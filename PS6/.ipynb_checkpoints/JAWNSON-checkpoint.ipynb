{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info 3950 Problem Set 6\n",
    "\n",
    "**due Thu evening 20 Apr 2023 23:00**\n",
    "\n",
    "Submit via [gradescope](https://gradescope.com/).\n",
    "\n",
    "<font size=\"-1\">[Note that these problem sets are not intended as group projects: the work you submit must be your own. You can discuss with other students at a high level, for example general methods or strategies to solve a problem, but you must cite the other student(s) in your submission. Any work you submit must be your own understanding of the solution, the details of which you personally and individually worked out, and written in your own words. In no cases should notebooks or code be shared. It is also good practice to attribute any other sources you've used.]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Name and netid (will be graded)\n",
    "\n",
    "**A.** Put your name and netid in the cell below (a suprising number of students did not succeed to do that for ps5):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "0"
   },
   "source": [
    "# Joseph Greene | jhg287"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** Check that your name and netid are correct in the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import shakespeare\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, cross_validate, KFold\n",
    "from collections import Counter\n",
    "from urllib.request import urlopen\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1"
   },
   "source": [
    "## 1) Stylometrics with stopwords\n",
    "\n",
    "Many noticed that in **1.D.ii** of ps5, three of the training points were misclassified by the k-means classifier trained on the top 9 principal components of the 60 most common stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1A"
   },
   "source": [
    "**A.** i) Train an SVM classifier for this problem, using default values of the `SVC()` parameters, on the 77 training points with the features PCA-reduced to just 2 dimensions. Then test on the 122 test points (similarly PCA-reduced). What are the training and test scores?\n",
    "\n",
    "ii) Make a two dimensional plot showing the decision boundaries, as in the first plot of [plot_rbf_parameters.html](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html) (or use the more visible colors in [lec20_rbfparams.ipynb](https://courses.cit.cornell.edu/info3950_2023sp/lec20_rbfparams.ipynb)). Determine best values of C,gamma for this problem by looking at cluster boundaries for intuition. (This may require some trial-and-error experimentation.) What are the training and test scores for those best values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'and', 'to', 'of', 'a', 'in', 'i', 'that', 'he', 'his', 'it', 'with', 'you', 'was', 'not', 'but', 'as', 'for', 'be', 'all', 'is', 's', 'had', 'him', 'her', 'on', 'this', 'at', 'my', 'so', 'by', 'from', 'she', 'what', 'me', 'or', 'they', 'have', 'no', 'their', 'there', 'which', 'said', 'one', 'now', 'we', 'will', 'were', 'then', 'your', 'more', 'them', 'if', 'who', 'when', 'been', 'do', 'would', 'are', 'd')\n",
      "(76, 60)\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "\n",
    "shf = [fileid for fileid in gutenberg.fileids() if fileid.startswith('shakespeare')]\n",
    "\n",
    "#now try four authors\n",
    "shw = [w.lower() for fileid in shf for w in gutenberg.words(fileid) if w[0].isalpha()]\n",
    "mpw = [w.lower() for w in gutenberg.words('milton-paradise.txt') if w[0].isalpha()]\n",
    "auw = [w.lower() for w in gutenberg.words('austen-persuasion.txt') if w[0].isalpha()]\n",
    "mdw_all = [w.lower() for w in gutenberg.words('melville-moby_dick.txt') if w[0].isalpha()]\n",
    "mdw = mdw_all[:80000] #truncate moby dick words to 16 5k pieces\n",
    "# len(shw),len(mpw),len(auw),len(mdw) #numbers of words\n",
    "\n",
    "\n",
    "chimneys_url = 'https://www.gutenberg.org/cache/epub/65238/pg65238.txt'\n",
    "chimneys = urlopen(chimneys_url).read().decode('utf-8')\n",
    "agw = [w.lower() for w in nltk.word_tokenize(chimneys[897:439351]) if w[0].isalpha()]\n",
    "\n",
    "fdist=Counter(shw + mpw + auw + mdw + agw) #for all five\n",
    "top60,_=zip(*fdist.most_common(60)) #now use top 60 from combined\n",
    "print (top60)\n",
    "\n",
    "M5=[]\n",
    "for corp in [shw,mpw,auw,mdw,agw]:\n",
    "#   print(len(corp)//5000)\n",
    "  for i in range(0,len(corp)-4999,5000):  # 13 blocks of shakespeare, 16 of milton, austen,moby, christie\n",
    "    fdist = Counter(corp[i:i+5000])\n",
    "    M5.append([fdist[w] for w in top60])\n",
    "M5 = np.array(M5).astype(np.float64)\n",
    "print(M5.shape)\n",
    "\n",
    "#center the data on zero, and normalize standard deviation\n",
    "scaler5 = preprocessing.StandardScaler()\n",
    "M5_scaled = scaler5.fit_transform(M5)\n",
    "\n",
    "pca = PCA(2)\n",
    "X_train = pca.fit_transform(M5_scaled)\n",
    "y_train = list(13*[0] + 16*[1] + 16*[2] + 16*[3] + 15*[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data\n",
    "\n",
    "mpr_text = urlopen('https://www.gutenberg.org/cache/epub/58/pg58.txt').read().decode('utf-8')\n",
    "\n",
    "#find beginning and end to strip off gutenberg stuff\n",
    "mpr_text.index('\\r\\n'*5 + 'Paradise Regained'), mpr_text.index(5*'\\r\\n' + '*** END OF THE PROJECT GUTENBERG')\n",
    "\n",
    "mpr_words = nltk.word_tokenize(mpr_text[767:93590])\n",
    "\n",
    "T=[]\n",
    "more_shwords=[]\n",
    "for fileid in shakespeare.fileids():\n",
    "    if fileid in ('hamlet.xml', 'j_caesar.xml', 'macbeth.xml'): continue  #already in training data\n",
    "    more_shwords += shakespeare.words(fileid)\n",
    "more_shwords = [w.lower() for w in more_shwords if w[0].isalpha()]\n",
    "\n",
    "mpr = [w.lower() for w in mpr_words if w[0].isalpha()]\n",
    "\n",
    "aue= [w.lower() for w in gutenberg.words('austen-emma.txt') if w[0].isalpha()]\n",
    "aus= [w.lower() for w in gutenberg.words('austen-sense.txt') if w[0].isalpha()]\n",
    "\n",
    "\n",
    "links_url = 'https://www.gutenberg.org/cache/epub/58866/pg58866.txt'\n",
    "links = urlopen(links_url).read().decode('utf-8')\n",
    "links_words = [w.lower() for w in nltk.word_tokenize(links[829:373271]) if w[0].isalpha()]\n",
    "\n",
    "\n",
    "T5=[]\n",
    "for i in range(0,len(more_shwords)-4999,5000):  # 13 blocks of shakespeare, 16 of milton, austen,moby\n",
    "    fdist = Counter(more_shwords[i:i+5000])\n",
    "    T5.append([fdist[w] for w in top60])\n",
    "for i in range(0,len(mpr)-4999,5000):  # milton paradise regained\n",
    "    fdist = Counter(mpr[i:i+5000])\n",
    "    T5.append([fdist[w] for w in top60])\n",
    "for i in range(0,len(aus+aue)-4999,5000): #austen\n",
    "    fdist = Counter((aus+aue)[i:i+5000])\n",
    "    T5.append([fdist[w] for w in top60])\n",
    "for i in range(0,len(mdw_all[80000:])-4999,5000):  #rest of melville\n",
    "    fdist = Counter(mdw_all[i+80000:i+85000])\n",
    "    T5.append([fdist[w] for w in top60])\n",
    "for i in range(0,len(links_words)-4999,5000):  # milton paradise regained\n",
    "    fdist = Counter(links_words[i:i+5000])\n",
    "    T5.append([fdist[w] for w in top60])\n",
    "len(T5)\n",
    "\n",
    "X_test = pca.transform(scaler5.transform(T5))\n",
    "y_test = list(24*[0] + 3*[1] + 56*[2] + 27*[3] + 12*[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 100.00%, or 122/122\n",
      " Testing Score: 86.07%,  or 105/122\n"
     ]
    }
   ],
   "source": [
    "# 1Ai)\n",
    "\n",
    "# training\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# testing\n",
    "\n",
    "print(f'Training Score: {model.score(X_train, y_train)*100:.2f}%, or {model.score(X_train, y_train)*122:.0f}/122',\n",
    "      f' Testing Score: {model.score(X_test, y_test)*100:.2f}%,  or {model.score(X_test, y_test)*122:.0f}/122',\n",
    "     sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Make a two dimensional plot showing the decision boundaries, as in the first plot of [plot_rbf_parameters.html](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html) (or use the more visible colors in [lec20_rbfparams.ipynb](https://courses.cit.cornell.edu/info3950_2023sp/lec20_rbfparams.ipynb)). Determine best values of C,gamma for this problem by looking at cluster boundaries for intuition. (This may require some trial-and-error experimentation.) What are the training and test scores for those best values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Aii)\n",
    "\n",
    "# from lec20 rbfparams\n",
    "\n",
    "# looping through values for C and gamma\n",
    "C_2d_range = [.01, 1, 100]\n",
    "gamma_2d_range = [.1, 1, 10]\n",
    "classifiers = []\n",
    "for C in C_2d_range:\n",
    "    for gamma in gamma_2d_range:\n",
    "        clf = SVC(C=C, gamma=gamma)\n",
    "        clf.fit(X_train, y_train)\n",
    "        classifiers.append((C, gamma, clf))\n",
    "        \n",
    "        \n",
    "\n",
    "clrs = list(13*'b' + 16*'g' + 16*'r' + 16*'y')\n",
    "testclrs = list(24*'b' + 3*'g' + 56*'r' + 27*'y') + ['C6']*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 0.18329807108324356, 'gamma': 0.03792690190732246}:\n",
      "\tTraining score: 100.00\n",
      "\tTesting score: 82.79\n"
     ]
    }
   ],
   "source": [
    "C_range = np.logspace(-2, 10, 20)\n",
    "gamma_range = np.logspace(-9, 3, 20)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"The best parameters are %s:\\n\\tTraining score: %0.2f\\n\\tTesting score: %.2f\"\n",
    "    % (grid.best_params_, grid.best_score_*100,grid.score(X_test,y_test)*100)\n",
    ")\n",
    "\n",
    "best_C, best_gamma = grid.best_params_.values()\n",
    "\n",
    "scores = grid.cv_results_[\"mean_test_score\"].reshape(len(C_range), len(gamma_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.colors import LinearSegmentedColormap\n",
    "# cwlist = [plt.cm.coolwarm(0.),  plt.cm.coolwarm(.25), (1,1,1,1),\n",
    "#                        plt.cm.coolwarm(.75), plt.cm.coolwarm(1.)]\n",
    "# my_coolwarm = LinearSegmentedColormap.from_list('cw', cwlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 6))\n",
    "# xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))\n",
    "# for k, (C, gamma, clf) in enumerate(classifiers):\n",
    "#     # evaluate decision function in a grid\n",
    "#     Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "\n",
    "#     # visualize decision function for these parameters\n",
    "#     plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n",
    "#     if k < len(gamma_2d_range): plt.title(f'gamma={gamma}')\n",
    "#     if k%len(gamma_2d_range) == 0: plt.ylabel(f'C={C}' +' '*12, rotation=0, fontsize=12)\n",
    "\n",
    "#     # visualize parameter's effect on decision function\n",
    "#     plt.pcolormesh(xx, yy, -Z, cmap=my_coolwarm)\n",
    "#     plt.scatter(*X_2d.T, c=np.array(['r','b'])[y_2d], edgecolors=\"gray\")\n",
    "#     plt.xticks(())\n",
    "#     plt.yticks(())\n",
    "#     plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1B"
   },
   "source": [
    "**B.** Again for those best C,gamma values in part A, what are the training and test scores on the same training and test data, but using PCA reduction down to 3, 9 (as in ps5), and the full 60 dimensionsional data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-dimensional data:\n",
      "Training Score: 100.00%, or 122/122\n",
      "Testing Score:  100.00%, or 122/122\n",
      "\n",
      "9-dimensional data:\n",
      "Training Score: 100.00%, or 122/122\n",
      "Testing Score:  99.18%, or 121/122\n",
      "\n",
      "60-dimensional data:\n",
      "Training Score: 80.26%, or 98/122\n",
      "Testing Score:  69.67%, or 85/122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n_components in [3,9,60]:\n",
    "    pca = PCA(n_components)\n",
    "    \n",
    "    # train data\n",
    "    X_train = pca.fit_transform(M5_scaled)\n",
    "\n",
    "    # test data\n",
    "    X_test = pca.transform(scaler5.transform(T5))\n",
    "    \n",
    "    # training\n",
    "    model = SVC(C=best_C, gamma=best_gamma)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # testing\n",
    "    print(f'{n_components}-dimensional data:',\n",
    "          f'Training Score: {model.score(X_train, y_train)*100:.2f}%, or {model.score(X_train, y_train)*122:.0f}/122',\n",
    "          f'Testing Score:  {model.score(X_test,  y_test) *100:.2f}%, or {model.score(X_test, y_test)*122:.0f}/122\\n',\n",
    "         sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.18329807108324356, 'gamma': 0.03792690190732246} \n",
      "\n",
      "3-dimensional data:\n",
      "Training Score: 100.00%, or 122/122\n",
      "Testing Score:  100.00%, or 122/122\n",
      "\n",
      "9-dimensional data:\n",
      "Training Score: 100.00%, or 122/122\n",
      "Testing Score:  99.18%, or 121/122\n",
      "\n",
      "60-dimensional data:\n",
      "Training Score: 80.26%, or 98/122\n",
      "Testing Score:  69.67%, or 85/122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_,\"\"\"\n",
    "\n",
    "3-dimensional data:\n",
    "Training Score: 100.00%, or 122/122\n",
    "Testing Score:  100.00%, or 122/122\n",
    "\n",
    "9-dimensional data:\n",
    "Training Score: 100.00%, or 122/122\n",
    "Testing Score:  99.18%, or 121/122\n",
    "\n",
    "60-dimensional data:\n",
    "Training Score: 80.26%, or 98/122\n",
    "Testing Score:  69.67%, or 85/122\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "2"
   },
   "source": [
    "## 2) Stylometrics with letter 3-grams\n",
    "\n",
    "Data: download and unpack the file [ps6data.tar.gz](https://courses.cit.cornell.edu/info3950_2023sp/ps6data.tar.gz) (or equivalently [ps6data.zip](https://courses.cit.cornell.edu/info3950_2023sp/ps6data.zip)).\n",
    "It has a file auths.txt giving the names of the 20 NYTimes columnists used in the data, and subdirectories train/ and test/:<br>\n",
    "The train/ subdirectory in turn has 20 subdirectories, labelled by author initials, each of which contains 32 text files by that author. Those are the training and validation files. The test/ subdirectory also has 20 subdirectories, each of which contains 10 additional text files by that author.\n",
    "\n",
    "You should calculate the letter 3-gram features for each item as in [lec18_l3g_sty.ipynb](https://nbviewer.org/url/courses.cit.cornell.edu/info3950_2023sp/lec18_l3g_sty.ipynb), with **one modification**.\n",
    "\n",
    "---\n",
    "*Calculate the most common features to use as usual, by counting the total number of occurrences in the training set.\n",
    "But when using those to calculate the features for each individual item, **divide the number counts by the length of the text** (since the texts are of varying lengths, this will prevent the properties of longer texts from dominating).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "2A"
   },
   "source": [
    "**A.** i) Start with the most common 1500 features as in [lec18_l3g_sty.ipynb](https://nbviewer.org/url/courses.cit.cornell.edu/info3950_2023sp/lec18_l3g_sty.ipynb). On the training set (the 32 items for each of 20 authors), use cv=8 cross validation (which will result in 32/8 = 4 items in each test split for each author) to find the best values of the parameters C and gamma, using GridSearchCV as in lec18_l3g_sty.ipynb.\n",
    "\n",
    "ii) Plot the training and test scores for the 8 splits using those best values of C and gamma, and indicate the average test and training scores.\n",
    "\n",
    "iii) Plot a confusion matrix for the full `32*20 = 640` test items. Which authors are the most confused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2d4cd37e0afd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauth_dirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ps6data/train/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{train_path}/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_fileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{train_path}{auth}/**\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mauth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauth_dirs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_fileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{test_path}{auth}/**\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mauth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauth_dirs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_path' is not defined"
     ]
    }
   ],
   "source": [
    "auth_dirs = sorted([path.replace('ps6data/train/', '') for path in glob.glob(f\"{train_path}/*\")])\n",
    "\n",
    "train_fileids = np.array([glob.glob(f\"{train_path}{auth}/**\") for auth in auth_dirs]).flatten()\n",
    "test_fileids = np.array([glob.glob(f\"{test_path}{auth}/**\") for auth in auth_dirs]).flatten()\n",
    "\n",
    "y_train = np.array([[i]*32 for i in range(len(auth_dirs))]).flatten()\n",
    "y_test = np.array([[i]*10 for i in range(len(auth_dirs))]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1500\n",
    "allcounts = Counter()\n",
    "for f in train_fileids:\n",
    "    txt = re.sub('\\s+',' ', open(f).read().lower())\n",
    "\n",
    "    counter = Counter([txt[i:i+3] for i in range(len(txt))])\n",
    "    allcounts += Counter(counter)\n",
    "top1500 = dict(allcounts.most_common(N)).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training x_data\n",
    "M = []\n",
    "for f in train_fileids:\n",
    "    txt = re.sub('\\s+',' ', open(f).read().lower())\n",
    "    c = Counter([txt[i:i+3] for i in range(len(txt))])\n",
    "    c = Counter({key: value/len(txt) for key, value in c.items()})\n",
    "    M.append([c[g3] for g3 in top1500])\n",
    "\n",
    "M = np.array(M)\n",
    "M.shape\n",
    "\n",
    "# testing x_data\n",
    "T = []\n",
    "\n",
    "for f in test_fileids:\n",
    "    txt = re.sub('\\s+',' ', open(f).read().lower())\n",
    "    c = Counter([txt[i:i+3] for i in range(len(txt))])\n",
    "    c = Counter({key: value/len(txt) for key, value in c.items()})\n",
    "    T.append([c[g3] for g3 in top1500])\n",
    "\n",
    "T = np.array(T)\n",
    "T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAYBE DELETE THE PRINT OR MOST OF IT (KEEP BEST PARAMS PART?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "C_range = np.logspace(-2, 10, 20)\n",
    "gamma_range = np.logspace(-9, 3, 20)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "grid = GridSearchCV(SVC(kernel='rbf'),\n",
    "                   param_grid, cv=8, return_train_score=True).fit(M, y_train)\n",
    "\n",
    "print(\n",
    "    \"The best parameters are %s:\\n\\tTraining score: %0.2f\\n\\tTesting score:  %.2f\"\n",
    "    % (grid.best_params_, grid.best_score_*100,grid.score(T,y_test)*100)\n",
    ")\n",
    "\n",
    "best_C, best_gamma = grid.best_params_.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Plot the training and test scores for the 8 splits using those best values of C and gamma, and indicate the average test and training scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_scores = [grid.cv_results_[f'split{i}_train_score'][grid.best_index_] for i in range(8)]\n",
    "test_scores = [grid.cv_results_[f'split{i}_test_score'][grid.best_index_] for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_scores, label='Test Score')\n",
    "plt.plot(train_scores, label='Train Score')\n",
    "\n",
    "title = 'Mean Train Score: {:.2f}%; Mean Test Score: {:.2f}%'.format(np.mean(train_scores)*100,np.mean(test_scores)*100)\n",
    "plt.title(title)\n",
    "plt.xlabel('CV Split')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Plot a confusion matrix for the full 32*20 = 640 test items. Which authors are the most confused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(grid,M,y_train,\n",
    "                    display_labels=auth_dirs, ax=ax)\n",
    "\n",
    "plt.title('Training Items')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No authors are confused using the optimal parameters found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "problem": "2B"
   },
   "source": [
    "**B.**  i) Now determine the accuracy score on the test data (the additional 10 items for each of the 20 authors), and draw a contingency matrix for performance on the test data for the 20 authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(grid,T,y_test,\n",
    "                    display_labels=auth_dirs, ax=ax)\n",
    "\n",
    "plt.title(f'Test Items | Test Accuracy Score: {grid.score(T,y_test)*100:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''Authors most confused:\n",
    "cb (8 incorrect)\n",
    "mg (5 incorrect)\n",
    "bs (4 incorrect)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Plot the training and test scores as a function of the number of features, as in [lec18_l3g_sty.ipynb](https://nbviewer.org/url/courses.cit.cornell.edu/info3950_2023sp/lec18_l3g_sty.ipynb), from 1 to 2000 features (or more), using the optimal C,gamma from part A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.concatenate([M,T])\n",
    "y_data = np.concatenate([y_train,y_test])\n",
    "\n",
    "clf = SVC(C=grid.best_params_['C'],gamma=grid.best_params_['gamma'],kernel='rbf')\n",
    "nfs = [1,10,25,50,75,100,250,500,750,1000,1500,2000] # + list(range(1000,5100,500))\n",
    "test_means = []\n",
    "train_means = []\n",
    "for Nf in nfs:\n",
    "    cv10 = cross_validate(clf, X_data[:,:Nf], y_data, cv=5, return_train_score=True)\n",
    "    test_means.append(cv10['test_score'].mean())\n",
    "    train_means.append(cv10['train_score'].mean())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.08571428571428572,\n",
       "  0.2773809523809524,\n",
       "  0.4023809523809524,\n",
       "  0.525,\n",
       "  0.6,\n",
       "  0.6523809523809524,\n",
       "  0.7869047619047619,\n",
       "  0.8428571428571429,\n",
       "  0.8547619047619047,\n",
       "  0.875,\n",
       "  0.8797619047619047,\n",
       "  0.8797619047619047],\n",
       " [0.10684523809523809,\n",
       "  0.34851190476190474,\n",
       "  0.5136904761904761,\n",
       "  0.7148809523809524,\n",
       "  0.8258928571428571,\n",
       "  0.8842261904761906,\n",
       "  0.975297619047619,\n",
       "  0.9991071428571429,\n",
       "  0.9994047619047619,\n",
       "  0.9997023809523811,\n",
       "  1.0,\n",
       "  1.0])"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_means, train_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
