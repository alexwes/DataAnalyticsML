{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lec6.1, 9 Feb 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import textwrap\n",
    "from collections import defaultdict,Counter\n",
    "wrap = textwrap.TextWrapper(85, break_long_words=False, break_on_hyphens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recall data files bio.txt and phys.txt available at [abstracts.zip](https://www.cs.cornell.edu/~ginsparg/6010/spr20/abstracts.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn\n",
    "\n",
    "Now use scikit-learn routines for the Naive bayes binary text classification in\n",
    "[lec2b.ipynb](https://nbviewer.jupyter.org/url/www.cs.cornell.edu/~ginsparg/6010/spr20/lec2b.ipynb). See:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with BernoulliNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the texts as a list of lines\n",
    "bio = open('bio.txt').readlines()\n",
    "phys = open('phys.txt').readlines()\n",
    "# see https://docs.python.org/3/library/re.html\n",
    "import re\n",
    "def words(txt): return re.findall(r\"['\\w]+\", txt.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvocab = Counter() #won't need smoothing this time, BernoulliNB will do it\n",
    "for txt in phys[:900]:\n",
    "  for w in set(words(txt)): pvocab[w] += 1\n",
    "bvocab = Counter()\n",
    "for txt in bio[:900]:\n",
    "  for w in set(words(txt)): bvocab[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sizes of bio and phys vocab\n",
    "len(bvocab), len(pvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab in common between the two\n",
    "len(set(bvocab) & set(pvocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_words = bvocab + pvocab\n",
    "len(all_train_words) #roughly 15k combined vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [lec4.ipynb](https://nbviewer.jupyter.org/url/courses.cit.cornell.edu/info3950_2021sp/lec4.ipynb), we used the most common words in the scikit-learn `BernoulliNB()` classifier.\n",
    "\n",
    "Instead we can try to use the terms that are most discriminating, in the sense of having the largest disparities in numbers of occurrences between the biology and physics texts. We still want terms that have occurred in at least some minimal number of abstracts, in order to be useful and informative, so we could start with the terms that occur in at least 10 different abstracts. The most informative features are the ones that have either the largest or smallest ratio of appearances between biology and physics abstracts. Since one or the other can sometimes be zero, we have already used a \".5 smoothing\", adding .5 to each numerical value (whether or not zero). Thus a term like 'gravity', which occurs in 170 physics abstracts and 0 biology abstracts would be assigned a phys:bio ratio of (170 + .5) / (0 + .5) = 341, and a term like 'network' that occurs in 162 biology and 7 physics abstracts would be assigned a bio:phys ratio of (162.5 / 7.5) = 21.67. (The ratios are always calculated with the larger number in the numerator. There would be an additional factor of the ratio of sizes+1 of the two classes if the classes were different in size.)\n",
    "\n",
    "Below we'll try most discriminating vocabularies of size Nf = 100 and 1000 (and we could also repeat the entire exercise from lec4 plotting the accuracy on the test set against sizes from 2 to 2048)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {w: (bvocab[w]+.5)/(pvocab[w]+.5) for w in all_train_words if all_train_words[w]>=10}\n",
    "disc = {w: max(r[w],1/r[w]) for w in r}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nf = 100 #number of features = length of vocab to keep, for fun only 100\n",
    "vocab = sorted(disc, key=disc.get, reverse=True)[:Nf]\n",
    "word_index = {w:k for k,w in enumerate(vocab)} #assigns each word an index by rank\n",
    "set_vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(txt):\n",
    "    f = np.zeros(Nf)\n",
    "    for w in set(words(txt)) & set_vocab: f[word_index[w]] = 1\n",
    "    return f  #array of zeros and ones for whether or not feature words occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top ten most discriminating features in vocab\n",
    "print (vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the length 100 feature vector for a synthetic text\n",
    "# note there's a 1 for words in the above vocab that occur one or more times\n",
    "features ('here is some gauge gauge gauge and brain gravity plus cell genome text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use first 900 bio and first 900 phys documents as training set, and remaining 100+100=200 documents as test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data is a list of feature vectors\n",
    "X_train = np.array([features(txt) for txt in bio[:900] + phys[:900]])\n",
    "#training labels a corresponding list\n",
    "y_train = np.array([0]*900 + [1]*900)  #0 = bio, 1=phys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data is a list of feature vectors\n",
    "X_test = np.array([features(txt) for txt in bio[900:] + phys[900:]])\n",
    "#test labels a corresponding list\n",
    "y_test = np.array([0]*100 + [1]*100)  #0 = bio, 1=phys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have another look at what these features look like... printed out below are the 17th through 27th vocabulary words, along with the the corresponding entries for the first 10 documents.  1 or 0 signifies whether or not a word appears, so for example of the ten words below the third document contains only 'gene' and 'environment'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (vocab[16:26])\n",
    "X_train[:10, 16:26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document also has a label, 0 or 1, meaning 'bio' or 'physics', so `y_train` is a list of 900 0's followed by 900 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('total=', len(y_train), ', middle 10:', y_train[895:905])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BernoulliNB(alpha=.5)\n",
    "clf.fit(X_train, y_train) #fit the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all bio correct and only three phys wrong (even though only 100 words, not full 14664)\n",
    "clf.predict(X_test[:100]), clf.predict(X_test[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test) #197/200  [cell [31] referred to below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 10 bio probabilities\n",
    "clf.predict_proba(X_test[:10]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phys also by wide margins\n",
    "clf.predict_proba(X_test[100:110]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train) #percentage correct (training set accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets 46 wrong! turns out 0 bio and 46 phys\n",
    "1754/1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the feature sets for the ones it gets wrong (just too few relevant content words)\n",
    "for i, in np.argwhere(clf.predict(X_test) != y_test):\n",
    "     print ('doc {:4d} labelled {:>4s}, but predicted bio={:.3f}, phys={:.3f}'.format(\n",
    "                 i, ['bio','phys'][y_test[i]], *clf.predict_proba(X_test[i:i+1])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "run this to see the full original texts\n",
    "\n",
    "    for i, in np.argwhere(clf.predict(X) != Y):\n",
    "        print(i, ['bio','phys'][Y[i]], wrap.fill((bio[:900] + phys[:900])[i]),'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this to see the full original texts wrong, and the features used\n",
    "# interpretable AI ...\n",
    "\n",
    "for i, in np.argwhere(clf.predict(X_test) != y_test):\n",
    "    txt = (bio[900:] + phys[900:])[i]\n",
    "    print('{} ({}) {}:'.format(i, ['bio','phys'][y_test[i]], len(set(words(txt)) & set_vocab)),\n",
    "          wrap.fill(txt),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(clf.predict(X_test[100:110]) != y_test[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nf = 1000 #number of features = length of vocab to keep, now 1000\n",
    "vocab = sorted(disc, key=disc.get, reverse=True)[:Nf]\n",
    "word_index = {w:k for k,w in enumerate(vocab)} #assigns each word an index by rank\n",
    "set_vocab = set(vocab)\n",
    "\n",
    "X_train = np.array([features(txt) for txt in bio[:900] + phys[:900]])\n",
    "y_train = np.array([0]*900 + [1]*900)  #0 = bio, 1=phys\n",
    "\n",
    "X_test = np.array([features(txt) for txt in bio[900:] + phys[900:]])\n",
    "y_test = np.array([0]*100 + [1]*100)  #0 = bio, 1=phys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train) #fit the training data\n",
    "clf.score(X_test, y_test) #percentage correct (test set accuracy), gets 1/200 wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(clf.predict(X_test) != y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba([X_test[11],]) #thinks it's physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio[911]  #arxiv.org/abs/1808.09546 q-bio.BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in set(words(bio[911])) & set_vocab: print (w,round(r[w],2), ' \\tp' if r[w]<1 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train) #percentage correct (training set accuracy 1798/1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(clf.predict(X_train) != y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X_train[[1260,1733]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys[1260-900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thinks it's bio, more interpretable AI\n",
    "for w in set(words(phys[1260-900])) & set_vocab: print (w,round(r[w],2), ' \\tb' if r[w]>1 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys[1733-900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in set(words(phys[1733-900])) & set_vocab: print (w,round(r[w],2), ' \\tb' if r[w]>1 else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note the training and test set accuracies are certainly not usually 100% (can't always fit the entire training set, can't always predict the entire test set).\n",
    "\n",
    "Exercise: look back at the above for Nf=100, just a hundred word vocabulary, and have a look at the words that survive for the ones wrong to see why the ambiguities arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "It could have been that our 90/10 train/test split was particularly lucky (or unlucky).\n",
    "\n",
    "To correct for this, it's standard to use \"cross-validation\": use a number of different splits, train on multiple different training sets, and score on the corresponding test set, then average over the resulting scores.\n",
    "\n",
    "If we do this for 10 different splits, it would be called \"10-fold cross validation\".\n",
    "\n",
    "The code below breaks the original data into 10 blocks, each balanced with equal numbers of bio and phys abstracts. We can then use each successive 10% block as the test data, and the remaining 90% as the training data, giving the 10 splits for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nf = 100 #number of features = length of vocab to keep, for fun again only 100\n",
    "vocab = sorted(disc, key=disc.get, reverse=True)[:Nf]\n",
    "word_index = {w:k for k,w in enumerate(vocab)} #assigns each word an index by rank\n",
    "set_vocab = set(vocab)\n",
    "\n",
    "#First get features for all 2000 abstracts\n",
    "X_data = np.array([features(txt) for txt in bio + phys])\n",
    "y_data = np.array([0]*1000 + [1]*1000)  #0 = bio, 1=phys\n",
    "\n",
    "#then break it up into stratified blocks of ten each,\n",
    "#including equal amounts of data from the two classes\n",
    "X_data_blocks = [X_data[i:i+100] for i in range(0,2000,100)]\n",
    "y_data_blocks = [y_data[i:i+100] for i in range(0,2000,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 9  # use the last block as the test data, should be same result as above\n",
    "\n",
    "X_train = np.concatenate([X_data_blocks[i] for i in range(20) if i not in (j,j+10)])\n",
    "y_train = np.concatenate([y_data_blocks[i] for i in range(20) if i not in (j,j+10)])\n",
    "\n",
    "X_test = np.concatenate([X_data_blocks[i] for i in (j,j+10)])\n",
    "y_test = np.concatenate([y_data_blocks[i] for i in (j,j+10)])\n",
    "\n",
    "X_train.shape # check that we're getting 1800 abstracts, each with 100 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same result as cell [31]\n",
    "clf.fit(X_train,y_train).score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now average the scores over the ten splits\n",
    "scores=[]\n",
    "for j in range(10):\n",
    "    X_train = np.concatenate([X_data_blocks[i] for i in range(20) if i not in (j,j+10)])\n",
    "    y_train = np.concatenate([y_data_blocks[i] for i in range(20) if i not in (j,j+10)])\n",
    "    X_test = np.concatenate([X_data_blocks[i] for i in (j,j+10)])\n",
    "    y_test = np.concatenate([y_data_blocks[i] for i in (j,j+10)])\n",
    "    clf.fit(X_train,y_train)\n",
    "    print (j,'training score=', clf.score(X_train,y_train),\n",
    "             ', test score=', clf.score(X_test,y_test))\n",
    "    scores.append(clf.score(X_test,y_test))\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(10),scores, 'o-')\n",
    "plt.ylim(.9,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but all of the above is common enough that it's already implemented, see:\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first try the test_train_split using 10% of the data for test:\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_data,y_data,test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train).score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could use train_test_split() ten times, but that too can be done automatically:\n",
    "\n",
    "scores = cross_val_score(clf, X_data, y_data, cv=10) # or try cv = 20 for 95%/5% splits\n",
    "scores, scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(10),scores, 'o-')\n",
    "plt.ylim(.9,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 fold cross-validation, with more diagnostics:\n",
    "output = cross_validate(clf, X_data, y_data, return_train_score=True, cv=10)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(10),output['train_score'], 'o-', label='train scores')\n",
    "plt.plot(range(10),output['test_score'], 'o-', label='test scores')\n",
    "plt.title('10-fold validation')\n",
    "plt.xticks(range(10))\n",
    "plt.grid(axis='y', linestyle='dotted')\n",
    "plt.ylim(.9,1)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
