{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from scikit-learn [plot_logistic_multinomial](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html),\n",
    "with significant modifications (beyond just updating the color scheme...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcolors = np.array(['#00bbFF', '#FFbb00', '#00FF00']) #marker colors\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] #default colorcycle\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "bog = LinearSegmentedColormap.from_list('bog', colors[:3], 3) #blue|orange|green\n",
    "#white -> blue, orange, green colormaps:\n",
    "w_bog = [LinearSegmentedColormap.from_list(c,[(1,1,1),c]) for c in colors[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 3-class dataset for classification\n",
    "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
    "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
    "#made 334 0s, 333 1s, 333 2s\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.gca().set_aspect(1)\n",
    "plt.scatter(*X.T, c=mcolors[y], ec='k', s=20);\n",
    "plt.title('original blobs')\n",
    "\n",
    "transformation = [[0.2, -0.4], [1.2, 0.4]] \n",
    "X = X @ transformation # stretches/squeezes and rotates blobs\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.gca().set_aspect(1)\n",
    "plt.scatter(*X.T, c=mcolors[y], ec='k', s=20)\n",
    "plt.title('after transformation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min,y_min = X.min(0)-1\n",
    "x_max,y_max = X.max(0)+1\n",
    "xg = np.arange(x_min, x_max, .02) #x grid\n",
    "yg = np.arange(y_min, y_max, .02) #y grid\n",
    "xmm = np.array([x_min, x_max])\n",
    "\n",
    "xx,yy = np.meshgrid(xg,yg) # a grid of the x and y values\n",
    "xy = np.transpose([xx,yy],(1,0,2)) #to calculate z values\n",
    "\n",
    "def plot_decision_surface(): #define function since will use twice\n",
    "    plt.contourf(xg, yg, Z, cmap=bog)\n",
    "    plt.title(f\"Decision surface of LogisticRegression ({mclass[m]})\", fontsize=10)\n",
    "    plt.axis('tight')\n",
    "    plt.gca().set_aspect(1)\n",
    "    plt.scatter(*X.T, c=mcolors[y], ec=edges, s=20) #training points\n",
    "    # Plot the three one-against-all classifiers (arg of logit = 0)   \n",
    "    for i, color in zip(clf.classes_, mcolors):\n",
    "        yline = -(coeff[i,0]*xmm + intcpt[i])/coeff[i,1] #arg of logistic is 0\n",
    "        plt.plot(xmm, yline, ls=\"--\", color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,11))\n",
    "mclass = ('ovr', 'multinomial')\n",
    "for m in range(2):\n",
    "    clf = LogisticRegression(solver='sag', max_iter=100, random_state=42,\n",
    "                             multi_class=mclass[m]).fit(X, y)\n",
    "    print(\"training score: {:.3f} ({})\".format(clf.score(X, y), mclass[m]))\n",
    "    coeff, intcpt = (clf.coef_, clf.intercept_)\n",
    "    edges = np.full(len(X), 'k')\n",
    "    edges[clf.predict(X) != y] = 'r' # the ones wrong\n",
    "\n",
    "    # decision boundary: assign color to each point in mesh\n",
    "    # ravel is like .flatten, only 1000x faster, np.c_ stacks as two columns\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)    \n",
    "\n",
    "    #save these results for later:\n",
    "    if m==0: ovr_coeff,ovr_intcpt,Z_ovr, eco = (coeff, intcpt, Z, edges)\n",
    "    else: multi_coeff,multi_intcpt,Z_multi, ecm = (coeff, intcpt, Z, edges)\n",
    "    \n",
    "    plt.subplot(1,2,m+1)\n",
    "    plot_decision_surface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider first the plot on the left above, showing the result for the ovr (=one vs rest) classifier.\n",
    "The dotted lines indicate the .5 probability for the logistic classifier, i.e., indicate the best choice for dividing a class vs not in the class. So for example the blue dotted line is the best choice for dividing the blue from non-blue (i.e., green or orange) points, and similarly for the green dotted line.\n",
    "\n",
    "Due to the nature of the data, the orange line does not fare as well: too much to the left and it inclues too many non-orange points, and too much to the right and it misses too many orange points. The classification boundaries are determined by which of the three ovr probabilities is largest at a given point on the plane, and the 24 misclassified points are indicated by red edges. These three lines are the best one can do using only the in-class vs out-of-class distinctions during training.\n",
    "\n",
    "[The nature of these lines may be more evident in the plots below. The plots in the top line reproduce the two above, and the three plots lower plots in the left column show the logistic ovr probabilties for each of the blue, orange, and green classes separately. The fully saturated color means probability 1, and white means probability 0. The blue and green have narrow transitions, whereas the orange has a much wider uncertainty range, due the difficulty of accommodating the points in the central region.]\n",
    "\n",
    "The plot on the right above shows the result of the multinomial logistic regression, which uses instead the labels for all three classes simultaneously to fit the decision boundaries, by taking the largest of the three probabilities at any point on the plane. Unlike the ovr, it is able to shift the orange dotted line to the left, giving higher orange likelihood to the previously misclassified points, because it can simultaneously move the blue green dotted lines closer together. The result is to extend the orange wedge to the left, correctly classifying most of the points missed by the ovr, but not misclassifying any additional points because the shifted blue and green lines make the wedge narrower. These coordinated changes now leave only 5 points misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(x): return 1/ (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,14))\n",
    "for m in range(2):\n",
    "  if m==0: coeff,intcpt,Z = (ovr_coeff, ovr_intcpt, Z_ovr)\n",
    "  else: coeff,intcpt,Z = (multi_coeff, multi_intcpt, Z_multi)\n",
    "  zz = logit((coeff @ xy).transpose([1,0,2]) + intcpt[:,None,None])\n",
    "  plt.subplot(4,2, m+1)\n",
    "  plot_decision_surface()\n",
    "    \n",
    "  for i in range(3):\n",
    "    plt.subplot(4,2, 2*i + m+3)\n",
    "    plt.contourf(xg, yg, zz[i], levels=10, cmap=w_bog[i])\n",
    "    plt.scatter(*X.T, c=mcolors[y], ec='k', s=20) #training points\n",
    "    plt.axis('tight')\n",
    "    plt.gca().set_aspect(1)\n",
    "    \n",
    "    yline = -(coeff[i,0]*xmm + intcpt[i])/ coeff[i,1]\n",
    "    plt.plot(xmm, yline, ls=\"--\", color= 'w') #mcolors[i])\n",
    "    plt.title('{} prob weight, class {}'.format(mclass[m], i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show the decision boundaries for each of the three classes independently.\n",
    "\n",
    "The two plots below show them pieced together as in cell [5] above, but now with the most likely class probability function in each of the three decision regions.  The center region that had significant uncertainty for all three classes for the ovr classifier is replaced by the much higher likelihood of orange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_inter(coeff,intcpt):\n",
    "    \"\"\"simple algebra to find x value of intersection of two boundary lines\"\"\"\n",
    "    numer = (intcpt[0]-intcpt[2]) / (coeff[0,1]-coeff[2,1])\n",
    "    numer -= (intcpt[0]-intcpt[1]) / (coeff[0,1]-coeff[1,1])\n",
    "    denom = (coeff[0,0] - coeff[1,0]) / (coeff[0,1]-coeff[1,1])\n",
    "    denom -= (coeff[0,0] - coeff[2,0]) / (coeff[0,1]-coeff[2,1])\n",
    "    return (numer / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,10))\n",
    "for m in range(2):\n",
    "  plt.subplot(1,2,m+1)\n",
    "  if m==0: coeff,intcpt,Z,edges = (ovr_coeff, ovr_intcpt, Z_ovr, eco)\n",
    "  else: coeff,intcpt,Z,edges = (multi_coeff, multi_intcpt, Z_multi, ecm)\n",
    "  zz = logit((coeff @ xy).transpose([1,0,2]) + intcpt[:,None,None])\n",
    "  xti = triple_inter(coeff,intcpt)\n",
    "  plt.title(\"Decision surface of LogisticRegression ({})\".format(mclass[m]))\n",
    "\n",
    "  for i in range(3):   \n",
    "    mask = np.zeros_like(Z, dtype=bool)\n",
    "    mask[Z!=i] = True\n",
    "    plt.contourf(xg,yg, np.ma.array(zz[i], mask=mask), levels=10, cmap=w_bog[i])\n",
    "    plt.scatter(*X.T, c=mcolors[y], ec=edges, s=20) #training points\n",
    "\n",
    "#    yline = -(coeff[i,0]*xmm + intcpt[i])/coeff[i,1]  #the dotted lines now redundant\n",
    "#    plt.plot(xmm, yline, ls=\"--\", color=mcolors[i])\n",
    "    \n",
    "    j = (i+1)%3  #plot i vs j boundaries\n",
    "    wij = coeff[i] - coeff[j]\n",
    "    bij = intcpt[i] - intcpt[j]\n",
    "    xtm = np.array([x_min,xti]) if wij[1] < 0 else np.array([xti, x_max])\n",
    "    yline = -(wij[0]*xtm + bij)/ wij[1]\n",
    "    plt.plot(xtm, yline, ls=\"dotted\", color='grey')\n",
    "    plt.axis([x_min,x_max,y_min,y_max])\n",
    "    plt.gca().set_aspect(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
