{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info 3950 Problem Set 7\n",
    "\n",
    "**due Tue evening 2 May 2023 23:00**\n",
    "\n",
    "Remember to include your name and netid in the first cell. Submit via [gradescope](https://gradescope.com/).\n",
    "\n",
    "<font size=\"-1\">[Also note that these problem sets are not intended as group projects: the work you submit must be your own. You can discuss with other students at a high level, for example general methods or strategies to solve a problem, but you must cite the other student(s) in your submission. Any work you submit must be your own understanding of the solution, the details of which you personally and individually worked out, and written in your own words. In no cases should notebooks or code be shared.]</font>\n",
    "\n",
    "### <font color='red'>Alex Weseley alw269</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/_C.cpython-38-darwin.so, 0x0002): Library not loaded: '@rpath/libopenblas.0.dylib'\n  Referenced from: '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib'\n  Reason: tried: '/private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9d63z49rj_/croot/pytorch_1681837279022/_build_env/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/../../../../libopenblas.0.dylib' (no such file), '/private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9d63z49rj_/croot/pytorch_1681837279022/_build_env/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/../../../../libopenblas.0.dylib' (no such file), '/private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9d63z49rj_/croot/pytorch_1681837279022/_build_env/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/../../../../libopenblas.0.dylib' (no such file), '/private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9d63z49rj_/croot/pytorch_1681837279022/_build_env/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/../../../../libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/../../../libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/../../../libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/bin/../lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/bin/../lib/libopenblas.0.dylib' (no such file), '/usr/local/lib/libopenblas.0.dylib' (no such file), '/usr/lib/libopenblas.0.dylib' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glob\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/__init__.py:218\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    217\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/_C.cpython-38-darwin.so, 0x0002): Library not loaded: '@rpath/libopenblas.0.dylib'\n  Referenced from: '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib'\n  Reason: tried: '/private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9d63z49rj_/croot/pytorch_1681837279022/_build_env/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/../../../../libopenblas.0.dylib' (no such file), '/private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9d63z49rj_/croot/pytorch_1681837279022/_build_env/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/../../../../libopenblas.0.dylib' (no such file), '/private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9d63z49rj_/croot/pytorch_1681837279022/_build_env/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/../../../../libopenblas.0.dylib' (no such file), '/private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9d63z49rj_/croot/pytorch_1681837279022/_build_env/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/../../../../libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/../../../libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/lib/python3.8/site-packages/torch/../../../libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/bin/../lib/libopenblas.0.dylib' (no such file), '/Users/alexweseley/opt/anaconda3/bin/../lib/libopenblas.0.dylib' (no such file), '/usr/local/lib/libopenblas.0.dylib' (no such file), '/usr/lib/libopenblas.0.dylib' (no such file)"
     ]
    }
   ],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "%matplotlib inline\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, cross_validate, \n",
    "                                     GridSearchCV, RepeatedStratifiedKFold)\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)  \n",
    "\n",
    "In this problem we'll reconsider the dataset from the second problem in ps6. (To reconstruct the data, you can use your own code, or the simple code from [s6.ipynb](https://nbviewer.org/url/courses.cit.cornell.edu/info3950_2023sp/s6.ipynb), or even better to save the ps6#2 train/test data as a `.npy` file for reloading, as demo'd for the ps5 data used in ps6 in [s6_supp.ipynb](https://nbviewer.org/url/courses.cit.cornell.edu/info3950_2023sp/s6_supp.ipynb).\n",
    "\n",
    "**A.** Start with the standard Scaler applied to 2000 features, and as in [lec24_torch_mnist.ipynb](https://nbviewer.org/url/courses.cit.cornell.edu/info3950_2023sp/lec24_torch_mnist.ipynb), train a neural net with the following architecture: a Linear layer reducing the 2000 input features to 1024 (followed by ReLU, and Dropout), then a second Linear layer reducing from 1024 to the output 20 (again followed by Dropout, but not Relu since it's the output).<br>\n",
    "Use the same learning rate, loss_fn, and optimizer as in the lec24_torch_mnist.ipynb examples.\n",
    "\n",
    "You should experiment with the values of the Dropouts, but you should find that with both Dropouts in the p=.75 range (i.e., high) that you achieve accuracy on the test set at least in the mid 80s percent level after a few hundred epochs. (The data set is small so epochs are rapid, but it takes more of them to train properly.)<br>\n",
    "Make plots of i) the train and test losses, and ii) the train and test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "  nn.Flatten(),\n",
    "\n",
    "  nn.Linear(2000,1024),   #128 node layer, fully connected\n",
    "  nn.ReLU(), # or .Tanh(), or .Sigmoid()\n",
    "  nn.Dropout(.75), #improves training\n",
    "    \n",
    "  nn.Linear(1024,20),    # connects to 10 node output layer\n",
    "  nn.Dropout(.75), #improves training\n",
    ")\n",
    "\n",
    "learning_rate = 1e-3;\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcount = Counter()\n",
    "for auth in sorted(glob('ps6data/train/*/')):\n",
    "    for f in sorted(glob(f'{auth}/*')):\n",
    "        txt = open(f).read()\n",
    "        allcount += Counter([txt[i:i+3] for i in range(len(txt)-2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 2000 trigrams\n",
    "top2000,_ = zip(*allcount.most_common(2000)) #we'll only use first 1500 here, rest for part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2000[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest the train set\n",
    "X_train = []\n",
    "y_train = []\n",
    "for auth in sorted(glob('ps6data/train/*')):\n",
    "    for f in sorted(glob(f'{auth}/*')):\n",
    "        txt = open(f).read()\n",
    "        c = Counter([txt[i:i+3] for i in range(len(txt)-2)])\n",
    "        X_train.append([c[tg]/len(txt) for tg in top2000])\n",
    "        y_train.append(auth.split('/')[-1])\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train_e = le.transform(y_train)\n",
    "y_train = torch.tensor(y_train_e)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for auth in sorted(glob('ps6data/test/*')):\n",
    "    for f in sorted(glob(f'{auth}/*')):\n",
    "        txt = open(f).read()\n",
    "        c = Counter([txt[i:i+3] for i in range(len(txt)-2)])\n",
    "        X_test.append([c[tg]/len(txt) for tg in top2000])\n",
    "        y_test.append(auth.split('/')[-1])\n",
    "\n",
    "le.fit(y_test)\n",
    "y_test_e = le.transform(y_test)\n",
    "y_test = torch.tensor(y_test_e)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 200  #about four seconds per epoch\n",
    "scores2 = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for t in range(epochs):\n",
    "\n",
    "    model.train() #turns on dropout\n",
    "   \n",
    "    pred = model(X_train)\n",
    "    loss = loss_fn(pred, y_train)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()  #turns off dropout\n",
    "    train_score = (model(X_train).argmax(1) == y_train).sum().item() / len(X_train)\n",
    "    test_score = (model(X_test).argmax(1) == y_test).sum().item() / len(X_test)\n",
    "    if(t % 25 == 0):\n",
    "        print(f'Epoch {t+1}: ', end='')\n",
    "        print(f'loss={loss.item():.3f}, training_score={train_score:.1%}, test_score={test_score:.1%}')\n",
    "    scores2.append((train_score, test_score))\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    test_loss = loss_fn(model(X_test), y_test)\n",
    "    test_losses.append(test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the train and test losses\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the train and test accuracies\n",
    "train_accuracies = [score[0] for score in scores2]\n",
    "test_accuracies = [score[1] for score in scores2]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), train_accuracies, '.-', label='Train Accuracy')\n",
    "plt.plot(range(1, epochs + 1), test_accuracies, '.-', label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracies')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** Now increase from 2000 to 4000 features (as shown in the final plots of the ps6 solutions), and also increase the middle layer from 1024 to 2048 neurons, leaving the rest of the architecture the same. Repeat part A but you should find with similar Dropout values test set values at least in the upper 80s percent level after a few hundred epochs.\n",
    "\n",
    "Note: this problem is one that out of curiosity while writing the ps6 solutions, and it was fun to see how easy it was to improve on the SVM performance by mindlessly inserting a hidden layer exactly as in the spiral example in class -- generalizing from 2 input features and 4 output classes to 4000 input features and 32 output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nn.Sequential(\n",
    "  nn.Flatten(),\n",
    "\n",
    "  nn.Linear(4000,2048),   #128 node layer, fully connected\n",
    "  nn.ReLU(), # or .Tanh(), or .Sigmoid()\n",
    "  nn.Dropout(.691), #improves training\n",
    "    \n",
    "  nn.Linear(2048,20),    # connects to 10 node output layer\n",
    "  nn.Dropout(.691), #improves training\n",
    ")\n",
    "\n",
    "learning_rate = 1e-3;\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 4000 trigrams\n",
    "top4000,_ = zip(*allcount.most_common(4000)) #we'll only use first 1500 here, rest for part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top4000[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest the train set\n",
    "X_train2 = []\n",
    "y_train2 = []\n",
    "for auth in sorted(glob('ps6data/train/*')):\n",
    "    for f in sorted(glob(f'{auth}/*')):\n",
    "        txt = open(f).read()\n",
    "        c = Counter([txt[i:i+3] for i in range(len(txt)-2)])\n",
    "        X_train2.append([c[tg]/len(txt) for tg in top4000])\n",
    "        y_train2.append(auth.split('/')[-1])\n",
    "\n",
    "        \n",
    "le = LabelEncoder()\n",
    "le.fit(y_train2)\n",
    "y_train2_e = le.transform(y_train2)\n",
    "y_train2 = torch.tensor(y_train2_e)\n",
    "\n",
    "X_train2_scaled = scaler.fit_transform(X_train2)\n",
    "X_train2 = torch.tensor(X_train2_scaled, dtype=torch.float32)\n",
    "\n",
    "X_train2.shape, y_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test2 = []\n",
    "y_test2 = []\n",
    "for auth in sorted(glob('ps6data/test/*')):\n",
    "    for f in sorted(glob(f'{auth}/*')):\n",
    "        txt = open(f).read()\n",
    "        c = Counter([txt[i:i+3] for i in range(len(txt)-2)])\n",
    "        X_test2.append([c[tg]/len(txt) for tg in top4000])\n",
    "        y_test2.append(auth.split('/')[-1])\n",
    "\n",
    "le.fit(y_test2)\n",
    "y_test2_e = le.transform(y_test2)\n",
    "y_test2 = torch.tensor(y_test2_e)\n",
    "\n",
    "X_test2_scaled = scaler.transform(X_test2)\n",
    "X_test2 = torch.tensor(X_test2_scaled, dtype=torch.float32)\n",
    "\n",
    "X_test2.shape, y_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 300  #about four seconds per epoch\n",
    "scores2 = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for t in range(epochs):\n",
    "\n",
    "    model2.train() #turns on dropout\n",
    "    pred = model2(X_train2)\n",
    "    loss = loss_fn(pred, y_train2)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model2.eval()  #turns off dropout\n",
    "    train_score = (model2(X_train2).argmax(1) == y_train2).sum().item() / len(X_train2)\n",
    "    test_score = (model2(X_test2).argmax(1) == y_test2).sum().item() / len(X_test2)\n",
    "    if(t % 25 == 0):\n",
    "        print(f'Epoch {t+1}: ', end='')\n",
    "        print(f'loss={loss.item():.3f}, training_score={train_score:.1%}, test_score={test_score:.1%}')\n",
    "    scores2.append((train_score, test_score))\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    test_loss = loss_fn(model2(X_test2), y_test2)\n",
    "    test_losses.append(test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the train and test losses\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the train and test accuracies\n",
    "train_accuracies = [score[0] for score in scores2]\n",
    "test_accuracies = [score[1] for score in scores2]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), train_accuracies, '.-', label='Train Accuracy')\n",
    "plt.plot(range(1, epochs + 1), test_accuracies, '.-', label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracies')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)\n",
    "\n",
    "(This is very simple and quick, but useful to understand.)\n",
    "\n",
    "**A.** Cell [19] of [lec24_torch_mnist.ipynb](https://nbviewer.org/url/courses.cit.cornell.edu/info3950_2023sp/lec24_torch_mnist.ipynb) gives the number of parameters for each layer of the '99.1%' model III.<br>\n",
    "Explain what the numbers mean in terms of weights and biases.<br>\n",
    "(For example, the last layer which takes 1024 neurons to the ten output neurons has ten objects each of the form $wx+b$ so there are 10 biases $b_i$ feeding into the $i^{\\rm th}$ output neuron, but $x$ is a list of 1024 features so each of the 10 associated $w$'s has weights for 1024 items, a total of 10240 parameters.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer:**\n",
    "\n",
    "There are three layers in the convolutional neural network shown in cell 19 of lecture 24.\n",
    "\n",
    "The first one is a convolutional layer with 1 input channel and 32 output channels. The shape of the weight tensor in this layer is (32, 1, 5, 5), indicating that there are 32 filters, each with a kernel size of 5x5 and 1 input channel. The shape of the bias tensor is (32), representing the biases associated with each output channel. \n",
    "\n",
    "The second layer is also a convolutional layer which has 32 input channels and 64 output channels. The shape of the weight tensor in this layer is (64, 32, 5, 5), indicating that there are 64 filters, each with a kernel size of 5x5 and 32 input channels. The shape of the bias tensor is (64), representing the biases associated with each output channel.\n",
    "\n",
    "The third and final layer is a fully connected linear layer that takes the flattened input from the previous layer. The shape of the weight tensor in this layer is (10, 6444), indicating that there are 10 output neurons and each neuron has weights for all the flattened input features (64x4x4 = 1024). The shape of the bias tensor is (10), representing the biases associated with each output neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** Do the same parameter counts, with interpretations, and find the total number of parameters for:<br>\n",
    "i) the '98.1%' model II in cell [9] of lec24_torch_mnist.ipynb<br>\n",
    "ii) the model in problem 1A above<br>\n",
    "iii) the model in problem 1B aove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer (i):**\n",
    "\n",
    "This network only makes use of fully connected layers. The first fully connected layer takes a flattened input of size 28x28 (784) and produces 128 output neurons. The shape of the weight tensor in this layer is (128, 784), indicating that there are 128 output neurons and each neuron has weights for all the flattened input features (784). The shape of the bias tensor is (128), representing the biases associated with each output neuron. \n",
    "\n",
    "The second fully connected layer takes the output from the previous layer. The shape of the weight tensor in this layer is (10, 128), indicating that there are 10 output neurons and each neuron has weights for the 128 input features. The shape of the bias tensor is (10), representing the biases associated with each output neuron. \n",
    "\n",
    "\n",
    "**answer (ii):**\n",
    "\n",
    "Similar to the previous neural network, this one only uses 2 linear layers. The first linear layer takes an input of size 2000 and produces 1024 output neurons. The shape of the weight tensor in this layer is (1024, 2000), indicating that there are 1024 output neurons and each neuron has weights for all the 2000 input features. The shape of the bias tensor is (1024), representing the biases associated with each output neuron.\n",
    "\n",
    "The second linear layer takes the output from the previous layer. The shape of the weight tensor in this layer is (20, 1024), indicating that there are 20 output neurons and each neuron has weights for the 1024 input features. The shape of the bias tensor is (20), representing the biases associated with each output neuron. \n",
    "\n",
    "\n",
    "**answer (iii):**\n",
    "\n",
    "This neural network is extremely similar to the previous one with a small change to some of the input and output sizes. The first linear layer takes an input of size 4000 and produces 2048 output neurons. The shape of the weight tensor in this layer is (2048, 4000), indicating that there are 2048 output neurons and each neuron has weights for all the 4000 input features. The shape of the bias tensor is (2048), representing the biases associated with each output neuron.\n",
    "\n",
    "The second linear layer takes the output from the previous layer. The shape of the weight tensor in this layer is (20, 2048), indicating that there are 20 output neurons and each neuron has weights for the 2048 input features. The shape of the bias tensor is (20), representing the biases associated with each output neuron. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "3"
   },
   "source": [
    "# 3)\n",
    "Lectures 23 and 24 (see\n",
    "[lec24_torch_mnist.ipynb](https://nbviewer.org/url/courses.cit.cornell.edu/info3950_2023sp/lec24_torch_mnist.ipynb)\n",
    "considered four models for the 28x28 [MNIST](https://en.wikipedia.org/wiki/MNIST_database) classification task, with increasing test accuracies of 92.6%, 98.1%, 99.1%, 99.7%. (A 20 year history of progress is given in the above wikipedia link, including a result of 99.82% reported in table 2 of [arXiv:1805.01890](https://arxiv.org/abs/1805.01890)).\n",
    "The methods used for this task are now ubiquitious in other applications, including general image classification, face recognition, text classification, generation, and translation, self-driving cars, medical diagnostics, and so on.\n",
    "\n",
    "For thie problem, you can use `conda install pytorch` to run locally, or run on google colaboratory where it is installed by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "3A",
    "tags": []
   },
   "source": [
    "### A.\n",
    "\n",
    "This part considers a three layer model intermediate between the non-convolutional model II (cell [9] of lec24_torch_mnist.ipynb) and the two convolutional layers of model III (cell [18] of lec24_torch_mnist.ipynb). \n",
    "\n",
    "Starting from model II, replace the `nn.Flatten()` at the beginning with a layer that learns eight 4x4 convolutional filters, using\n",
    "    \n",
    "    nn.Conv2d(1, 8, kernel_size=(4,4)),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "The output of this layer will be 25x25 images for each of the 8 filters (a total of 5000 features).\n",
    "Follow that by the `nn.Flatten()` layer and also the `nn.Linear()` layer (as in model II cell 9 of lec24_torch_mnist, though without the `nn.Dropout()` layer for the time being, and you'll need to specify the input size of that layer).<br>\n",
    "\n",
    "Using a batchsize of 64, train it for 15 epochs (i.e., it'll look at 64 training samples before each parameter update, and eventually make 10 full passes through the data -- though you're free to experiment with these parameters).\n",
    "\n",
    "For testing you might want to use epochs set to 1, until everything works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nn.Sequential(\n",
    "  nn.Conv2d(1, 8, kernel_size=(4,4)),\n",
    "  nn.ReLU(),\n",
    "  nn.Flatten(),\n",
    "\n",
    "  nn.Linear(5000,10),\n",
    "  nn.ReLU(), \n",
    ")\n",
    "\n",
    "learning_rate = 1e-3;\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root=\"./\",\n",
    "    train=True,\n",
    "   # download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# Test data\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"./\",\n",
    "    train=False,\n",
    "   # download=True, #already downloaded with train data above\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores99 = []\n",
    "for epoch in range(15):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        # pass data through network\n",
    "        pred = model2(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        train_accuracy.append((pred.argmax(1) == y).sum().item() / len(pred))        \n",
    "        \n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    for i, (x, y) in enumerate(test_loader):\n",
    "        # pass data through network\n",
    "        pred = model2(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        test_loss.append(loss.item())\n",
    "        test_accuracy.append((pred.argmax(1) == y).sum().item() / len(pred))\n",
    "        \n",
    "    print(f'epoch: {epoch}, train loss: {np.mean(train_loss):.5f}, test loss: {np.mean(test_loss):.5f}, ' +\\\n",
    "          f'train_score: {np.mean(train_accuracy):.1%}, test_score: {np.mean(test_accuracy):.1%}')\n",
    "    scores99.append((np.mean(train_loss),np.mean(test_loss),np.mean(train_accuracy),np.mean(test_accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "3Ai"
   },
   "source": [
    "**i.** Evaluate the test score on the 10,000 image MNIST test data (you should find 98% or above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "3Aii"
   },
   "source": [
    "**ii.** Visualize the eight 4x4 filters you've trained (they can be in two rows of four), as was done for the 32 5x5 filters in cell [24] of lec24_torch_mnist.ipynb for model III. You should see filters that detect horizontal, vertical, and diagonal edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "3Aiii"
   },
   "source": [
    "**iii.** Now visualize the output of the convolutional layer, as in cell [29] of lec24_torch_mnist.ipynb, except now there are only eight filters, so more training examples can be considered. It could look something like the below\n",
    "(which uses training samples [1,3,5,7,2,0,13,15,17,4] to have ten digits in sequence, and where f0-f7 designate the outputs from the eight filters). E.g., in the below f0 appears optimized for roughly horizontal edges, and f2 for roughly vertical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('f8.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.\n",
    "\n",
    "Now add a `nn.Dropout(.2)` layer after the `nn.Flatten()` layer (so that 20% of the neuron outputs in that layer are randomly ignored each training pass).<br>\n",
    "You should find an improved test accuracy after 15 epochs.<br>\n",
    "(Feel free to experiment with the value of the dropout probability.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "3B"
   },
   "source": [
    "### C.\n",
    "Now augment the model above to two convolutional layers and a dropout (as in cell [18] of lec24_torch_mnist.ipynb for model III, but without the maxpooling layers). For the second convolutional layer (right after the first), try\n",
    "\n",
    "    nn.Conv2D(8, 16, kernel_size=(2, 2)),\n",
    "    nn.Relu(),\n",
    "    \n",
    "to train a total of 16 2x2 filters on the outputs of the first layer (i.e., learning 16 new filters, each of which combines together the 2x2 set of outputs of all eight filters from the previous layer).\n",
    "\n",
    "Follow that with `nn.Flatten()` and `nn.Linear()` layers as in **A.**, but between them add a `Dropout(0.5)` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "3Bi"
   },
   "source": [
    "**i.** Using a batchsize of 64, train the model for 15 epochs, and evaluate on the MNIST `x_test` data. You should now find a score above 98.5%.<br>\n",
    "(Again feel free to experiment with the dropout rate.)\n",
    "\n",
    "**ii)** Visualize the outputs of the 2nd layer of filters for this model.<br>\n",
    "(They might not be as easily interpretable as in part A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
