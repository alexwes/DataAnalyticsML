{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joseph Greene | jhg287"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info 3950 ps1\n",
    "**due Thu evening 9 Feb 2023 23:00**\n",
    "\n",
    "Remember to include your name and netid in the first cell. Submit via [gradescope](https://gradescope.com) -- remember to click the 'code' button to ensure that it renders properly, and it is your final saved version.\n",
    "\n",
    "<font size=\"-1\">[Also note that these problem sets are not intended as group projects: the work you submit must be your own. You can discuss with other students at a high level, for example general methods or strategies to solve a problem, but you must cite the other student(s) in your submission. Any work you submit must be your own understanding of the solution, the details of which you personally and individually worked out, and written in your own words. In no cases should notebooks or code be shared.]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup for inline plotting\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "%matplotlib inline\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.stats import norm,binom,poisson\n",
    "\n",
    "import re\n",
    "\n",
    "def words(txt): return re.findall(\"[a-z0-9']+\", txt.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "#1A"
   },
   "source": [
    "# 1)\n",
    "\n",
    "These are two more \"review\" problems from past early problem sets in Info 2950 (since the material was really from intro probability/statistics courses), to diagnose whether there are any systematic deficits that the TAs need to review.\n",
    "\n",
    "**A.** In [ps0](https://nbviewer.jupyter.org/url/courses.cit.cornell.edu/info3950_2023sp/ps0.ipynb), we considered the statistics for the number of heads recorded by 15000 Cornell undergraduate students. Consider the same number of Cornell undergraduates instead rolling a die 120 times each, and examine the number of 6's they record. (For this you can use either np.random.randint(6), or more generally check whether np.random.rand() is < 1/6.)\n",
    "\n",
    "**i)** Simulate the above, and determine for your simulation a) the mean number of 6's recorded, b) the median, c) the variance, d) the standard deviation.\n",
    "\n",
    "**ii)** What are the theoretically expected values of the mean and standard deviation in this situation?\n",
    "\n",
    "**iii)** Plot a histogram of the results in a figure as in the ps0 notebook, together with the expected normal distribution.\n",
    "\n",
    "**iv)** For your simulated data, what percentage of the number of 6's are within 1,2, and 3 standard deviations of the mean?<br>\n",
    "(As in the notebook, this will agree better with the values expected for a normal distribution if you use a strict inequality on only one of upper or lower boundary (i.e., < or >, rather than <= or >=, on one boundary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0002\n",
      "20.0\n",
      "16.626866626666665\n",
      "4.077605501598538\n"
     ]
    }
   ],
   "source": [
    "# 1A)\n",
    "#  i)\n",
    "\n",
    "sim_rolls = np.random.randint(6, size=(15000,120))\n",
    "\n",
    "sixes = [sim_rolls[i][sim_rolls[i]==5].shape[0] for i in range(len(sim_rolls))]\n",
    "\n",
    "print(np.average(sixes)) # a\n",
    "print(np.median(sixes))  # b\n",
    "print(np.var(sixes))     # c\n",
    "print(np.std(sixes))     # d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n",
      "4.08248290463863\n"
     ]
    }
   ],
   "source": [
    "# ii)\n",
    "\n",
    "ex_mean = 120/6 # expected mean\n",
    "\n",
    "ex_stdev = np.sqrt(120/6*(5/6)) # expected std. dev.\n",
    "\n",
    "print(ex_mean)\n",
    "print(ex_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iii)\n",
    "\n",
    "plt.plot(norm(loc=ex_mean, scale=ex_stdev).pdf(range(max(sixes)))*15000,label=\"Theoretical\")\n",
    "plt.hist(sixes,bins=len(set(sixes)),label=\"Simulated\", alpha=.8)\n",
    "\n",
    "plt.xlabel(\"Number of times 6 is rolled\")\n",
    "plt.ylabel(\"Frequency of number of times 6 is rolled\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733% of the number of 6's within 1 standard deviations of the mean\n",
      "0.965% of the number of 6's within 2 standard deviations of the mean\n",
      "0.998% of the number of 6's within 3 standard deviations of the mean\n"
     ]
    }
   ],
   "source": [
    "# iv)\n",
    "\n",
    "avg = np.average(sixes)\n",
    "stdev = np.std(sixes)\n",
    "\n",
    "cum_sum = 0\n",
    "for q in range(3):\n",
    "    cum_sum += Counter([np.floor(abs((i-avg)/stdev)) for i in sixes])[q]/15000\n",
    "    print(f\"{round(cum_sum,3)}% of the number of 6's within {q+1} standard deviations of the mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "#1D"
   },
   "source": [
    "**B.** In the \"birthday paradox\" problem in\n",
    "[ps0](https://nbviewer.jupyter.org/url/courses.cit.cornell.edu/info3950_2023sp/ps0.ipynb),\n",
    "you calculated the probability of *at least one* coincident birthday. (Continue to assume only 365 days and birthdays uniformly distributed in the below.) Empirically, we found in lecs 2,3 that there were at least two pairs of coincident birthdays within the classroom (and there are of course many more).\n",
    "\n",
    "You can easily simulate this by choosing 50 random integers in the range 0 to 364 (try it):\n",
    "\n",
    "    Counter(np.random.randint(365, size=50)).most_common(10)\n",
    "\n",
    "to see days that occur multiple times. If you rerun a few times, you'll see different 'dates' and different numbers of collisions that occur (sometimes triples, and so on). Applying another `Counter()` to the values of the above (i.e., without the `.most_common()` method) will give the number of singles, doubles, triples, etc. Repeating this, say, 1000 times and averaging will give the expected numbers of singles, doubles, triples, etc, for this number of people.\n",
    "\n",
    "**i)** Do this simulation 1000 times for the full class, assuming `n=94`\n",
    "\n",
    "**ii)** As explained in ps0, the expected number of each type of coincidence can also be estimated by considering a Bernoulli-like process. The idea was to think about throwing 94 balls (the students) randomly into 365 bins (the days). The average number of balls per bin at the end of the process is 94/365, so most bins will be unoccupied, but many bins will get 1 ball, and a small number of \"lucky\" bins will get two, three, or even four balls. The bins are the birthdays, and the ones with multiple balls are the days with coincident birthdays.\n",
    "\n",
    "From the standpoint of any given bin, this is like flipping a coin 94 times, only the probability of \"success\" for each flip is only 1/365. That means that the bins follow the `binom.pmf(range(1,6), n, 1/365)` probability distribution for the number of coincident birthdays, and multiplying by 365 will give an exact estimate of the expected number of days with singles, doubles, triples. Add this \"theoretical estimate\" to the plot in **i)**.  (Your plot should look something like the below, which was for `n=50` [for which the probability of at least one collision was 97%], except use `n=` 94).\n",
    "\n",
    "**iii)** `p=1/365` is small enough that the binomial distribution deforms to the so-called Poisson distribution (`scipy.stats.poisson`), which depends only on the mean `n/365`, rather than independently on the two parameters `n` and `p=1/365`. Replot the simulated data from i) along with `365*poisson.pmf(range(1,6), n/365)` instead as the \"theory\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1B)\n",
    "#  i)\n",
    "singles = []\n",
    "doubles = []\n",
    "triples = []\n",
    "quads = []\n",
    "quints = []\n",
    "# sexts = []\n",
    "# septs = []\n",
    "\n",
    "for i in range(1000):\n",
    "    counts = Counter(Counter(np.random.randint(365, size=94)).values())\n",
    "    singles.append(counts.get(1,0))\n",
    "    doubles.append(counts.get(2,0))\n",
    "    triples.append(counts.get(3,0))\n",
    "    quads.append(counts.get(4,0))\n",
    "    quints.append(counts.get(5,0))\n",
    "#     sexts.append(counts.get(6,0))\n",
    "#     septs.append(counts.get(7,0))\n",
    "\n",
    "freq = [np.average(i) for i in [singles,doubles,triples,quads,quints]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ii)\n",
    "\n",
    "plt.bar(range(1,6), freq, label='simulation')\n",
    "plt.plot(range(1,6),binom.pmf(range(1,6), 94, 1/365)*365, 'o-', color='C1', label='theory')\n",
    "plt.xlabel('# people with coincident birthday')\n",
    "plt.ylabel('expected number of such days')\n",
    "plt.title(f'coincident birthdays for n={94} people')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iii)\n",
    "\n",
    "plt.bar(range(1,6), freq, label='simulation')\n",
    "plt.plot(range(1,6),365*poisson.pmf(range(1,6), 94/365), 'o-', color='C1', label='theory')\n",
    "plt.xlabel('# people with coincident birthday')\n",
    "plt.ylabel('expected number of such days')\n",
    "plt.title(f'coincident birthdays for n={94} people')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "#2"
   },
   "source": [
    "## 2) Naive Bayes text classifier\n",
    "\n",
    "\n",
    "An example used in [lec3.ipynb](https://nbviewer.jupyter.org/url/courses.cit.cornell.edu/info3950_2023sp/lec3.ipynb) involved building a Bernoulli Naive Bayes classifier based on 1000 abstracts each from a single physics category and a single biology category (1800 training texts, and 200 test texts; \"Bernoulli\" in this case just means consider only whether or not a word occurs in a document, not how many times). This dataset [ps1data.py.gz](https://courses.cit.cornell.edu/info3950_2023sp/ps1data.py.gz) expands on that with similar data from twelve categories. gunzip that file and use\n",
    "\n",
    "    from ps1data import absdata\n",
    "\n",
    "to import absdata, which will then be a dictionary of twelve subject areas in fields of astrophysics (GA = galaxies), condensed matter, computer science (CV= computer vision, HC = human computer interactions, LG = machine learning), two closely related areas of high energy physics (phenomenology and theory), math, applied physics, computational physics, biology (NC= neurons and cognition), and quantum physics. The value of each is a list of the texts of 1000 recent abstracts, for a total of 12000 abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf497dee50>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.logspace(100,10,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['astro-ph.GA', 'cond-mat.mes-hall', 'cs.CV', 'cs.HC', 'cs.LG', 'hep-ph', 'hep-th', 'math.AP', 'physics.app-ph', 'physics.comp-ph', 'q-bio.NC', 'quant-ph']\n"
     ]
    }
   ],
   "source": [
    "from ps1data import absdata\n",
    "print (sorted(absdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"
     ]
    }
   ],
   "source": [
    "print ([len(absdata[cat]) for cat in absdata])  #cat = \"category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data-driven decision making related to individuals has become increasingly pervasive, but the issue concerning the potential discrimination has been raised by recent studies. In response, researchers have made efforts to propose and implement fairness measures and algorithms, but those efforts have not been translated to the real-world practice of data-driven decision making. As such, there is still an urgent need to create a viable tool to facilitate fair decision making. We propose FairSight, a visual analytic system to address this need; it is designed to achieve different notions of fairness in ranking decisions through identifying the required actions -- understanding, measuring, diagnosing and mitigating biases -- that together lead to fairer decision making. Through a case study and user study, we demonstrate that the proposed visual analytic and diagnostic modules in the system are effective in understanding the fairness-aware decision pipeline and obtaining more fair outcomes.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first cs.HC abstract\n",
    "absdata['cs.HC'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in lecs 3,4, it's necessary to tokenize, or split the texts into something like words. Text can be split on whitespace using the `.split()` method, but that leaves punctuation characters.\n",
    "For a more useful spliting, words can be considered strings of lower case letters a-z plus apostrophes, and extracted as a list. For example, for the first cs.HC text above, and using a regular expression that finds all strings with a-z and ':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'driven',\n",
       " 'decision',\n",
       " 'making',\n",
       " 'related',\n",
       " 'to',\n",
       " 'individuals',\n",
       " 'has',\n",
       " 'become',\n",
       " 'increasingly',\n",
       " 'pervasive',\n",
       " 'but',\n",
       " 'the',\n",
       " 'issue',\n",
       " 'concerning',\n",
       " 'the',\n",
       " 'potential',\n",
       " 'discrimination',\n",
       " 'has',\n",
       " 'been',\n",
       " 'raised',\n",
       " 'by',\n",
       " 'recent',\n",
       " 'studies',\n",
       " 'in',\n",
       " 'response',\n",
       " 'researchers',\n",
       " 'have',\n",
       " 'made',\n",
       " 'efforts',\n",
       " 'to',\n",
       " 'propose',\n",
       " 'and',\n",
       " 'implement',\n",
       " 'fairness',\n",
       " 'measures',\n",
       " 'and',\n",
       " 'algorithms',\n",
       " 'but',\n",
       " 'those',\n",
       " 'efforts',\n",
       " 'have',\n",
       " 'not',\n",
       " 'been',\n",
       " 'translated',\n",
       " 'to',\n",
       " 'the',\n",
       " 'real',\n",
       " 'world',\n",
       " 'practice',\n",
       " 'of',\n",
       " 'data',\n",
       " 'driven',\n",
       " 'decision',\n",
       " 'making',\n",
       " 'as',\n",
       " 'such',\n",
       " 'there',\n",
       " 'is',\n",
       " 'still',\n",
       " 'an',\n",
       " 'urgent',\n",
       " 'need',\n",
       " 'to',\n",
       " 'create',\n",
       " 'a',\n",
       " 'viable',\n",
       " 'tool',\n",
       " 'to',\n",
       " 'facilitate',\n",
       " 'fair',\n",
       " 'decision',\n",
       " 'making',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'fairsight',\n",
       " 'a',\n",
       " 'visual',\n",
       " 'analytic',\n",
       " 'system',\n",
       " 'to',\n",
       " 'address',\n",
       " 'this',\n",
       " 'need',\n",
       " 'it',\n",
       " 'is',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'different',\n",
       " 'notions',\n",
       " 'of',\n",
       " 'fairness',\n",
       " 'in',\n",
       " 'ranking',\n",
       " 'decisions',\n",
       " 'through',\n",
       " 'identifying',\n",
       " 'the',\n",
       " 'required',\n",
       " 'actions',\n",
       " 'understanding',\n",
       " 'measuring',\n",
       " 'diagnosing',\n",
       " 'and',\n",
       " 'mitigating',\n",
       " 'biases',\n",
       " 'that',\n",
       " 'together',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'fairer',\n",
       " 'decision',\n",
       " 'making',\n",
       " 'through',\n",
       " 'a',\n",
       " 'case',\n",
       " 'study',\n",
       " 'and',\n",
       " 'user',\n",
       " 'study',\n",
       " 'we',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'the',\n",
       " 'proposed',\n",
       " 'visual',\n",
       " 'analytic',\n",
       " 'and',\n",
       " 'diagnostic',\n",
       " 'modules',\n",
       " 'in',\n",
       " 'the',\n",
       " 'system',\n",
       " 'are',\n",
       " 'effective',\n",
       " 'in',\n",
       " 'understanding',\n",
       " 'the',\n",
       " 'fairness',\n",
       " 'aware',\n",
       " 'decision',\n",
       " 'pipeline',\n",
       " 'and',\n",
       " 'obtaining',\n",
       " 'more',\n",
       " 'fair',\n",
       " 'outcomes']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[a-z0-9']+\", absdata['cs.HC'][0].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(`re.findall(\"[a-z0-9']+\",txt)` just finds all contiguous strings made up of the characters a-z or digits 0-9 or apostrophe ', and returns them as a list.)\n",
    "\n",
    "This will return a list of all the 'words' in that document (try it). For the time being, we only want to count the *number of documents* in which a word occurs (not the number of *occurrences* within the document), so we apply `set()` to the above list so that each word only appears once.\n",
    "Finally, it's convenient to use the Counter object (https://docs.python.org/3/library/collections.html#collections.Counter), which is just a dictionary to accumulate counts for the words. The result is (try it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 8),\n",
       " ('the', 7),\n",
       " ('and', 6),\n",
       " ('decision', 5),\n",
       " ('making', 4),\n",
       " ('in', 4),\n",
       " ('fairness', 3),\n",
       " ('a', 3),\n",
       " ('data', 2),\n",
       " ('driven', 2),\n",
       " ('has', 2),\n",
       " ('but', 2),\n",
       " ('been', 2),\n",
       " ('have', 2),\n",
       " ('efforts', 2),\n",
       " ('propose', 2),\n",
       " ('of', 2),\n",
       " ('is', 2),\n",
       " ('need', 2),\n",
       " ('fair', 2)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter( re.findall(\"[a-z0-9']+\", absdata['cs.HC'][0].lower()) ).most_common(20)\n",
    "# 20 most common, 'the' should appear 7 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "#3"
   },
   "source": [
    "Here you will train a simple binary classifier, for cs.HC = Human computer interactions vs. cs.LG = machine learning, based on the first 900 abstracts in each of those two categories, and test it on the last 100 \"test\" abstracts from each category. This follows the example given in [lec3.ipynb](https://nbviewer.jupyter.org/url/courses.cit.cornell.edu/info3950_2023sp/lec3.ipynb).\n",
    "\n",
    "[Parts of the remainder of this notebook are useful only if it isn't clear how to proceed.]\n",
    "\n",
    "As in the above [lec3.ipynb](https://nbviewer.jupyter.org/url/courses.cit.cornell.edu/info3950_2023sp/lec3.ipynb), we'll count only the *number of documents* in which a word occurs (not the number of *occurrences* within the document). To this end, we apply set() to the list of words so that each word only appears once.\n",
    "Again using the `Counter()` object, the result of inserting `set()` is (try it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'facilitate': 1,\n",
       "         'analytic': 1,\n",
       "         'lead': 1,\n",
       "         'been': 1,\n",
       "         'diagnostic': 1,\n",
       "         'decision': 1,\n",
       "         'in': 1,\n",
       "         'study': 1,\n",
       "         'system': 1,\n",
       "         'there': 1,\n",
       "         'real': 1,\n",
       "         'case': 1,\n",
       "         'effective': 1,\n",
       "         'studies': 1,\n",
       "         'we': 1,\n",
       "         'measuring': 1,\n",
       "         'user': 1,\n",
       "         'this': 1,\n",
       "         'concerning': 1,\n",
       "         'pervasive': 1,\n",
       "         'designed': 1,\n",
       "         'fairsight': 1,\n",
       "         'that': 1,\n",
       "         'aware': 1,\n",
       "         'issue': 1,\n",
       "         'raised': 1,\n",
       "         'potential': 1,\n",
       "         'increasingly': 1,\n",
       "         'biases': 1,\n",
       "         'pipeline': 1,\n",
       "         'viable': 1,\n",
       "         'obtaining': 1,\n",
       "         'practice': 1,\n",
       "         'but': 1,\n",
       "         'visual': 1,\n",
       "         'identifying': 1,\n",
       "         'by': 1,\n",
       "         'measures': 1,\n",
       "         'implement': 1,\n",
       "         'proposed': 1,\n",
       "         'response': 1,\n",
       "         'made': 1,\n",
       "         'such': 1,\n",
       "         'efforts': 1,\n",
       "         'algorithms': 1,\n",
       "         'ranking': 1,\n",
       "         'driven': 1,\n",
       "         'the': 1,\n",
       "         'is': 1,\n",
       "         'required': 1,\n",
       "         'making': 1,\n",
       "         'fairer': 1,\n",
       "         'a': 1,\n",
       "         'together': 1,\n",
       "         'individuals': 1,\n",
       "         'actions': 1,\n",
       "         'achieve': 1,\n",
       "         'modules': 1,\n",
       "         'propose': 1,\n",
       "         'fairness': 1,\n",
       "         'related': 1,\n",
       "         'fair': 1,\n",
       "         'need': 1,\n",
       "         'world': 1,\n",
       "         'urgent': 1,\n",
       "         'notions': 1,\n",
       "         'address': 1,\n",
       "         'has': 1,\n",
       "         'of': 1,\n",
       "         'through': 1,\n",
       "         'are': 1,\n",
       "         'different': 1,\n",
       "         'more': 1,\n",
       "         'data': 1,\n",
       "         'decisions': 1,\n",
       "         'not': 1,\n",
       "         'become': 1,\n",
       "         'demonstrate': 1,\n",
       "         'create': 1,\n",
       "         'understanding': 1,\n",
       "         'to': 1,\n",
       "         'discrimination': 1,\n",
       "         'as': 1,\n",
       "         'researchers': 1,\n",
       "         'mitigating': 1,\n",
       "         'have': 1,\n",
       "         'an': 1,\n",
       "         'translated': 1,\n",
       "         'it': 1,\n",
       "         'still': 1,\n",
       "         'outcomes': 1,\n",
       "         'and': 1,\n",
       "         'diagnosing': 1,\n",
       "         'recent': 1,\n",
       "         'tool': 1,\n",
       "         'those': 1})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(set(re.findall(\"[a-z0-9']+\",absdata['cs.HC'][0].lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number counts (number of documents in which word occurs) can be accumulated as was done for bvocab and pvocab in cell [10] of [lec3.ipynb](https://nbviewer.jupyter.org/url/courses.cit.cornell.edu/info3950_2023sp/lec3.ipynb), perhaps call them `HCvocab` and `LGvocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'achieve',\n",
       " 'actions',\n",
       " 'address',\n",
       " 'algorithms',\n",
       " 'an',\n",
       " 'analytic',\n",
       " 'and',\n",
       " 'are',\n",
       " 'as',\n",
       " 'aware',\n",
       " 'become',\n",
       " 'been',\n",
       " 'biases',\n",
       " 'but',\n",
       " 'by',\n",
       " 'case',\n",
       " 'concerning',\n",
       " 'create',\n",
       " 'data',\n",
       " 'decision',\n",
       " 'decisions',\n",
       " 'demonstrate',\n",
       " 'designed',\n",
       " 'diagnosing',\n",
       " 'diagnostic',\n",
       " 'different',\n",
       " 'discrimination',\n",
       " 'driven',\n",
       " 'effective',\n",
       " 'efforts',\n",
       " 'facilitate',\n",
       " 'fair',\n",
       " 'fairer',\n",
       " 'fairness',\n",
       " 'fairsight',\n",
       " 'has',\n",
       " 'have',\n",
       " 'identifying',\n",
       " 'implement',\n",
       " 'in',\n",
       " 'increasingly',\n",
       " 'individuals',\n",
       " 'is',\n",
       " 'issue',\n",
       " 'it',\n",
       " 'lead',\n",
       " 'made',\n",
       " 'making',\n",
       " 'measures',\n",
       " 'measuring',\n",
       " 'mitigating',\n",
       " 'modules',\n",
       " 'more',\n",
       " 'need',\n",
       " 'not',\n",
       " 'notions',\n",
       " 'obtaining',\n",
       " 'of',\n",
       " 'outcomes',\n",
       " 'pervasive',\n",
       " 'pipeline',\n",
       " 'potential',\n",
       " 'practice',\n",
       " 'propose',\n",
       " 'proposed',\n",
       " 'raised',\n",
       " 'ranking',\n",
       " 'real',\n",
       " 'recent',\n",
       " 'related',\n",
       " 'required',\n",
       " 'researchers',\n",
       " 'response',\n",
       " 'still',\n",
       " 'studies',\n",
       " 'study',\n",
       " 'such',\n",
       " 'system',\n",
       " 'that',\n",
       " 'the',\n",
       " 'there',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'together',\n",
       " 'tool',\n",
       " 'translated',\n",
       " 'understanding',\n",
       " 'urgent',\n",
       " 'user',\n",
       " 'viable',\n",
       " 'visual',\n",
       " 'we',\n",
       " 'world'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(words(absdata['cs.HC'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCvocab = defaultdict(lambda: .5)\n",
    "for txt in absdata['cs.HC'][:900]:\n",
    "    for w in set(words(txt)): HCvocab[w] += 1\n",
    "\n",
    "LGvocab = defaultdict(lambda: .5)\n",
    "for txt in absdata['cs.LG'][:900]:\n",
    "    for w in set(words(txt)): LGvocab[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data-driven decision making related to individuals has become increasingly pervasive, but the issue concerning the potential discrimination has been raised by recent studies. In response, researchers have made efforts to propose and implement fairness measures and algorithms, but those efforts have not been translated to the real-world practice of data-driven decision making. As such, there is still an urgent need to create a viable tool to facilitate fair decision making. We propose FairSight, a visual analytic system to address this need; it is designed to achieve different notions of fairness in ranking decisions through identifying the required actions -- understanding, measuring, diagnosing and mitigating biases -- that together lead to fairer decision making. Through a case study and user study, we demonstrate that the proposed visual analytic and diagnostic modules in the system are effective in understanding the fairness-aware decision pipeline and obtaining more fair outcomes.',\n",
       " 'Eyewear devices, such as augmented reality displays, increasingly integrate eye tracking but the first-person camera required to map a user\\'s gaze to the visual scene can pose a significant threat to user and bystander privacy. We present PrivacEye, a method to detect privacy-sensitive everyday situations and automatically enable and disable the eye tracker\\'s first-person camera using a mechanical shutter. To close the shutter in privacy-sensitive situations, the method uses a deep representation of the first-person video combined with rich features that encode users\\' eye movements. To open the shutter without visual input, PrivacEye detects changes in users\\' eye movements alone to gauge changes in the \"privacy level\" of the current situation. We evaluate our method on a first-person video dataset recorded in daily life situations of 17 participants, annotated by themselves for privacy sensitivity, and show that our method is effective in preserving privacy in this challenging setting.',\n",
       " 'What we eat is one of the most frequent and important health decisions we make in daily life, yet it remains notoriously difficult to capture and understand. Effective food journaling is thus a grand challenge in personal health informatics. In this paper we describe a system for food journaling called I Ate This, which is inspired by the Remote Food Photography Method (RFPM). I Ate This is simple: you use a smartphone app to take a photo and give a very basic description of any food or beverage you are about to consume. Later, a qualified dietitian will evaluate your photo, giving you feedback on how you did and where you can improve. The aim of I Ate This is to provide a convenient, visual and reliable way to help users learn from their eating habits and nudge them towards better choices each and every day. Ultimately, this incremental approach can lead to long-term behaviour change. Our goal is to bring RFPM to a wider audience, through APIs that can be incorporated into other apps.',\n",
       " 'Ubiquitous sensing is tightly coupled with activity recognition. This survey reviews recent advances in Ubiquitous sensing and looks ahead on promising future directions. In particular, Ubiquitous sensing crosses new barriers giving us new ways to interact with the environment or to inspect our psyche. Through sensing paradigms that parasitically utilise stimuli from the noise of environmental, third-party pre-installed systems, sensing leaves the boundaries of the personal domain. Compared to previous environmental sensing approaches, these new systems mitigate high installation and placement cost by providing a robustness towards process noise. On the other hand, sensing focuses inward and attempts to capture mental activities such as cognitive load, fatigue or emotion through advances in, for instance, eye-gaze sensing systems or interpretation of body gesture or pose. This survey summarises these developments and discusses current research questions and promising future directions.',\n",
       " 'Virtual reality setups are particularly suited to create a tight bond between players and their avatars up to a degree where we start perceiving the virtual representation as our own body. We hypothesize that such an illusion of virtual body ownership (IVBO) has a particularly high, yet overlooked potential for nonhumanoid avatars. To validate our claim, we use the example of three very different creatures---a scorpion, a rhino, and a bird---to explore possible avatar controls and game mechanics based on specific animal abilities. A quantitative evaluation underpins the high game enjoyment arising from embodying such nonhuman morphologies, including additional body parts and obtaining respective superhuman skills, which allows us to derive a set of novel design implications. Furthermore, the experiment reveals a correlation between IVBO and game enjoyment, which is a further indication that nonhumanoid creatures offer a meaningful design space for VR games worth further investigation.',\n",
       " 'This paper overviews the state of the art, research challenges, and future opportunities in an emerging research direction: Social Sensing based Edge Computing (SSEC). Social sensing has emerged as a new sensing application paradigm where measurements about the physical world are collected from humans or from devices on their behalf. The advent of edge computing pushes the frontier of computation, service, and data along the cloud-to-things continuum. The merging of these two technical trends generates a set of new research challenges that need to be addressed. In this paper, we first define the new SSEC paradigm that is motivated by a few underlying technology trends. We then present a few representative real-world case studies of SSEC applications and several key research challenges that exist in those applications. Finally, we envision a few exciting research directions in future SSEC. We hope this paper will stimulate discussions of this emerging research direction in the community.',\n",
       " 'Many professional services are provided through text and voice systems, from voice calls over the internet to messaging and emails. There is a growing need for both individuals and organizations to understand these online conversations better and find actionable insights. One method that allows the user to explore insights is to build intuitive and rich visualizations that illustrate the content of the conversation. In this paper, we present a systematic survey of the various methods of visualizing a conversation and research papers involving interactive visualizations and human participants. Findings from the survey show that there have been attempts to visualize most, if not all, of the types of conversation that are taking place digitally, from speech to messages and emails. Through this survey, we make two contributions. One, we summarize the current practices in the domain of visualizing dyadic conversations. Two, we provide suggestions for future dialogue visualization research.',\n",
       " 'Shopping is difficult for people with motor impairments. This includes online shopping. Proprietary software can emulate mouse and keyboard via head tracking. However, such a solution is not common for smartphones. Unlike desktop and laptop computers, they are also much easier to carry indoors and outdoors.To address this, we implement and open source button that is sensitive to head movements tracked from the front camera of iPhone X. This allows developers to integrate in eCommerce applications easily without requiring specialized knowledge. Other applications include gaming and use in hands-free situations such as during cooking, auto-repair. We built a sample online shopping application that allows users to easily browse between items from various categories and take relevant action just by head movements. We present results of user studies on this sample application and also include sensitivity studies based on two independent tests performed at 3 different distances to the screen.',\n",
       " 'Advances in user interfaces, pattern recognition, and ubiquitous computing continue to pave the way for better navigation towards our health goals. Quantitative methods which can guide us towards our personal health goals will help us optimize our daily life actions, and environmental exposures. Ubiquitous computing is essential for monitoring these factors and actuating timely interventions in all relevant circumstances. We need to combine the events recognized by different ubiquitous systems and derive actionable causal relationships from an event ledger. Understanding of user habits and health should be teleported between applications rather than these systems working in silos, allowing systems to find the optimal guidance medium for required interventions. We propose a method through which applications and devices can enhance the user experience by leveraging event relationships, leading the way to more relevant, useful, and, most importantly, pleasurable health guidance experience.',\n",
       " '- Like consumer electronic products, medical devices are becoming more complicated, with performance doubling every two years. With multiple commands and systems to negotiate, cognitive load can make it difficult for users to execute commands effectively. In the case of medical devices, which use advanced technology and require multidisciplinary inputs for design and development, cognitive workload is a significant factor. As these devices are very expensive and operators require specialized training, effective and economical methods are needed to evaluate the user experience. Heuristic evaluation is an effective method of identifying major usability problems and related issues. This study used heuristic evaluation to assess the usability of a CT scan and associated physical and mental loads for Saudi Arabian users. The findings indicate a gender difference in terms of consistency, flexibility, and document attributes, with a statistically significant gender difference in mental load.',\n",
       " 'Browsing and finding relevant information for Bangladeshi laws is a challenge faced by all law students and researchers in Bangladesh, and by citizens who want to learn about any legal procedure. Some law archives in Bangladesh are digitized, but lack proper tools to organize the data meaningfully. We present a text visualization tool that utilizes machine learning techniques to make the searching of laws quicker and easier. Using Doc2Vec to layout law article nodes, link mining techniques to visualize relevant citation networks, and named entity recognition to quickly find relevant sections in long law articles, our tool provides a faster and better search experience to the users. Qualitative feedback from law researchers, students, and government officials show promise for visually intuitive search tools in the context of governmental, legal, and constitutional data in developing countries, where digitized data does not necessarily pave the way towards an easy access to information.',\n",
       " \"The Google's frugal Cardboard solution for immersive Virtual Reality experiences has come a long way in the VR market. The Google Cardboard VR applications will support us in the fields such as education, virtual tourism, entertainment, gaming, design etc. Recently, Qualcomm's Vuforia SDK has introduced support for developing mixed reality applications for Google Cardboard which can combine Virtual and Augmented Reality to develop exciting and immersive experiences. In this work, we present a comprehensive review of Google Cardboard for AR and also highlight its technical and subjective limitations by conducting a feasibility study through the inspection of a Desktop computer use-case. Additionally, we recommend the future avenues for the Google Cardboard in AR. This work also serves as a guide for Android/iOS developers as there are no published scholarly articles or well documented studies exclusively on Google Cardboard with both user and developer's experience captured at one place.\",\n",
       " 'We propose technology to enable a new medium of expression, where video elements can be looped, merged, and triggered, interactively. Like audio, video is easy to sample from the real world but hard to segment into clean reusable elements. Reusing a video clip means non-linear editing and compositing with novel footage. The new context dictates how carefully a clip must be prepared, so our end-to-end approach enables previewing and easy iteration.\\n  We convert static-camera videos into loopable sequences, synthesizing them in response to simple end-user requests. This is hard because a) users want essentially semantic-level control over the synthesized video content, and b) automatic loop-finding is brittle and leaves users limited opportunity to work through problems. We propose a human-in-the-loop system where adding effort gives the user progressively more creative control. Artists help us evaluate how our trigger interfaces can be used for authoring of videos and video-performances.',\n",
       " 'Mobile devices such as smartphones, smartwatches or smart glasses have revolutionized how we interact. We are interested in smart glasses because they have the advantage of providing a simultaneous view of both physical and digital worlds. Despite this potential, pointing task on smart glasses is not really widespread. In this paper, we compared four interaction techniques for selecting targets : (a) the Absolute Head Movement and (b) the Relative Head Movement, where head movement controls the cursor on smart glasses in absolute or relative way, (c) the Absolute Free Hand interaction, where the forefinger control the cursor and (d) the Tactile Surface interaction, where the user controls the cursor via a small touchpad connected to smart glasses. We conducted an experiment with 18 participants. The Tactile Surface and Absolute Head Movement were the most efficient. The Relative Head Movement and Absolute Free Hand interactions were promising and require more exploration for other tasks.',\n",
       " \"The goal of visual analytics is to create a symbiosis between human and computer by leveraging their unique strengths. While this model has demonstrated immense success, we are yet to realize the full potential of such a human-computer partnership. In a perfect collaborative mixed-initiative system, the computer must possess skills for learning and anticipating the users' needs. Addressing this gap, we propose a framework for inferring focus areas from passive observations of the user's actions, thereby allowing accurate predictions of future events. We evaluate this technique with a crime map and demonstrate that users' clicks appear in our prediction set 95% - 97% of the time. Further analysis shows that we can achieve high prediction accuracy typically after three clicks. Altogether, we show that passive observations of interaction data can reveal valuable information that will allow the system to learn and anticipate future events, laying the foundation for next-generation tools.\",\n",
       " 'Recommender systems rely heavily on the predictive accuracy of the learning algorithm. Most work on improving accuracy has focused on the learning algorithm itself. We argue that this algorithmic focus is myopic. In particular, since learning algorithms generally improve with more and better data, we propose shaping the feedback generation process as an alternate and complementary route to improving accuracy. To this effect, we explore how changes to the user interface can impact the quality and quantity of feedback data -- and therefore the learning accuracy. Motivated by information foraging theory, we study how feedback quality and quantity are influenced by interface design choices along two axes: information scent and information access cost. We present a user study of these interface factors for the common task of picking a movie to watch, showing that these factors can effectively shape and improve the implicit feedback data that is generated while maintaining the user experience.',\n",
       " 'In recent years, bike-sharing systems have been deployed in many cities, which provide an economical lifestyle. With the prevalence of bike-sharing systems, a lot of companies join the market, leading to increasingly fierce competition. To be competitive, bike-sharing companies and app developers need to make strategic decisions for mobile apps development. Therefore, it is significant to predict and compare the popularity of different bike-sharing apps. However, existing works mostly focus on predicting the popularity of a single app, the popularity contest among different apps has not been explored yet. In this paper, we aim to forecast the popularity contest between Mobike and Ofo, two most popular bike-sharing apps in China. We develop CompetitiveBike, a system to predict the popularity contest among bike-sharing apps. Moreover, we conduct experiments on real-world datasets collected from 11 app stores and Sina Weibo, and the experiments demonstrate the effectiveness of our approach.',\n",
       " 'Digital ink promises to combine the flexibility and aesthetics of handwriting and the ability to process, search and edit digital text. Character recognition converts handwritten text into a digital representation, albeit at the cost of losing personalized appearance due to the technical difficulties of separating the interwoven components of content and style. In this paper, we propose a novel generative neural network architecture that is capable of disentangling style from content and thus making digital ink editable. Our model can synthesize arbitrary text, while giving users control over the visual appearance (style). For example, allowing for style transfer without changing the content, editing of digital ink at the word level and other application scenarios such as spell-checking and correction of handwritten text. We furthermore contribute a new dataset of handwritten text with fine-grained annotations at the character level and report results from an initial user evaluation.',\n",
       " 'Advances in machine learning have produced systems that attain human-level performance on certain visual tasks, e.g., object identification. Nonetheless, other tasks requiring visual expertise are unlikely to be entrusted to machines for some time, e.g., satellite and medical imagery analysis. We describe a human-machine cooperative approach to visual search, the aim of which is to outperform either human or machine acting alone. The traditional route to augmenting human performance with automatic classifiers is to draw boxes around regions of an image deemed likely to contain a target. Human experts typically reject this type of hard highlighting. We propose instead a soft highlighting technique in which the saliency of regions of the visual field is modulated in a graded fashion based on classifier confidence level. We report on experiments with both synthetic and natural images showing that soft highlighting achieves a performance synergy surpassing that attained by hard highlighting.',\n",
       " 'The article presents the dynamics of social networks users increase, depending on the total world population from 2010 to 2018. It also identifies the most popular social networks in Ukraine. The systematic risk indicator of using social networks relative to the total number of Internet resources users is determined. Types of social intercourse in the process of the higher education institution image creation are presented. The peculiarities of using social networks in the formation of a positive image of an educational institution are highlighted. The statistical indicators of user actions in the official group of the Faculty of Mathematics and Information Technologies of Vasyl Stus Donetsk National University in January, February and March 2019 are presented, as well as the average attraction coefficient of users depending on the subject of publications. The main technologies of astroturfing in the creation process of the higher education institution negative image are considered.',\n",
       " \"This work covers multiple aspects of overt visual attention on 3D renders: measurement, projection, visualization, and application to studying the influence of material appearance on looking behaviour. In the scope of this work, we ran an eye-tracking experiment in which the observers are presented with animations of rotating 3D objects. The objects were rendered to simulate different metallic appearance, particularly smooth (glossy), rough (matte), and coated gold. The eye-tracking results illustrate how material appearance itself influences the observer's attention, while all the other parameters remain unchanged. In order to make visualization of the attention maps more natural and also make the analysis more accurate, we develop a novel technique of projection of gaze fixations on the 3D surface of the figure itself, instead of the conventional 2D plane of the screen. The proposed methodology will be useful for further studies of attention and saliency in the computer graphics domain.\",\n",
       " \"Three-dimensional (3D) applications have come to every corner of life. We present 3DTouch, a novel 3D wearable input device worn on the fingertip for interacting with 3D applications. 3DTouch is self-contained, and designed to universally work on various 3D platforms. The device employs touch input for the benefits of passive haptic feedback, and movement stability. Moreover, with touch interaction, 3DTouch is conceptually less fatiguing to use over many hours than 3D spatial input devices such as Kinect. Our approach relies on relative positioning technique using an optical laser sensor and a 9-DOF inertial measurement unit. We implemented a set of 3D interaction techniques including selection, translation, and rotation using 3DTouch. An evaluation also demonstrates the device's tracking accuracy of 1.10 mm and 2.33 degrees for subtle touch interaction in 3D space. With 3DTouch project, we would like to provide an input device that reduces the gap between 3D applications and users.\",\n",
       " 'Lecture notes are important for students to review and understand the key points in the class. Unfortunately, the students often miss or lose part of the lecture notes. In this paper, we design and implement an infrared sensor based system, InfraNotes, to automatically record the notes on the board by sensing and analyzing hand gestures of the lecturer. Compared with existing techniques, our system does not require special accessories with lecturers such as sensor-facilitated pens, writing surfaces or the video-taping infrastructure. Instead, it only has an infrared-sensor module on the eraser holder of black/white board to capture handwritten trajectories. With a lightweight framework for handwritten trajectory processing, clear lecture notes can be generated automatically. We evaluate the quality of lecture notes by three standard character recognition techniques. The results indicate that InfraNotes is a promising solution to create clear and complete lectures to promote the education.',\n",
       " 'Cartograms combine statistical and geographical information in thematic maps, where areas of geographical regions (e.g., countries, states) are scaled in proportion to some statistic (e.g., population, income). Cartograms make it possible to gain insight into patterns and trends in the world around us and have been very popular visualizations for geo-referenced data for over a century. This work surveys cartogram research in visualization, cartography and geometry, covering a broad spectrum of different cartogram types: from the traditional rectangular and table cartograms, to Dorling and diffusion cartograms. A particular focus is the study of the major cartogram dimensions: statistical accuracy, geographical accuracy, and topological accuracy. We review the history of cartograms, describe the algorithms for generating them, and consider task taxonomies. We also review quantitative and qualitative evaluations, and we use these to arrive at design guidelines and research challenges.',\n",
       " 'This paper outlines the development of a wearable game controller incorporating vibrotacticle haptic feedback that provides a low cost, versatile and intuitive interface for controlling digital games. The device differs from many traditional haptic feedback implementation in that it combines vibrotactile based haptic feedback with gesture based input, thus becoming a two way conduit between the user and the virtual environment. The device is intended to challenge what is considered an \"interface\" and draws on work in the area of Actor-Network theory to purposefully blur the boundary between man and machine. This allows for a more immersive experience, so rather than making the user feel like they are controlling an aircraft the intuitive interface allows the user to become the aircraft that is controlled by the movements of the user\\'s hand. This device invites playful action and thrill. It bridges new territory on portable and low cost solutions for haptic controllers in a gaming context.',\n",
       " \"Expert crowdsourcing marketplaces have untapped potential to empower workers' career and skill development. Currently, many workers cannot afford to invest the time and sacrifice the earnings required to learn a new skill, and a lack of experience makes it difficult to get job offers even if they do. In this paper, we seek to lower the threshold to skill development by repurposing existing tasks on the marketplace as mentored, paid, real-world work experiences, which we refer to as micro-internships. We instantiate this idea in Atelier, a micro-internship platform that connects crowd interns with crowd mentors. Atelier guides mentor-intern pairs to break down expert crowdsourcing tasks into milestones, review intermediate output, and problem-solve together. We conducted a field experiment comparing Atelier's mentorship model to a non-mentored alternative on a real-world programming crowdsourcing task, finding that Atelier helped interns maintain forward progress and absorb best practices.\",\n",
       " 'Surveillance is essential for the safety of power substation. The detection of whether wearing safety helmets or not for perambulatory workers is the key component of overall intelligent surveillance system in power substation. In this paper, a novel and practical safety helmet detection framework based on computer vision, machine learning and image processing is proposed. In order to ascertain motion objects in power substation, the ViBe background modelling algorithm is employed. Moreover, based on the result of motion objects segmentation, real-time human classification framework C4 is applied to locate pedestrian in power substation accurately and quickly. Finally, according to the result of pedestrian detection, the safety helmet wearing detection is implemented using the head location, the color space transformation and the color feature discrimination. Extensive compelling experimental results in power substation illustrate the efficiency and effectiveness of the proposed framework.',\n",
       " 'Humans are the most effective integrators and producers of information, directly and through the use of information-processing inventions. As these inventions become increasingly sophisticated, the substantive role of humans in processing information will tend toward capabilities that derive from our most complex cognitive processes, e.g., abstraction, creativity, and applied world knowledge. Through the advancement of human computation - methods that leverage the respective strengths of humans and machines in distributed information-processing systems - formerly discrete processes will combine synergistically into increasingly integrated and complex information processing systems. These new, collective systems will exhibit an unprecedented degree of predictive accuracy in modeling physical and techno-social processes, and may ultimately coalesce into a single unified predictive organism, with the capacity to address societies most wicked problems and achieve planetary homeostasis.',\n",
       " 'In this research, a literature in human-computer interaction is reviewed and the technology aspect of human computer interaction related with digital academic supportive devices is also analyzed. According to all these concerns, recommendations to design good human-computer digital academic supportive devices are analyzed and proposed. Due to improvements in both hardware and software, digital devices have unveiled continuous advances in efficiency and processing capacity. However, many of these systems are also becoming larger and increasingly more complex. Although such complexity usually poses no difficulties for many users, it often creates barriers for academic users while using digital devices. Usually, in designing those digital devices, the human-computer interaction is left behind without consideration. To achieve dependable, usable, and well-engineered interactive digital academic supportive devices requires applied human computer interaction research and awareness of its issues.',\n",
       " 'Deep learning is one of the fastest growing technologies in computer science with a plethora of applications. But this unprecedented growth has so far been limited to the consumption of deep learning experts. The primary challenge being a steep learning curve for learning the programming libraries and the lack of intuitive systems enabling non-experts to consume deep learning. Towards this goal, we study the effectiveness of a no-code paradigm for designing deep learning models. Particularly, a visual drag-and-drop interface is found more efficient when compared with the traditional programming and alternative visual programming paradigms. We conduct user studies of different expertise levels to measure the entry level barrier and the developer load across different programming paradigms. We obtain a System Usability Scale (SUS) of 90 and a NASA Task Load index (TLX) score of 21 for the proposed visual programming compared to 68 and 52, respectively, for the traditional programming methods.',\n",
       " \"The widespread popularity of Pok\\\\'emon GO presents the first opportunity to observe the geographic effects of location-based gaming at scale. This paper reports the results of a mixed methods study of the geography of Pok\\\\'emon GO that includes a five-country field survey of 375 Pok\\\\'emon GO players and a large scale geostatistical analysis of game elements. Focusing on the key geographic themes of places and movement, we find that the design of Pok\\\\'emon GO reinforces existing geographically-linked biases (e.g. the game advantages urban areas and neighborhoods with smaller minority populations), that Pok\\\\'emon GO may have instigated a relatively rare large-scale shift in global human mobility patterns, and that Pok\\\\'emon GO has geographically-linked safety risks, but not those typically emphasized by the media. Our results point to geographic design implications for future systems in this space such as a means through which the geographic biases present in Pok\\\\'emon GO may be counteracted.\",\n",
       " 'With the large number of partially or completely visually impaired persons in society, their integration as productive, educated and capable members of society is hampered heavily by a pervasively high level of braille illiteracy. This problem is further compounded by the fact that braille printers are prohibitively expensive - generally starting from two thousand US dollars, beyond the reach of the common man. Over the period of a year, the authors have tried to develop a Braille printer which attempts to overcome the problems inherent in commercial printers. The purpose of this paper, therefore, is to introduce two prototypes - the first with an emphasis of cost-effectiveness, and the second prototype, which is more experimental and aims to eliminate several demerits of Braille printing. The first prototype has been constructed at a cost significantly less than the existing commercial braille printers. Both the prototypes of the device have been constructed, which will be shown.',\n",
       " \"Screencasts, where computer screen is broadcast to a large audience on the web, are becoming popular as an online educational tool. Among various types of screencast content, popular are the contents that involve text editing, including computer programming. There are emerging platforms that support such text-based screencasts by recording every character insertion/deletion from the creator and reconstructing its playback on the viewer's screen. However, these platforms lack rich support for creating and editing the screencast itself, mainly due to the difficulty of manipulating recorded text changes; the changes are tightly coupled in sequence, thus modifying arbitrary part of the sequence is not trivial. We present a non-linear editing tool for text-based screencasts. With the proposed selective history rewrite process, our editor allows users to substitute an arbitrary part of a text-based screencast while preserving overall consistency of the rest of the text-based screencast.\",\n",
       " 'Designing and building automated systems with which people can interact naturally is one of the emerging objective of Mechatronics. In this perspective multimodality and adaptivity represent focal issues, enabling users to communicate more freely and naturally with automated systems. One of the basic problem of multimodal interaction is the fusion process. Current approaches to fusion are mainly two: the former implements the multimodal fusion at dialogue management level, whereas the latter at grammar level. In this paper, we propose a multimodal attribute grammar, that provides constructions both for representing input symbols from different modalities and for modeling semantic and temporal features of multimodal input symbols, enabling the specification of multimodal languages. Moreover, an application of the proposed approach in the context of a multimodal language specification to control a driver assistance system, as robots using different integrated interaction modalities, is given.',\n",
       " \"The paper analyzes the interaction between humans and computers in terms of response time in solving the image-based CAPTCHA. In particular, the analysis focuses on the attitude of the different Internet users in easily solving four different types of image-based CAPTCHAs which include facial expressions like: animated character, old woman, surprised face, worried face. To pursue this goal, an experiment is realized involving 100 Internet users in solving the four types of CAPTCHAs, differentiated by age, Internet experience, and education level. The response times are collected for each user. Then, association rules are extracted from user data, for evaluating the dependence of the response time in solving the CAPTCHA from age, education level and experience in internet usage by statistical analysis. The results implicitly capture the users' psychological states showing in what states the users are more sensible. It reveals to be a novelty and a meaningful analysis in the state-of-the-art.\",\n",
       " 'We consider a crowdsourcing model in which $n$ workers are asked to rate the quality of $n$ items previously generated by other workers. An unknown set of $\\\\alpha n$ workers generate reliable ratings, while the remaining workers may behave arbitrarily and possibly adversarially. The manager of the experiment can also manually evaluate the quality of a small number of items, and wishes to curate together almost all of the high-quality items with at most an $\\\\epsilon$ fraction of low-quality items. Perhaps surprisingly, we show that this is possible with an amount of work required of the manager, and each worker, that does not scale with $n$: the dataset can be curated with $\\\\tilde{O}\\\\Big(\\\\frac{1}{\\\\beta\\\\alpha^3\\\\epsilon^4}\\\\Big)$ ratings per worker, and $\\\\tilde{O}\\\\Big(\\\\frac{1}{\\\\beta\\\\epsilon^2}\\\\Big)$ ratings by the manager, where $\\\\beta$ is the fraction of high-quality items. Our results extend to the more general setting of peer prediction, including peer grading in online classrooms.',\n",
       " 'The Audio/Visual Emotion Challenge and Workshop (AVEC 2019) \"State-of-Mind, Detecting Depression with AI, and Cross-cultural Affect Recognition\" is the ninth competition event aimed at the comparison of multimedia processing and machine learning methods for automatic audiovisual health and emotion analysis, with all participants competing strictly under the same conditions. The goal of the Challenge is to provide a common benchmark test set for multimodal information processing and to bring together the health and emotion recognition communities, as well as the audiovisual processing communities, to compare the relative merits of various approaches to health and emotion recognition from real-life data. This paper presents the major novelties introduced this year, the challenge guidelines, the data used, and the performance of the baseline systems on the three proposed tasks: state-of-mind recognition, depression assessment with AI, and cross-cultural affect sensing, respectively.',\n",
       " \"Throughout the course of product experience, a user employs multiple senses, including vision, hearing, and touch. Previous cross-modal studies have shown that multiple senses interact with each other and change perceptions. In this paper, we propose a methodology for designing multisensory product experiences by applying cross-modal effect to simultaneous stimuli. In this methodology, we first obtain a model of the comprehensive cognitive structure of user's multisensory experience by applying Kansei modeling methodology and extract opportunities of cross-modal effect from the structure. Second, we conduct experiments on these cross-modal effects and formulate them by obtaining a regression curve through analysis. Finally, we find solutions to improve the product sensory experience from the regression model of the target cross-modal effects. We demonstrated the validity of the methodology with SLR cameras as a case study, which is a typical product with multisensory perceptions.\",\n",
       " 'Recent advances in haptic hardware and software technology have generated interest in novel, multimodal interfaces based on the sense of touch. Such interfaces have the potential to revolutionize the way we think about human computer interaction and open new possibilities for simulation and training in a variety of fields. In this paper we review several frameworks, APIs and toolkits for haptic user interface development. We explore these software components focusing on minimally invasive surgical simulation systems. In the area of medical diagnosis, there is a strong need to determine mechanical properties of biological tissue for both histological and pathological considerations. Therefore we focus on the development of affordable visuo-haptic simulators to improve practice-based education in this area. We envision such systems, designed for the next generations of learners that enhance their knowledge in connection with real-life situations while they train in mandatory safety conditions.',\n",
       " 'The regular K-10 curriculums often do not get the necessary of affordable technology involving interactive ways of teaching the prescribed curriculum with effective analytical skill building. In this paper, we present \"PlutoAR\", a paper-based augmented reality interpreter which is scalable, affordable, portable and can be used as a platform for skill building for the kids. PlutoAR manages to overcome the conventional albeit non-interactive ways of teaching by incorporating augmented reality (AR) through an interactive toolkit to provide students with the best of both worlds. Students cut out paper \"tiles\" and place these tiles one by one on a larger paper surface called \"Launchpad\" and use the PlutoAR mobile application which runs on any Android device with a camera and uses augmented reality to output each step of the program like an interpreter. PlutoAR has inbuilt AR experiences like stories, maze solving using conditional loops, simple elementary mathematics and the intuition of gravity.',\n",
       " \"While a number of touch-based visualization systems have appeared in recent years, relatively little work has been done to evaluate these systems. The prevailing methods compare these systems to desktop-class applications or utilize traditional training-based usability studies. We argue that existing studies, while useful, fail to address a key aspect of mobile application usage - initial impression and discoverability-driven usability. Over the past few years, we have developed a tablet-based visualization system, Tangere, for analyzing tabular data in a multiple coordinated view configuration. This article describes a discoverability-based user study of Tangere in which the system is compared to a commercially available visualization system for tablets - Tableau's Vizable. The study highlights aspects of each system's design that resonate with study participants, and we reflect upon those findings to identify design principles for future tablet-based data visualization systems.\",\n",
       " \"This bachelor's thesis describes the conception and implementation of an augmented reality application for the Android platform. The intention is to demonstrate some possibilities of interaction within an augmented reality environment on mobile devices. For that purpose, a 3D-model is displayed on the devices' touchscreen using marker-based tracking. This enables the user to translate, rotate or scale the model as he wishes. He can additionally select and highlight preassigned parts of the model to display specific information for that element. To assist developers in modifying the application for changing requirements without re-writing large portions of the source code, the information for each part have been encapsulated into its own data type. After an introduction to augmented reality, its underlying technology and the Android platform, some possible usage scenarios and the resulting functionalities are outlined. Finally, the design as well as the developed implementation are described.\",\n",
       " 'Thanks to mobile apps such as Periscope and Facebook Live, live-streaming video is having a moment again. It has not been clear, however, to what extent the current ubiquity of smartphones is impacting this technology\\'s acceptance in everyday social situations and how mobile contexts or affordances will affect and be affected by shifts in social norms and policy debates regarding privacy, surveillance and intellectual property. This ethnographic-style research explores familiarity with and attitudes about mobile live-streaming video and related legal and ethical issues among a sample of \"Middle America\" participants at two typical outdoor social events: sports tailgating and a rooftop party. In situ observations of n=110 bystanders to the use of a smartphone, including interviews with n=20, revealed that many are not fully aware of when their image or speech is being live-streamed in a casual context and want stronger notifications of and ability to consent to such broadcasting.',\n",
       " 'It remains uncertain regarding the safety of driving in autonomous vehicles that, after a long, passive control and inattention to the driving situation, how the drivers will be effectively informed to take-over the control in emergency. In particular, the active role of vehicle force feedback on the driver\\'s risk perception on curves has not been fully explored. To investigate it, the current paper examined the driver\\'s cognitive and visual responses to the whole-body haptic feedback during curve negotiations. The effects of force feedback on drivers\\' responses on curves were investigated in a high-fidelity driving simulator while measuring EEG and visual gaze over ten participants. The preliminary analyses of the first two participants revealed that pupil diameter and fixation time on the curves were significantly longer when the driver received whole-body feedback, compared to none. The findings suggest that whole-body feedback can be used as an effective \"advance notification\" of hazards.',\n",
       " 'The aim of the study is the description of problem of developing web design for people with color blindness. The objectives of the study are familiarising with the exiting algorithms of simulation color blindness and searching the most appropriate color models to realize a filter of disputed colors. The object of the study is the convertation of color models and algorithms of filtration. The subject of the study are methods of recognition disputed colors. In the study were investigated the problems of color blind people, examined the basic concepts of trichromatic color vision theory, substantiated the necessity of changing different types of color models, given formulas convertation from RGB-color model to HSL-color model, systematized the algorithms of imitation and filtration of colors for different types of dichromacy: protanopia, deuteranopia and tritanopia. The results of the study are planned using in development of adapting website design for people with color blindness.',\n",
       " 'We are developing a system for human-robot communication that enables people to communicate with robots in a natural way and is focused on solving problems in a shared space. Our strategy for developing this system is fundamentally data-driven: we use data from multiple input sources and train key components with various machine learning techniques. We developed a web application that is collecting data on how two humans communicate to accomplish a task, as well as a mobile laboratory that is instrumented to collect data on how two humans communicate to accomplish a task in a physically shared space. The data from these systems will be used to train and fine-tune the second stage of our system, in which the robot will be simulated through software. A physical robot will be used in the final stage of our project. We describe these instruments, a test-suite and performance metrics designed to evaluate and automate the data gathering process as well as evaluate an initial data set.',\n",
       " 'In this paper, we examine the statistical soundness of comparative assessments within the field of recommender systems in terms of reliability and human uncertainty. From a controlled experiment, we get the insight that users provide different ratings on same items when repeatedly asked. This volatility of user ratings justifies the assumption of using probability densities instead of single rating scores. As a consequence, the well-known accuracy metrics (e.g. MAE, MSE, RMSE) yield a density themselves that emerges from convolution of all rating densities. When two different systems produce different RMSE distributions with significant intersection, then there exists a probability of error for each possible ranking. As an application, we examine possible ranking errors of the Netflix Prize. We are able to show that all top rankings are more or less subject to high probabilities of error and that some rankings may be deemed to be caused by mere chance rather than system quality.',\n",
       " 'Riemannian geometry has been successfully used in many brain-computer interface (BCI) classification problems and demonstrated superior performance. In this paper, for the first time, it is applied to BCI regression problems, an important category of BCI applications. More specifically, we propose a new feature extraction approach for Electroencephalogram (EEG) based BCI regression problems: a spatial filter is first used to increase the signal quality of the EEG trials and also to reduce the dimensionality of the covariance matrices, and then Riemannian tangent space features are extracted. We validate the performance of the proposed approach in reaction time estimation from EEG signals measured in a large-scale sustained-attention psychomotor vigilance task, and show that compared with the traditional powerband features, the tangent space features can reduce the root mean square estimation error by 4.30-8.30%, and increase the estimation correlation coefficient by 6.59-11.13%.',\n",
       " 'Prison facilities, mental correctional institutions, sports bars and places of public protest are prone to sudden violence and conflicts. Surveillance systems play an important role in mitigation of hostile behavior and improvement of security by detecting such provocative and aggressive activities. This research proposed using automatic aggressive behavior and anger detection to improve the effectiveness of the surveillance systems. An emotion and aggression aware component will make the surveillance system highly responsive and capable of alerting the security guards in real time. This research proposed facial expression, head, hand and body movement and speech tracking for detecting anger and aggressive actions. Recognition was achieved using support vector machines and rule based features. The multimodal affect recognition precision rate for anger improved by 15.2% and recall rate improved by 11.7% when behavioral rule based features were used in aggressive action detection.',\n",
       " 'Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.',\n",
       " 'Even in the digital age, designers largely rely on physical material samples to illustrate their products, as existing visual representations fail to sufficiently reproduce the look and feel of real world materials. Here, we investigate the use of interactive material sonification as an additional sensory modality for communicating well-established material qualities like softness, pleasantness or value. We developed a custom application for touchscreen devices that receives tactile input and translate it into material rubbing sound using granular synthesis. We used this system to perform a psychophysical study, in which the ability of the user to rate subjective material qualities is evaluated, with the actual material samples serving as reference stimulus. Our experimental results indicate that the considered audio cues do not significantly contribute to the perception of material qualities but are able to increase the level of immersion when interacting with digital samples.',\n",
       " \"Music history, referring to the records of users' listening or downloading history in online music services, is the primary source for music service providers to analyze users' preferences on music and thus to provide personalized recommendations to users. In order to engage users into the service and to improve user experience, it would be beneficial to provide visual analyses of one user's music history as well as visualized recommendations to that user. In this paper, we take a user-centric approach to the design of such visual analyses. We start by investigating user needs on such visual analyses and recommendations, then propose several different visualization schemes, and perform a pilot study to collect user feedback on the designed schemes. We further conduct user studies to verify the utility of the proposed schemes, and the results not only demonstrate the effectiveness of our proposed visualization, but also provide important insights to guide the visualization design in the future.\",\n",
       " 'Social media enjoys a growing popularity as a platform to seek and share personal health information. For sleep studies using data from social media, most researchers focused on inferring sleep-related artifacts from self-reported anecdotal pointers to sleep patterns or issues such as insomnia. The data shared by \"quantified-selfers\" on social media presents an opportunity to study more quantitative and objective measures of sleep. We propose and validate the approach of collecting and analyzing sleep logs that are generated and shared through a sleep-tracking mobile application. We highlight the value of this data by combining it with users\\' social media data. The results provide a validation of using social media for sleep studies as the collected sleep data is aligned with sleep data from other sources. The results of combining social media data with sleep data provide preliminary evidence that higher social media activity is associated with lower sleep duration and quality.',\n",
       " \"Recent methods to automatically calibrate stationary eye trackers were shown to effectively reduce inherent calibration distortion. However, these methods require additional information, such as mouse clicks or on-screen content. We propose the first method that only requires users' eye movements to reduce calibration distortion in the background while users naturally look at an interface. Our method exploits that calibration distortion makes straight saccade trajectories appear curved between the saccadic start and end points. We show that this curving effect is systematic and the result of distorted gaze projection plane. To mitigate calibration distortion, our method undistorts this plane by straightening saccade trajectories using image warping. We show that this approach improves over the common six-point calibration and is promising for reducing distortion. As such, it provides a non-intrusive solution to alleviating accuracy decrease of eye tracker during long-term use.\",\n",
       " 'There are numerous possibilities and motivations for an adaptive BCI, which may not be easy to clarify and organize for a newcomer to the field. To our knowledge, there has not been any work done in classifying the literature on adaptive BCI in a comprehensive and structured way. We propose a conceptual framework, a taxonomy of adaptive BCI methods which encompasses most important approaches to fit them in such a way that a reader can clearly visualize which elements are being adapted and for what reason. In the interest of having a clear review of existing adaptive BCIs, this framework considers adaptation approaches for both the user and the machine, i.e., using instructional design observations as well as the usual machine learning techniques. This framework not only provides a coherent review of such extensive literature but also enables the reader to perceive gaps and flaws in the current BCI systems, which would hopefully bring novel solutions for an overall improvement.',\n",
       " 'In this paper we measure the step-wise latency in the pipeline of three kinds of interactive mobile video applications that are rapidly gaining popularity, namely Remote Graphics Rendering (RGR) of which we focus on mobile cloud gaming, Mobile Augmented Reality (MAR), and Mobile Virtual Reality (MVR). The applications differ from each other by the way in which the user interacts with the application, i.e., video I/O and user controls, but they all share in common the fact that their user experience is highly sensitive to end-to-end latency. Long latency between a user control event and display update renders the application unusable. Hence, understanding the nature and origins of latency of these applications is of paramount importance. We show through extensive measurements that control input and display buffering have a substantial effect on the overall delay. Our results shed light on the latency bottlenecks and the maturity of technology for seamless user experience with these applications.',\n",
       " \"In this paper, we report a method of intuitively transmitting symbolic information to untrained users via only their hands without using any visual or auditory cues. Our simple concept is presenting three-dimensional letter trajectories to the user's hand via a stylus which is mechanically manipulated. By this simple method, in our experiments, participants were able to read 14 mm-high lower-case letters displayed at a rate of one letter per second with an accuracy rate of 71.9% in their first trials, which was improved to 91.3% after a five-minute training period. These results showed small individual differences among participants (standard deviation of 12.7% in the first trials and 6.7% after training). We also found that this accuracy was still retained to a high level (85.1% with SD of 8.2%) even when the letters were reduced to a height of 7 mm. Thus, we revealed that sighted adults potentially possess the ability to read small letters accurately at normal writing speed using their hands.\",\n",
       " 'Haptic interfaces have untapped the sense of touch to assist multimodal music learning. We have recently seen various improvements of interface design on tactile feedback and force guidance aiming to make instrument learning more effective. However, most interfaces are still quite static; they cannot yet sense the learning progress and adjust the tutoring strategy accordingly. To solve this problem, we contribute an adaptive haptic interface based on the latest design of haptic flute. We first adopted a clutch mechanism to enable the interface to turn on and off the haptic control flexibly in real time. The interactive tutor is then able to follow human performances and apply the \"teacher force\" only when the software instructs so. Finally, we incorporated the adaptive interface with a step-by-step dynamic learning strategy. Experimental results showed that dynamic learning dramatically outperforms static learning, which boosts the learning rate by 45.3% and shrinks the forgetting chance by 86%.',\n",
       " 'Signatures have been traditionally acquired in pen-based office-like scenarios using devices specifically designed for signing. However, the high deployment of devices such as smartphones and tablets has given rise to new and thriving scenarios where signatures can be performed using not only a pen stylus but also the finger. Some preliminary studies have highlighted the challenge of this new scenario and the necessity of further research on the topic. The main contribution of this work is to propose a new signature verification architecture adapted to the signature complexity in order to tackle this new and challenging scenario. Additionally, an exhaustive comparative analysis of both pen- and touch-based scenarios using our proposed methodology is carried out along with a review of the most relevant and recent studies in the field. Significant improvements of biometric verification performance and practical insights are extracted for the application of signature verification in real scenarios.',\n",
       " 'We consider the problem of designing an artificial agent capable of interacting with humans in collaborative dialogue to produce creative, engaging narratives. In this task, the goal is to establish universe details, and to collaborate on an interesting story in that universe, through a series of natural dialogue exchanges. Our model can augment any probabilistic conversational agent by allowing it to reason about universe information established and what potential next utterances might reveal. Ideally, with each utterance, agents would reveal just enough information to add specificity and reduce ambiguity without limiting the conversation. We empirically show that our model allows control over the rate at which the agent reveals information and that doing so significantly improves accuracy in predicting the next line of dialogues from movies. We close with a case-study with four professional theatre performers, who preferred interactions with our model-augmented agent over an unaugmented agent.',\n",
       " \"In this study, a novel control algorithm for a P-300 based brain-computer interface is fully developed to control a 2-DoF robotic arm. Eight subjects including 5 men and 3 women, perform a 2-dimensional target tracking task in a simulated environment. Their EEG signals from visual cortex are recorded and P-300 components are extracted and evaluated to perform a real-time BCI based controller. The volunteer's intention is recognized and will be decoded as an appropriate command to control the cursor. The final goal of the system is to control a simulated robotic arm in a 2-dimensional space for writing some English letters. The results show that the system allows the robot end-effector to move between arbitrary positions in a point-to-point session with the desired accuracy. This model is tested on and compared with Dataset II of the BCI Competition. The best result is obtained with a multi-class SVM solution as the classifier, with a recognition rate of 97 percent, without pre-channel selection.\",\n",
       " \"The vast majority of recommender systems model preferences as static or slowly changing due to observable user experience. However, spontaneous changes in user preferences are ubiquitous in many domains like media consumption and key factors that drive changes in preferences are not directly observable. These latent sources of preference change pose new challenges. When systems do not track and adapt to users' tastes, users lose confidence and trust, increasing the risk of user churn. We meet these challenges by developing a model of novelty preferences that learns and tracks latent user tastes. We combine three innovations: a new measure of item similarity based on patterns of consumption co-occurrence; model for {\\\\em spontaneous} changes in preferences; and a learning agent that tracks each user's dynamic preferences and learns individualized policies for variety. The resulting framework adaptively provides users with novelty tailored to their preferences for change per se.\",\n",
       " 'Across- and within-recording variabilities in electroencephalographic (EEG) activity is a major limitation in EEG-based brain-computer interfaces (BCIs). Specifically, gradual changes in fatigue and vigilance levels during long EEG recording durations and BCI system usage bring along significant fluctuations in BCI performances even when these systems are calibrated daily. We address this in an experimental offline study from EEG-based BCI speller usage data acquired for one hour duration. As the main part of our methodological approach, we propose the concept of adversarial invariant feature learning for BCIs as a regularization approach on recently expanding EEG deep learning architectures, to learn nuisance-invariant discriminative features. We empirically demonstrate the feasibility of adversarial feature learning on eliminating drowsiness effects from event related EEG activity features, by using temporal recording block ordering as the source of drowsiness variability.',\n",
       " 'In this paper, we propose a novel continuous authentication system for smartphone users. The proposed system entirely relies on unlabeled phone movement patterns collected through smartphone accelerometer. The data was collected in a completely unconstrained environment over five to twelve days. The contexts of phone usage were identified using k-means clustering. Multiple profiles, one for each context, were created for every user. Five machine learning algorithms were employed for classification of genuine and impostors. The performance of the system was evaluated over a diverse population of 57 users. The mean equal error rates achieved by Logistic Regression, Neural Network, kNN, SVM, and Random Forest were 13.7%, 13.5%, 12.1%, 10.7%, and 5.6% respectively. A series of statistical tests were conducted to compare the performance of the classifiers. The suitability of the proposed system for different types of users was also investigated using the failure to enroll policy.',\n",
       " \"Online labor platforms, such as the Amazon Mechanical Turk, provide an effective framework for eliciting responses to judgment tasks. Previous work has shown that workers respond best to financial incentives, especially to extra bonuses. However, most of the tested incentives involve describing the bonus conditions in formulas instead of plain English. We believe that different incentives given in English (or in qualitative framing) will result in differences in workers' performance, especially when task difficulties vary. In this paper, we report the preliminary results of a crowdsourcing experiment comparing workers' performance using only qualitative framings of financial incentives. Our results demonstrate a significant increase in workers' performance using a specific well-formulated qualitative framing inspired by the Peer Truth Serum. This positive effect is observed only when the difficulty of the task is high, while when the task is easy there is no difference of which incentives to use.\",\n",
       " 'Situationally-induced impairments and disabilities (SIIDs) make it difficult for users of interactive computing systems to perform tasks due to context (e.g., listening to a phone call when in a noisy crowd) rather than a result of a congenital or acquired impairment (e.g., hearing damage). SIIDs are a great concern when considering the ubiquitousness of technology in a wide range of contexts. Considering our daily reliance on technology, and mobile technology in particular, it is increasingly important that we fully understand and model how SIIDs occur. Similarly, we must identify appropriate methods for sensing and adapting technology to reduce the effects of SIIDs. In this workshop, we will bring together researchers working on understanding, sensing, modelling, and adapting technologies to ameliorate the effects of SIIDs. This workshop will provide a venue to identify existing research gaps, new directions for future research, and opportunities for future collaboration.',\n",
       " \"Scatter plots carry an implicit if subtle message about causality. Whether we look at functions of one variable in pure mathematics, plots of experimental measurements as a function of the experimental conditions, or scatter plots of predictor and response variables, the value plotted on the vertical axis is by convention assumed to be determined or influenced by the value on the horizontal axis. This is a problem for the public understanding of scientific results and perhaps also for professional scientists' interpretations of scatter plots. To avoid suggesting a causal relationship between the x and y values in a scatter plot, we propose a new type of data visualization, the diamond plot. Diamond plots are essentially 45 degree rotations of ordinary scatter plots; by visually jarring the viewer they clearly indicate that she should not draw the usual distinction between independent/predictor variable and dependent/response variable. Instead, she should see the relationship as purely correlative.\",\n",
       " \"The emergence of smartwatches poses new challenges to information security. Although there are mature touch-based authentication methods for smartphones, the effectiveness of using these methods on smartwatches is still unclear. We conducted a user study (n=16) to evaluate how authentication methods (PIN and Pattern), UIs (Square and Circular), and display sizes (38mm and 42mm) affect authentication accuracy, speed, and security. Circular UIs are tailored to smartwatches with fewer UI elements. Results show that 1) PIN is more accurate and secure than Pattern; 2) Pattern is much faster than PIN; 3) Square UIs are more secure but less accurate than Circular UIs; 4) display size does not affect accuracy or speed, but security; 5) Square PIN is the most secure method of all. The study also reveals a security concern that participants' favorite method is not the best in any of the measures. We finally discuss implications for future touch-based smartwatch authentication design.\",\n",
       " \"Collective urban mobility embodies the residents' local insights on the city. Mobility practices of the residents are produced from their spatial choices, which involve various considerations such as the atmosphere of destinations, distance, past experiences, and preferences. The advances in mobile computing and the rise of geo-social platforms have provided the means for capturing the mobility practices; however, interpreting the residents' insights is challenging due to the scale and complexity of an urban environment, and its unique context. In this paper, we present MobInsight, a framework for making localized interpretations of urban mobility that reflect various aspects of the urbanism. MobInsight extracts a rich set of neighborhood features through holistic semantic aggregation, and models the mobility between all-pairs of neighborhoods. We evaluate MobInsight with the mobility data of Barcelona and demonstrate diverse localized and semantically-rich interpretations.\",\n",
       " 'Sensory evaluation is used to assess the consumer acceptance of foods or other consumer products, so as to improve industrial processes and marketing strategies. The procedures currently involved are time-consuming because they require a statistical approach from measurements and feedback reports from a wide set of evaluators under a well-established measurement setup. In this paper, we propose to collect directly the signal of the perceived quality of the food from Event-related potentials (ERPs) that are the outcome of the processing of visual stimuli. This permits to narrow the number of evaluators since errors related to psychological factors are by-passed. We present the design of a wearable system for ERP measurement and we present preliminary results on the use of ERP to give a quantitative measure to the appearance of a food product. The system is developed to be wearable and our experiments demonstrate that is possible to use it to identify and classify the grade of acceptance of the food.',\n",
       " 'Drones are being used in many industries for a variety of applications, including inspecting bridges, surveying farm land, and delivering cargo. Automating these kinds of scenarios requires more than following a sequence of GPS waypoints; they require integrating on-device hardware with real-time analysis to provide feedback and control to the drone. Currently, implementing these kinds of advanced scenarios is a complex task, requiring skilled software engineers programming with drone APIs. We envision an alternate model to enable drone operators to orchestrate advanced behaviors using a card-based approach. We describe the design of our card-based programming model, position it relative to other visual programming metaphors, share results from our paper prototype user study, and discuss our learnings from its implementation. Results suggest that a wide range of scenarios can be implemented with moderate mental effort and learning, balanced by intuitiveness and engagement.',\n",
       " 'In online communities, recent studies have strongly improved our knowledge about the different types or profiles of contributors, from casual to very involved ones, through focused people. However they do so by using very complex methodologies (qualitative-quantitative mix, with a high workload to manually codify/characterize the edits), making their replication for the practitioners limited. These studies are on the English Wikipedia only. The objective of this paper is to highlight different profiles of contributors with clustering techniques. The originality is to show how using only the edits, and their distribution over time, allows to build these contributors profiles with a good accuracy and stability amongst languages. The methodology is validated with both Romanian and Danish wikis. The highlighted profiles are identifiable early in the history of involvement, suggesting that light monitoring of newcomers may be sufficient to adapt the interaction with them and increase the retention rate.',\n",
       " 'Despite gaining traction in North America, live streaming has not reached the popularity it has in China, where livestreaming has a tremendous impact on the social behaviors of users. To better understand this socio-technological phenomenon, we conducted a mixed methods study of live streaming practices in China. We present the results of an online survey of 527 live streaming users, focusing on their broadcasting or viewing practices and the experiences they find most engaging. We also interviewed 14 active users to explore their motivations and experiences. Our data revealed the different categories of content that was broadcasted and how varying aspects of this content engaged viewers. We also gained insight into the role reward systems and fan group-chat play in engaging users, while also finding evidence that both viewers and streamers desire deeper channels and mechanisms for interaction in addition to the commenting, gifting, and fan groups that are available today.',\n",
       " \"We want to understand the human capabilities to perceive amplitude similarities between a haptic and an audio signal. So, four psychophysical experiments were performed. Three of them measured the asynchrony JND (Just Noticeable Difference) at the signals' attack, release and decay, while the forth experiment measured the amplitude decrease on the middle of the signal. All the experiments used a combination of the constant stimulus and staircase methods to present two stimuli, while the participants' (N=12) task was to identify which of the two stimuli was synchronized. The audiotactile stimulus was defined using an stereo audio signal with an ADSR (Attack Decay Sustain Release) envelope. The partial results reveal JNDs for temporal asynchrony of: 54ms for attack, 265ms for decay and 57ms for release. Also the results reveal an amplitude decrease JND of 25\\\\%. Although for decay the results were to disperse, therefore we suspect that the participants were not able to the changes on the haptic signal.\",\n",
       " 'The emergence of social virtual reality (VR) experiences, such as Facebook Spaces, Oculus Rooms, and Oculus Venues, will generate increased interest from users who want to share real places (both personal and public) with their fellow users in VR. At the same time, advances in scanning and reconstruction technology are making the realistic capture of real places more and more feasible. These complementary pressures mean that the representation of real places in virtual reality will be an increasingly common use case for VR. Despite this, there has been very little research into how users perceive such replicated spaces. This paper reports the results from a series of three user studies investigating this topic. Taken together, these results show that getting the scale of the space correct is the most important factor for generating a \"feeling of reality\", that it is important to avoid incoherent behaviors (such as floating objects), and that lighting makes little difference to perceptual similarity.',\n",
       " 'The landscape of analytics is changing rapidly. Much of online user analytics, however, is based on collection of various user analytics numbers. Understanding these numbers, and then relating them to higher numerical analysis for the evaluation of key performance indicators (KPIs) can be quite challenging, especially with large volumes of data. There is a plethora of tools and software packages that one can employ. However, these tools and packages require a quantitative competence and analytical sophistication that average end users often do not possess. Additionally, they often do little to reduce the complexity of numerical data in a manner that allows ease of use in decision making and communication. Dealing with numbers poses cognitive challenges for individuals who often do cannot recall many numbers at a time. Here, we explore the concept of automatic analytics by demonstrating use case examples and discussion on the current state and future of automated insights.',\n",
       " 'Multimodal features play a key role in wearable sensor-based human activity recognition (HAR). Selecting the most salient features adaptively is a promising way to maximize the effectiveness of multimodal sensor data. In this regard, we propose a \"collect fully and select wisely\" principle as well as an interpretable parallel recurrent model with convolutional attentions to improve the recognition performance. We first collect modality features and the relations between each pair of features to generate activity frames, and then introduce an attention mechanism to select the most prominent regions from activity frames precisely. The selected frames not only maximize the utilization of valid features but also reduce the number of features to be computed effectively. We further analyze the accuracy and interpretability of the proposed model based on extensive experiments. The results show that our model achieves competitive performance on two benchmarked datasets and works well in real life scenarios.',\n",
       " 'A well-designed control-to-display (CD) gain function can improve pointing performance with an indirect pointing device such as a trackpad. However, the design of gain functions has been challenging and mostly based on trial and error. AutoGain is an unobtrusive method to obtain a gain function for an indirect pointing device in contexts where cursor trajectories can be tracked. It gradually improves pointing efficiency by using a novel submovement-level tracking+optimization technique. In a study, we show that AutoGain can produce gain functions with performance comparable to commercial designs in less than a half hour of active use. This is attributable to reductions in aiming error (undershooting/overshooting) for each submovement. Our second study shows that AutoGain can be used to obtain gain functions for emerging input devices (here, a Leap Motion controller) for which no good gain function may exist yet. Finally, we discuss deployment in a real interactive system.',\n",
       " 'Crowdsourcing systems accomplish large tasks with scale and speed by breaking work down into independent parts. However, many types of complex creative work, such as fiction writing, have remained out of reach for crowds because work is tightly interdependent: changing one part of a story may trigger changes to the overall plot and vice versa. Taking inspiration from how expert authors write, we propose a technique for achieving interdependent complex goals with crowds. With this technique, the crowd loops between reflection, to select a high-level goal, and revision, to decompose that goal into low-level, actionable tasks. We embody this approach in Mechanical Novel, a system that crowdsources short fiction stories on Amazon Mechanical Turk. In a field experiment, Mechanical Novel resulted in higher-quality stories than an iterative crowdsourcing workflow. Our findings suggest that orienting crowd work around high-level goals may enable workers to coordinate their effort to accomplish complex work.',\n",
       " \"Crowd workers are distributed and decentralized. While decentralization is designed to utilize independent judgment to promote high-quality results, it paradoxically undercuts behaviors and institutions that are critical to high-quality work. Reputation is one central example: crowdsourcing systems depend on reputation scores from decentralized workers and requesters, but these scores are notoriously inflated and uninformative. In this paper, we draw inspiration from historical worker guilds (e.g., in the silk trade) to design and implement crowd guilds: centralized groups of crowd workers who collectively certify each other's quality through double-blind peer assessment. A two-week field experiment compared crowd guilds to a traditional decentralized crowd work model. Crowd guilds produced reputation signals more strongly correlated with ground-truth worker quality than signals available on current crowd working platforms, and more accurate than in the traditional model.\",\n",
       " 'Leveraging human grasping skills to teach a robot to perform a manipulation task is appealing, but there are several limitations to this approach: time-inefficient data capture procedures, limited generalization of the data to other grasps and objects, and inability to use that data to learn more about how humans perform and evaluate grasps. This paper presents a data capture protocol that partially addresses these deficiencies by asking participants to specify ranges over which a grasp is valid. The protocol is verified both qualitatively through online survey questions (where 95.38% of within-range grasps are identified correctly with the nearest extreme grasp) and quantitatively by showing that there is small variation in grasps ranges from different participants as measured by joint angles, contact points, and position. We demonstrate that these grasp ranges are valid through testing on a physical robot (93.75% of grasps interpolated from grasp ranges are successful).',\n",
       " \"The Experience Sampling Method (ESM) introduces in-situ sampling of human behaviour, and provides researchers and behavioural therapists with ecologically valid and timely assessments of a person's psychological state. This, in turn, opens up new opportunities for understanding behaviour at a scale and granularity that was not possible just a few years ago. The practical applications are many, such as the delivery of personalised and agile behaviour interventions. Mobile computing devices represent a revolutionary platform for improving ESM. They are an inseparable part of our daily lives, context-aware, and can interact with people at suitable moments. Furthermore, these devices are equipped with sensors, and can thus take part of the reporting burden off the participant, and collect data automatically. The goal of this survey is to discuss recent advancements in using mobile technologies for ESM (mESM), and present our vision of the future of mobile experience sampling.\",\n",
       " 'In this article, we study the Cyber-Human Interaction (CHI) based approach that the \"Human\" part sets a list of location-based objectives and makes the pathway decision whereas the \"Cyber\" part provides the pathway suggestion, infer heuristics from the environment along the pathway and incrementally resolve the location-based objectives with new heuristics for indoor localization. For this study, we implement a CHI-based system on mobile phone. The CHI-based system offers the pathway suggestion and the solution of the location-based objectives based on its trajectory management. Without any priori knowledge on the area of interest and any aid from other equipments, a laborer can achieve his location-based objectives by walking through the area of interest and simultaneously online interacting with the CHI-based system installed in his phone. In evaluation, we conduct the experiments and show the advantage CHI in reducing the time cost and the expense cost for the laborer.',\n",
       " 'In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory. These techniques often require researchers to engineer abstract \"features\" that encode chemical concepts into a mathematical form compatible with the input to machine-learning models. However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features. We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem. The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots. The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection. It is built to be scalable, and integrate with existing infrastructure.',\n",
       " \"Process Mining is a famous technique which is frequently applied to Software Development Processes, while being neglected in Human-Computer Interaction (HCI) recommendation applications. Organizations usually train employees to interact with required IT systems. Often, employees, or users in general, develop their own strategies for solving repetitive tasks and processes. However, organizations find it hard to detect whether employees interact efficiently with IT systems or not. Hence, we have developed a method which detects inefficient behavior assuming that at least one optimal HCI strategy is known. This method provides recommendations to gradually adapt users' behavior towards the optimal way of interaction considering satisfaction of users. Based on users' behavior logs tracked by a Java application suitable for multi-application and multi-instance environments, we demonstrate the applicability for a specific task in a common Windows environment utilizing realistic simulated behaviors of users.\",\n",
       " 'With the popularization of cell phones, laptops, and tablets, Liquid Crystal Displays (LCDs) have become one of the main types of User Interface (UI) in the modern world. While LCDs are widely used for retrieving text information, the impact of text formatting on the legibility is often overlooked. With the goal of improving recognition efficiency (RE) on LCDs, this paper studies the impact of font-background colors on RE of texts being presented on LCD. For this purpose, difference between font/background color combinations, Primary Color Difference (PCD), is introduced that brings efficient RE assessment under wider spectrum. Accordingly, a testing platform is designed in C#. NET that captures participants response time to different font-background color combination stimuli. Based on the results, black background and green font color outperform other tested colors especially when the PCD is maximized. In correspond to results, Implications for using research outcome in prototype LCDs are suggested.',\n",
       " \"Scientists increasingly rely on simulation runs of complex models in lieu of cost-prohibitive or infeasible experimentation. The data output of many controlled simulation runs, the ensemble, is used to verify correctness and quantify uncertainty. However, due to their size and complexity, ensembles are difficult to visually analyze because the working set often exceeds strict memory limitations. We present a navigable ensemble analysis tool, NEA, for interactive exploration of ensembles. NEA's pre-processing component takes advantage of the data similarity characteristics of ensembles to represent the data in a new, spatially-efficient data structure which does not require fully reconstructing the original data at visualization time. This data structure allows a fine degree of control in working set management, which enables interactive ensemble exploration while fitting within memory limitations. Scientists can also gain new insights from the data-similarity analysis in the pre-processing component.\",\n",
       " 'Camera arrays (CamArrays) are widely used in commercial filming projects for achieving special visual effects such as bullet time effect, but are very expensive to set up. We propose CamSwarm, a low-cost and lightweight alternative to professional CamArrays for consumer applications. It allows the construction of a collaborative photography platform from multiple mobile devices anywhere and anytime, enabling new capturing and editing experiences that a single camera cannot provide. Our system allows easy team formation; uses real-time visualization and feedback to guide camera positioning; provides a mechanism for synchronized capturing; and finally allows the user to efficiently browse and edit the captured imagery. Our user study suggests that CamSwarm is easy to use; the provided real-time guidance is helpful; and the full system achieves high quality results promising for non-professional use.\\n  A demo video is provided at https://www.youtube.com/watch?v=LgkHcvcyTTM.',\n",
       " 'One of the fundamental aspects of ubiquitous computing is the instrumentation of the real world by smart devices. This instrumentation constitutes an opportunity to rethink the interactions between human beings and their environment on the one hand, and between the components of this environment on the other. In this paper we discuss what this understanding of ubiquitous computing can bring to geographic science and particularly to GIS technology. Our main idea is the instrumentation of the geographic environment through the instrumentation of geographic objects composing it. And then investigate how this instrumentation can meet the current limitations of GIS technology, and offers a new stage of rapprochement between the earth and its abstraction. As result, the current research work proposes a new concept we named Smart Geographic Object SGO. The latter is a convergence point between the smart objects and geographic objects, two concepts appertaining respectively to t',\n",
       " 'Research on science fiction (sci-fi) in scientific publications has indicated the usage of sci-fi stories, movies or shows to inspire novel Human-Computer Interaction (HCI) research. Yet no studies have analysed sci-fi in a top-ranked computer science conference at present. For that reason, we examine the CHI main track for the presence and nature of sci-fi referrals in relationship to HCI research. We search for six sci-fi terms in a dataset of 5812 CHI main proceedings and code the context of 175 sci-fi referrals in 83 papers indexed in the CHI main track. In our results, we categorize these papers into five contemporary HCI research themes wherein sci-fi and HCI interconnect: 1) Theoretical Design Research; 2) New Interactions; 3) Human-Body Modification or Extension; 4) Human-Robot Interaction and Artificial Intelligence; and 5) Visions of Computing and HCI. In conclusion, we discuss results and implications located in the promising arena of sci-fi and HCI research.',\n",
       " 'It is often desirable to analyse trajectory data in local coordinates relative to a reference location. Similarly, temporal data also needs to be transformed to be relative to an event. Together, temporal and spatial contextualisation permits comparative analysis of similar trajectories taken across multiple reference locations. To the GIS professional, the procedures to establish a reference frame at a location and reproject the data into local coordinates are well known, albeit tedious. However, GIS tools are now often used by subject matter experts who may not have the deep knowledge of coordinate frames and projections required to use these techniques effectively.\\n  We introduce a novel method for representing spatio-temporal reference frames using ordinary geographic objects available in GIS tools. We argue that our method both reduces the number of manual steps required to reproject data to a local reference frame, in addition to reducing the number of concepts a novice user would need to learn.',\n",
       " 'Parallel coordinates plotting is one of the most popular methods for multivariate visualization. However, when applied to larger data sets, there tends to be a \"black screen problem,\" with the screen becoming so cluttered and full that patterns are difficult or impossible to discern. Xie and Matloff (2014) proposed remedying this problem by plotting only the most frequently-appearing patterns, with frequency defined in terms of nonparametrically estimated multivariate density. This approach displays \"typical\" patterns, which may reveal important insights for the data. However, this remedy does not cover variables that are discrete or categorical. An alternate method, still frequency-based, is presented here for such cases. We discretize all continuous variables, retaining the discrete/categorical ones, and plot the patterns having the highest counts in the dataset. In addition, we propose some novel approaches to handling missing values in parallel coordinates settings.',\n",
       " 'We introduce Code Park, a novel tool for visualizing codebases in a 3D game-like environment. Code Park aims to improve a programmer\\'s understanding of an existing codebase in a manner that is both engaging and intuitive, appealing to novice users such as students. It achieves these goals by laying out the codebase in a 3D park-like environment. Each class in the codebase is represented as a 3D room-like structure. Constituent parts of the class (variable, member functions, etc.) are laid out on the walls, resembling a syntax-aware \"wallpaper\". The users can interact with the codebase using an overview, and a first-person viewer mode. We conducted two user studies to evaluate Code Park\\'s usability and suitability for organizing an existing project. Our results indicate that Code Park is easy to get familiar with and significantly helps in code understanding compared to a traditional IDE. Further, the users unanimously believed that Code Park was a fun tool to work with.',\n",
       " \"Both dialogue systems and chatbots aim at putting into action communication between humans and computers. However, instead of focusing on sophisticated techniques to perform natural language understanding, as the former usually do, chatbots seek to mimic conversation. Since Eliza, the first chatbot ever, developed in 1966, there were many interesting ideas explored by the chatbots' community. Actually, more than just ideas, some chatbots' developers also provide free resources, including tools and large-scale corpora. It is our opinion that this know-how and materials should not be neglected, as they might be put to use in the human-computer communication field (and some authors already do it). Thus, in this paper we present a historical overview of the chatbots' developments, we review what we consider to be the main contributions of this community, and we point to some possible ways of coupling these with current work in the human-computer communication research line.\",\n",
       " 'Recent years have seen an explosion in the availability of Voice User Interfaces. However, user surveys suggest that there are issues with respect to usability, and it has been hypothesised that contemporary voice-enabled systems are missing crucial behaviours relating to user engagement and vocal interactivity. However, it is well established that such ostensive behaviours are ubiquitous in the animal kingdom, and that vocalisation provides a means through which interaction may be coordinated and managed between individuals and within groups. Hence, this paper reports results from a study aimed at identifying generic mechanisms that might underpin coordinated collective vocal behaviour with a particular focus on closed-loop negative-feedback control as a powerful regulatory process. A computer-based real-time simulation of vocal interactivity is described which has provided a number of insights, including the enumeration of a number of key control variables that may be worthy of further investigation.',\n",
       " \"The delivery of mental health interventions via ubiquitous devices has shown much promise. A conversational chatbot is a promising oracle for delivering appropriate just-in-time interventions. However, designing emotionally-aware agents, specially in this context, is under-explored. Furthermore, the feasibility of automating the delivery of just-in-time mHealth interventions via such an agent has not been fully studied. In this paper, we present the design and evaluation of EMMA (EMotion-Aware mHealth Agent) through a two-week long human-subject experiment with N=39 participants. EMMA provides emotionally appropriate micro-activities in an empathetic manner. We show that the system can be extended to detect a user's mood purely from smartphone sensor data. Our results show that our personalized machine learning model was perceived as likable via self-reports of emotion from users. Finally, we provide a set of guidelines for the design of emotion-aware bots for mHealth.\",\n",
       " 'The multimodal web elements such as text and images are associated with inherent memory costs to store and transfer over the Internet. With the limited network connectivity in developing countries, webpage rendering gets delayed in the presence of high-memory demanding elements such as images (relative to text). To overcome this limitation, we propose a Canonical Correlation Analysis (CCA) based computational approach to replace high-cost modality with an equivalent low-cost modality. Our model learns a common subspace for low-cost and high-cost modalities that maximizes the correlation between their visual features. The obtained common subspace is used for determining the low-cost (text) element of a given high-cost (image) element for the replacement. We analyze the cost-saving performance of the proposed approach through an eye-tracking experiment conducted on real-world webpages. Our approach reduces the memory-cost by at least 83.35% by replacing images with text.',\n",
       " \"This paper presents a novel approach to simulate human wayfinding behaviour incorporating visual cognition into a software agent for a computer aided evaluation of wayfinding systems in large infrastructures. The proposed approach follows the Sense-Plan-Act paradigm comprised of a model for visual attention, navigation behaviour and pedestrian movement. Stochastic features of perception are incorporated to enhance generality and diversity of the developed wayfinding simulation to reflect a variety of behaviours. The validity of the proposed approach was evaluated based on empirical data collected through wayfinding experiments with 20 participants in an immersive virtual reality environment using a life-sized 3D replica of Vienna's new central railway station. The results show that the developed cognitive agent-based simulation provides a further contribution to the simulation of human wayfinding and subsequently a further step to an effective evaluation tool for the planning of wayfinding and signage.\",\n",
       " 'Given the choice, users produce passwords reflecting common strategies and patterns that ease recall but offer uncertain and often weak security. System-assigned passwords provide measurable security but suffer from poor memorability. To address this usability-security tension, we argue that systems should assign random passwords but also help with memorization and recall. We investigate the feasibility of this approach with CuedR, a novel cued-recognition authentication scheme that provides users with multiple cues (visual, verbal, and spatial) and lets them choose the cues that best fit their learning process for later recognition of system-assigned keywords. In our lab study, all 37 of our participants could log in within three attempts one week after registration (mean login time: 38.0 seconds). A pilot study on using multiple CuedR passwords also showed 100% recall within three attempts. Based on our results, we suggest appropriate applications for CuedR, such as financial and e-commerce accounts.',\n",
       " 'Promoting exercise and promoting fun in sport and activity is a common goal of schools. However, children and adolescents do not exercise enough, which can favor a number of chronic illnesses. Exercise and sports often require coordination of visual perception and reaction, which is an additional barrier for visually impaired (blind and partially sighted) people. Due to their highly motivating appeal, games promoting physical activity (exertion games) have become increasingly popular. Although accessible exertion games have been developed, they do not consider the different abilities of players. Especially in team sports player roles that consider individual abilities can foster inclusion. To personalize roles and assign certain abilities to players, wearable technology can play an important role. In this position paper we present ideas how digital objects can be used to design exertion games for visually impaired students and we reflect how wearable technology can be used for personalized player roles.',\n",
       " \"Is it possible to predict the affect of a user just by observing her behavioral interaction through a video? How can we, for instance, predict a user's arousal in games by merely looking at the screen during play? In this paper we address these questions by employing three dissimilar deep convolutional neural network architectures in our attempt to learn the underlying mapping between video streams of gameplay and the player's arousal. We test the algorithms in an annotated dataset of 50 gameplay videos of a survival shooter game and evaluate the deep learned models' capacity to classify high vs low arousal levels. Our key findings with the demanding leave-one-video-out validation method reveal accuracies of over 78% on average and 98% at best. While this study focuses on games and player experience as a test domain, the findings and methodology are directly relevant to any affective computing area, introducing a general and user-agnostic approach for modeling affect.\",\n",
       " 'In human computer interaction (HCI), it is common to evaluate the value of HCI designs, techniques, devices, and systems in terms of their benefit to users. It is less common to discuss the benefit of HCI to computers. Every HCI task allows a computer to receive some data from the user. In many situations, the data received by the computer embodies human knowledge and intelligence in handling complex problems, and/or some critical information without which the computer cannot proceed. In this paper, we present an information-theoretic framework for quantifying the knowledge received by the computer from its users via HCI. We apply information-theoretic measures to some common HCI tasks as well as HCI tasks in complex data intelligence processes. We formalize the methods for estimating such quantities analytically and measuring them empirically. Using theoretical reasoning, we can confirm the significant but often undervalued role of HCI in data intelligence workflows.',\n",
       " \"Tools for quadrotor trajectory design have enabled single videographers to create complex aerial video shots that previously required dedicated hardware and several operators. We build on this prior work by studying film-maker's working practices which informed a system design that brings expert workflows closer to end-users. For this purpose, we propose WYFIWYG, a new quadrotor camera tool which (i) allows to design a video solely via specifying its frames, (ii) encourages the exploration of the scene prior to filming and (iii) allows to continuously frame a camera target according to compositional intentions. Furthermore, we propose extensions to an existing algorithm, generating more intuitive angular camera motions and producing spatially and temporally smooth trajectories. Finally, we conduct a user study where we evaluate how end-users work with current videography tools. We conclude by summarizing the findings of work as implications for the design of UIs and algorithms of quadrotor camera tools.\",\n",
       " 'Exploring data requires a fast feedback loop from the analyst to the system, with a latency below about 10 seconds because of human cognitive limitations. When data becomes large or analysis becomes complex, sequential computations can no longer be completed in a few seconds and data exploration is severely hampered. This article describes a novel computation paradigm called Progressive Computation for Data Analysis or more concisely Progressive Analytics, that brings at the programming language level a low-latency guarantee by performing computations in a progressive fashion. Moving this progressive computation at the language level relieves the programmer of exploratory data analysis systems from implementing the whole analytics pipeline in a progressive way from scratch, streamlining the implementation of scalable exploratory data analysis systems. This article describes the new paradigm through a prototype implementation called ProgressiVis, and explains the requirements it implies through examples.',\n",
       " 'In the complex manufacturing sector a considerable amount of resources are focused on developing new skills and training workers. In that context, increasing the effectiveness of those processes and reducing the investment required is an outstanding issue. In this paper we present an experiment that shows how modern Human Computer Interaction (HCI) metaphors such as collaborative mixed-reality can be used to transmit procedural knowledge and could eventually replace other forms of face-to-face training. We implement a real-time Immersive Augmented Reality (IAR) setup with see-through cameras that allows for collaborative interactions that can simulate conventional forms of training. The obtained results indicate that people who took the IAR training achieved the same performance than people in the conventional face-to-face training condition. These results, their implications for future training and the use of HCI paradigms in this context are discussed in this paper.',\n",
       " \"Thispaper presents the design and realization of a context-aware wireless health monitoring system for recording the heartbeat (HR) and respiration (RR) rate based on an indirect measurement approach. The system consists of a contact-less medical sensor as well as a communication infrastructure for handling the transmission and reception of the measured results. The contact-less sensor includes a highly sensitive tri-axial accelerometer, an accurate temperature and air pressure sensor that enable one to inspect patients' health condition by continuously monitoring of two critical signs related to the cardiorespiratory system. The developed system can also be utilized in performing a number of long-term inspection on the heart and lungs while measuring the HR and RR values in addition to calculating the HR and RR ratio, which is denoted by HRR. The obtained results show the potential of the developed system for versatile monitoring applications applied to telemedicine\",\n",
       " \"The head-up display (HUD) is an emerging device which can project information on a transparent screen. The HUD has been used in airplanes and vehicles, and it is usually placed in front of the operator's view. In the case of the vehicle, the driver can see not only various information on the HUD but also the backgrounds (driving environment) through the HUD. However, the projected information on the HUD may interfere with the colors in the background because the HUD is transparent. For example, a red message on the HUD will be less noticeable when there is an overlap between it and the red brake light from the front vehicle. As the first step to solve this issue, how to evaluate the mutual interference between the information on the HUD and backgrounds is important. Therefore, this paper proposes a method to evaluate the mutual interference based on saliency. It can be evaluated by comparing the HUD part cut from a saliency map of a measured image with the HUD image.\",\n",
       " 'A lack of awareness surrounding secure online behaviour can lead to end-users, and their personal details becoming vulnerable to compromise. This paper describes an ongoing research project in the field of usable security, examining the relationship between end-user-security behaviour, and the use of affective feedback to educate end-users. Part of the aforementioned research project considers the link between categorical information users reveal about themselves online, and the information users believe, or report that they have revealed online. The experimental results confirm a disparity between information revealed, and what users think they have revealed, highlighting a deficit in security awareness. Results gained in relation to the affective feedback delivered are mixed, indicating limited short-term impact. Future work seeks to perform a long-term study, with the view that positive behavioural changes may be reflected in the results as end-users become more knowledgeable about security awareness.',\n",
       " \"Online communities have been able to develop large, open-source software (OSS) projects like Linux and Firefox throughout the successful collaborations carried out by their members over the Internet. However, online communities also involve creative arts domains such as animation, video games, and music. Despite their growing popularity, the factors that lead to successful collaborations in these communities are not entirely understood. In this paper, we present a study on creative collaboration in a music community where authors write songs together by 'overdubbing,' that is, by mixing a new track with an existing audio recording. We analyzed the relationship between song- and author-related measures and the likelihood of a song being overdubbed. We found that recent songs, as well as songs with many reactions, are more likely to be overdubbed; authors with a high status in the community and a recognizable identity write songs that the community tends to build upon.\",\n",
       " \"Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers' hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.\",\n",
       " 'Cognitive processes involved in both allocation of attention during decision making as well as surprise when making mistakes trigger release of the neurotransmitter norepinephrine, which has been shown to be correlated with an increase in pupil dilation, in turn reflecting raised levels of arousal. Extending earlier experiments based on the Attention Network Test (ANT), separating the neural components of alertness and spatial re-orientation from the attention involved in more demanding conflict resolution tasks, we demonstrate that these signatures of attention are so robust that they may be retrieved even when applying low cost eye tracking in an everyday mobile computing context. Furthermore we find that the reaction of surprise elicited when committing mistakes in a decision task, which in the neuroimaging EEG literature have been referred to as a negativity feedback error correction signal, may likewise be retrieved solely based on an increase in pupil dilation.',\n",
       " 'Recent years have seen significant market penetration for voice-based personal assistants such as Apple\\'s Siri. However, despite this success, user take-up is frustratingly low. This position paper argues that there is a habitability gap caused by the inevitable mismatch between the capabilities and expectations of human users and the features and benefits provided by contemporary technology. Suggestions are made as to how such problems might be mitigated, but a more worrisome question emerges: \"is spoken language all-or-nothing\"? The answer, based on contemporary views on the special nature of (spoken) language, is that there may indeed be a fundamental limit to the interaction that can take place between mismatched interlocutors (such as humans and machines). However, it is concluded that interactions between native and non-native speakers, or between adults and children, or even between humans and dogs, might provide critical inspiration for the design of future speech-based human-machine interaction.',\n",
       " 'Modern interactive visualizations are akin to distributed systems, where user interactions, background data processing, remote requests, and streaming data read and modify the interface at the same time. This concurrency is crucial to provide an interactive user experience---forbidding it can cripple responsiveness. However, it is notoriously challenging to program distributed systems, and concurrency can easily lead to ambiguous or confusing interface behaviors. In this paper, we present DIEL, a declarative programming model to help developers reason about and reconcile concurrency-related issues. Using DIEL, developers no longer need to procedurally describe how the interface should update based on different input events, but rather declaratively specify what the state of the interface should be as queries over event history. We show that resolving conflicts from concurrent processes in real-world interactive visualizations can be done in a few lines of DIEL code.',\n",
       " \"The growing popularity of chatbots has brought new needs for HCI since it has changed the patterns of human interactions with computers. The conversational aspect of the interaction increases the necessity for chatbots to present social behaviors that are habitual in human-human conversations. In this survey, we argue that chatbots should be enriched with social characteristics that are coherent with users' expectations, ultimately avoiding frustration and dissatisfaction. We bring together the literature on disembodied, text-based chatbots to derive a conceptual model of social characteristics for chatbots. We analyzed 58 papers from various domains to understand how social characteristics can benefit the interactions and identify the challenges and strategies to designing them. Additionally, we discussed how characteristics may influence one another. Our results provide relevant opportunities to both researchers and designers to advance human-chatbot interactions.\",\n",
       " \"Lifestyle and environment interacting with our biological machine are primarily responsible for shaping our health and wellbeing. Continuous, multi-modal, and quantitative approaches to understanding and controlling these factors will allow each person to better reach their desired quality of life. A navigational paradigm can help users towards a specified health goal by using constantly captured measurements to feed estimations of how a user's health is continuously changing in order to provide actionable guidance. As various actions are taken by the user, measurements of the resulting effects loop back into the estimation and the next step of guidance. This perpetual cycle of measuring, estimating, guiding, and acting articulates a Personal Health Navigation information and actuation framework. Personal Health Navigation focuses on fulfilling a user's health goals by ensuring minimal deviation from healthy states, rather than treating disease or symptoms after derailment from proper biological function.\",\n",
       " 'The P300 speller is a well known Brain-Computer Interface paradigm that has been used for over two decades. A new P300 speller paradigm (XP300) is proposed. It includes several characteristics: (i) the items are not intensified by using rows and columns, (ii) the order of the visual stimuli is pseudo-random, (iii) a visual feedback is added on each item to increase the stimulus meaning, which is the main novelty. XP300 has been tested on ten healthy subjects on copy spelling mode, with only eight sensors. It has been compared with the classical P300 paradigm (CP300). With five repetitions, the average recognition rate across subjects is 85.25% for XP300 and 77.25% for CP300. Single-trial detection is significantly higher with XP300 by comparing the AUC (Area Under Curve) of the ROC (Receiver Operating Characteristic) curve. The mean AUC is 0.86 for XP300, 0.80 for CP300. More importantly, XP300 has also been judged as more convenient and user-friendly than CP300, hence being able to allow longer sessions.',\n",
       " 'The quality of high-end videoconferencing systems has improved significantly over the last few years enabling a class of applications known as \"telepresence\" wherein the users engaged in a communication session experience a feeling of mutual presence in a shared virtual space. Telepresence systems have reached a maturity level that seriously challenges the old familiar truism that a face-to-face meeting is always better than a technology-mediated alternative. To explore the state of the art in telepresence technology and outline future opportunities, this paper proposes an optimality condition, expressed as a \"Turing Test,\" whereby the subjective experience of using a telepresence system is compared to the corresponding face-to-face situation. The requirements and challenges of designing a system passing such a Turing Test for telepresence are analyzed with respect to the limits of human perception, and the feasibility of achieving this goal with currently available or near future technology is discussed.',\n",
       " 'Joint narratives are often used in the context of reconciliation interventions for people in social conflict situations, which arise, for example, due to ethnic or religious differences. The interventions aim to encourage a change in attitudes of the participants towards each other. Typically, a human mediator is fundamental for achieving a successful intervention. In this work, we present an automated approach to support remote interactions between pairs of participants as they contribute to a shared story in their own language. A key component is an automated cognitive tutor that guides the participants through a controlled escalation/de-escalation process during the development of a joint narrative. We performed a controlled study comparing a trained human mediator to the automated mediator. The results demonstrate that an automated mediator, although simple at this stage, effectively supports interactions and helps to achieve positive outcomes comparable to those attained by the trained human mediator.',\n",
       " 'The need for interpretable and accountable intelligent system gets sensible as artificial intelligence plays more role in human life. Explainable artificial intelligence systems can be a solution by self-explaining the reasoning behind the decisions and predictions of the intelligent system. Researchers from different disciplines work together to define, design and evaluate interpretable intelligent systems for the user. Our work supports the different evaluation goals in interpretable machine learning research by a thorough review of evaluation methodologies used in machine-explanation research across the fields of human-computer interaction, visual analytics, and machine learning. We present a 2D categorization of interpretable machine learning evaluation methods and show a mapping between user groups and evaluation measures. Further, we address the essential factors and steps for a right evaluation plan by proposing a nested model for design and evaluation of explainable artificial intelligence systems.',\n",
       " 'Brain-Computer Interface (BCI) uses brain signals in order to provide a new method for communication between human and outside world. Feature extraction, selection and classification are among the main matters of concerns in signal processing stage of BCI. In this article, we present our findings about the most effective features and classifiers in some brain tasks. Six different groups of classical features and twelve classifiers have been examined in nine datasets of brain signal. The results indicate that energy of brain signals in {\\\\alpha} and \\\\b{eta} frequency bands, together with some statistical parameters are more effective, comparing to the other types of extracted features. In addition, Bayesian classifier with Gaussian distribution assumption and also Support Vector Machine (SVM) show to classify different BCI datasets more accurately than the other classifiers. We believe that the results can give an insight about a strategy for blind classification of brain signals in brain-computer interface.',\n",
       " 'Using Low Cost Portable Eye Tracking for Biometric Identification Or Verification: Eye tracking technologies have in recent years become available outside of specialised labs, and are starting to become integrated in tablets and virtual reality headsets. This offers new opportunities for use in common office- and home environments, such as for biometric recognition (identification or verification), alone or in combination with other technologies. This paper exposes two fundamentally different approaches that have been suggested, based on spatial and temporal signatures respectively. While deploying different stimulation paradigms for recording, it also proposes an alternative way to analyze spatial domain signatures using Fourier transformation. Empirical data recorded from two subjects over two weeks, three months apart, are found to support previous results. Further, variations and stability of some of the proposed signatures are analyzed over the extended timeframe and under slightly varying conditions.',\n",
       " 'Gait adaptation is an important part of gait analysis and its neuronal origin and dynamics has been studied extensively. In neurorehabilitation, it is important as it perturbs neuronal dynamics and allows patients to restore some of their motor function. Exoskeletons and robotics of the lower limbs are increasingly used to facilitate rehabilitation as well as supporting daily function. Their efficiency and safety depends on how well can sense the human intention to move and adapt the gait accordingly. This paper presents a gait adaptation scheme in natural settings. It allows monitoring of subjects in more realistic environment without the requirement of specialized equipment such as treadmill and foot pressure sensors. We extract gait characteristics based on a single RBG camera whereas wireless EEG signals are monitored simultaneously. We demonstrate that the method can not only successfully detect adaptation steps but also detect efficiently whether the subject adjust their pace to higher or lower speed.',\n",
       " 'Smartphone usage while driving is unanimously considered to be a really dangerous habit due to strong correlation with road accidents. In this paper, the problem of detecting whether the driver is using the phone during a trip is addressed. To do this, high-frequency data from the triaxial inertial measurement unit (IMU) integrated in almost all modern phone is processed without relying on external inputs so as to provide a self-contained approach. By resorting to a frequency-domain analysis, it is possible to extract from the raw signals the useful information needed to detect when the driver is using the phone, without being affected by the effects that vehicle motion has on the same signals. The selected features are used to train a Support Vector Machine (SVM) algorithm. The performance of the proposed approach are analyzed and tested on experimental data collected during mixed naturalistic driving scenarios, proving the effectiveness of the proposed approach.',\n",
       " 'Human-human joint-action in short-cycle repetitive handover tasks was investigated for a bottle handover task using a three-fold approach: work-methods field studies in multiple supermarkets, simulation analysis using an ergonomics software package and by conducting an in-house lab experiment on human-human collaboration by re-creating the environment and conditions of a supermarket. Evaluation included both objective and subjective measures. Subjective evaluation was done taking a psychological perspective and showcases among other things, the differences in the way a common joint-action is being perceived by individual team partners depending upon their role (giver or receiver). The proposed approach can provide a systematic method to analyze similar tasks. Combining the results of all the three analyses, this research gives insight into the science of joint-action for short-cycle repetitive tasks and its implications for human-robot collaborative system design.',\n",
       " 'Consider unsupervised clustering of objects drawn from a discrete set, through the use of human intelligence available in crowdsourcing platforms. This paper defines and studies the problem of universal clustering using responses of crowd workers, without knowledge of worker reliability or task difficulty. We model stochastic worker response distributions by incorporating traits of memory for similar objects and traits of distance among differing objects. We are particularly interested in two limiting worker types---temporary workers who retain no memory of responses and long-term workers with memory. We first define clustering algorithms for these limiting cases and then integrate them into an algorithm for the unified worker model. We prove asymptotic consistency of the algorithms and establish sufficient conditions on the sample complexity of the algorithm. Converse arguments establish necessary conditions on sample complexity, proving that the defined algorithms are asymptotically order-optimal in cost.',\n",
       " 'Diabetes is an epidemic disease of the 21st century and is growing globally. Although, final diabetes treatments and cure are still on research phase, related complications of diabetes endanger life of diabetic patients. Diabetic coma which happens with extreme high or low blood glucose is one of the risk factor for diabetic patients and if it remains unattended will lead to patient death or permanent brain damage. To reduce the risk of such deaths or damages, a novel algorithm for wearable devices application, especially for smart watches are proposed. Such application can inform the patients relatives or emergency centers, if the person falls in coma or irresponsive condition based on readouts from smart watches sensors including mobility, heart rate and skin moisture. However; such an application is not a final solution to detect all types of coma, but it potentially could save lives of many patients, if widely used among the diabetic patients around the world.',\n",
       " \"In this paper we describe the ways participants of the Scratch online community, primarily young people, engage in remixing of each others' shared animations, games, and interactive projects. In particular, we try to answer the following questions: How do users respond to remixing in a social media environment where remixing is explicitly permitted? What qualities of originators and their projects correspond to a higher likelihood of plagiarism accusations? Is there a connection between plagiarism complaints and similarities between a remix and the work it is based on? Our findings indicate that users have a very wide range of reactions to remixing and that as many users react positively as accuse remixers of plagiarism. We test several hypotheses that might explain the high number of plagiarism accusations related to original project complexity, cumulative remixing, originators' integration into remixing practice, and remixee-remixer project similarity, and find support for the first and last explanations.\",\n",
       " 'Hiring robots for the workplaces is a challenging task as robots have to cater to customer demands, follow organizational protocols and behave with social etiquette. In this study, we propose to have a humanoid social robot, Nadine, as a customer service agent in an open social work environment. The objective of this study is to analyze the effects of humanoid robots on customers at work environment, and see if it can handle social scenarios. We propose to evaluate these objectives through two modes, namely, survey questionnaire and customer feedback. We also propose a novel approach to analyze customer feedback data (text) using sentic computing methods. Specifically, we employ aspect extraction and sentiment analysis to analyze the data. From our framework, we detect sentiment associated to the aspects that mainly concerned the customers during their interaction. This allows us to understand customers expectations and current limitations of robots as employees.',\n",
       " 'Smartphones are equipped with sensors such as accelerometers, gyroscope, and GPS in one cost-effective device with an acceptable level of accuracy. There have been some research studies carried out in terms of using smartphones to measure the pavement roughness. However, a little attention has been paid to investigate the validity of the measured pavement roughness by smartphones via other subjective methods such as the user opinion. This paper aims at calculating the pavement roughness data with a smartphone using its embedded sensors and investigating its correlation with a user opinion about the ride quality. In addition, the applicability of using smartphones to assess the pavement surface distresses is examined. Furthermore, to validate the smartphone sensor outputs objectively, the Road Surface Profiler is applied. Finally, a good roughness model is developed which demonstrates an acceptable level of correlation between the pavement roughness measured by smartphones and the ride quality rated by users.',\n",
       " 'Websites and applications that match and connect individuals for romantic purposes are commonly used in the Western world. However, there have not been many previous investigations focusing on cultural factors that affect the adoption of similar technologies in religiously conservative non-Western cultures. In this study, we examine the socio-technical and cultural factors that influence the perceptions and use of matchmaking technologies in Saudi Arabia. We report the methods and findings of interviews with 18 Saudi nationals (nine males and nine females) with diverse demographics and backgrounds. We provide qualitatively generated insights into the major themes reported by our participants related to the common approaches to matchmaking, the current role of technology, and concerns regarding matchmaking technologies in this cultural con-text. We relate these themes to specific implications for designing marital matchmaking technologies in Saudi Arabia and we outline opportunities for future investigations.',\n",
       " 'Latest research revealed a considerable lack of reliability within user feedback and discussed striking impacts for the assessment of adaptive web systems and content personalisation approaches, e.g. ranking errors, systematic biases to accuracy metrics as well as its natural offset (the magic barrier). In order to perform holistic assessments and to improve web systems, a variety of strategies have been proposed to deal with this so-called human uncertainty. In this contribution we discuss the most relevant strategies to handle uncertain feedback and demonstrate that these approaches are more or less ineffective to fulfil their objectives. In doing so, we consider human uncertainty within a purely probabilistic framework and utilise hypothesis testing as well as a generalisation of the magic barrier to compare the effects of recently proposed algorithms. On this basis we recommend a novel strategy of acceptance which turns away from mere filtering and discuss potential benefits for the community of the WWW.',\n",
       " 'In this paper we argue, drawing from the perspectives of cybersecurity and social psychology, that Internet-based manipulation of an individual or group reality using ambient tactical deception is possible using only software and changing words in a web browser. We call this attack Ambient Tactical Deception (ATD). Ambient, in artificial intelligence, describes software that is \"unobtrusive,\" and completely integrated into a user\\'s life. Tactical deception is an information warfare term for the use of deception on an opposing force. We suggest that an ATD attack could change the sentiment of text in a web browser. This could alter the victim\\'s perception of reality by providing disinformation. Within the limit of online communication, even a pause in replying to a text can affect how people perceive each other. The outcomes of an ATD attack could include alienation, upsetting a victim, and influencing their feelings about an election, a spouse, or a corporation.',\n",
       " 'In recent years, deep learning poses a deep technical revolution in almost every field and attracts great attentions from industry and academia. Especially, the convolutional neural network (CNN), one representative model of deep learning, achieves great successes in computer vision and natural language processing. However, simply or blindly applying CNN to the other fields results in lower training effects or makes it quite difficult to adjust the model parameters. In this poster, we propose a general methodology named V-CNN by introducing data visualizing for CNN. V-CNN introduces a data visualization model prior to CNN modeling to make sure the data after processing is fit for the features of images as well as CNN modeling. We apply V-CNN to the network intrusion detection problem based on a famous practical dataset: AWID. Simulation results confirm V-CNN significantly outperforms other studies and the recall rate of each invasion category is more than 99.8%.',\n",
       " 'Head gesture is a natural means of face-to-face communication between people but the recognition of head gestures in the context of virtual reality and use of head gesture as an interface for interacting with virtual avatars and virtual environments have been rarely investigated. In the current study, we present an approach for real-time head gesture recognition on head-mounted displays using Cascaded Hidden Markov Models. We conducted two experiments to evaluate our proposed approach. In experiment 1, we trained the Cascaded Hidden Markov Models and assessed the offline classification performance using collected head motion data. In experiment 2, we characterized the real-time performance of the approach by estimating the latency to recognize a head gesture with recorded real-time classification data. Our results show that the proposed approach is effective in recognizing head gestures. The method can be integrated into a virtual reality system as a head gesture interface for interacting with virtual worlds.',\n",
       " 'We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this method we have assembled SciQ, a dataset of 13.7K multiple choice science exam questions (Dataset available at http://allenai.org/data.html). We demonstrate that the method produces in-domain questions by providing an analysis of this new dataset and by showing that humans cannot distinguish the crowdsourced questions from original questions. When using SciQ as additional training data to existing questions, we observe accuracy improvements on real science exams.',\n",
       " \"Modern software developers rely on an extensive set of social media tools and communication channels. The adoption of team communication platforms has led to the emergence of conversation-based tools and integrations, many of which are chatbots. Understanding how software developers manage their complex constellation of collaborators in conjunction with the practices and tools they use can bring valuable insights into socio-technical collaborative work in software development and other knowledge work domains.\\n  In this paper, we explore how chatbots can help reduce the friction points software developers face when working collaboratively. Using a socio-technical model for collaborative work, we identify three main areas for conflict: friction stemming from team interactions with each other, an individual's interactions with technology, and team interactions with technology. Finally, we provide a set of open questions for discussion within the research community.\",\n",
       " 'In real settings, natural body movements can be erroneously recognized by whole-body input systems as explicit input actions. We call body activity not intended as input actions \"background activity.\" We argue that understanding background activity is crucial to the success of always-available whole-body input in the real world. To operationalize this argument, we contribute a reusable study methodology and software tools to generate standardized background activity datasets composed of data from multiple Kinect cameras, a Vicon tracker, and two high-definition video cameras. Using our methodology, we create an example background activity dataset for a television-oriented living room setting. We use this dataset to demonstrate how it can be used to redesign a gestural interaction vocabulary to minimize conflicts with the real world. The software tools and initial living room dataset are publicly available (http://www.dgp.toronto.edu/~dustin/backgroundactivity/).',\n",
       " \"Speech emotion recognition is an important task in human-machine interaction. However, it faces many challenges such as the ambiguity of emotion expression and the lack of training samples. To solve these problems, we propose a novel 'Pairwise discriminative task', which attempts to learn the similarity and distinction between two audios rather than specific labels. In the task, pairwise audios are fed into audio encode networks to extract audio vectors, followed with discrimination networks behind to judge whether audios belong to the same emotion category or not. The system is optimized in an end-to-end manner to minimize the loss function, which cooperates cosine similarity loss and cross entropy loss together. To verify the performance of audio representation vectors extracted from the system, we test them on IEMOCAP database-a common evaluation corpus. We gain 56.33% unweighted accuracy on the test database, which surpasses above 5% compared with traditional end-to-end speech emotion recognition networks.\",\n",
       " 'This paper examines some of the potential challenges associated with enabling a seamless web experience on underpowered mobile devices such as Google Glass from the perspective of web content providers, device, and the network. We conducted experiments to study the impact of webpage complexity, individual web components and different application layer protocols while accessing webpages on the performance of Glass browser, by measuring webpage load time, temperature variation and power consumption and compare it to a smartphone. Our findings suggest that (a) performance of Glass compared to a smartphone in terms of power consumption and webpage load time deteriorates with increasing webpage complexity (b) execution time for popular JavaScript benchmarks is about 3-8 times higher on Glass compared to a smartphone, (c) WebP is more energy efficient image format than JPEG and PNG, and (d) seven out of 50 websites studied are optimized for content delivery to Glass.',\n",
       " \"We introduce Blocks, a mobile application that enables people to co-create AR structures that persist in the physical environment. Using Blocks, end users can collaborate synchronously or asynchronously, whether they are colocated or remote. Additionally, the AR structures can be tied to a physical location or can be accessed from anywhere. We evaluated how people used Blocks through a series of lab and field deployment studies with over 160 participants, and explored the interplay between two collaborative dimensions: space and time. We found that participants preferred creating structures synchronously with colocated collaborators. Additionally, they were most active when they created structures that were not restricted by time or place. Unlike most of today's AR experiences, which focus on content consumption, this work outlines new design opportunities for persistent and collaborative AR experiences that empower anyone to collaborate and create AR content.\",\n",
       " 'Static and dynamic hand movements are basic way for human-machine interactions. To recognize and classify these movements, first these movements are captured by the cameras mounted on the augmented reality (AR) or virtual reality (VR) wearable devices. The hand is segmented using segmentation method and its gestures are passed to hand gesture recognition algorithm, which depends on depth-wise separable convolutional neural network for training, testing and finally running smoothly on mobile AR/VR devices, while maintaining the accuracy and balancing the load. A number of gestures are processed for identification of right gesture and to classify the gesture and ignore the all intermittent gestures. With proposed method, a user can write letters and numbers in air by just moving his/her hand in air. Gesture based operations are performed, and trajectory of hand is recorded as handwritten text. Finally, that handwritten text is processed for the text recognition.',\n",
       " 'Virtual globes have progressed from little-known technology to broadly popular software in a mere few years. We investigated this phenomenon through a survey and discovered that, while virtual globes are en vogue, their use is restricted to a small set of tasks so simple that they do not involve any spatial thinking. Spatial thinking requires that users ask \"what is where\" and \"why\"; the most common virtual globe tasks only include the \"what\". Based on the results of this survey, we have developed a multi-touch virtual globe derived from an adapted virtual globe paradigm designed to widen the potential uses of the technology by helping its users to inquire about both the \"what is where\" and \"why\" of spatial distribution. We do not seek to provide users with full GIS (geographic information system) functionality, but rather we aim to facilitate the asking and answering of simple \"why\" questions about general topics that appeal to a wide virtual globe user base.',\n",
       " 'Providing vibrotactile feedback that corresponds to the state of the virtual texture surfaces allows users to sense haptic properties of them. However, hand-tuning such vibrotactile stimuli for every state of the texture takes much time. Therefore, we propose a new approach to create models that realize the automatic vibrotactile generation from texture images or attributes. In this paper, we make the first attempt to generate the vibrotactile stimuli leveraging the power of deep generative adversarial training. Specifically, we use conditional generative adversarial networks (GANs) to achieve generation of vibration during moving a pen on the surface. The preliminary user study showed that users could not discriminate generated signals and genuine ones and users felt realism for generated signals. Thus our model could provide the appropriate vibration according to the texture images or the attributes of them. Our approach is applicable to any case where the users touch the various surfaces in a predefined way.',\n",
       " \"Working memory accounts for the ability of humans to perform cognitive processing, by handling both the representation of information (the mental picture forming the situation awareness) and the space required for processing these information (skill processing). The more complex the skills are, the more processing space they require, the less space becomes available for storage of information. This interplay between situation awareness and skills is critical in many applications. Theoretically, it is less understood in cognition and neuroscience. In the meantime, and practically, it is vital when analysing the mental processes involved in safety-critical domains.\\n  In this paper, we use the Sudoku game as a vehicle to study this trade-off. This game combines two features that are present during a user interaction with a software in many safety critical domains: scanning for information and processing of information. We use a society of agents for investigating how this trade-off influences player's proficiency.\",\n",
       " \"This paper outlines the development of a sensory feedback device providing a tangible interface for controlling digital environments, in this example a flight simulator, where the intention for the device is that it is relatively low cost, versatile and intuitive. Gesture based input allows for a more immersive experience, so rather than making the user feel like they are controlling an aircraft the intuitive interface allows the user to become the aircraft that is controlled by the movements of the user's hand. The movements are designed to allow a sense of immersion that would be difficult to achieve with an alternative interface. A vibrotactile based haptic feedback is incorporated in the device to further enhance the connection between the user and the game environment by providing immediate confirmation of game events. When used for navigating an aircraft simulator, this device invites playful action and thrill. It bridges new territory on portable, low cost solutions for haptic devices in gaming contexts.\",\n",
       " 'A carefully constructed scatterplot can reveal plenty about an underlying data set. However, in most cases visually mining and understanding a large multivariate data set requires more finesse, and greater level of interactivity to really grasp the full spectrum of the information being presented. We present a paradigm for glyph design and use in the creation of single plots presenting multiple variables of information. We center our design on two key concepts. The first concept is that visually it is easier to discriminate between completely distinct shapes rather than subtly different ones, specially when partially occluded. The second one is that users ingest information in layers, i.e. in an order of visual relevance. Using this paradigm, we present complex data as binned into desired and relevant discrete categories. We show results in the areas of high energy physics and security, displaying over 6 distinct data variables in each single plot, yielding a clear, highly readable, and effective visualization.',\n",
       " 'Many real-world networks are globally sparse but locally dense. Typical examples are social networks, biological networks, and information networks. This double structural nature makes it difficult to adopt a homogeneous visualization model that clearly conveys an overview of the network and the internal structure of its communities at the same time. As a consequence, the use of hybrid visualizations has been proposed. For instance, NodeTrix combines node-link and matrix-based representations (Henry et al., 2007). In this paper we describe ChordLink, a hybrid visualization model that embeds chord diagrams, used to represent dense subgraphs, into a node-link diagram, which shows the global network structure. The visualization is intuitive and makes it possible to interactively highlight the structure of a community while keeping the rest of the layout stable. We discuss the intriguing algorithmic challenges behind the ChordLink model, present a prototype system, and illustrate case studies on real-world networks.',\n",
       " 'Virtual reality (VR) games are gradually becoming more elaborated and feature-rich, but fail to reach the complexity of traditional digital games. One common feature that is used to extend and organize complex gameplay is the in-game inventory, which allows players to obtain and carry new tools and items throughout their journey. However, VR imposes additional requirements and challenges that impede the implementation of this important feature and hinder games to unleash their full potential. Our current work focuses on the design space of inventories in VR games. We introduce this sparsely researched topic by constructing a first taxonomy of the underlying design considerations and building blocks. Furthermore, we present three different inventories that were designed using our taxonomy and evaluate them in an early qualitative study. The results underline the importance of our research and reveal promising insights that show the huge potential for VR games.',\n",
       " 'We describe the experimental procedures for a dataset that we have made publicly available at https://doi.org/10.5281/zenodo.1494163 in mat and csv formats. This dataset contains electroencephalographic (EEG) recordings of 24 subjects doing a visual P300 Brain-Computer Interface experiment on PC.  The visual P300 is an event-related potential elicited by visual stimulation, peaking 240-600 ms after stimulus onset. The experiment was designed in order to compare the use of a P300-based brain-computer interface on a PC with and without adaptive calibration using Riemannian geometry. The brain-computer interface is based on electroencephalography (EEG). EEG data were recorded thanks to 16 electrodes. Data were recorded during an experiment taking place in the GIPSA-lab, Grenoble, France, in 2013 (Congedo, 2013). Python code for manipulating the data is available at https://github.com/plcrodrigues/py.BI.EEG.2013-GIPSA. The ID of this dataset is BI.EEG.2013-GIPSA.',\n",
       " 'Unhealthy lifestyles could cause many chronic diseases, which bring patients and their families much burden. Research has shown the potential of digital technologies for supporting health behavior change to help us prevent these chronic diseases. The HCI community has contributed to the research on health behavior change for more than a decade. In this paper, we aim to explore the research trends and patterns of health behavior change in HCI. Our systematic review showed that physical activity drew much more attention than other behaviors. Most of the participants in the reviewed studies were adults, while children and the elderly were much less addressed. Also, we found there is a lack of standardized approaches to evaluating the user experience of interventions for health behavior change in HCI. Based on the reviewed studies, we provide suggestions and research opportunities on six topics, e.g., game integration, social support, and relevant AI application.',\n",
       " 'This paper reports on a simple visual technique that boils extracting a subgraph down to two operations---pivots and filters---that is agnostic to both the data abstraction, and its visual complexity scales independent of the size of the graph. The system\\'s design, as well as its qualitative evaluation with users, clarifies exactly when and how the user\\'s intent in a series of pivots is ambiguous---and, more usefully, when it is not. Reflections on our results show how, in the event of an ambiguous case, this innately practical operation could be further extended into \"smart pivots\" that anticipate the user\\'s intent beyond the current step. They also reveal ways that a series of graph pivots can expose the semantics of the data from the user\\'s perspective, and how this information could be leveraged to create adaptive data abstractions that do not rely as heavily on a system designer to create a comprehensive abstraction that anticipates all the user\\'s tasks.',\n",
       " 'Tactile displays have a wide potential field of applications, ranging from enhancing Virtual-Reality scenarios up to aiding telesurgery as well as in fundamental psychological and neurophysiological research. In this paper, we describe an open source hardware and software architecture that is designed to drive a variety of different tactile displays. For demonstration purposes, a tactile computer mouse featuring a simple tactile display, based on lateral piezoelectric (PZT) actuators, is presented. Even though we will focus on driving mechanical actuators in this paper, the system can be extended to different working principles. The suggested architecture is supplied with a custom, easy to use, software stack allowing a simple definition of tactile scenarios as well as user studies while being especially tailored to non-computer scientists. By releasing the OpenTactile system under MIT license we hope to ease the burden of controlling tactile displays as well as designing and reproducing the related experiments.',\n",
       " 'In order for people to be able to trust and take advantage of the results of advanced machine learning and artificial intelligence solutions for real decision making, people need to be able to understand the machine rationale for given output. Research in explain artificial intelligence (XAI) addresses the aim, but there is a need for evaluation of human relevance and understandability of explanations. Our work contributes a novel methodology for evaluating the quality or human interpretability of explanations for machine learning models. We present an evaluation benchmark for instance explanations from text and image classifiers. The explanation meta-data in this benchmark is generated from user annotations of image and text samples. We describe the benchmark and demonstrate its utility by a quantitative evaluation on explanations generated from a recent machine learning algorithm. This research demonstrates how human-grounded evaluation could be used as a measure to qualify local machine-learning explanations.',\n",
       " \"We use the term borg to refer to the complex organizations composed of people, machines, and processes with which users frequently interact using computer interfaces and websites. Unlike interfaces to pure machines, we contend that borg-human interaction (BHI) happens in a context combining the anthropomorphization of the interface, conflict with users, and dramatization of the interaction process. We believe this context requires designers to construct the human facet of the borg, a structure encompassing the borg's personality, social behavior, and embodied actions; and the strategies to co-create dramatic narratives with the user. To design the human facet of a borg, different concepts and models are explored and discussed, borrowing ideas from psychology, sociology, and arts. Based on those foundations, we propose six design methodologies to complement traditional computer-human interface design techniques, including play-and-freeze enactment of conflicts and the use of giant puppets as interface prototypes.\",\n",
       " 'The objective of this research was to find out how the two search engines Google and Bing perform when users work freely on pre-defined tasks, and judge the relevance of the results immediately after finishing their search session. In a user study, 64 participants conducted two search tasks each, and then judged the results on the following: (1) The quality of the results they selected in their search sessions, (2) The quality of the results they were presented with in their search sessions (but which they did not click on), (3) The quality of the results from the competing search engine for their queries (which they did not see in their search session). We found that users heavily relied on Google, that Google produced more relevant results than Bing, that users were well able to select relevant results from the results lists, and that users judged the relevance of results lower when they regarded a task as difficult and did not find the correct information.',\n",
       " \"As the quantity of human knowledge increasing rapidly, it is harder and harder to evaluate a knowledge worker's knowledge quantitatively. There are lots of demands for evaluating a knowledge worker's knowledge. For example, accurately finding out a researcher's research concentrations for the last three years; searching for common topics for two scientists with different academic backgrounds; helping a researcher discover his deficiencies on a research field etc. This paper proposes a method named knowledge model to evaluate a knowledge worker's knowledge quantitatively without taking an examination. It records and analyzes an individual's each learning experience, discovering all the involved knowledge points and calculating their shares by analyzing the text learning contents with topic model. It calculates a score for a knowledge point by accumulating the effects of one's all learning experiences about it. A preliminary knowledge evaluating system is developed to testify the practicability of knowledge model.\",\n",
       " 'As people store more personal data in their smartphones, the consequences of having it stolen or lost become an increasing concern. A typical counter-measure to avoid this risk is to set up a secret code that has to be entered to unlock the device after a period of inactivity. However, for blind users, PINs and passwords are inadequate, since entry 1) consumes a non-trivial amount of time, e.g. using screen readers, 2) is susceptible to observation, where nearby people can see or hear the secret code, and 3) might collide with social norms, e.g. disrupting personal interactions. Tap-based authentication methods have been presented and allow unlocking to be performed in a short time and support naturally occurring inconspicuous behavior (e.g. concealing the device inside a jacket) by being usable with a single hand. This paper presents a study with blind users (N = 16) where an authentication method based on tap phrases is evaluated. Results showed the method to be usable and to support the desired inconspicuity.',\n",
       " 'Mutually beneficial behavior in repeated games can be enforced via the threat of punishment, as enshrined in game theory\\'s well-known \"folk theorem.\" There is a cost, however, to a player for generating these disincentives. In this work, we seek to minimize this cost by computing a \"Stackelberg punishment,\" in which the player selects a behavior that sufficiently punishes the other player while maximizing its own score under the assumption that the other player will adopt a best response. This idea generalizes the concept of a Stackelberg equilibrium. Known efficient algorithms for computing a Stackelberg equilibrium can be adapted to efficiently produce a Stackelberg punishment. We demonstrate an application of this idea in an experiment involving a virtual autonomous vehicle and human participants. We find that a self-driving car with a Stackelberg punishment policy discourages human drivers from bullying in a driving scenario requiring social negotiation.',\n",
       " 'Crowdsourcing is a popular means to obtain labeled data at moderate costs, for example for tweets, which can then be used in text mining tasks. To alleviate the problem of low-quality labels in this context, multiple human factors have been analyzed to identify and deal with workers who provide such labels. However, one aspect that has been rarely considered is the inherent difficulty of tweets to be labeled and how this affects the reliability of the labels that annotators assign to such tweets. Therefore, we investigate in this preliminary study this connection using a hierarchical sentiment labeling task on Twitter. We find that there is indeed a relationship between both factors, assuming that annotators have labeled some tweets before: labels assigned to easy tweets are more reliable than those assigned to difficult tweets. Therefore, training predictors on easy tweets enhances the performance by up to 6% in our experiment. This implies potential improvements for active learning techniques and crowdsourcing.',\n",
       " 'Web portals are being considered as excellent means for conducting teaching and learning activities electronically. The number of online services such as course enrollment, tutoring through online course materials, evaluation and even certification through web portals is increasing day by day. However, the effectiveness of an educational web portal depends on its accessibility to a wide range of students irrespective of their age, and physical abilities. Accessibility of web portals largely depends on their userfriendliness in terms of design, contents, assistive features, and online support. In this paper, we have critically analyzed the web portals of thirty Indian Universities of different categories based on the WCAG 2.0 guidelines. The purpose of this study is to point out the deficiencies that are commonly observed in web portals and help web designers to remove such deficiencies from the academic web portals with a view to enhance their accessibility.',\n",
       " 'Social media are a rich source of insight for data mining and user-centred research, but the question of consent arises when studying such data without the express knowledge of the creator. Case studies that mine social data from users of online services such as Facebook and Twitter are becoming increasingly common. This has led to calls for an open discussion into how researchers can best use these vast resources to make innovative findings while still respecting fundamental ethical principles. In this position paper we highlight some key considerations for this topic and argue that the conditions of informed consent are often not being met, and that using social media data that some deem free to access and analyse may result in undesirable consequences, particularly within the domain of health research and other sensitive topics. We posit that successful exploitation of online personal data, particularly for health and other sensitive research, requires new and usable methods of obtaining consent from the user.',\n",
       " \"As online health communities (OHCs) grow, users find it challenging to properly search, read, and contribute to the community because of its overwhelming content. Our goal is to understand OHC users' needs and requirements for better delivering large-scale OHC content. We interviewed 14 OHC users with interests in diabetes to investigate their attitudes and needs towards using OHCs and 2 OHC administrators to assess our findings. Four personas -Coddlers, Scientists, Adventurers, and Opportunists- emerged, which inform users' interaction behavior and attitudes with OHCs. An individual can possess the characteristics of multiple personas, which can also change over time. Our personas uniquely describe users' OHC participation intertwined with illness contexts compared to existing social types in general online communities. We discuss broader implications back to the literature and how our findings apply to other illness contexts in OHCs. We end with requirements for personalized delivery of large-scale OHC content.\",\n",
       " 'The process of learning involves interaction with the learning environment through our five senses (sight, hearing, touch, smell, and taste). Until recently, distance education focused only on the first two of those senses, sight and sound. Internet-based learning environments are predominantly visual with auditory components. With the advent of haptic technology we can now simulate/generate forces and, as a result, the sense of touch. The gaming industry has promoted the \"touch\" on the \"wire\", allowing complex multi-modal interactions online. In this article we provide a brief overview of the evolution of haptic technology, its potential for education, and existing challenges. We review recent data on 21st century students\\' behaviors, and share our experiences in designing interactive haptic environments for education. From the \"Community of Inquiry\" framework perspective, we discuss the potential impact of haptic feedback on cognitive and social presence.',\n",
       " 'Traditional literature on camera network design focuses on constructing automated algorithms. These require problem specific input from experts in order to produce their output. The nature of the required input is highly unintuitive leading to an unpractical workflow for human operators. In this work we focus on developing a virtual reality user interface allowing human operators to manually design camera networks in an intuitive manner. From real world practical examples we conclude that the camera networks designed using this interface are highly competitive with, or superior to those generated by automated algorithms, but the associated workflow is much more intuitive and simple. The competitiveness of the human-generated camera networks is remarkable because the structure of the optimization problem is a well known combinatorial NP-hard problem. These results indicate that human operators can be used in challenging geometrical combinatorial optimization problems given an intuitive visualization of the problem.',\n",
       " 'We are experiencing an upcoming trend of using head mounted display systems in games and serious games, which is likely to become an established practice in the near future. While these systems provide highly immersive experiences, many users have been reporting discomfort symptoms, such as nausea, sickness, and headaches, among others. When using VR for health applications, this is more critical, since the discomfort may interfere a lot in treatments. In this work we discuss possible causes of these issues, and present possible solutions as design guidelines that may mitigate them. In this context, we go deeper within a dynamic focus solution to reduce discomfort in immersive virtual environments, when using first-person navigation. This solution applies an heuristic model of visual attention that works in real time. This work also discusses a case study (as a first-person spatial shooter demo) that applies this solution and the proposed design guidelines.',\n",
       " 'Get-Up-and-Go Test is commonly used for assessing the physical mobility of the elderly by physicians. This paper presents a method for automatic analysis and classification of human gait in the Get-Up-and-Go Test using a Microsoft Kinect sensor. Two types of features are automatically extracted from the human skeleton data provided by the Kinect sensor. The first type of feature is related to the human gait (e.g., number of steps, step duration, and turning duration); whereas the other one describes the anatomical configuration (e.g., knee angles, leg angle, and distance between elbows). These features characterize the degree of human physical mobility. State-of-the-art machine learning algorithms (i.e. Bag of Words and Support Vector Machines) are used to classify the severity of gaits in 12 subjects with ages ranging between 65 and 90 enrolled in a pilot study. Our experimental results show that these features can discriminate between patients who have a high risk for falling and patients with a lower fall risk.',\n",
       " 'We design a system, SolarGest, which can recognize hand gestures near a solar-powered device by analyzing the patterns of the photocurrent. SolarGest is based on the observation that each gesture interferes with incident light rays on the solar panel in a unique way, leaving its distinguishable signature in harvested photocurrent. Using solar energy harvesting laws, we develop a model to optimize design and usage of SolarGest. To further improve the robustness of SolarGest under non-deterministic operating conditions, we combine dynamic time warping with Z-score transformation in a signal processing pipeline to pre-process each gesture waveform before it is analyzed for classification. We evaluate SolarGest with both conventional opaque solar cells as well as emerging see-through transparent cells. Our experiments with 6,960 gesture samples for 6 different gestures reveal that even with transparent cells, SolarGest can detect 96% of the gestures while consuming 44% less power compared to light sensor based systems.',\n",
       " 'This work describes a new human-in-the-loop (HitL) assistive grasping system for individuals with varying levels of physical capabilities. We investigated the feasibility of using four potential input devices with our assistive grasping system interface, using able-bodied individuals to define a set of quantitative metrics that could be used to assess an assistive grasping system. We then took these measurements and created a generalized benchmark for evaluating the effectiveness of any arbitrary input device into a HitL grasping system. The four input devices were a mouse, a speech recognition device, an assistive switch, and a novel sEMG device developed by our group that was connected either to the forearm or behind the ear of the subject. These preliminary results provide insight into how different interface devices perform for generalized assistive grasping tasks and also highlight the potential of sEMG based control for severely disabled individuals.',\n",
       " 'Brain Computer Interface (BCI) can help patients of neuromuscular diseases restore parts of the movement and communication abilities that they have lost. Most of BCIs rely on mapping brain activities to device instructions, but limited number of brain activities decides the limited abilities of BCIs. To deal with the problem of limited ablility of BCI, this paper verified the feasibility of constructing BCI based on decoding imagined speech electroencephalography (EEG). As sentences decoded from EEG can have rich meanings, BCIs based on EEG decoding can achieve numerous control instructions. By combining a modified EEG feature extraction mehtod with connectionist temporal classification (CTC), this paper simulated decoding imagined speech EEG using synthetic EEG data without help of speech signal. The performance of decoding model over synthetic data to a certain extent demonstrated the feasibility of constructing BCI based on imagined speech brain signal.',\n",
       " 'Online creative communities allow creators to share their work with a large audience, maximizing opportunities to showcase their work and connect with fans and peers. However, sharing in-progress work can be technically and socially challenging in environments designed for sharing completed pieces. We propose an online creative community where sharing process, rather than showcasing outcomes, is the main method of sharing creative work. Based on this, we present Mosaic---an online community where illustrators share work-in-progress snapshots showing how an artwork was completed from start to finish. In an online deployment and observational study, artists used Mosaic as a vehicle for reflecting on how they can improve their own creative process, developed a social norm of detailed feedback, and became less apprehensive of sharing early versions of artwork. Through Mosaic, we argue that communities oriented around sharing creative process can create a collaborative environment that is beneficial for creative growth.',\n",
       " 'Online deliberation offers a way for citizens to collectively discuss an issue and provide input for policy makers. The overall experience of online deliberation can be affected by multiple factors. We decided to investigate the effects of moderation and opinion heterogeneity on the perceived deliberation experience, by running the first online deliberation experiment in Singapore. Our study took place in three months with three phases. In phase 1, our 2,006 participants answered a survey, that we used to create groups of different opinion heterogeneity. During the second phase, 510 participants discussed about the population issue on the online platform we developed. We gathered data on their online deliberation experience during phase 3. We found out that higher levels of moderation negatively impact the experience of deliberation on perceived procedural fairness, validity claim and policy legitimacy; and that high opinion heterogeneity is important in order to get a fair assessment of the deliberation experience.',\n",
       " \"Web-based systems for assessment or homework are commonly used in many different domains. Several studies show that these systems can have positive effects on learning outcomes. Many research efforts also have made these systems quite flexible with respect to different item formats and exercise styles. However, there is still a lack of support for complex exercises in several domains at university level. Although there are systems that allow for quite sophisticated operations for generating exercise contents, there is less support for using similar operations for evaluating students' input and for feedback generation. This paper elaborates on filling this gap in the specific case of statistics. We present both the conceptional requirements for this specific domain as well as a fully implemented solution. Furthermore, we report on using this solution for formative and summative assessments in lectures with large numbers of participants at a big university.\",\n",
       " 'Digital health interventions have been emerging in the last decade. Due to their interdisciplinary nature, digital health interventions are guided and influenced by theories (e.g., behavioral theories, behavior change technologies, persuasive technology) from different research communities. However, digital health interventions are always coded using various taxonomies and reported in insufficient perspectives. The inconsistency and incomprehensiveness will bring difficulty for conducting systematic reviews and sharing contributions among communities. Based on existing related work, therefore, we propose a holistic framework that embeds behavioral theories, behavior change technique (BCT) taxonomy, and persuasive system design (PSD) principles. Including four development steps, two toolboxes, and one workflow, our framework aims to guide digital health intervention developers to design, evaluate, and report their work in a formative and comprehensive way.',\n",
       " \"Breast cancer is the most common cancer in women both in developed and developing countries. More than half of all cancer mobile application concern breast cancer. Gamification is widely used in mobile software applications created for health-related services. Current prevalence of gamification in breast cancer apps is unknown and detection must be manually performed. The purpose of this study is to describe and produce a tool allowing automatic detection of apps which contain gamification elements and thus empowering researchers to study gamification using large data samples. Predictive logistic regression model was designed on data extracted from breast cancer apps' title and description text available in app stores. Model was validated comparing estimated and benchmark values, observed by gamification specialists. Study's outcome can be applied as a screening tool to efficiently identify gamification presence in breast cancer apps for further research.\",\n",
       " 'Conversational agents are systems with a conversational interface that afford interaction in spoken language. These systems are becoming prevalent and are preferred in various contexts and for many users. Despite their increasing success, the automated testing infrastructure to support the effective and efficient development of such systems compared to traditional software systems is still limited. Automated testing framework for conversational systems can improve the quality of these systems by assisting developers to write, execute, and maintain test cases. In this paper, we introduce our work-in-progress automated testing framework, and its realization in the Python programming language. We discuss some research problems in the development of such an automated testing framework for conversational agents. In particular, we point out the problems of the specification of the expected behavior, known as test oracles, and semantic comparison of utterances.',\n",
       " \"We present RealPen, an augmented stylus for capacitive tablet screens that recreates the physical sensation of writing on paper with a pencil, ball-point pen or marker pen. The aim is to create a more engaging experience when writing on touch surfaces, such as screens of tablet computers. This is achieved by re-generating the friction-induced oscillation and sound of a real writing tool in contact with paper. To generate realistic tactile feedback, our algorithm analyses the frequency spectrum of the friction oscillation generated when writing with traditional tools, extracts principal frequencies, and uses the actuator's frequency response profile for an adjustment weighting function. We enhance the realism by providing the sound feedback aligned with the writing pressure and speed. Furthermore, we investigated the effects of superposition and fluctuation of several frequencies on human tactile perception, evaluated the performance of RealPen, and characterized users' perception and preference of each feedback type.\",\n",
       " 'This article proposes the analysis on novel human computer interaction (HCI) platform based college mathematical education methodology. Above for the application of virtual reality technology in teaching the problems in the study, only through the organization focus on the professional and technical personnel, and constantly improve researchers in development process of professional knowledge, close to the actual needs of the teaching can we achieve the satisfactory result. To obtain better education output, we combine the Kinect to form the HCI based teaching environment. We firstly review the latest HCI technique and principles of college math courses, then we introduce basic components of the Kinect including the gesture segmentation, systematic implementation and the primary characteristics of the platform. As the further step, we implement the system with the re-write of script code to build up the personalized HCI assisted education scenario. The verification and simulation proves the feasibility of our method.',\n",
       " 'With the arrival of digital maps, the ubiquity of maps has increased sharply and new map functionalities have become available such as changing the scale on the fly or displaying/hiding layers. Users can now interact with maps on multiple devices (e.g. smartphones, desktop computers, large-scale displays, head-mounted displays) using different means of interaction such as touch, voice or gestures. However, ensuring map functionalities and good user experience across these devices and modalities frequently entails dedicated development efforts for each combination. In this paper, we argue that introducing an abstract representation of what a map contains and affords can unlock new opportunities. For this purpose, we propose the concept of map plasticity, the capability of a map-based system to support different contexts of use while preserving usability and functionality. Based on this definition, we discuss core components and an example. We also propose a research agenda for realising map plasticity and its benefits.',\n",
       " \"The success of smart environments largely depends on their smartness of understanding the environments' ongoing situations. Accordingly, this task is an essence to smart environment central processors. Obtaining knowledge from the environment is often through sensors, and the response to a particular circumstance is offered by actuators. This can be improved by getting user feedback, and capturing environmental changes. Machine learning techniques and semantic reasoning tools are widely used in this area to accomplish the goal of interpretation. In this paper, we have proposed a hybrid approach utilizing both machine learning and semantic reasoning tools to derive a better understanding from sensors. This method uses situation templates jointly with a decision tree to adapt the system knowledge to the environment. To test this approach we have used a simulation process which has resulted in a better precision for detecting situations in an ongoing environment involving living agents while capturing its dynamic nature.\",\n",
       " \"Public sharing is integral to online platforms. This includes the popular multimedia messaging application Snapchat, on which public sharing is relatively new and unexplored in prior research. In mobile-first applications, sharing contexts are dynamic. However, it is unclear how context impacts users' sharing decisions. As platforms increasingly rely on user-generated content, it is important to also broadly understand user motivations and considerations in public sharing. We explored these aspects of content sharing through a survey of 1,515 Snapchat users. Our results indicate that users primarily have intrinsic motivations for publicly sharing Snaps, such as to share an experience with the world, but also have considerations related to audience and sensitivity of content. Additionally, we found that Snaps shared publicly were contextually different from those privately shared. Our findings suggest that content sharing systems can be designed to support sharing motivations, yet also be sensitive to private contexts.\",\n",
       " 'Recent research in the enteric nervous system, sometimes called the second brain, has revealed potential of the digestive system in predicting emotion. Even though people regularly experience changes in their gastrointestinal (GI) tract which influence their mood and behavior multiple times per day, robust measurements and wearable devices are not quite developed for such phenomena. However, other manifestations of the autonomic nervous system such as electrodermal activity, heart rate, and facial muscle movement have been extensively used as measures of emotions or in biofeedback applications, while neglecting the gut.  We expose electrogastrography (EGG), i.e., recordings of the myoelectric activity of the GI tract, as a possible measure for inferring human emotions. In this paper, we also wish to bring into light some fundamental questions about emotions, which are often taken for granted in the field of Human Computer Interaction, but are still a great debate in the fields of cognitive neuroscience and psychology.',\n",
       " 'Through Augmented Reality (AR), virtual graphics can transform the physical world. This offers benefits to mobile tourism, where points of interest (POIs) can be annotated on a smartphone screen. Although several of these applications exist, usability issues can discourage adoption. User-centred design (UCD) solicits frequent feedback, often contributing to usable products. While AR mock-ups have been constructed through UCD, we develop a novel and functional tourism app. We solicit requirements through a synthesis of domain analysis, tourist observation and semi-structured interviews. Through four rounds of iterative development, users test and refine the app. The final product, dubbed ToARist, is evaluated by 20 participants, who engage in a tourism task around a UK city. Users regard the system as usable, but find technical issues can disrupt AR. We finish by reflecting on our design and critiquing the challenges of a strict user-centred methodology.',\n",
       " 'A building design aiding tool for space allocation and thermal performance optimization is being developed to help practitioners during the building space planning phase, predicting how it will behave regarding energy consumption and thermal comfort. The tool evaluates, ranks, and optimizes generated floor plans according to thermal performance criteria, using the dynamic simulation program EnergyPlus. The tool is currently able to use a wide variety of EnergyPlus objects, allowing for various template and detailed HVAC, DHW, and thermal and electrical energy production systems and components, as well as numerous internal gains types, construction elements and energy saving controls, to be accounted for and simulated in the generated buildings. This paper presents the tool overall concept as well as the main features regarding dynamic simulation. Some performance results are presented for distinct systems to illustrate the use and potential of the tool.',\n",
       " 'This paper is the basis paper for the accepted IJCNN challenge One-Minute Gradual-Emotion Recognition (OMG-Emotion) by which we hope to foster long-emotion classification using neural models for the benefit of the IJCNN community. The proposed corpus has as the novelty the data collection and annotation strategy based on emotion expressions which evolve over time into a specific context. Different from other corpora, we propose a novel multimodal corpus for emotion expression recognition, which uses gradual annotations with a focus on contextual emotion expressions. Our dataset was collected from Youtube videos using a specific search strategy based on restricted keywords and filtering which guaranteed that the data follow a gradual emotion expression transition, i.e. emotion expressions evolve over time in a natural and continuous fashion. We also provide an experimental protocol and a series of unimodal baseline experiments which can be used to evaluate deep and recurrent neural models in a fair and standard manner.',\n",
       " 'Dynamic hand tracking and gesture recognition is a hard task since there are many joints on the fingers and each joint owns many degrees of freedom. Besides, object occlusion is also a thorny issue in finger tracking and posture recognition. Therefore, we propose a robust and customized system for realtime hand tracking and gesture recognition under occlusion environment. First, we model the angles between hand keypoints and encode their relative coordinate vectors, then we introduce GAN to generate raw discrete sequence dataset. Secondly we propose a time series forecasting method in the prediction of defined hand keypoint location. Finally, we define a sliding window matching method to complete gesture recognition. We analyze 11 kinds of typical gestures and show how to perform gesture recognition with the proposed method. Our work can reach state of the art results and contribute to build a framework to implement customized gesture recognition task.',\n",
       " '\"How common is interactive visualization on the web?\" \"What is the most popular visualization design?\" \"How prevalent are pie charts really?\" These questions intimate the role of interactive visualization in the real (online) world. In this paper, we present our approach (and findings) to answering these questions. First, we introduce Beagle, which mines the web for SVG-based visualizations and automatically classifies them by type (i.e., bar, pie, etc.). With Beagle, we extract over 41,000 visualizations across five different tools and repositories, and classify them with 86% accuracy, across 24 visualization types. Given this visualization collection, we study usage across tools. We find that most visualizations fall under four types: bar charts, line charts, scatter charts, and geographic maps. Though controversial, pie charts are relatively rare in practice. Our findings also indicate that users may prefer tools that emphasize a succinct set of visualization types, and provide diverse expert visualization examples.',\n",
       " 'This paper suggests that recent developments in video game technology have occurred in parallel to play being moved from public into private spaces, which has had impact on the way people interact with games. The paper also argues and that there is potentially value in the creation of public play spaces to create opportunities to utilise both technology and body for the benefit of community culture and experiences through gaming. Co-located social gaming coupled with tangible interfaces offer alternative possibilities for the local video game scene. This paper includes a descriptive account of Rabble Room Arcade, an experimental social event combining custom-built tangible interface devices and multiplayer video games. The event was designed around games that promoted a return to simplicity through the use of unique tangible controllers to allow casual gamers to connect to the game and to each other, whilst also transforming the event into a spectacle.',\n",
       " 'Thermal imaging-based physiological and affective computing is an emerging research area enabling technologies to monitor our bodily functions and understand psychological and affective needs in a contactless manner. However, up to recently, research has been mainly carried out in very controlled lab settings. As small size and even low-cost versions of thermal video cameras have started to appear on the market, mobile thermal imaging is opening its door to ubiquitous and real-world applications. Here we review the literature on the use of thermal imaging to track changes in physiological cues relevant to affective computing and the technological requirements set so far. In doing so, we aim to establish computational and methodological pipelines from thermal images of the human skin to affective states and outline the research opportunities and challenges to be tackled to make ubiquitous real-life thermal imaging-based affect monitoring a possibility.',\n",
       " 'Live animation of 2D characters has recently become a popular way for storytelling, and has potential application scenarios like tele-present agents or robots. As an extension of human-human communication, there is a need for augmenting the emotional communication experience of live animation. In this paper, we explore the emotional expressiveness issue of 2D live animation. In particular, we propose a descriptive emotion command model to bind a triggering action, the semantic meaning, psychology measurements, and behaviors of an emotional expression. Based on the model, we designed and implemented a proof-of-concept 2D live animation system, where a novel visual programming tool for editing the behaviors of 2D digital characters, and an emotion command recommendation algorithm are proposed. Through a user evaluation, we showcase the usability of our system and its potential for boosting creativity and enhancing the emotional communication experience.',\n",
       " 'Point sets in 2D with multiple classes are a common type of data. A canonical visualization design for them are scatterplots, which do not scale to large collections of points. For these larger data sets, binned aggregation (or binning) is often used to summarize the data, with many possible design alternatives for creating effective visual representations of these summaries. There are a wide range of designs to show summaries of 2D multi-class point data, each capable of supporting different analysis tasks. In this paper, we explore the space of visual designs for such data, and provide design guidelines for different analysis scenarios. To support these guidelines, we compile a set of abstract tasks and ground them in concrete examples using multiple sample datasets. We then assess designs, and survey a range of design decisions, considering their appropriateness to the tasks. In addition, we provide a web-based implementation to experiment with design choices, supporting the validation of designs based on task needs.',\n",
       " 'Developing cross-device multi-user interfaces (UIs) is a challenging problem. There are numerous ways in which content and interactivity can be distributed. However, good solutions must consider multiple users, their roles, their preferences and access rights, as well as device capabilities. Manual and rule-based solutions are tedious to create and do not scale to larger problems nor do they adapt to dynamic changes, such as users leaving or joining an activity. In this paper, we cast the problem of UI distribution as an assignment problem and propose to solve it using combinatorial optimization. We present a mixed integer programming formulation which allows real-time applications in dynamically changing collaborative settings. It optimizes the allocation of UI elements based on device capabilities, user roles, preferences, and access rights. We present a proof-of-concept designer-in-the-loop tool, allowing for quick solution exploration. Finally, we compare our approach to traditional paper prototyping in a lab study.',\n",
       " 'Most tabular data visualization techniques focus on overviews, yet many practical analysis tasks are concerned with investigating individual items of interest. At the same time, relating an item to the rest of a potentially large table is important. In this work we present Taggle, a tabular visualization technique for exploring and presenting large and complex tables. Taggle takes an item-centric,spreadsheet-like approach, visualizing each row in the source data individually using visual encodings for the cells. At the same time, Taggle introduces data-driven aggregation of data subsets. The aggregation strategy is complemented by interaction methods tailored to answer specific analysis questions, such as sorting based on multiple columns and rich data selection and filtering capabilities. We evaluate Taggle using a qualitative user study and a case study conducted by a domain expert on complex genomics data analysis for the purpose of drug discovery.',\n",
       " 'At present many blind assistive systems have been implemented but there is no such kind of good system to navigate a blind person and also to track the movement of a blind person and rescue him/her if he/she is lost. In this paper, we have presented a blind assistive and tracking embedded system. In this system the blind person is navigated through a spectacle interfaced with an android application. The blind person is guided through Bengali/English voice commands generated by the application according to the obstacle position. Using voice command a blind person can establish voice call to a predefined number without touching the phone just by pressing the headset button. The blind assistive application gets the latitude and longitude using GPS and then sends them to a server. The movement of the blind person is tracked through another android application that points out the current position in Google map. We took distances from several surfaces like concrete and tiles floor in our experiment where the error rate is 5%.',\n",
       " 'Today, in the digital age, the mobile devices are more and more used to aid people in the struggle to improve or maintain their health. In this paper, the mobile eHealth solution for remote patient monitoring during clinical trials is presented, together with the outcomes of quantitative and qualitative performance evaluation. The evaluation is a third step to improve the quality of the application after earlier Good Clinical Practice certification and validation with the participation of 10 patients and three general practitioners. This time, the focus was on the usability which was evaluated by the seventeen participants divided into three age groups (18-28, 29-50, and 50+). The results, from recorded sessions and the eye tracking, show that there is no difference in performance between the first group and the second group, while for the third group the performance was worse, however, it was still good enough to complete task within reasonable time.',\n",
       " 'To enhance the performance of affective models and reduce the cost of acquiring physiological signals for real-world applications, we adopt multimodal deep learning approach to construct affective models from multiple physiological signals. For unimodal enhancement task, we indicate that the best recognition accuracy of 82.11% on SEED dataset is achieved with shared representations generated by Deep AutoEncoder (DAE) model. For multimodal facilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE) achieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets, respectively, which are much superior to the state-of-the-art approaches. For cross-modal learning task, our experimental results demonstrate that the mean accuracy of 66.34% is achieved on SEED dataset through shared representations generated by EEG-based DAE as training samples and shared representations generated by eye-based DAE as testing sample, and vice versa.',\n",
       " 'Young people worldwide are participating in ever-increasing numbers in online fan communities. Far from mere shallow repositories of pop culture, these sites are accumulating significant evidence that sophisticated informal learning is taking place online in novel and unexpected ways. In order to understand and analyze in more detail how learning might be occurring, we conducted an in-depth nine-month ethnographic investigation of online fanfiction communities, including participant observation and fanfiction author interviews. Our observations led to the development of a theory we term distributed mentoring, which we present in detail in this paper. Distributed mentoring exemplifies one instance of how networked technology affords new extensions of behaviors that were previously bounded by time and space. Distributed mentoring holds potential for application beyond the spontaneous mentoring observed in this investigation and may help students receive diverse, thoughtful feedback in formal learning environments as well.',\n",
       " \"This is the preprint version of our paper on Personal and Ubiquitous Computing. There is an increasing interest in creating pervasive games based on emerging interaction technologies. In order to develop touch-less, interactive and augmented reality games on vision-based wearable device, a touch-less motion interaction technology is designed and evaluated in this work. Users interact with the augmented reality games with dynamic hands/feet gestures in front of the camera, which triggers the interaction event to interact with the virtual object in the scene. Three primitive augmented reality games with eleven dynamic gestures are developed based on the proposed touch-less interaction technology as proof. At last, a comparing evaluation is proposed to demonstrate the social acceptability and usability of the touch-less approach, running on a hybrid wearable framework or with Google Glass, as well as workload assessment, user's emotions and satisfaction.\",\n",
       " 'Virtual learning environments are a useful modality for engaging students in the classroom by affording them a sense of presence and immersion. The motivation of this project was to create an open-source augmented reality electrical circuit application for use in lower division engineering courses to teach students about electricity fundamentals. Softwares that are readily available for use on virtual and augmented reality devices do not typically apply to all disciplines and do not necessarily have a pedagogical or accessibility focus. Considering this lack of appropriate educational applications for the current virtual and augmented reality devices, a team of interdisciplinary students was assembled to create such software. With extensive usability studies, the application was designed for quick adoption and improve accessibility by providing multimodal access such as voice assistant, gray scaling for depth perception and daltonize the app. The software is available as part of VITaL Laboratory, Sonoma State University.',\n",
       " 'An attempt is made to develop a smart toy to help the children suffering with communication disorders. The children suffering with such disorders need additional attention and guidance to understand different types of social events and life activities. Various issues and features of the children with speech disorders are identified in this study and based on the inputs from the study, a working architecture is proposed with suitable policies. A prediction module with a checker component is designed in this work to produce alerts in at the time of abnormal behaviour of the child with communication disorder. The model is designed very sensitively to the behaviour of the child for a particular voice tone, based on which the smart toy will change to tones automatically. Such an arrangement proved to be helpful for the children to improve the communication with other due to the inclusion of continuous training for the smart toy from the prediction module.',\n",
       " 'Distributed systems technologies supporting 3D visualization and social collaboration will be increasing in frequency and type over time. An emerging type of head-mounted display referred to as the head-mounted projection display (HMPD) was recently developed that only requires ultralight optics (i.e., less than 8 g per eye) that enables immersive multiuser, mobile augmented reality 3D visualization, as well as remote 3D collaborations. In this paper a review of the development of lightweight HMPD technology is provided, together with insight into what makes this technology timely and so unique. Two novel emerging HMPD-based technologies are then described: a teleportal HMPD(T-HMPD) enabling face-to-face communication and visualization of shared 3D virtual objects, and a mobile HMPD (M-HMPD) designed for outdoor wearable visualization and communication. Finally, the use of HMPD in medical visualization and training, as well as in infospaces, two applications developed in the ODA and MIND labs respectively, are discussed.',\n",
       " \"Virtual Reality (VR) provides immersive experiences in the virtual world, but it may reduce users' awareness of physical surroundings and cause safety concerns and psychological discomfort. Hence, there is a need of an ambient information design to increase users' situational awareness (SA) of physical elements when they are immersed in VR environment. This is challenging, since there is a tradeoff between the awareness in reality and the interference with users' experience in virtuality. In this paper, we design five representations (indexical, symbolic, and iconic with three emotions) based on two dimensions (vividness and emotion) to address the problem. We conduct an empirical study to evaluate participants' SA, perceived breaks in presence (BIPs), and perceived engagement through VR tasks that require movement in space. Results show that designs with higher vividness evoke more SA, designs that are more consistent with the virtual environment can mitigate the BIP issue, and emotion-evoking designs are more engaging.\",\n",
       " 'How could we gather affect annotations in a rapid, unobtrusive, and accessible fashion? How could we still make sure that these annotations are reliable enough for data-hungry affect modelling methods? This paper addresses these questions by introducing PAGAN, an accessible, general-purpose, online platform for crowdsourcing affect labels in videos. The design of PAGAN overcomes the accessibility limitations of existing annotation tools, which often require advanced technical skills or even the on-site involvement of the researcher. Such limitations often yield affective corpora that are restricted in size, scope and use, as the applicability of modern data-demanding machine learning methods is rather limited. The description of PAGAN is accompanied by an exploratory study which compares the reliability of three continuous annotation tools currently supported by the platform. Our key results reveal higher inter-rater agreement when annotation traces are processed in a relative manner and collected via unbounded labelling.',\n",
       " 'Animated GIFs are increasingly popular in text-based communication. Finding the perfect GIF can make conversations funny, interesting, and engaging, but GIFs also introduce potentials for miscommunication. Through 24 in-depth qualitative interviews, this empirical, exploratory study examines the nuances of communication practices with animated GIFs to better understand why and how GIFs can send unintentional messages. We find participants leverage contexts like source material and interpersonal relationship to find the perfect GIFs for different communication scenarios, while these contexts are also the primary reason for miscommunication and some technical usability issues in GIFs. This paper concludes with a discussion of the important role that different types of context play in the use and interpretations of GIFs, and argues that nonverbal communication tools should account for complex contexts and common ground that communication media rely on.',\n",
       " 'Socially Assistive Robots (SARs) offer great promise for improving outcomes in paediatric rehabilitation. However, the design of software and interactive capabilities for SARs must be carefully considered in the context of their intended clinical use. While previous work has explored specific roles and functionalities to support paediatric rehabilitation, few have considered the design of such capabilities in the context of ongoing clinical deployment. In this paper we present a two-phase In-situ design process for SARs in health care, emphasising stakeholder engagement and on-site development. We explore this in the context of developing the humanoid social robot NAO as a socially assistive rehabilitation aid for children with cerebral palsy. We present and evaluate our design process, outcomes achieved, and preliminary results from ongoing clinical testing with 9 patients and 5 therapists over 14 sessions. We argue that our in-situ Design methodology has been central to the rapid and successful deployment of our system.',\n",
       " 'The aim of this research is development of rule based decision model for emotion recognition. This research also proposes using the rules for augmenting inter-corporal recognition accuracy in multimodal systems that use supervised learning techniques. The classifiers for such learning based recognition systems are susceptible to over fitting and only perform well on intra-corporal data. To overcome the limitation this research proposes using rule based model as an additional modality. The rules were developed using raw feature data from visual channel, based on human annotator agreement and existing studies that have attributed movement and postures to emotions. The outcome of the rule evaluations was combined during the decision phase of emotion recognition system. The results indicate rule based emotion recognition augment recognition accuracy of learning based systems and also provide better recognition rate across inter corpus emotion test data.',\n",
       " 'Crowdsourcing has revolutionized the process of knowledge building on the web. Wikipedia and StackOverflow are witness to this uprising development. However, the dynamics behind the process of crowdsourcing in the domain of knowledge building is an area relatively unexplored. It has been observed that an ecosystem exists in the collaborative knowledge building environments (KBE), which puts users of a KBE into various categories based on their expertise. Classical cognitive theories indicate triggering among the knowledge units to be one of the most important reasons behind accelerated knowledge building in collaborative KBEs. We use the concept of ecosystem and the triggering phenomenon to highlight the necessity for the right mix of users in a KBE. We provide a hill climbing based algorithm which gives the ideal mixture of users in a KBE, given the amount of triggering that takes place among the users of various categories. The study will help the portal designers to accordingly build suitable crowdsourced environments.',\n",
       " \"Location sensing is a key enabling technology for Ubicomp to support contextual interaction. However, the laboratories where calibrated testing of location technologies is done are very different to the domestic situations where `context' is a problematic social construct. This study reports measurements of Bluetooth beacons, informed by laboratory studies, but done in diverse domestic settings. The design of these surveys has been motivated by the natural environment implied in the Bluetooth beacon standards - relating the technical environment of the beacon to the function of spaces within the home. This research method can be considered as a situated, `ethnographic' technical response to the study of physical infrastructure that arises through social processes. The results offer insights for the future design of `seamful' approaches to indoor location sensing, and to the ways that context might be constructed and interpreted in a seamful manner.\",\n",
       " 'Information visualization limits itself, per definition, to the domain of symbolic information. This paper discusses arguments why the field should also consider forms of data that are not symbolically encoded, including physical traces and material indicators. Continuing a provocation presented by Pat Hanrahan in his 2004 IEEE Vis capstone address, this paper compares physical traces to visualizations and describes the techniques and visual practices for producing, revealing, and interpreting them. By contrasting information visualization with a speculative counter model of autographic visualization, this paper examines the design principles for material data. Autographic visualization addresses limitations of information visualization, such as the inability to directly reflect the material circumstances of data generation. The comparison between the two models allows probing the epistemic assumptions behind information visualization and uncovers linkages with the rich history of scientific visualization and trace reading.',\n",
       " 'Landmark-based navigation systems have proven benefits relative to traditional turn-by-turn systems that use street names and distances. However, one obstacle to the implementation of landmark-based navigation systems is the complex challenge of selecting salient local landmarks at each decision point for each user. In this paper, we present Pharos, a novel system that extends turn-by-turn navigation instructions using a single global landmark (e.g. the Eiffel Tower, the Burj Khalifa, municipal TV towers) rather than multiple, hard-to-select local landmarks. We first show that our approach is feasible in a large number of cities around the world through the use of computer vision to select global landmarks. We then present the results of a study demonstrating that by including global landmarks in navigation instructions, users navigate more confidently and build a more accurate mental map of the navigated area than using turn-by-turn instructions.',\n",
       " 'We study the performance of brain computer interface (BCI) system in a virtual reality (VR) environment and compare it to 2D regular displays. First, we design a headset that consists of three components: a wearable electroencephalography (EEG) device, a VR headset and an interface. Recordings of brain and behavior from human subjects, performing a wide variety of tasks using our device are collected. The tasks consist of object rotation or scaling in VR using either mental commands or facial expression (smile and eyebrow movement). Subjects are asked to repeat similar tasks on regular 2D monitor screens. The performance in 3-D virtual reality environment is considerably higher compared to the to the 2D screen. Particularly, the median number of success rate across trials for VR setting is double of that for the 2D setting (8 successful command in VR setting compared to 4 successful command in 2D screen in 1 minute trials). Our results suggest that the design of future BCI systems can remarkably benefit from the VR setting.',\n",
       " \"Mental State Transition Network (MSTN) is a basic concept of approximating to human psychological and mental responses. A stimulus calculated by Emotion Generating Calculations (EGC) method can cause the transition of mood from an emotional state to others. In this paper, the agent can interact with human to realize smooth communication by an adaptive learning method of the user's personality trait based mood. The learning method consists of the profit sharing (PS) method and the recurrent neural network (RNN). An emotion for sensor inputs to MSTN is calculated by EGC and the variance of emotion leads to the change of mental state, and then the sequence of states forms an episode. In order to learn the tendency of personality trait effectively, the ineffective rules should be removed from the episode. PS method finds out a detour in episode and should be deleted. Furthermore, RNN works to realize the variance of user's mood. Some experimental results were shown the success of representing a various human's delicate emotion.\",\n",
       " 'Users\\' persistent social media contents like posts on Facebook Timeline are presented as an \"exhibition\" about the person to others, and managing these exhibitional contents for impression management needs intentional and manual efforts. To raise awareness of and facilitate impression management around past contents, we developed a prototype called PersonalityInsight. The system employs computational psycho-linguistic analysis to help users visualize the way their past text posts might convey impressions of their personality and allowed users to modify their posts based on these visualizations. We conducted a user study to evaluate the design; users overall found that such a tool raised awareness of the fact and the ways personality might be conveyed through their past content as one aspect of impression management, but that it needs design improvement to offer action-able suggestions for content modification, as well as careful thinking about impression management as one of many values people have about their digital past.',\n",
       " 'Usability is a key quality attribute of successful software systems. Unfortunately, there is no common understanding of the factors influencing usability and their interrelations. Hence, the lack of a comprehensive basis for designing, analyzing, and improving user interfaces. This paper proposes a 2-dimensional model of usability that associates system properties with the activities carried out by the user. By separating activities and properties, sound quality criteria can be identified, thus facilitating statements concerning their interdependencies. This model is based on a tested quality meta-model that fosters preciseness and completeness. A case study demonstrates the manner by which such a model aids in revealing contradictions and omissions in existing usability standards. Furthermore, the model serves as a central and structured knowledge base for the entire quality assurance process, e.g. the automatic generation of guideline documents.',\n",
       " \"This study describes a detailed analysis of museum visitors' decoding process as they used a visualization designed to support exploration of a large, complex dataset. Quantitative and qualitative analyses revealed that it took, on average, 43 seconds for visitors to decode enough of the visualization to see patterns and relationships in the underlying data represented, and 54 seconds to arrive at their first correct data interpretation. Furthermore, visitors decoded throughout and not only upon initial use of the visualization. The study analyzed think-aloud data to identify issues visitors had mapping the visual representations to their intended referents, examine why they occurred, and consider if and how these decoding issues were resolved. The paper also describes how multiple visual encodings both helped and hindered decoding and concludes with implications on the design and adaptation of visualizations for informal science learning venues.\",\n",
       " 'Quality of Experience (QoE) typically involves conducting experiments in which stimuli are presented to participants and their judgments as well as behavioral data are collected. Nowadays, many experiments require software for the presentation of stimuli and the data collection from participants. While different software solutions exist, these are not tailored to conduct experiments on QoE. Moreover, replicating experiments or repeating the same experiment in different settings (e. g., laboratory vs. crowdsourcing) can further increase the software complexity. TheFragebogen is an open-source, versatile, extendable software framework for the implementation of questionnaires - especially for research on QoE. Implemented questionnaires can be presented with a state-of-the-art web browser to support a broad range of devices while the use of a web server being optional. Out-of-the-box, TheFragebogen provides graphical exact scales as well as free-hand input, the ability to collect behavioral data, and playback multimedia content.',\n",
       " \"With eye tracking being increasingly integrated into virtual and augmented reality (VR/AR) head-mounted displays, preserving users' privacy is an ever more important, yet under-explored, topic in the eye tracking community. We report a large-scale online survey (N=124) on privacy aspects of eye tracking that provides the first comprehensive account of with whom, for which services, and to what extent users are willing to share their gaze data. Using these insights, we design a privacy-aware VR interface that uses differential privacy, which we evaluate on a new 20-participant dataset for two privacy sensitive tasks: We show that our method can prevent user re-identification and protect gender information while maintaining high performance for gaze-based document type classification. Our results highlight the privacy challenges particular to gaze data and demonstrate that differential privacy is a potential means to address them. Thus, this paper lays important foundations for future research on privacy-aware gaze interfaces.\",\n",
       " 'Social media platforms such as Twitter are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve and evade traditional detection techniques. In this work, we propose VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling techniques, which offer new insights that enable the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We conducted a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool.',\n",
       " 'Large-scale labeled datasets are the indispensable fuel that ignites the AI revolution as we see today. Most such datasets are constructed using crowdsourcing services such as Amazon Mechanical Turk which provides noisy labels from non-experts at a fair price. The sheer size of such datasets mandates that it is only feasible to collect a few labels per data point. We formulate the problem of test-time label aggregation as a statistical estimation problem of inferring the expected voting score in an ideal world where all workers label all items. By imitating workers with supervised learners and using them in a doubly robust estimation framework, we prove that the variance of estimation can be substantially reduced, even if the learner is a poor approximation. Synthetic and real-world experiments show that by combining the doubly robust approach with adaptive worker/item selection, we often need as low as 0.1 labels per data point to achieve nearly the same accuracy as in the ideal world where all workers label all data points.',\n",
       " \"Near distances are overestimated in virtual reality, and far distances are underestimated, but an explanation for these distortions remains elusive. One potential concern is that whilst the eye rotates to look at the virtual scene, the virtual cameras remain static. Could using eye-tracking to change the perspective of the virtual cameras as the eye rotates improve depth perception in virtual reality? This paper identifies 14 distinct perspective distortions that could in theory occur from keeping the virtual cameras fixed whilst the eye rotates in the context of near-eye displays. However, the impact of eye movements on the displayed image depends on the optical, rather than physical, distance of the display. Since the optical distance of most head-mounted displays is over 1m, most of these distortions will have only a negligible effect. The exception are 'gaze-contingent disparities', which will leave near virtual objects looking displaced from physical objects that are meant to be at the same distance in augmented reality.\",\n",
       " \"Physical agents that can autonomously generate engaging, life-like behaviour will lead to more responsive and interesting robots and other autonomous systems. Although many advances have been made for one-to-one interactions in well controlled settings, future physical agents should be capable of interacting with humans in natural settings, including group interaction. In order to generate engaging behaviours, the autonomous system must first be able to estimate its human partners' engagement level. In this paper, we propose an approach for estimating engagement from behaviour and use the measure within a reinforcement learning framework to learn engaging interactive behaviours. The proposed approach is implemented in an interactive sculptural system in a museum setting. We compare the learning system to a baseline using pre-scripted interactive behaviours. Analysis based on sensory data and survey data shows that adaptable behaviours within a perceivable and understandable range can achieve higher engagement and likeability.\",\n",
       " 'Due to the increasing number of mobile robots including domestic robots for cleaning and maintenance in developed countries, human activity recognition is inevitable for congruent human-robot interaction. Needless to say that this is indeed a challenging task for robots, it is expedient to learn human activities for autonomous mobile robots (AMR) for navigating in an uncontrolled environment without any guidance. Building a correct classifier for complex human action is non-trivial since simple actions can be combined to recognize a complex human activity. In this paper, we trained a model for human activity recognition using convolutional neural network. We trained and validated the model using the Vicon physical action dataset and also tested the model on our generated dataset (VMCUHK). Our experiment shows that our method performs with high accuracy, human activity recognition task both on the Vicon physical action dataset and VMCUHK dataset.',\n",
       " 'The Unified Modeling Language (UML) is a widely used general purpose modeling language. Together with the Object Constraint Language (OCL), formal models can be described by defining the structure and behavior with UML and additional OCL constraints. In the development process for formal models, it is important to make sure that these models are (a) correct, i.e. consistent and complete, and (b) testable in the sense that the developer is able to interactively check model properties. The USE tool (UML-based Specification Environment) allows both characteristics to be studied. We demonstrate how the tool supports modelers to analyze, validate and verify UML and OCL models via the use of several graphical means that assist the modeler in interpreting and visualizing formal model descriptions. In particular, we discuss how the so-called USE model validator plugin is integrated into the USE environment in order to allow non domain experts to use it and construct object models that help to verify properties like model consistency.',\n",
       " 'Drawing principles, or aesthetics, are important in graph drawing. They are used as criteria for algorithm design and for quality evaluation. Current aesthetics are described as visual properties that a drawing is required to have to be visually pleasing. However, most of these aesthetics are originally proposed without consideration of graph structure information. Therefore their ability in visually revealing graph structural features are not guaranteed and indeed mixed results have been reported in the literature regarding their impact on user graph comprehension. In this paper, we propose to derive aesthetics based on graph internal structural features. Further, graphs are often evaluated based on controlled experiments with simple perception tasks to avoid possible confounding factors caused by complex tasks. This leaves their value in supporting complex tasks unevaluated. To fill this gap, we also discuss the possibility of applying evaluation methodologies used in the Cognitive Load Theory research for graph evaluation.',\n",
       " 'We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.',\n",
       " 'We present some results concerning the dialogue behavior and inferred sentiment of a group of older adults interacting with a computer-based avatar. Our avatar is unique in its ability to hold natural dialogues on a wide range of everyday topics---27 topics in three groups, developed with the help of gerontologists. The three groups vary in ``degrees of intimacy\", and as such in degrees of difficulty for the user. Each participant interacted with the avatar for 7-9 sessions over a period of 3-4 weeks; analysis of the dialogues reveals correlations such as greater verbosity for more difficult topics, increasing verbosity with successive sessions, especially for more difficult topics, stronger sentiment on topics concerned with life goals rather than routine activities, and stronger self-disclosure for more intimate topics. In addition to their intrinsic interest, these results also reflect positively on the sophistication of our dialogue system.',\n",
       " 'Regional Intergovernmental Organizations (RIGOs) are constituted by the local governments within their respective regions and are supported by the active engagement of the regions community and citizens. Metropolitan Statistical Areas (MSAs), on the other hand, are classified by the federal government based on commuting and commerce patterns. They do not adhere to any local government. The Graduate School of Policy and International Affairs Center for Metropolitan Studies (GSPIA) at the University of Pittsburgh have been researching the boundaries of RIGOs and the characteristics defining them. In this paper, we propose, design, and implement an approach to enhance the current visualization by visualizing two categorical data: RIGOs and MSAs and the overlapping between them. We attempted to use a combination of visual attributes that leverage human perception system and do not impose cognitive and mental effort. The overall result of the evaluation shows that our work proved to be more effective than the current visualization.',\n",
       " \"By collecting the data of eyeball movement of pilots, it is possible to monitor pilot's operation in the future flight in order to detect potential accidents. In this paper, we designed a novel SVS system that is integrated with an eye tracking device, and is able to achieve the following functions:1) A novel method that is able to learn from the eyeball movements of pilots and preload or render the terrain data in various resolutions, in order to improve the quality of terrain display by comprehending the interested regions of the pilot. 2) A warning mechanism that may detect the risky operation via analyzing the aviation information from the SVS and the eyeball movement from the eye tracking device, in order to prevent the maloperations or human factor accidents. The user study and experiments show that the proposed SVS-Eyetracking system works efficiently and is capable of avoiding potential risked caused by fatigue in the flight simulation.\",\n",
       " 'In this position paper we describe select aspects of our experience with health-related self-tracking, the data generated, and processes surrounding those. In particular we focus on how bilateral patient-clinician engagement may be fostered by the combination of technology and method. We exemplify with a case study where a PTSD-suffering veteran has been self-tracking a specific symptom precursor. The availability of high-resolution self-tracking data on the occurrences of even a single symptom created new opportunities in the therapeutic process for identifying underlying triggers of symptoms. The patient was highly engaged in self-tracking and sharing the collected data. We suggest a key reason was the collaborative effort in defining the data collection protocol and discussion of the data. The therapist also engaged highly in the self-tracking data, as it supported the existing therapeutic process in reaching insights otherwise unobtainable.',\n",
       " \"This paper studies the use of eye tracking in a First-Person Shooter (FPS) game as a~mechanism to: (1) control the attention of the player's avatar according to the attention deployed by the player, and (2) guide the gameplay and game's procedural content generation, accordingly. This results in a more natural use of eye tracking in comparison to a use in which the eye tracker directly substitutes control input devices, such as gamepads. The study was conducted on a custom endless runner FPS, Zombie Runner, using an affordable eye tracker. Evaluation sessions showed that the proposed use of eye tracking provides a more challenging and immersive experience to the player, when compared to its absence. However, a strong correlation between eye tracker calibration problems and player's overall experience was found. This means that eye tracking technology still needs to evolve but also means that once technology gets mature enough players are expected to benefit greatly from the inclusion of eye tracking in their gaming experience.\",\n",
       " \"Microtask crowdsourcing has enabled dataset advances in social science and machine learning, but existing crowdsourcing schemes are too expensive to scale up with the expanding volume of data. To scale and widen the applicability of crowdsourcing, we present a technique that produces extremely rapid judgments for binary and categorical labels. Rather than punishing all errors, which causes workers to proceed slowly and deliberately, our technique speeds up workers' judgments to the point where errors are acceptable and even expected. We demonstrate that it is possible to rectify these errors by randomizing task order and modeling response latency. We evaluate our technique on a breadth of common labeling tasks such as image verification, word similarity, sentiment analysis and topic classification. Where prior work typically achieves a 0.25x to 1x speedup over fixed majority vote, our approach often achieves an order of magnitude (10x) speedup.\",\n",
       " 'User Experience (UX) has been a buzzword in agile literature in recent years. However, often UX remains as a vague concept and it may be hard to understand the very nature of it in the context of agile software development. This paper explores the multifaceted UX literature, emphasizes the multi-dimensional nature of the concept and organizes the current state-of-the-art knowledge. As a starting point to better understand the contemporary meaning of UX assigned by practitioners, we selected four UX blogs and performed an analysis using a framework derived from the literature review. The preliminary results show that the practitioners more often focus on interaction between product and user and view UX from design perspective predominantly. While the economical perspective receives little attention in literature, it is evident in practitioners writings. Our study opens up a promising line of request of the contemporary meaning of UX in practice.',\n",
       " \"In this paper, we explore the potential impact of Internet of Things (IoT) technology may have on the cosplay community. We developed a costume (an IoT Skullfort) and embedded IoT technology to enhance its capabilities and user interactions. Sensing technologies are widely used in many different wearable domains including cosplay scenarios. However, in most of these scenarios, typical interaction pattern is that the costume responds to its environment or the player's behaviour (e.g., colour of lights may get changed when player moves hands). In contrast, our research focuses on exploring scenarios where the audience (third party) get to manipulate the costume behaviour (e.g., the audience get to change the colour of the Skullfort using a mobile application). We believe such an audience (third party) influenced cosplay brings new opportunities for enhanced entertainment. However, it also creates significant challenges. We report the results gathered through a focus group conducted in collaboration with cosplay community experts.\",\n",
       " \"As the capability and complexity of UAVs continue to increase, the human-robot interface community has a responsibility to design better ways of specifying the complex 3D flight paths necessary for instructing them. Immersive interfaces, such as those afforded by virtual reality (VR), have several unique traits which may improve the user's ability to perceive and specify 3D information. These traits include stereoscopic depth cues which induce a sense of physical space as well as six degrees of freedom (DoF) natural head-pose and gesture interactions. This work introduces an open-source platform for 3D aerial path planning in VR and compares it to existing UAV piloting interfaces. Our study has found statistically significant improvements in safety and subjective usability over a manual control interface, while achieving a statistically significant efficiency improvement over a 2D touchscreen interface. The results illustrate that immersive interfaces provide a viable alternative to touchscreen interfaces for UAV path planning.\",\n",
       " \"Mobile Learning Games (MLGs) show great potential for increasing engagement, creativity and authentic learning. Yet, despite their great potential for education, the use of MLGs by teachers, remains limited. This is partly due to the fact that MLGs are often designed to match a specific learning context, and thus cannot be reusable for other contexts. Therefore, researchers have recently designed various types of MLG authoring tools. However, these authoring tools are not always adapted to non-computer-scientists or non-game-designers. Hence, we propose in this paper to focus on five existing MLG authoring tools, in order to assess their features and usability with the help of five teachers, who are used to organizing educational field trips. In the second part of this paper, we present an approach for designing a MLG authoring tool, based on the lacks identified through the analysis, and tailored to the teachers' different profiles and needs.\",\n",
       " 'Individual thermal comfort perception gives important feedback signals for energy efficient control of smart buildings. However, there is no effective method to measure real-time thermal comfort status of individual occupant until now. For overcoming this challenge, a novel macro posed based non-invasive perception method for thermal comfort (NIMAP) was presented. The occupant pose images were captured by normal phone camera (computer or cell phone) and the corresponding 2D coordinates can be obtained. Based on this, a novel pose recognition algorithm for thermal comfort, including 12 sub-algorithms, was presented. The 12 thermal comfort related macro poses can be recognized. Further, based on Fanger theory, 369 subjects were invited for subjective questionnaire survey. 3 human occupants participated in the validation of the proposed method and massive data were collected. All the 12 thermal comfort related poses can be recognized effectively.',\n",
       " 'Brain Computer Interfaces (BCIs) based on visual evoked potentials (VEP) allow for spelling from a keyboard of flashing characters. Among VEP BCIs, code-modulated visual evoked potentials (c-VEPs) are designed for high-speed communication . In c-VEPs, all characters flash simultaneously. In particular, each character flashes according to a predefined 63-bit binary sequence (m-sequence), circular-shifted by a different time lag. For a given character, the m-sequence evokes a VEP in the electroencephalogram (EEG) of the subject, which can be used as a template. This template is obtained during a calibration phase at the beginning of each session. Then, the system outputs the desired character after a predefined number of repetitions by estimating its time lag with respect to the template. Our work avoids the calibration phase, by extracting from the VEP relative lags between successive characters, and predicting the full word using a dictionary.',\n",
       " 'One of the big restrictions in brain computer interface field is the very limited training samples, it is difficult to build a reliable and usable system with such limited data. Inspired by generative adversarial networks, we propose a conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks method to generate more artificial EEG signal automatically for data augmentation to improve the performance of convolutional neural networks in brain computer interface field and overcome the small training dataset problems. We evaluate the proposed cDCGAN method on BCI competition dataset of motor imagery. The results show that the generated artificial EEG data from Gaussian noise can learn the features from raw EEG data and has no less than the classification accuracy of raw EEG data in the testing dataset. Also by using generated artificial data can effectively improve classification accuracy at the same model with limited training data.',\n",
       " 'Developing information technology to democratize scientific knowledge and support citizen empowerment is a challenging task. In our case, a local community suffered from air pollution caused by industrial activity. The residents lacked the technological fluency to gather and curate diverse scientific data to advocate for regulatory change. We collaborated with the community in developing an air quality monitoring system which integrated heterogeneous data over a large spatial and temporal scale. The system afforded strong scientific evidence by using animated smoke images, air quality data, crowdsourced smell reports, and wind data. In our evaluation, we report patterns of sharing smoke images among stakeholders. Our survey study shows that the scientific knowledge provided by the system encourages agonistic discussions with regulators, empowers the community to support policy making, and rebalances the power relationship between stakeholders.',\n",
       " \"Assessing the influence of a scholar's work is an important task for funding organizations, academic departments, and researchers. Common methods, such as measures of citation counts, can ignore much of the nuance and multidimensionality of scholarly influence. We present an approach for generating dynamic visualizations of scholars' careers. This approach uses an animated node-link diagram showing the citation network accumulated around the researcher over the course of the career in concert with key indicators, highlighting influence both within and across fields. We developed our design in collaboration with one funding organization---the Pew Biomedical Scholars program---but the methods are generalizable to visualizations of scholarly influence. We applied the design method to the Microsoft Academic Graph, which includes more than 120 million publications. We validate our abstractions throughout the process through collaboration with the Pew Biomedical Scholars program officers and summative evaluations with their scholars.\",\n",
       " 'BRICKxAR is an Augmented Reality-based construction method applied to LEGO as a case study. With BRICKxAR, real LEGO brick construction is guided by virtual bricks in the right place at the right time, step by step. Virtual and real object occlusions are implemented to enable a natural appearance of virtual bricks inside real bricks. LEGO players\\' hand detection and occlusion are realized to allow a realistic immersive AR experience, in which virtual bricks can be \"grasped\" by the real hand, facilitating hand-eye coordination in AR. High accuracy and high precision of registration, i.e. the virtual - physical model alignment, are achieved. In the best case, the average error of registration is less than 1mm throughout the model. BRICKxAR is expected to enhance Learning Through Play, as well as assembly and construction by human workers. In the experiment, LEGO Arc de Triomphe is built completely with BRICKxAR, without the instruction booklet.',\n",
       " 'As various driving automation system (DAS) are commonly used in the vehicle, the over-trust in the DAS may put the driver in the risk. In order to prevent the over-trust while driving, the trust state of the driver should be recognized. However, description variables of the trust state are not distinct. This paper assumed that the outward expressions of a driver can represent the trust state of him/her-self. The explicit behaviors when driving with DAS is seen as those outward expressions. In the experiment, a driving simulator with a driver monitoring system was used for simulating a vehicle with the adaptive cruise control (ACC) and observing the motion information of the driver. Results show that if the driver completely trusted in the ACC, then 1) the participants were likely to put their feet far away from the pedals; 2) the operational intervention of the driver will delay in dangerous situations. In the future, a machine learning model will be tried to predict the trust state by using the motion information of the driver.',\n",
       " \"The recent growth of sophisticated digital gaming technologies has spawned an \\\\$8.1B industry around using these games for pedagogical purposes. Though Digital Game-Based Learning Systems have been adopted by industries ranging from military to medical applications, these systems continue to rely on traditional measures of explicit interactions to gauge player performance which can be subject to guessing and other factors unrelated to actual performance. This study presents a novel implicit eye-tracking based metric for digital game-based learning environments. The proposed metric introduces a weighted eye-tracking measure of traditional in-game scoring to consider the mental schema of a player's decision making. In order to validate the efficacy of this metric, we conducted an experiment with 25 participants playing a game designed to evaluate Chinese cultural competency and communication. This experiment showed strong correlation between the novel eye-tracking performance metric and traditional measures of in-game performance.\",\n",
       " 'Audio description, a form of trans-modal media translation, allows people who are blind or visually impaired access to visually-oriented, socio-cultural, or historical public discourse alike. Although audio description has gained more prominence in media policy and research lately, it rarely has been studied empirically. Yet this paper presents quantitative and qualitative survey data on its challenges and opportunities, through the analysis of responses from 483 participants in a national sample, with 334 of these respondents being blind. Our results give insight into audio description use in broadcast TV, streaming services, for physical media, such as DVDs, and in movie theaters. We further discover a multiplicity of barriers and hindrances which prevent a better adoption and larger proliferation of audio description. In our discussion, we present a possible answer to these problems - the UniDescription Project - a media ecosystem for the creation, curation, and dissemination of audio description for multiple media platforms.',\n",
       " 'Static visualizations have analytic and expressive value. However, many interactive tasks cannot be completed using static visualizations. As datasets grow in size and complexity, static visualizations start losing their analytic and expressive power for interactive data exploration. Despite this limitation of static visualizations, there are still many cases where visualizations are limited to being static (e.g., visualizations on presentation slides or posters). We believe in many of these cases, static visualizations will benefit from allowing users to perform interactive tasks on them. Inspired by the introduction of numerous commercial personal augmented reality (AR) devices, we propose an AR solution that allows interactive data exploration of datasets on static visualizations. In particular, we present a prototype system named VisAR that uses the Microsoft Hololens to enable users to complete interactive tasks on static visualizations.',\n",
       " \"Current technologies have enabled us to track and quantify our physical state and behavior. Self-tracking aims to achieve increased awareness to decrease undesired behaviors and lead to a healthier lifestyle. However, inappropriately communicated self-tracking results might cause the opposite effect. In this work, we propose a subtle self-tracking feedback by mirroring the self's state into an artificial agent. By eliciting empathy towards the artificial agent and fostering helping behaviors, users would help themselves as well. Finally, we reflected on the implications of this design framework, and the methodology to design and implement it. A series of interviews to expert designers pointed out to the importance of having multidisciplinary teams working in parallel. Moreover, an agile methodology with a sprint zero for the initial design, and shifted user research, design, and implementation sprints were proposed. Similar systems with data flow and hardware dependencies would also benefit from the proposed agile design process.\",\n",
       " \"Online writers and journalism media are increasingly combining visualization (and other multimedia content) with narrative text to create narrative visualizations. Often, however, the two elements are presented independently of one another. We propose an approach to automatically integrate text and visualization elements. We begin with a writer's narrative that presumably can be supported with visual data evidence. We leverage natural language processing, quantitative narrative analysis, and information visualization to (1) automatically extract narrative components (who, what, when, where) from data-rich stories, and (2) integrate the supporting data evidence with the text to develop a narrative visualization. We also employ bidirectional interaction from text to visualization and visualization to text to support reader exploration in both directions. We demonstrate the approach with a case study in the data-rich field of sports journalism.\",\n",
       " \"Engagement in educational games, a recently popular academic topic, has been shown to increase learning performance, as well as a number of attitudinal factors, such as intrinsic interest and motivation. However, there is a lack of research on how games can be designed to promote engagement. This mixed methods case study aimed to discover effective game elements for promoting 17-18 year old high school students' engagement with an educational game. Using within-case and cross-case analyses and triangulated data, 10 elements emerged and were categorized into the constructs of story, gameplay, and atmosphere. Examples and connections to the literature for each element are reported. Findings implicate that educational game design for both learning and engagement is composed of educational-game specific elements, game design for solely engagement is similar for both educational and entertainment games, and a gap on educational game design technique instead of theory should be addressed to further benefit educational game development.\",\n",
       " 'We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.',\n",
       " 'Intelligent personal assistants (IPAs) are supposed to help us multitask. Yet the impact of IPA use on multitasking is not clearly quantified, particularly in situations where primary tasks are also language based. Using a dual task paradigm, our study observes how IPA interactions impact two different types of writing primary tasks; copying and generating content. We found writing tasks that involve content generation, which are more cognitively demanding and share more of the resources needed for IPA use, are significantly more disrupted by IPA interaction than less demanding tasks such as copying content. We discuss how theories of cognitive resources, including multiple resource theory and working memory, explain these results. We also outline the need for future work how interruption length and relevance may impact primary task performance as well as the need to identify effects of interruption timing in user and IPA led interruptions.',\n",
       " 'Designing for situational awareness could lead to better solutions for disabled people, likewise, exploring the needs of disabled people could lead to innovations that can address situational impairments. This in turn can create non-stigmatising assistive technology for disabled people from which eventually everyone could benefit. In this paper, we investigate the potential for advanced haptics to compliment the graphical user interface of mobile devices, thereby enhancing user experiences of all people in some situations (e.g. sunlight interfering with interaction) and visually impaired people. We explore technical solutions to this problem space and demonstrate our justification for a focus on the creation of kinaesthetic force feedback. We propose initial design concepts and studies, with a view to co-create delightful and expressive haptic interactions with potential users motivated by scenarios of situational and permanent impairments.',\n",
       " \"Engaging residential communities with each other and with management remains a challenge. Housing providers deploy a variety of engagement strategies, some of which are supported by digital technologies. Their individual success is varied and integrated, multipronged approaches are seen to be more successful. As part of those, it is important to address people's perceptions of community and places, as well as any practical issues that they face. We present the design and evaluation of Photo Screen, a situated, public photo taking and viewing screen which was deployed in the context of a new flagship housing estate as part of a range of community engagement measures. In a new context, we confirm the high levels of engagement that can be achieved with this simple mechanism. We propose that photo 'tagging' might offer a second-stage engagement mechanism and enable meaningful dialogue between residents and management. Finally, we discuss how this playful activity allowed residents to positively shape the perception of their community.\",\n",
       " \"Determining and identifying opportune moments for interruptions is a challenging task in Ubiquitous Computing and Human-Computer-Interaction. The current state-of-the-art approaches do this by identifying breakpoints either in user tasks, activities or by processing social relationships and contents of interruptions. However, from a psychological perspective, not all of these breakpoints represent opportune moments for interruptions. In this paper, we propose a new concept in the field of interruptibility. The concept is based on role theory and psychological interruption research. In particular, we argue that social roles which define sets of norms, expectations, rules and behaviours can provide useful information about the user's current context that can be used to enhance interruption management systems. Based on this concept, we propose a prototype system architecture that uses social roles to detect opportune moments for interruptions.\",\n",
       " \"Situation awareness (SA) is an important constituent in human information processing and essential in pilots' decision-making processes. Acquiring and maintaining appropriate levels of SA is critical in aviation environments as it affects all decisions and actions taking place in flights and air traffic control. This paper provides an overview of recent measurement models and approaches to establishing and enhancing SA in aviation environments. Many aspects of SA are examined including the classification of SA techniques into six categories, and different theoretical SA models from individual, to shared or team, and to distributed or system levels. Quantitative and qualitative perspectives pertaining to SA methods and issues of SA for unmanned vehicles are also addressed. Furthermore, future research directions regarding SA assessment approaches are raised to deal with shortcomings of the existing state-of-the-art methods in the literature.\",\n",
       " 'Recently, increasing attention has been directed to the study of the speech emotion recognition, in which global acoustic features of an utterance are mostly used to eliminate the content differences. However, the expression of speech emotion is a dynamic process, which is reflected through dynamic durations, energies, and some other prosodic information when one speaks. In this paper, a novel local dynamic pitch probability distribution feature, which is obtained by drawing the histogram, is proposed to improve the accuracy of speech emotion recognition. Compared with most of the previous works using global features, the proposed method takes advantage of the local dynamic information conveyed by the emotional speech. Several experiments on Berlin Database of Emotional Speech are conducted to verify the effectiveness of the proposed method. The experimental results demonstrate that the local dynamic information obtained with the proposed method is more effective for speech emotion recognition than the traditional global features.',\n",
       " 'Understanding human mobility is important for the development of intelligent mobile service robots as it can provide prior knowledge and predictions of human distribution for robot-assisted activities. In this paper, we propose a probabilistic method to model human motion behaviors which is determined by both internal and external factors in an indoor environment. While the internal factors are represented by the individual preferences, aims and interests, the external factors are indicated by the stimulation of the environment. We model the randomness of human macro-level movement, e.g., the probability of visiting a specific place and staying time, under the Bayesian framework, considering the influence of both internal and external variables. We use two case studies in a shopping mall and in a college student dorm building to show the effectiveness of our proposed probabilistic human mobility model. Real surveillance camera data are used to validate the proposed model together with survey data in the case study of student dorm.',\n",
       " 'We live in an emerging hyper-connected era in which people are in contact and interacting with an increasing number of other people and devices. Increasingly, modern IT systems form networks of humans and machines that interact with one another. As machines take a more active role in such networks, they exert an in-creasing level of influence on other participants. We review the existing literature on agency and propose a definition of agency that is practical for describing the capabilities and impact human and machine actors may have in a human-machine network. On this basis, we discuss and demonstrate the impact and trust implica-tions for machine actors in human-machine networks for emergency decision support, healthcare and future smart homes. We maintain that machine agency not only facilitates human to machine trust, but also interpersonal trust; and that trust must develop to be able to seize the full potential of future technology.',\n",
       " 'The use of computer-aided and web-based educational technologies such as Virtual Learning Environments (VLE) has increased significantly in the recent past. One example of such a VLE is Virtual Interactive Engineering on the Web (VIEW). VIEW is a 3D virtual, interactive, student centered, framework of web-based modules based on the Extensible 3D standard. These modules are dedicated to the improvement of student success and learning. In this paper, an overview of the recent developments in VIEW along with associated assessment results is presented. An experimental study was also performed to compare the learning experience and performance of students in a physical dissection activity vs. that in a virtual dissection activity using a VIEW module. The results of this study show that students can meet given learning objectives and that there is limited difference in their learning and performance irrespective of a physical or virtual setting.',\n",
       " 'A goal of Interactive Machine Learning (IML) is to enable people without specialized training to teach agents how to perform tasks. Many of the existing machine learning algorithms that learn from human instructions are evaluated using simulated feedback and focus on how quickly the agent learns. While this is valuable information, it ignores important aspects of the human-agent interaction such as frustration. In this paper, we present the Newtonian Action Advice agent, a new method of incorporating human verbal action advice with Reinforcement Learning (RL) in a way that improves the human-agent interaction. In addition to simulations, we validated the Newtonian Action Advice algorithm by conducting a human-subject experiment. The results show that Newtonian Action Advice can perform better than Policy Shaping, a state-of-the-art IML algorithm, both in terms of RL metrics like cumulative reward and human factors metrics like frustration.',\n",
       " 'We propose a toolkit for creating Tangible Out-of-Body Experiences: exposing the inner states of users using physiological signals such as heart rate or brain activity. Tobe can take the form of a tangible avatar displaying live physiological readings to reflect on ourselves and others. Such a toolkit could be used by researchers and designers to create a multitude of potential tangible applications, including (but not limited to) educational tools about Science Technologies Engineering and Mathematics (STEM) and cognitive science, medical applications or entertainment and social experiences with one or several users or Tobes involved. Through a co-design approach, we investigated how everyday people picture their physiology and we validated the acceptability of Tobe in a scientific museum. We also give a practical example where two users relax together, with insights on how Tobe helped them to synchronize their signals and share a moment.',\n",
       " 'We explain and provide examples of a formalism that supports the methodology of discovering how to adapt and personalize technology by combining randomized experiments with variables associated with user models. We characterize a formal relationship between the use of technology to conduct A/B experiments and use of technology for adaptive personalization. The MOOClet Formalism [11] captures the equivalence between experimentation and personalization in its conceptualization of modular components of a technology. This motivates a unified software design pattern that enables technology components that can be compared in an experiment to also be adapted based on contextual data, or personalized based on user characteristics. With the aid of a concrete use case, we illustrate the potential of the MOOClet formalism for a methodology that uses randomized experiments of alternative micro-designs to discover how to adapt technology based on user characteristics, and then dynamically implements these personalized improvements in real time.',\n",
       " 'Today, many of the aid systems deployed for visually impaired people are mostly made for a single purpose. Be it navigation, object detection, or distance perceiving. Also, most of the deployed aid systems use indoor navigation which requires a pre-knowledge of the environment. These aid systems often fail to help visually impaired people in the unfamiliar scenario. In this paper, we propose an aid system developed using object detection and depth perceivement to navigate a person without dashing into an object. The prototype developed detects 90 different types of objects and compute their distances from the user. We also, implemented a navigation feature to get input from the user about the target destination and hence, navigate the impaired person to his/her destination using Google Directions API. With this system, we built a multi-feature, high accuracy navigational aid system which can be deployed in the wild and help the visually impaired people in their daily life by navigating them effortlessly to their desired destination.',\n",
       " 'Web search is among the most ubiquitous online activities, commonly used to acquire new knowledge and to satisfy learning-related objectives through informational search sessions. The importance of learning as an outcome of web search has been recognized widely, leading to a variety of research at the intersection of information retrieval, human computer interaction and learning-oriented sciences. Given the lack of explicit information, understanding of users and their learning needs has to be derived from their search behavior and resource interactions. In this paper, we introduce the involved research challenges and survey related work on the detection of learning needs, understanding of users, e.g. with respect to their knowledge state, learning tasks and learning progress throughout a search session as well as the actual consideration of learning needs throughout the retrieval and ranking process. In addition, we summarise our own research contributing to the aforementioned tasks and describe our research agenda in this context.',\n",
       " 'Throughout the past decade, many studies have classified human emotions using only a single sensing modality such as face video, electroencephalogram (EEG), electrocardiogram (ECG), galvanic skin response (GSR), etc. The results of these studies are constrained by the limitations of these modalities such as the absence of physiological biomarkers in the face-video analysis, poor spatial resolution in EEG, poor temporal resolution of the GSR etc. Scant research has been conducted to compare the merits of these modalities and understand how to best use them individually and jointly. Using multi-modal AMIGOS dataset, this study compares the performance of human emotion classification using multiple computational approaches applied to face videos and various bio-sensing modalities. Using a novel method for compensating physiological baseline we show an increase in the classification accuracy of various approaches that we use. Finally, we present a multi-modal emotion-classification approach in the domain of affective computing research.',\n",
       " 'Analytical tools in business management are understood as a combination of information technologies and quantitative methods used to assist stakeholders to make better decisions. The contemporary business environment is dramatically changing by the massive accumulation of data. Now, as never before, the use of analytical tools must be expanded to take advantage of this growing digital universe. This article will apply the laddering technique to see how personal values (or managerial functions) influence a companys adoption of analytical tools. A set of ten in-depth interviews are conducted with CEOs, analytics consultants, academics and businessmen in order to establish quantitative relations among attributes, consequences and personal values. Two easy-to-read outputs are provided to interpret our results. The most important links are quantitatively associated through an implication matrix, and then visually represented on a hierarchical value map. Guidelines for improving the use of analytical tools are provided in the last section',\n",
       " \"Social Networking Site (SNS) is a great innovation of modern times. Facebook, Twitter etc. have become an everyday part of peoples' life. Among all SNSs, Facebook is the most popular social network all over the world. Bangladesh is no exception. People of Bangladesh use Facebook for social communication, online shopping, business, knowledge and experience sharing etc. As well as the various uses of SNSs, people sometimes find themselves involved in real life violence, provoked by some social media posts or activities. In this paper, we discussed some case studies in which real life violence is originated based on Facebook activities in Bangladesh. Facebook was used in these incidents intentionally or unintentionally mostly as a tool to trigger hatred and violence. We analyzed and discussed the real-world consequences of these virtual activities in social media. Lastly, we recommended possible future measurements to prevent such violence.\",\n",
       " 'As we increasingly delegate important decisions to intelligent systems, it is essential that users understand how algorithmic decisions are made. Prior work has often taken a technocentric approach to transparency. In contrast, we explore empirical user-centric methods to better understand user reactions to transparent systems. We assess user reactions to global and incremental feedback in two studies. In Study 1, users anticipated that the more transparent incremental system would perform better, but retracted this evaluation after experience with the system. Qualitative data suggest this may arise because incremental feedback is distracting and undermines simple heuristics users form about system operation. Study 2 explored these effects in depth, suggesting that users may benefit from initially simplified feedback that hides potential system errors and assists users in building working heuristics about system operation. We use these findings to motivate new progressive disclosure principles for transparency in intelligent systems.',\n",
       " \"Working in complex industrial facilities requires spatial navigation skills that people build up with time and field experience. Training sessions consisting in guided tours help discover places but they are insufficient to become intimately familiar with their layout. They imply passive learning postures, are time-limited and can be experienced only once because of organization constraints and potential interferences with ongoing activities in the buildings. To overcome these limitations and improve the acquisition of navigation skills, we developed Indy, a virtual reality system consisting in a collaborative game of treasure hunting. It has several key advantages: it focuses learners' attention on navigation tasks, implies their active engagement and provides them with feedbacks on their achievements. Virtual reality makes it possible to multiply the number and duration of situations that learners can experience to better consolidate their skills. This paper discusses the main design principles and a typical usage scenario of Indy.\",\n",
       " \"This paper proposes an image-processing-based method for personalization of calorie consumption assessment during exercising. An experiment is carried out where several actions are required in an exercise called broadcast gymnastics, especially popular in Japan and China. We use Kinect, which captures body actions by separating the body into joints and segments that contain them, to monitor body movements to test the velocity of each body joint and capture the subject's image for calculating the mass of each body joint that differs for each subject. By a kinetic energy formula, we obtain the kinetic energy of each body joint, and calories consumed during exercise are calculated in this process. We evaluate the performance of our method by benchmarking it to Fitbit, a smart watch well-known for health monitoring during exercise. The experimental results in this paper show that our method outperforms a state-of-the-art calorie assessment method, which we base on and improve, in terms of the error rate from Fitbit's ground-truth values.\",\n",
       " 'While clustering is one of the most popular methods for data mining, analysts lack adequate tools for quick, iterative clustering analysis, which is essential for hypothesis generation and data reasoning. We introduce Clustrophile, an interactive tool for iteratively computing discrete and continuous data clusters, rapidly exploring different choices of clustering parameters, and reasoning about clustering instances in relation to data dimensions. Clustrophile combines three basic visualizations -- a table of raw datasets, a scatter plot of planar projections, and a matrix diagram (heatmap) of discrete clusterings -- through interaction and intermediate visual encoding. Clustrophile also contributes two spatial interaction techniques, $\\\\textit{forward projection}$ and $\\\\textit{backward projection}$, and a visualization method, $\\\\textit{prolines}$, for reasoning about two-dimensional projections obtained through dimensionality reductions.',\n",
       " 'Comparing many long time series is challenging to do by hand. Clustering time series enables data analysts to discover relevance between and anomalies among multiple time series. However, even after reasonable clustering, analysts have to scrutinize correlations between clusters or similarities within a cluster. We developed SAX Navigator, an interactive visualization tool, that allows users to hierarchically explore global patterns as well as individual observations across large collections of time series data. Our visualization provides a unique way to navigate time series that involves a \"vocabulary of patterns\" developed by using a dimensionality reduction technique,Symbolic Aggregate approXimation(SAX). With SAX, the time series data clusters efficiently and is quicker to query at scale. We demonstrate the ability of SAX Navigator to analyze patterns in large time series data based on three case studies for an astronomy data set. We verify the usability of our system through a think-aloud study with an astronomy domain scientist.',\n",
       " 'With an ever-increasing number of mobile devices competing for our attention, quantifying when, how often, or for how long users visually attend to their devices has emerged as a core challenge in mobile human-computer interaction. Encouraged by recent advances in automatic eye contact detection using machine learning and device-integrated cameras, we provide a fundamental investigation into the feasibility of quantifying visual attention during everyday mobile interactions. We identify core challenges and sources of errors associated with sensing attention on mobile devices in the wild, including the impact of face and eye visibility, the importance of robust head pose estimation, and the need for accurate gaze estimation. Based on this analysis, we propose future research directions and discuss how eye contact detection represents the foundation for exciting new applications towards next-generation pervasive attentive user interfaces.',\n",
       " 'Clinical decision support tools (DST) promise improved healthcare outcomes by offering data-driven insights. While effective in lab settings, almost all DSTs have failed in practice. Empirical research diagnosed poor contextual fit as the cause. This paper describes the design and field evaluation of a radically new form of DST. It automatically generates slides for clinicians\\' decision meetings with subtly embedded machine prognostics. This design took inspiration from the notion of \"Unremarkable Computing\", that by augmenting the users\\' routines technology/AI can have significant importance for the users yet remain unobtrusive. Our field evaluation suggests clinicians are more likely to encounter and embrace such a DST. Drawing on their responses, we discuss the importance and intricacies of finding the right level of unremarkableness in DST design, and share lessons learned in prototyping critical AI systems as a situated experience.',\n",
       " 'Environmental concerns have driven an interest in sustainable smart cities, through the monitoring and optimisation of networked infrastructure processes. At the same time, there are concerns about who these interventions and services are for, and who benefits. HCI researchers and designers interested in civic life have started to call for the democratisation of urban space through resistance and political action to challenge state and corporate claims. This paper aims to add to the growing body of critical and civic led smart city literature in HCI by leveraging concepts from the environmental humanities about more than human worlds, as a way to shift understandings within HCI of smart cities away from the exceptional and human centered, towards a more inclusive understanding that incorporates and designs for other others and other species. We illustrate through a case study that involved codesigning Internet of Things with urban agricultural communities, possibilities for creating more environmentally and socially just smart cities.',\n",
       " 'EMG-based gesture recognition shows promise for human-machine interaction. Systems are often afflicted by signal and electrode variability which degrades performance over time. We present an end-to-end system combating this variability using a large-area, high-density sensor array and a robust classification algorithm. EMG electrodes are fabricated on a flexible substrate and interfaced to a custom wireless device for 64-channel signal acquisition and streaming. We use brain-inspired high-dimensional (HD) computing for processing EMG features in one-shot learning. The HD algorithm is tolerant to noise and electrode misplacement and can quickly learn from few gestures without gradient descent or back-propagation. We achieve an average classification accuracy of 96.64% for five gestures, with only 7% degradation when training and testing across different days. Our system maintains this accuracy when trained with only three trials of gestures; it also demonstrates comparable accuracy with the state-of-the-art when trained with one trial.',\n",
       " 'In gaming, customizing individual characters, can create personal bonds between players and their characters. Hence, character customization is a standard component in many games. While mobile Augmented Reality (AR) games become popular, to date, no 3D character editor for AR games exists. We investigate the feasibility of 3D character customization for smartphone-based AR in an iterative design process.\\n  Specifically, we present findings from creating AR prototypes in a handheld AR setting. In a first user study, we found that a tangible AR prototype resulted in higher hedonistic measures than a camera-based approach. In a follow up study, we compared the tangible AR prototype with a non-AR touchscreen version for selection, scaling, translation and rotation tasks in a 3D character customization setting. The tangible AR version resulted in significantly better results for stimulation and novelty measures than the non-AR version. At the same time, it maintained a proficient level in pragmatic measures such as accuracy and efficiency.',\n",
       " 'According to Brooke* \"Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts.\" That is, one cannot speak of usability without specifying what that particular usability is characterized by. Driven by the feedback of a reviewer at an international conference, I explore in which way one can precisely specify the kind of usability they are investigating in a given setting. Finally, I come up with a formalism that defines usability as a quintuple comprising the elements level of usability metrics, product, users, goals and context of use. Providing concrete values for these elements then constitutes the investigated type of usability. The use of this formalism is demonstrated in two case studies.\\n  * J. Brooke. SUS: A \"quick and dirty\" usability scale. In P. W. Jordan, B. Thomas, B. A. Weerdmeester, and A. L. McClelland, editors, Usability Evaluation in Industry. Taylor and Francis, 1996.',\n",
       " \"Mood disorders are common and associated with significant morbidity and mortality. Early diagnosis has the potential to greatly alleviate the burden of mental illness and the ever increasing costs to families and society. Mobile devices provide us a promising opportunity to detect the users' mood in an unobtrusive manner. In this study, we use a custom keyboard which collects keystrokes' meta-data and accelerometer values. Based on the collected time series data in multiple modalities, we propose a deep personalized mood prediction approach, called {\\\\pro}, by integrating convolutional and recurrent deep architectures as well as exploring each individual's circadian rhythm. Experimental results not only demonstrate the feasibility and effectiveness of using smart-phone meta-data to predict the presence and severity of mood disturbances in bipolar subjects, but also show the potential of personalized medical treatment for mood disorders.\",\n",
       " 'In human-in-the-loop machine learning, the user provides information beyond that in the training data. Many algorithms and user interfaces have been designed to optimize and facilitate this human--machine interaction; however, fewer studies have addressed the potential defects the designs can cause. Effective interaction often requires exposing the user to the training data or its statistics. The design of the system is then critical, as this can lead to double use of data and overfitting, if the user reinforces noisy patterns in the data. We propose a user modelling methodology, by assuming simple rational behaviour, to correct the problem. We show, in a user study with 48 participants, that the method improves predictive performance in a sparse linear regression sentiment analysis task, where graded user knowledge on feature relevance is elicited. We believe that the key idea of inferring user knowledge with probabilistic user models has general applicability in guarding against overfitting and improving interactive machine learning.',\n",
       " 'Thermostats are primary interfaces for occupants of office buildings to express their comfort preferences. However, standard thermostats are often ineffective due to inaccessibility, lack of information, or limited responsiveness, leading to occupant discomfort. Software thermostats based on web or smartphone applications provide alternative interfaces to occupants with minimal deployment cost. However, their usage and effectiveness have not been studied extensively in real settings. In this paper we present Genie, a novel software-augmented thermostat that we deployed and studied at our university over a period of 21 months. Our data shows that providing wider thermal control to users does not lead to system abuse and that the effect on energy consumption is minimal while improving comfort and energy awareness. We believe that increased introduction of software thermostats in office buildings will have important effects on comfort and energy consumption and we provide key design recommendations for their implementation and deployment.',\n",
       " 'This paper presents a telepresence interaction framework based on touchscreen and telepresence-robot technologies. The core of the framework is a new user interface, Touchable live video Image based User Interface, called TIUI. The TIUI allows a remote operator to not just drive the telepresence robot but operate and interact with real objects by touching their live video images on a pad with finger touch gestures. We implemented a telepresence interaction system which is composed of a telepresence robot and tele-interactive objects located in a local space, the TIUI of a pad located in a remote space, and the wireless networks connecting the two spaces. Our system can be a perfect embodiment of a remote operator to do most of daily living tasks, such as opening a door, drawing a curtain, pushing a wheelchair, and other like tasks. The evaluation and demonstration results show the effectiveness and promising applications of our system.',\n",
       " 'The amount of visual communication we are facing is rapidly increasing, and skills to process, understand, and generate visual representations are in high demand. Especially students focusing on computer graphics and visualization can benefit from a more diverse education on visual literacy, as they often have to work on graphical representations for broad masses after their graduation. Our proposed teaching approach incorporates basic design thinking principles into traditional visualization and graphics education. Our course was inspired by the book Dear Data that was the subject of a lively discussion at the closing capstone of IEEE VIS 2017. The paper outlines our 12-week teaching experiment and summarizes the results extracted from accompanying questionnaires and interviews. In particular, we provide insights into the creation process and pain points of visualization novices, discuss the observed interplay between visualization tasks and design thinking, and finally draw design implications for visual literacy education in general.',\n",
       " 'In this paper, we consider the problem of tracking the eye-gaze of individuals while they engage in reading. Particularly, we develop ways to accurately track the line being read by an individual using commercially available eye tracking devices. Such an approach will enable futuristic functionalities such as comprehension evaluation, interest level detection, and user-assisting applications like hands-free navigation and automatic scrolling. Existing commercial eye trackers provide an estimated location of the eye-gaze fixations every few milliseconds. However, this estimated data is found to be very noisy. As such, commercial eye-trackers are unable to accurately track lines while reading. In this paper we propose several statistical models to bridge the commercial gaze tracker outputs and eye-gaze patterns while reading. We then employ hidden Markov models to parametrize these statistical models and to accurately detect the line being read. The proposed approach is shown to yield an improvement of over 20% in line detection accuracy.',\n",
       " 'When viewing stereoscopic displays, people may not always be able to stay exactly in front of the display. It is known that viewing stereoscopic display from different vertical angles lead to different visual discomfort. However, the effects of horizontal viewing angle on stereoscopic visual discomfort have been rarely investigated, especially for household stereoscopic displays. In this study, subjects were required to view a stereoscopic display from various horizontal viewing angles, and assessed their visual discomfort during viewing. The visual stimuli have various amount of disparities: positive disparity, negative disparity or zero disparity. Results showed that the visual discomfort changes with horizontal viewing angle, and greater angles generally lead to more serious visual discomfort. Furthermore, the relationship between visual discomfort and horizontal viewing angle can be approximately expressed by a quadratic function.',\n",
       " 'The rise of machine learning has brought closer scrutiny to intelligent systems, leading to calls for greater transparency and explainable algorithms. We explore the effects of transparency on user perceptions of a working intelligent system for emotion detection. In exploratory Study 1, we observed paradoxical effects of transparency which improves perceptions of system accuracy for some participants while reducing accuracy perceptions for others. In Study 2, we test this observation using mixed methods, showing that the apparent transparency paradox can be explained by a mismatch between participant expectations and system predictions. We qualitatively examine this process, indicating that transparency can undermine user confidence by causing users to fixate on flaws when they already have a model of system operation. In contrast transparency helps if users lack such a model. Finally, we revisit the notion of transparency and suggest design considerations for building safe and successful machine learning systems based on our insights.',\n",
       " \"We present the concept of X-Vision, an enhanced Augmented Reality (AR)-based visualization tool, with the real-time sensing capability in a tagged environment. We envision that this type of a tool will enhance the user-environment interaction and improve the productivity in factories, smart-spaces, home & office environments, maintenance/facility rooms and operation theatres, etc. In this paper, we describe the design of this visualization system built upon combining the object's pose information estimated by the depth camera and the object's ID & physical attributes captured by the RFID tags. We built a physical prototype of the system demonstrating the projection of 3D holograms of the objects encoded with sensed information like water-level and temperature of common office/household objects. The paper also discusses the quality metrics used to compare the pose estimation algorithms for robust reconstruction of the object's 3D data.\",\n",
       " \"Digitally presenting physiological signals as biofeedback to users raises awareness of both body and mind. This paper describes the effectiveness of conveying a physiological signal often overlooked for communication: breathing. We present the design and development of digital breathing patterns and their evaluation along three output modalities: visual, audio, and haptic. We also present Breeze, a wearable pendant placed around the neck that measures breathing and sends biofeedback in real-time. We evaluated how the breathing patterns were interpreted in a fixed environment and gathered qualitative data on the wearable device's design. We found that participants intentionally modified their own breathing to match the biofeedback, as a technique for understanding the underlying emotion. Our results describe how the features of the breathing patterns and the feedback modalities influenced participants' perception. We include guidelines and suggested use cases, such as Breeze being used by loved ones to increase connectedness and empathy.\",\n",
       " 'Even if only the acoustic channel is considered, human communication is highly multi-modal. Non-lexical cues provide a variety of information such as emotion or agreement. The ability to process such cues is highly relevant for spoken dialog systems, especially in assistance systems. In this paper we focus on the recognition of non-lexical confirmations such as \"mhm\", as they enhance the system\\'s ability to accurately interpret human intent in natural communication. The architecture uses a Support Vector Machine to detect confirmations based on acoustic features. In a systematic comparison, several feature sets were evaluated for their performance on a corpus of human-agent interaction in a setting with naive users including elderly and cognitively impaired people. Our results show that using stacked formants as features yield an accuracy of 84% outperforming regular formants and MFCC or pitch based features for online classification.',\n",
       " 'This paper presents an open source tool for testing the recognition accuracy of Chinese handwriting input methods. The tool consists of two modules, namely the PC and Android mobile client. The PC client reads handwritten samples in the computer, and transfers them individually to the Android client in accordance with the socket communication protocol. After the Android client receives the data, it simulates the handwriting on screen of client device, and triggers the corresponding handwriting recognition method. The recognition accuracy is recorded by the Android client. We present the design principles and describe the implementation of the test platform. We construct several test datasets for evaluating different handwriting recognition systems, and conduct an objective and comprehensive test using six Chinese handwriting input methods with five datasets. The test results for the recognition accuracy are then compared and analyzed.',\n",
       " 'Experts in different domains rely increasingly on simulation models of complex processes to reach insights, make decisions, and plan future projects. These models are often used to study possible trade-offs, as experts try to optimise multiple conflicting objectives in a single investigation. Understanding all the model intricacies, however, is challenging for a single domain expert. We propose a simple approach to support multiple experts when exploring complex model results. First, we reduce the model exploration space, then present the results on a shared interactive surface, in the form of a scatterplot matrix and linked views. To explore how multiple experts analyse trade-offs using this setup, we carried out an observational study focusing on the link between expertise and insight generation during the analysis process. Our results reveal the different exploration strategies and multi-storyline approaches that domain experts adopt during trade-off analysis, and inform our recommendations for collaborative model exploration systems.',\n",
       " \"The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams' challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by industry practitioners and solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address industry practitioners' needs.\",\n",
       " \"Most of today's scientific research relies on computers and software not only for administrational tasks, but also for processing scientific information. Examples of such computer-aided research are the analysis of experimental data or the simulation of phenomena based on theoretical models. With the rapid increase of computational power, scientific software has integrated more and more complex scientific knowledge in a black-box fashion. As a consequence, its users do not know, and don't even have a chance of finding out, which models or assumptions their computations are based on. The black-box nature of scientific software has thereby become a major cause of mistakes. The present work starts with an analysis of this situation from the point of view of human-computer interaction in scientific research. It identifies the key role of digital scientific notations at the human-computer interface, and describes a proof-of-concept implementation of such a digital scientific notation for scientific models formulated as mathematical equations.\",\n",
       " 'Virtual laboratories are the new online educational trend for communicating to students practical skills of science. In this paper we report on a comparison of techniques for familiarizing distance learning students with a 3D virtual biology laboratory, in order to prepare them for their microscopy experiment in their physical wet lab. Initial training for these students was provided at a distance, via Skype. Their progress was assessed through Pre and Post-tests and compared to those of students who opted to only prepare for their wet lab using the conventional face-to-face educational method, which was provided for all students. Our results provide preliminary answers to questions such as whether the incorporation of a virtual lab in the educational process will improve the quality of distance learning education and whether a virtual lab can be a valuable educational supplement to students enrolled in laboratory courses on Biology.',\n",
       " \"We consider a crowdsourcing platform where workers' responses to questions posed by a crowdsourcer are used to determine the hidden state of a multi-class labeling problem. As workers may be unreliable, we propose to perform sequential questioning in which the questions posed to the workers are designed based on previous questions and answers. We propose a Partially-Observable Markov Decision Process (POMDP) framework to determine the best questioning strategy, subject to the crowdsourcer's budget constraint. As this POMDP formulation is in general intractable, we develop a suboptimal approach based on a $q$-ary Ulam-R\\\\'enyi game. We also propose a sampling heuristic, which can be used in tandem with standard POMDP solvers, using our Ulam-R\\\\'enyi strategy. We demonstrate through simulations that our approaches outperform a non-sequential strategy based on error correction coding and which does not utilize workers' previous responses.\",\n",
       " 'Current machine algorithms for analysis of unstructured data typically show low accuracies due to the need for human-like intelligence. Conversely, though humans are much better than machine algorithms on analyzing unstructured data, they are unpredictable, slower and can be erroneous or even malicious as computing agents. Therefore, a hybrid platform that can intelligently orchestrate machine and human computing resources would potentially be capable of providing significantly better benefits compared to either type of computing agent in isolation. In this paper, we propose a new hybrid human-machine computing platform with integrated service level objectives (SLO) management for complex tasks that can be decomposed into a dependency graph where nodes represent subtasks. Initial experimental results are highly encouraging. To the best of our knowledge, ours is the first work that attempts to design such a hybrid human-machine computing platform with support for addressing the three SLO parameters of accuracy, budget and completion time.',\n",
       " 'In this article, we explore the availability of head-mounted display (HMD) devices which can be coupled in a seamless way with P300-based brain-computer interfaces (BCI) using electroencephalography (EEG). The P300 is an event-related potential appearing about 300ms after the onset of a stimulation. The recognition of this potential on the ongoing EEG requires the knowledge of the exact onset of the stimuli. In other words, the stimulations presented in the HMD must be perfectly synced with the acquisition of the EEG signal. This is done through a process called tagging. The tagging must be performed in a reliable and robust way so as to guarantee the recognition of the P300 and thus the performance of the BCI. An HMD device should also be able to render images fast enough to allow an accurate perception of the stimulations, and equally to not perturb the acquisition of the EEG signal. In addition, an affordable HMD device is needed for both research and entertainment purposes. In this study, we selected and tested two HMD configurations.',\n",
       " 'Nowadays, the development of Web applications supporting distributed user interfaces (DUI) is straightforward. However, it is still hard to find Web sites supporting this kind of user interaction. Although studies on this field have demonstrated that DUI would improve the user experience, users are not massively empowered to manage these kinds of interactions. In this setting, we propose to move the responsibility of distributing both the UI and user interaction, from the application (a Web application) to the client (the Web browser), giving also rise to inter-application interaction distribution. This paper presents a platform for client-side DUI, built on the foundations of Web augmentation and End User Development. The idea is to empower end users to apply an augmentation layer over existing Web applications, considering both frequent use and opportunistic DUI requirements. In this work, we present the architecture and a prototype tool supporting this approach and illustrate the incorporation of some DUI features through case studies.',\n",
       " \"Virtual museums aim to promote access to cultural artifacts. However, they often face the challenge of getting audiences to read and understand a large amount of information in an uncontrolled online environment. Inspired by successful practices in physical museums, we investigated the possible use of guiding questions to engage audiences in virtual museums. To this end, we first identified how to construct questions that are likely to attract audiences through domain expert interviews and mining cultural-related posts in a popular question and answer community. Then in terms of the proactive level for attracting users' attention, we designed two mechanisms to interactively prompt questions: active and passive. Through an online experiment with 150 participants, we showed that having interactive guiding questions encourages browsing and improves content comprehension. We discuss reasons why they are useful by conducting another qualitative comparison and obtained insights about the influence of question category and interaction mechanism.\",\n",
       " 'Critical infrastructure is vulnerable to a broad range of hazards. Timely and effective recovery of critical infrastructure after extreme events is crucial. However, critical infrastructure disaster recovery planning is complicated and involves both domain- and user-centered characteristics and complexities. Recovery planning currently uses few quantitative computer-based tools and instead largely relies on expert judgment. Simulation modeling can simplify domain-centered complexities but not the human factors. Conversely, human-centered design places end-users at the center of design. We discuss the benefits of combining simulation modeling with human-centered design and refer it as human-centered simulation modeling. Human-centered simulation modeling has the capability to make recovery planning simpler and more understandable for critical infrastructure and emergency management experts and other recovery planning decision-makers.',\n",
       " 'Feedback tools help people to monitor information about themselves to improve their health, sustainability practices, or personal well-being. Yet reasoning about personal data (e.g., pedometer counts, blood pressure readings, or home electricity consumption) to gain a deep understanding of your current practices and how to change can be challenging with the data alone. We integrate quantitative feedback data within a personal digital calendar; this approach aims to make the feedback data readily accessible and more comprehensible. We report on an eight-week field study of an on-calendar visualization tool. Results showed that a personal calendar can provide rich context for people to reason about their feedback data. The on-calendar visualization enabled people to quickly identify and reason about regular patterns and anomalies. Based on our results, we also derived a model of the behavior feedback process that extends existing technology adoption models. With that, we reflected on potential barriers for the ongoing use of feedback tools.',\n",
       " 'Although information workers may complain about meetings, they are an essential part of their work life. Consequently, busy people spend a significant amount of time scheduling meetings. We present Calendar.help, a system that provides fast, efficient scheduling through structured workflows. Users interact with the system via email, delegating their scheduling needs to the system as if it were a human personal assistant. Common scheduling scenarios are broken down using well-defined workflows and completed as a series of microtasks that are automated when possible and executed by a human otherwise. Unusual scenarios fall back to a trained human assistant who executes them as unstructured macrotasks. We describe the iterative approach we used to develop Calendar.help, and share the lessons learned from scheduling thousands of meetings during a year of real-world deployments. Our findings provide insight into how complex information tasks can be broken down into repeatable components that can be executed efficiently to improve productivity.',\n",
       " \"This is the preprint version of our paper on 3rd International Workshop on Virtual and Augmented Assistive Technology (VAAT) at IEEE Virtual Reality 2015 (VR2015). An assistive training tool for rehabilitation of dysphonic patients is designed and developed according to the practical clinical needs. The assistive tool employs a space flight game as the attractive logic part, and microphone arrays as input device, which is getting rid of ambient noise by setting a specific orientation. The therapist can guide the patient to play the game as well as the voice training simultaneously side by side, while not interfere the patient voice. The voice information can be recorded and extracted for evaluating the long-time rehabilitation progress. This paper outlines a design science approach for the development of an initial useful software prototype of such a tool, considering 'Intuitive', 'Entertainment', 'Incentive' as main design factors.\",\n",
       " 'Linear and non-linear measures of heart rate variability (HRV) are widely investigated as non-invasive indicators of health. Stress has a profound impact on heart rate, and different meditation techniques have been found to modulate heartbeat rhythm. This paper aims to explore the process of identifying appropriate metrices from HRV analysis for sonification. Sonification is a type of auditory display involving the process of mapping data to acoustic parameters. This work explores the use of auditory display in aiding the analysis of HRV leveraged by unsupervised machine learning techniques. Unsupervised clustering helps select the appropriate features to improve the sonification interpretability. Vocal synthesis sonification techniques are employed to increase comprehension and learnability of the processed data displayed through sound. These analyses are early steps in building a real-time sound-based biofeedback training system.',\n",
       " 'In video production, inserting B-roll is a widely used technique to enrich the story and make a video more engaging. However, determining the right content and positions of B-roll and actually inserting it within the main footage can be challenging, and novice producers often struggle to get both timing and content right. We present B-Script, a system that supports B-roll video editing via interactive transcripts. B-Script has a built-in recommendation system trained on expert-annotated data, recommending users B-roll position and content. To evaluate the system, we conducted a within-subject user study with 110 participants, and compared three interface variations: a timeline-based editor, a transcript-based editor, and a transcript-based editor with recommendations. Users found it easier and were faster to insert B-roll using the transcript-based interface, and they created more engaging videos when recommendations were provided.',\n",
       " 'To use social media for health-related analysis, one key step is the detection of health-related labels for users. But unlike transient conditions like flu, social media users are less vocal about chronic conditions such as obesity, as users might not tweet \"I\\'m still overweight\". As, however, obesity-related conditions such as diabetes, heart disease, osteoarthritis, and even cancer are on the rise, this obese-or-not label could be one of the most useful for studies in public health.\\n  In this paper we investigate the feasibility of using profile pictures to infer if a user is overweight or not. We show that this is indeed possible and further show that the fraction of labeled-as-overweight users is higher in U.S. counties with higher obesity rates. Going from public to individual health analysis, we then find differences both in behavior and social networks, for example finding users labeled as overweight to have fewer followers.',\n",
       " 'A large number of statistical decision problems in the social sciences and beyond can be framed as a (contextual) multi-armed bandit problem. However, it is notoriously hard to develop and evaluate policies that tackle these types of problem, and to use such policies in applied studies. To address this issue, this paper introduces StreamingBandit, a Python web application for developing and testing bandit policies in field studies. StreamingBandit can sequentially select treatments using (online) policies in real time. Once StreamingBandit is implemented in an applied context, different policies can be tested, altered, nested, and compared. StreamingBandit makes it easy to apply a multitude of bandit policies for sequential allocation in field experiments, and allows for the quick development and re-use of novel policies. In this article, we detail the implementation logic of StreamingBandit and provide several examples of its use.',\n",
       " 'High performance computing (HPC) has driven collaborative science discovery for decades. Exascale computing platforms, currently in the design stage, will be deployed around 2022. The next generation of supercomputers is expected to utilize radically different computational paradigms, necessitating fundamental changes in how the community of scientific users will make the most efficient use of these powerful machines. However, there have been few studies of how scientists work with exascale or close-to-exascale HPC systems. Time as a metaphor is so pervasive in the discussions and valuation of computing within the HPC community that it is worthy of close study. We utilize time as a lens to conduct an ethnographic study of scientists interacting with HPC systems. We build upon recent CSCW work to consider temporal rhythms and collective time within the HPC sociotechnical ecosystem and provide considerations for future system design.',\n",
       " 'Appearance-based gaze estimation methods that only require an off-the-shelf camera have significantly improved but they are still not yet widely used in the human-computer interaction (HCI) community. This is partly because it remains unclear how they perform compared to model-based approaches as well as dominant, special-purpose eye tracking equipment. To address this limitation, we evaluate the performance of state-of-the-art appearance-based gaze estimation for interaction scenarios with and without personal calibration, indoors and outdoors, for different sensing distances, as well as for users with and without glasses. We discuss the obtained findings and their implications for the most important gaze-based applications, namely explicit eye input, attentive user interfaces, gaze-based user modelling, and passive eye monitoring. To democratise the use of appearance-based gaze estimation and interaction in HCI, we finally present OpenGaze (www.opengaze.org), the first software toolkit for appearance-based gaze estimation and interaction.',\n",
       " \"This paper introduces an Internet of Things (IoT)-based data acquisition and monitoring scheme for insulin pumps. The proposed work employs embedded system hardware (Keil LPC1768-board) for data acquisition and monitoring. The hardware is used as an abstract layer between the insulin pump and the cloud. Diabetes data are secured before they are sent to the cloud for storage. Each patient's record is digitally signed using a secure hash algorithm mechanism. The proposed work will protect the patient's records from being breached from unauthorized entities, and authenticates them from improper modifications. The design is tested and verified using $\\\\mu$Vision studio, the Keil board mentioned above, and an ALARIS 8100 infusion pump. Moreover, a test case for a real cloud example is presented with the help of the Center of Computationally Assisted System and Technology. This center provided the infrastructure service to test our work.\",\n",
       " 'When inspecting information visualizations under time critical settings, such as emergency response or monitoring the heart rate in a surgery room, the user only has a small amount of time to view the visualization \"at a glance\". In these settings, it is important to provide a quantitative measure of the visualization to understand whether or not the visualization is too \"complex\" to accurately judge at a glance. This paper proposes Pixel Approximate Entropy (PAE), which adapts the approximate entropy statistical measure commonly used to quantify regularity and unpredictability in time-series data, as a measure of visual complexity for line charts. We show that PAE is correlated with user-perceived chart complexity, and that increased chart PAE correlates with reduced judgement accuracy. We also find that the correlation between PAE values and participants\\' judgment increases when the user has less time to examine the line charts.',\n",
       " \"UI design languages, such as Google's Material Design, make applications both easier to develop and easier to learn by providing a set of standard UI components. Nonetheless, it is hard to assess the impact of design languages in the wild. Moreover, designers often get stranded by strong-opinionated debates around the merit of certain UI components, such as the Floating Action Button and the Navigation Drawer. To address these challenges, this short paper introduces a method for measuring the impact of design languages and informing design debates through analyzing a dataset consisting of view hierarchies, screenshots, and app metadata for more than 9,000 mobile apps. Our data analysis shows that use of Material Design is positively correlated to app ratings, and to some extent, also the number of installs. Furthermore, we show that use of UI components vary by app category, suggesting a more nuanced view needed in design debates.\",\n",
       " 'Chatbot has become an important solution to rapidly increasing customer care demands on social media in recent years. However, current work on chatbot for customer care ignores a key to impact user experience - tones. In this work, we create a novel tone-aware chatbot that generates toned responses to user requests on social media. We first conduct a formative research, in which the effects of tones are studied. Significant and various influences of different tones on user experience are uncovered in the study. With the knowledge of effects of tones, we design a deep learning based chatbot that takes tone information into account. We train our system on over 1.5 million real customer care conversations collected from Twitter. The evaluation reveals that our tone-aware chatbot generates as appropriate responses to user requests as human agents. More importantly, our chatbot is perceived to be even more empathetic than human agents.',\n",
       " 'How should an AI-based explanation system explain an agent\\'s complex behavior to ordinary end users who have no background in AI? Answering this question is an active research area, for if an AI-based explanation system could effectively explain intelligent agents\\' behavior, it could enable the end users to understand, assess, and appropriately trust (or distrust) the agents attempting to help them. To provide insights into this question, we turned to human expert explainers in the real-time strategy domain, \"shoutcaster\", to understand (1) how they foraged in an evolving strategy game in real time, (2) how they assessed the players\\' behaviors, and (3) how they constructed pertinent and timely explanations out of their insights and delivered them to their audience. The results provided insights into shoutcasters\\' foraging strategies for gleaning information necessary to assess and explain the players; a characterization of the types of implicit questions shoutcasters answered; and implications for creating explanations by using the patterns',\n",
       " 'In this paper, we propose a new interface for virtual reality headset: a touchpad in front of the headset. To demonstrate the feasibility of the front touch interface, we built a prototype device, explored VR UI design space expansion, and performed various user studies. We started with preliminary tests to see how intuitively and accurately people can interact with the front touchpad. Then, we further experimented various user interfaces such as a binary selection, a typical menu layout, and a keyboard. Two-Finger and Drag-n-Tap were also explored to find the appropriate selection technique. As a low-cost, light-weight, and in low power budget technology, a touch sensor can make an ideal interface for mobile headset. Also, front touch area can be large enough to allow wide range of interaction types such as multi-finger interactions. With this novel front touch interface, we paved a way to new virtual reality interaction methods.',\n",
       " 'Display advertising normally charges advertisers for every single ad impression. Specifically, if an ad in a webpage has been loaded in the browser, an ad impression is counted. However, due to the position and size of the ad slot, lots of ads are actually not viewed but still measured as impressions and charged. These fraud ad impressions indeed undermine the efficacy of display advertising. A perfect ad impression viewability measurement should match what the user has really viewed with a short memory. In this paper, we conduct extensive investigations on display ad impression viewability measurements on dimensions of ad creative displayed pixel percentage and exposure time to find which measurement provides the most accurate ad impression counting. The empirical results show that the most accurate measurement counts one ad impression if more than 75% of the ad creative pixels have been exposed for at least 2 continuous seconds.',\n",
       " 'Building practitioners (architects, engineers, energy managers) are showing a growing interest in the design of more energy efficient and livable buildings. The best way to predict how a building will behave regarding energy consumption and thermal comfort is to use a dynamic simulation tool. However, the use of this kind of tools is difficult on a daily basis practice due to the heuristic and exploratory nature of the architectural design process. To deal with this difficulty, the University of Coimbra and three companies have been working on the development of a prototype design aiding tool, specifically devoted to the space planning phase of building design, under the project GerAPlanO (Automatic Generation of Architecture Floor plans with Energy Optimization). This project aims to combine the capabilities of design generation techniques, thermal assessment programs, and design optimization methods to provide assistance to decision makers. This paper presents the overall concept, as well as the current status of development of this tool.',\n",
       " 'For more then a decade the term User Experience (UX) has been highly debated and defined in many ways. However, often UX remains as a vague concept and it may be hard to understand the very nature of it. In this paper we aimed at providing a better understanding of this concept. We explored the multi-faceted UX literature, reviewing the current state-of- the-art knowledge and emphasizing the multi-dimensional nature of the concept. Based on the literature review we built a conceptual framework of UX using the elements that are linked to it and reported in different studies. To show the potential use of the framework, we examined the UX delivered by different phone applications on different mobile devices using the elements in the framework. Several interesting insights have been obtained in terms of how the phone applications deliver different UX. Our study opens up a promising line of investigating the contemporary meaning of UX.',\n",
       " 'This article proposes a multistage framework for time series analysis of user activity on touch sensitive surfaces in noisy environments. Here multiple methods are put together in multi stage framework; including moving average, moving median, linear regression, kernel density estimation, partial differential equations and Kalman filter. The proposed three stage filter consisting of partial differential equation based denoising, Kalman filter and moving average method provides ~25% better noise reduction than other methods according to Mean Squared Error (MSE) criterion in highly noise susceptible environments. Apart from synthetic data, we also obtained real world data like hand writing, finger/stylus drags etc. on touch screens in the presence of high noise such as unauthorized charger noise or display noise and validated our algorithms. Furthermore, the proposed algorithm performs qualitatively better than the existing solutions for touch panels of the high end hand held devices available in the consumer electronics market qualitatively.',\n",
       " \"With respect to machine operation tasks, the experiences from different skill level operators, especially novices, can provide worthy understanding about the manner in which they perceive the operational environment and formulate knowledge to deal with various operation situations. In this study, we describe the operator's behaviors by utilizing the relations among their head, hand, and operation location (hotspot) during the operation. A total of 40 experiences associated with a sewing machine operation task performed by amateur operators was recorded via a head-mounted RGB-D camera. We examined important features of operational behaviors in different skill level operators and confirmed their correlation to the difficulties of the operation steps. The result shows that the pure-gazing behavior is significantly reduced when the operator's skill improved. Moreover, the hand-approaching duration and the frequency of attention movement before operation are strongly correlated to the operational difficulty in such machine operating environments.\",\n",
       " \"The trend towards mobile devices usage has put more than ever the Web as a ubiquitous platform where users perform all kind of tasks. In some cases, users access the Web with 'native' mobile applications developed for well-known sites, such as LinkedIn, Facebook, Twitter, etc. These native applications might offer further (e.g. location-based) functionalities to their users in comparison with their corresponding Web sites, because they were developed with mobile features in mind. However, most Web applications have not this native mobile counterpart and users access them using browsers in the mobile device. Users might eventually want to add mobile features on these Web sites even though those features were not supported originally. In this paper we present a novel approach to allow end users to augment their preferred Web sites with mobile features. This end-user approach is supported by a framework for mobile Web augmentation that we describe in the paper. We also present a set of supporting tools and a validation experiment with end users.\",\n",
       " \"The objective of the system presented in this paper is to give users tactile feedback while walking in a virtual world through an anthropomorphic finger motion interface. We determined that the synchrony between the first person perspective and proprioceptive information together with the motor activity of the user's fingers are able to induce an illusionary feeling that is equivalent to the sense of ownership of the invisible avatar's legs. Under this condition, the perception of the ground under the virtual avatar's foot is felt through the user's fingertip. The experiments indicated that using our method the scale of the tactile perception of the texture roughness was extended and that the enlargement ratio was proportional to the avatar's body (foot) size. In order to display the target tactile perception to the users, we have to control only the virtual avatar's body (foot) size and the roughness of the tactile texture. Our results suggest that in terms of tactile perception fingers can be a replacement for legs in locomotion interfaces.\",\n",
       " 'As the internet of things (IoT) has integrated physical and digital technologies, designing for multiple sensory media (mulsemedia) has become more attainable. Designing technology for multiple senses has the capacity to improve virtual realism, extend our ability to process information, and more easily transfer knowledge between physical and digital environments. HCI researchers are beginning to explore the viability of integrating multimedia into virtual experiences, however research has yet to consider whether mulsemedia truly enhances realism, immersion and knowledge transfer. My work developing StreamBED, a VR training platform to train citizen science water monitors plans to consider the role of mulsemedia in immersion and learning goals. Future findings about the role of mulsemedia in learning contexts will potentially allow learners to experience, connect to, learn from spaces that are impossible to experience firsthand.',\n",
       " 'Predicting human performance in interaction tasks allows designers or developers to understand the expected performance of a target interface without actually testing it with real users. In this work, we present a deep neural net to model and predict human performance in performing a sequence of UI tasks. In particular, we focus on a dominant class of tasks, i.e., target selection from a vertical list or menu. We experimented with our deep neural net using a public dataset collected from a desktop laboratory environment and a dataset collected from hundreds of touchscreen smartphone users via crowdsourcing. Our model significantly outperformed previous methods on these datasets. Importantly, our method, as a deep model, can easily incorporate additional UI attributes such as visual appearance and content semantics without changing model architectures. By understanding about how a deep learning model learns from human behaviors, our approach can be seen as a vehicle to discover new patterns about human behaviors to advance analytical modeling.',\n",
       " 'Interaction methods based on computer-vision hold the potential to become the next powerful technology to support breakthroughs in the field of human-computer interaction. Non-invasive vision-based techniques permit unconventional interaction methods to be considered, including use of movements of the face and head for intentional gestural control of computer systems. Facial gesture interfaces open new possibilities for assistive input technologies. This chapter gives an overview of research aimed at developing vision-based head and face-tracking interfaces. This work has important implications for future assistive input devices. To illustrate this concretely we describe work from our own research in which we developed two vision-based facial feature tracking algorithms for human computer interaction and assistive input. Evaluation forms a critical component of this research and we provide examples of new quantitative evaluation tasks as well as the use of model real-world applications for the qualitative evaluation of new interaction styles.',\n",
       " \"We evaluate the performance and usability of mouse-based, touch-based, and tangible interaction for manipulating objects in a 3D virtual environment. This comparison is a step toward a better understanding of the limitations and benefits of these existing interaction techniques, with the ultimate goal of facilitating the integration of different 3D data exploration environments into a single interaction continuum. For this purpose we analyze participants' performance in 3D manipulation using a docking task. We measured completion times, docking precision, as well as subjective criteria such as fatigue, workload, and preference. Our results show that the three input modalities provide similar levels of precision but require different interaction times. We also discuss our qualitative observations as well as people's preferences and put our findings into context of the practical application domain of 3D data analysis environments.\",\n",
       " 'Several changes occur in the brain in response to voluntary and involuntary activities performed by a person. The ability to retrieve data from the brain within a time space provides basis for in-depth analyses that offer insight on what changes occur in the brain during its decision making processes. In this work, we present the technical description and software implementation of an electroencephalographic (EEG) based intelligent communication system. We use EEG dry sensors to read brain waves data in real-time with which we compute the likelihood that a voluntary eye blink has been made by a person and use the decision to trigger buttons on a user interface in order to produce text using a modification of the T9 algorithm. Our results indicate that EEG-based technology can be effectively applied in facilitating speech for people with severe speech and muscular disabilities, providing a foundation for future work in the area.',\n",
       " 'Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice are yet to benefit from the capacity of VEs. In this paper, we examine this perplexity from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theatre-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.',\n",
       " \"Alphanumeric text entry is a challenge for Virtual Reality (VR) applications. VR enables new capabilities, impossible in the real world, such as an unobstructed view of the keyboard, without occlusion by the user's physical hands. Several hand representations have been proposed for typing in VR on standard physical keyboards. However, to date, these hand representations have not been compared regarding their performance and effects on presence for VR text entry. Our work addresses this gap by comparing existing hand representations with minimalistic fingertip visualization. We study the effects of four hand representations (no hand representation, inverse kinematic model, fingertip visualization using spheres and video inlay) on typing in VR using a standard physical keyboard with 24 participants. We found that the fingertip visualization and video inlay both resulted in statistically significant lower text entry error rates compared to no hand or inverse kinematic model representations. We found no statistical differences in text entry speed.\",\n",
       " 'We present a crowdsourcing workflow to collect image annotations for visually similar synthetic categories without requiring experts. In animals, there is a direct link between taxonomy and visual similarity: e.g. a collie (type of dog) looks more similar to other collies (e.g. smooth collie) than a greyhound (another type of dog). However, in synthetic categories such as cars, objects with similar taxonomy can have very different appearance: e.g. a 2011 Ford F-150 Supercrew-HD looks the same as a 2011 Ford F-150 Supercrew-LL but very different from a 2011 Ford F-150 Supercrew-SVT. We introduce a graph based crowdsourcing algorithm to automatically group visually indistinguishable objects together. Using our workflow, we label 712,430 images by ~1,000 Amazon Mechanical Turk workers; resulting in the largest fine-grained visual dataset reported to date with 2,657 categories of cars annotated at 1/20th the cost of hiring experts.',\n",
       " 'This study examines the acceptance of technology and behavioral intention to use learning management systems (LMS). In specific, the aim of this research is to examine whether students ultimately accept and use educational learning systems such as e-class and the impact of behavioral intention on their decision to use them. An extended version of technology acceptance model has been proposed and used by employing the System Usability Scale to measure perceived ease of use. 345 university students participated in the study and the data analysis was based on partial least squares method. The results were confirmed in most of the research hypotheses. In particular, social norm, system access and self-efficacy significantly affect behavioral intention to use. As a result, it is suggested that e-learning developers and stakeholders should focus on these factors to increase acceptance and effectiveness of learning management systems.',\n",
       " 'In this paper we present Organic Primitives, an enabling toolbox that expands upon the library of input-output devices in HCI and facilitates the design of interactions with organic, fluid-based systems. We formulated color, odor and shape changing material primitives which act as sensor-actuators that convert pH signals into human-readable outputs. Food-grade organic molecules anthocyanin, vanillin, and chitosan were employed as dopants to synthesize materials which output a spectrum of colors, degrees of shape deformation, and switch between odorous and non-odorous states. We evaluated the individual output properties of our sensor-actuators to assess the rate, range, and reversibility of the changes as a function of pH 2-10. We present a design space with techniques for enhancing the functionality of the material primitives, and offer passive and computational methods for controlling the material interfaces. Finally, we explore applications enabled by Organic Primitives under four contexts: environmental, cosmetic, edible, and interspecies.',\n",
       " \"The number of informal caregivers for family members with Alzheimer's Disease (AD) is rising dramatically in the United States. AD caregivers disproportionately experience numerous health problems and are often isolated with little support. An active lifestyle can help prevent and mitigate physical and psychological health concerns amongst AD caregivers. Research has demonstrated how pervasive exergames can encourage physical activity (PA) in the general population, yet little work has explored how these tools can address the significant PA barriers that AD caregivers face. To identify opportunities for design, we conducted semi-structured interviews and participatory design sessions with 14 informal caregivers of family members with AD. Our findings characterize how becoming an AD caregiver profoundly impacts one's ability to be active, perspectives on being active, and the ways that exergames might best support this population. We discuss implications for design and how our findings challenge existing technological approaches to PA promotion.\",\n",
       " 'This study investigates the effect of a physical robot taking the role of a teacher or exercise partner in a language learning exercise. In order to investigate this, an application was developed enabling a 2:nd language learning vocabulary exercise in three different conditions. In the first condition the learner would receive tutoring from a disembodied voice, in the second condition the tutor would be embodied by an animated avatar on a computer screen, and in the final condition the tutor was a physical robotic head with a 3D animated face mask. A Russian language vocabulary exercise with 15 subjects was conducted. None of the subjects reported any Russian language skills prior to the exercises. Each subject were taught a set of 9 words in each of the three conditions during a practice phase, and were then asked to recall the words in a test phase. Results show that the recall of the words practiced with the physical robot were significantly higher than that of the words practiced with the avatar on the screen or with the disembodied voice.',\n",
       " 'This paper presents SAM, a modular and extensible JavaScript framework for self-adapting menus on webpages. SAM allows control of two elementary aspects for adapting web menus: (1) the target policy, which assigns scores to menu items for adaptation, and (2) the adaptation style, which specifies how they are adapted on display. By decoupling them, SAM enables the exploration of different combinations independently. Several policies from literature are readily implemented, and paired with adaptation styles such as reordering and highlighting. The process - including user data logging - is local, offering privacy benefits and eliminating the need for server-side modifications. Researchers can use SAM to experiment adaptation policies and styles, and benchmark techniques in an ecological setting with real webpages. Practitioners can make websites self-adapting, and end-users can dynamically personalise typically static web menus.',\n",
       " 'Heuristic inspections are often carried out in a rather restrictive manner in the sense that they often address one or two of User Experience aspects. These two generally being: usability and \"user experience\". This fails to consider UX as it should be [considered]: through a holistic approach. Thus, we suggest to go beyond that by opting for what we have called an Integrative Heu-ristic Inspection that takes into account issues of: accessibility, usability, emotions \\\\& motivation and persuasion, and that aims to simplify the overflow of recommendations UX professionals are faced with nowadays. We illustrate our proposal by a case study carried out on an insurance prospecting tablet application. We analyzed the results of the inspection separately for each dimension as well as combined across dimensions. Implications for a reflection on the struc-turing of the criteria for a general criteria-based approach in UX are discussed.',\n",
       " 'We propose a novel method to implement an optical see-through head mounted display which renders real aerial images with a wide viewing angle, called an Air Mounted Eyepiece (AME). To achieve the AMD design, we employ an off-the-shelf head mounted display and Transmissive Mirror Device (TMD) which is usually used in aerial real imaging systems. In the proposed method, we replicate the function of the head mounted display (HMD) itself, which is used in the air by using the TMD and presenting a real image of eyepiece in front of the eye. Moreover, it can realize a wide viewing angle 3D display by placing a virtual lens in front of the eye without wearing an HMD. In addition to enhancing the experience of mixed reality and augmented reality, our proposed method can be used as a 3D imaging method for use in other applications such as in automobiles and desktop work. We aim to contribute to the field of human-computer interaction and the research on eyepiece interfaces by discussing the advantages and the limitations of this near-eye optical system.',\n",
       " 'Multiple Sclerosis (MS) is an unpredictable, often disabling disease that can adversely affect any body function; this often requires persons with MS to be active patients who are able to self-manage. There are currently thousands of health applications available but it is unknown how many concern MS. We conducted a systematic review of all MS apps present in the most popular app stores (iTunes and Google Play store) on June 2016 to identify all relevant MS apps. After discarding non-MS related apps and duplicates, only a total of 25 MS apps were identified. App description contents and features were explored to assess target audience, functionalities, and developing entities. The vast majority of apps were focused on disease and treatment information with disease management being a close second. This is the first study that reviews MS apps and it highlights an interesting gap in the current repertoire of MS mHealth resources.',\n",
       " \"Mobile applications and on-body devices are becoming increasingly ubiquitous tools for physical activity tracking. We propose utilizing a self-tracker's habits to support continuous prediction of whether they will reach their daily step goal, thus enabling a variety of potential persuasive interventions. Our aim is to improve the prediction by leveraging historical data and other qualitative (motivation for using the systems, location, gender) and, quantitative (age) features. We have collected datasets from two activity tracking platforms (Moves and Fitbit) and aim to check if the model we derive from one is generalizable over the other. In the following paper we establish a pipeline for extracting the data and formatting it for modeling. We discuss the approach we took and our findings while selecting the features and classification models for the dataset. We further discuss the notion of generalizability of the model across different types of dataset and the probable inclusion of non standard features to further improve the model's accuracy.\",\n",
       " 'Machine learning advances have afforded an increase in algorithms capable of creating art, music, stories, games, and more. However, it is not yet well-understood how machine learning algorithms might best collaborate with people to support creative expression. To investigate how practicing designers perceive the role of AI in the creative process, we developed a game level design tool for Super Mario Bros.-style games with a built-in AI level designer. In this paper we discuss our design of the Morai Maker intelligent tool through two mixed-methods studies with a total of over one-hundred participants. Our findings are as follows: (1) level designers vary in their desired interactions with, and role of, the AI, (2) the AI prompted the level designers to alter their design practices, and (3) the level designers perceived the AI as having potential value in their design practice, varying based on their desired role for the AI.',\n",
       " 'The increasing pervasiveness of voice assistants in the home poses several privacy threats, including the stealthy recording of conversations. Little is known about user mental models of such threats, levels of comfort with their presence, and perceptions of potential interventions to eliminate them. In this paper, we present the design and prototyping of two such interventions as technology probes, \"Obfuscator\" and \"Blackout.\" We also present our findings from a field study that used both prototypes to better understand user mental models, preferences, and perceptions of privacy threats surrounding voice assistants. The findings from the field study have revealed several themes, including the importance of the aesthetics, ease-of-use, and form factor of the prototypes, all superseding the privacy-preserving features of the intervention. We discuss the design and research implications of our findings for the development of future interventions, the design of voice assistants, and our understanding of user privacy perceptions of voice assistants.',\n",
       " 'We present SigniFYI-CDN, an inspection method built from previously proposed methods combining Semiotic Engineering and the Cognitive Dimensions of Notations. Compared to its predecessors, SigniFYI-CDN simplifies procedural steps and supports them with more analytic scaffolds. It is especially fit for the study of interaction with technologies where notations are created and used by various people, or by a single person in various, and potentially distant, occasions. In such cases, notations may serve several purposes, like (mutual) comprehension, recall, coordination, negotiation, and documentation. We illustrate SigniFYI-CDN with highlights from the evaluation of a computer tool that supports qualitative data analysis. Our contribution is a simpler tool for researchers and practitioners to probe the power of combined communicability and usability analysis of interaction with increasingly complex data-intensive applications.',\n",
       " \"Asynchronous interfaces allow users to concurrently issue requests while existing ones are processed. While it is widely used to support non-blocking input when there is latency, it's not clear if people can make use of asynchrony as the data is updating, since the UI updates dynamically and the changes can be hard to interpret. Interactive data visualization presents an interesting context for studying the effects of asynchronous interfaces, since interactions are frequent, task latencies can vary widely, and results often require interpretation.\\n  In this paper, we study the effects of introducing asynchrony into interactive visualizations, under different latencies, and with different tasks. We observe that traditional asynchronous interfaces, where results update in place, induce users to wait for the result before interacting, not taking advantage of the asynchronous rendering of the results. However, when results are rendered cumulatively over the recent history, users perform asynchronous interactions and get faster task completion times.\",\n",
       " 'Researchers, technology reviewers, and governmental agencies have expressed concern that automation may necessitate the introduction of added displays to indicate vehicle intent in vehicle-to-pedestrian interactions. An automated online methodology for obtaining communication intent perceptions for 30 external vehicle-to-pedestrian display concepts was implemented and tested using Amazon Mechanic Turk. Data from 200 qualified participants was quickly obtained and processed. In addition to producing a useful early-stage evaluation of these specific design concepts, the test demonstrated that the methodology is scalable so that a large number of design elements or minor variations can be assessed through a series of runs even on much larger samples in a matter of hours. Using this approach, designers should be able to refine concepts both more quickly and in more depth than available development resources typically allow. Some concerns and questions about common assumptions related to the implementation of vehicle-to-pedestrian displays are posed.',\n",
       " 'Virtual Reality (VR) becomes accessible to mimic a \"real-like\" world now. People who have a VR experience usually can be impressed by the immersive feeling, they might consider themselves are actually existed in the VR space. Self-consciousness is important for people to identify their own characters in VR space, and illusory ownership can help people to \"build\" their \"bodies\". The rubber hand illusion can convince us a fake hand made by rubber is a part of our bodies under certain circumstances. Researches about autoscopic phenomena extend this illusory to the so-called full body illusion. We conducted 3 type of experiments to study the illusory ownership in VR space as it shows in Figure 1, and we learned: Human body must receive the synchronized visual signal and somatosensory stimulus at the same time; The visual signal must be the first person perceptive; the subject and the virtual body needs to be the same height as much as possible. All these illusory ownerships accompanied by the body temperature decreases, where the body is stimulated.',\n",
       " 'Mobile devices with touch keyboards have become ubiquitous, but text entry on these devices remains slow and errorprone. Understanding touch patterns during text entry could be useful in designing robust error-correction algorithms for soft keyboards. In this paper, we present an analysis of text input behaviors on a soft QWERTY keyboard in three different text entry postures: index finger only, one thumb, and two thumb. Our work expands on the work of [1] by considering the entire surface area of digit contact with the smartphone keyboard, rather than interpreting each touch as a single point. To do this, we captured touch areas for every key in a lab study with 8 participants and calculated offsets, error rates, and size measurements. We then repeated the original experiment described in [1] and showed that significant differences exist when basing offset calculations on touch area compared to touch points for two postures.',\n",
       " 'In this paper we present a fully autonomous and intrinsically motivated robot usable for HRI experiments. We argue that an intrinsically motivated approach based on the Predictive Information formalism, like the one presented here, could provide us with a pathway towards autonomous robot behaviour generation, that is capable of producing behaviour interesting enough for sustaining the interaction with humans and without the need for a human operator in the loop. We present a possible reactive baseline behaviour for comparison for future research. Participants perceive the baseline and the adaptive, intrinsically motivated behaviour differently. In our exploratory study we see evidence that participants perceive an intrinsically motivated robot as less intelligent than the reactive baseline behaviour. We argue that is mostly due to the high adaptation rate chosen and the design of the environment. However, we also see that the adaptive robot is perceived as more warm, a factor which carries more weight in interpersonal interaction than competence.',\n",
       " \"This paper presents a study on mutual speech variation influences in a human-computer setting. The study highlights behavioral patterns in data collected as part of a shadowing experiment, and is performed using a novel end-to-end platform for studying phonetic variation in dialogue. It includes a spoken dialogue system capable of detecting and tracking the state of phonetic features in the user's speech and adapting accordingly. It provides visual and numeric representations of the changes in real time, offering a high degree of customization, and can be used for simulating or reproducing speech variation scenarios. The replicated experiment presented in this paper along with the analysis of the relationship between the human and non-human interlocutors lays the groundwork for a spoken dialogue system with personalized speaking style, which we expect will improve the naturalness and efficiency of human-computer interaction.\",\n",
       " 'Conceptual design relies on extensive manipulation of morphological properties of real or virtual objects.This study investigates the nature of the perceptual information that could be retrieved from different representation modalities to reproduce structural properties of a complex object by drawing . The abstract and complex object (tensegrity simplex) was presented to two study populations (design experts/architects and non-experts) in three different representation modalities (2D image view explored visually only, digital 3D model explored visually using a computer mouse, the real object explored visually and manually. After viewing and exploring, observers had to draw the most critical parts of the structure by hand into a 2D reference frame. The results reveal a considerable performance advantage of digital 3D model exploration compared with real-world 3D object manipulation in the expert population.The results are discussed in terms of the specific nature of morphological cues made available through the different representation modalities.',\n",
       " 'Algorithms for data visualizations are essential tools for transforming data into useful narratives. Unfortunately, very few visualization algorithms can handle the large datasets of many real-world scenarios. In this study, we address the visualization of these datasets as a Multi-Objective Optimization Problem. We propose mQAPViz, a divide-and-conquer multi-objective optimization algorithm to compute large-scale data visualizations. Our method employs the Multi-Objective Quadratic Assignment Problem (mQAP) as the mathematical foundation to solve the visualization task at hand. The algorithm applies advanced sampling techniques originating from the field of machine learning and efficient data structures to scale to millions of data objects. The algorithm allocates objects onto a 2D grid layout. Experimental results on real-world and large datasets demonstrate that mQAPViz is a competitive alternative to existing techniques.',\n",
       " 'Constructive approaches to visualization authoring have been shown to offer advantages such as providing options for flexible outputs, scaffolding and ideation of new data mappings, personalized exploration of data, as well as supporting data understanding and literacy. However, visualization authoring tools based on a constructive approach do not scale well to larger datasets. As construction often involves manipulating small pieces of data and visuals, it requires a significant amount of time, effort, and repetitive steps. We present ReConstructor, an authoring tool in which a visualization is constructed by instantiating its structural and functional components through four interaction elements (objects, modifiers, activators, and tools). This design preserves most of the benefits of a constructive process while avoiding scalability issues by allowing designers to propagate individual mapping steps to all the elements of a visualization. We also discuss the perceived benefits of our approach and propose avenues for future research in this area.',\n",
       " \"This study investigated the essential of meaningful automated feedback for programming assignments. Three different types of feedback were tested, including (a) What's wrong - what test cases were testing and which failed, (b) Gap - comparisons between expected and actual outputs, and (c) Hint - hints on how to fix problems if test cases failed. 46 students taking a CS2 participated in this study. They were divided into three groups, and the feedback configurations for each group were different: (1) Group One - What's wrong, (2) Group Two - What's wrong + Gap, (3) Group Three - What's wrong + Gap + Hint. This study found that simply knowing what failed did not help students sufficiently, and might stimulate system gaming behavior. Hints were not found to be impactful on student performance or their usage of automated feedback. Based on the findings, this study provides practical guidance on the design of automated feedback.\",\n",
       " 'Computing devices such as laptops, tablets and mobile phones have become part of our daily lives. End users increasingly know more and more information about these devices. Further, more technically savvy end users know how such devices are being built and know how to choose one over the others. However, we cannot say the same about the Internet of Things (IoT) products. Due to its infancy nature of the marketplace, end users have very little idea about IoT products. To address this issue, we developed a method, a crowdsourced peer learning activity, supported by an online platform (OLYMPUS) to enable a group of learners to learn IoT products space better. We conducted two different user studies to validate that our tool enables better IoT education. Our method guide learners to think more deeply about IoT products and their design decisions. The learning platform we developed is open source and available for the community.',\n",
       " 'Data is omnipresent in the modern, digital world and a significant number of people need to make sense of data as part of their everyday social and professional life. Therefore, together with the rise of data, the design of graphical representations has gained importance and attention. Yet, although a large body of procedural knowledge about effective visualization exists, the quality of representations is often reported to be poor, proposedly because these guidelines are scattered, unstructured and sometimes perceived as contradictive. Therefore, this paper describes a literature research addressing these problems. The research resulted in the collection and structuring of 81 guidelines and 34 underlying propositions, as well as in the derivation of 7 foundational principles about graphical representation design, called the \"Physics of Diagrams\", which are illustrated with concrete, practical examples throughout the paper.',\n",
       " 'The need for data preservation and reproducible research is widely recognized in the scientific community. Yet, researchers often struggle to find the motivation to contribute to data repositories and to use tools that foster reproducibility. In this paper, we explore possible uses of gamification to support reproducible practices in High Energy Physics. To understand how gamification can be effective in research tools, we participated in a workshop and performed interviews with data analysts. We then designed two interactive prototypes of a research preservation service that use contrasting gamification strategies. The evaluation of the prototypes showed that gamification needs to address core scientific challenges, in particular the fair reflection of quality and individual contribution. Through thematic analysis, we identified four themes which describe perceptions and requirements of gamification in research: Contribution, Metrics, Applications and Scientific practice. Based on these, we discuss design implications for gamification in science.',\n",
       " 'Crisis situations are characterised by their sudden occurrence and an unclear information situation. In that context, social media platforms have become a highly utilised resource for collective information gathering to fill these gaps. However, there are indications that not only humans, but also social bots are active on these platforms during crisis situations. Although identifying the impact of social bots during extreme events seems to be a highly relevant topic, research remains sparse. To fill this research gap, we started a bigger project in analysing the influence of social bots during crisis situations. As a part of this project, we initially conducted a case study on the Manchester Bombing 2017 and analysed the social bot activity. Our results indicate that mainly benign bots are active during crisis situations. While the quantity of the bot accounts is rather low, their tweet activity indicates a high influence.',\n",
       " 'We propose that safe, beautiful, fulfilling vehicle HMI design must start from a rigorous consideration of minimalist design. Modern vehicles are changing from mechanical machines to mobile computing devices, similar to the change from landline phones to smartphones. We propose the approach of \"designing toward minimalism\", where we ask \"why?\" rather than \"why not?\" in choosing what information to display to the driver. We demonstrate this approach on an HMI case study of displaying vehicle speed. We first show that vehicle speed is what 87.6% of people ask for. We then show, through an online study with 1,038 subjects and 22,950 videos, that humans can estimate ego-vehicle speed very well, especially at lower speeds. Thus, despite believing that we need this information, we may not. In this way, we demonstrate a systematic approach of questioning the fundamental assumptions of what information is essential for vehicle HMI.',\n",
       " 'Digitalization offers chances as well as risks for industrial companies. This article describes how the area of Mixed Reality, with its manifestations Augmented and Virtual Reality, can support industrial applications in the age of digitalization. Starting from a historical perspective on Augmented and Virtual Reality, this article surveys recent developments in the domain of Mixed Reality, relevant for industrial use cases.\\n  ---\\n  Die Digitalisierung bietet f\\\\\"ur Industrieunternehmen neue Chancen, stellt diese jedoch auch vor Herausforderungen. Dieser Artikel beleuchtet wie das Gebiet der vermischten Realit\\\\\"at mit seinen Auspr\\\\\"agungen der erweiterten Realit\\\\\"at und der virtuellen Realit\\\\\"at f\\\\\"ur industriellen Anwendungen im Zeitalter der Digitalisierung Vorteile schaffen kann. Ausgehend von einer historischen Betrachtung, werden aktuelle Entwicklungen auf dem Gebiet der erweiterten und virtuellen Realit\\\\\"at diskutiert.',\n",
       " \"Telepresence is a necessity for present time as we can't reach everywhere and also it is useful in saving human life at dangerous places. A robot, which could be controlled from a distant location, can solve these problems. This could be via communication waves or networking methods. Also controlling should be in real time and smooth so that it can actuate on every minor signal in an effective way. This paper discusses a method to control a robot over the network from a distant location. The robot was controlled by hand gestures which were captured by the live camera. A DSP board TMS320DM642EVM was used to implement image pre-processing and fastening the whole system. PCA was used for gesture classification and robot actuation was done according to predefined procedures. Classification information was sent over the network in the experiment. This method is robust and could be used to control any kind of robot over distance.\",\n",
       " 'Understanding complex user behaviour under various conditions, scenarios and journeys can be fundamental to the improvement of the user-experience for a given system. Predictive models of user reactions, responses -- and in particular, emotions -- can aid in the design of more intuitive and usable systems. Building on this theme, the preliminary research presented in this paper correlates events and interactions in an online social network against user behaviour, focusing on personality traits. Emotional context and tone is analysed and modelled based on varying types of sentiments that users express in their language using the IBM Watson Developer Cloud tools. The data collected in this study thus provides further evidence towards supporting the hypothesis that analysing and modelling emotions, sentiments and personality traits provides valuable insight into improving the user experience of complex social computer systems.',\n",
       " 'Human Computer Interaction (HCI) has been redefined in this era. People want to interact with their devices in such a way that has physical significance in the real world, in other words, they want ergonomic input devices. In this paper, we propose a new method of interaction with computing devices having a consumer grade camera, that uses two colored markers (red and green) worn on tips of the fingers to generate desired hand gestures, and for marker detection and tracking we used template matching with kalman filter. We have implemented all the usual system commands, i.e., cursor movement, right click, left click, double click, going forward and backward, zoom in and out through different hand gestures. Our system can easily recognize these gestures and give corresponding system commands. Our system is suitable for both desktop devices and devices where touch screen is not feasible like large screens or projected screens.',\n",
       " 'Dimensionality reduction is a common method for analyzing and visualizing high-dimensional data. However, reasoning dynamically about the results of a dimensionality reduction is difficult. Dimensionality-reduction algorithms use complex optimizations to reduce the number of dimensions of a dataset, but these new dimensions often lack a clear relation to the initial data dimensions, thus making them difficult to interpret. Here we propose a visual interaction framework to improve dimensionality-reduction based exploratory data analysis. We introduce two interaction techniques, forward projection and backward projection, for dynamically reasoning about dimensionally reduced data. We also contribute two visualization techniques, prolines and feasibility maps, to facilitate the effective use of the proposed interactions. We apply our framework to PCA and autoencoder-based dimensionality reductions. Through data-exploration examples, we demonstrate how our visual interactions can improve the use of dimensionality reduction in exploratory data analysis.',\n",
       " 'Unrecognized hazards increase the likelihood of workplace fatalities and injuries substantially. However, recent research has demonstrated that a large proportion of hazards remain unrecognized in dynamic construction environments. Recent studies have suggested a strong correlation between viewing patterns of workers and their hazard recognition performance. Hence, it is important to study and analyze the viewing patterns of workers to gain a better understanding of their hazard recognition performance. The objective of this exploratory research is to explore hazard recognition as a visual search process to identifying various visual search factors that affect the process of hazard recognition. Further, the study also proposes a framework to develop a vision based tool capable of recording and analyzing viewing patterns of construction workers and generate feedback for personalized training and proactive safety management.',\n",
       " \"In a pointing task with time constraints, it was only possible to predict the user's error rate when pointing to a stationary target. This study presents a novel model for predicting pointing error rates regardless of the target motion. The model assumes that in the last submovement of the pointing trajectory just before the click, the timing to activate the button is anticipated by the user's internal clock decoding the temporal cues present in the relative movement between the cursor and the target. Then, based on the recent theory of temporal pointing, the model can predict the user's pointing error rate with a high R2 for both stationary (0.993) and moving targets (0.986) by analyzing the kinematic characteristics of the last submovement. In addition, empirical parameters obtained from the model fit succeeded in revealing differences in the cognitive characteristics of experts and novices in first-person shooter games.\",\n",
       " 'Research Objects (ROs) are semantically enhanced aggregations of resources associated to scientific experiments, such as data, provenance of these data, the scientific workflow used to run the experiment, intermediate results, logs and the interpretation of the results. As the number of ROs increases, it is becoming difficult to find ROs to be used, reused or re-purposed. New search and retrieval techniques are required to find the most appropriate ROs for a given researcher, paying attention to provide an intuitive user interface. In this paper we show CollabSpheres, a user interface that provides a new visual metaphor to find ROs by means of a recommendation system that takes advantage of the social aspects of ROs. The experimental evaluation of this tool shows that users perceive high values of usability, user satisfaction, usefulness and ease of use. From the analysis of these results we argue that users perceive the simplicity, intuitiveness and cleanness of this tool, as well as this tool increases collaboration and reuse of research objects.',\n",
       " \"Deaf individuals face great challenges in today's society. It can be very difficult to be able to understand different forms of media without a sense of hearing. Many videos and movies found online today are not captioned, and even fewer have a supporting video with an interpreter. Also, even with a supporting interpreter video provided, information is still lost due to the inability to look at both the video and the interpreter simultaneously. To alleviate this issue, we came up with a tool called closed interpreting. Similar to closed captioning, it will be displayed with an online video and can be toggled on and off. However, the closed interpreter is also user-adjustable. Settings, such as interpreter size, transparency, and location, can be adjusted. Our goal with this study is to find out what deaf and hard of hearing viewers like about videos that come with interpreters, and whether the adjustability is beneficial.\",\n",
       " 'Multimodal simulations augment the presentation of abstract concepts facilitating theoretical models understanding and learning. Most simulations only engage two of our five senses: sight and hearing. If we employ additional sensory communication channels in simulations, we may gain a deeper understanding of illustrated concepts by increasing the communication bandwidth and providing alternative perspectives. We implemented the sense of touch in 3D simulations to teach important concepts in introductory physics. Specifically, we developed a visual/haptic simulation for friction. We prove that interactive 3D haptic simulations, if carefully developed and deployed, are useful in engaging students and allowing them to understand concepts faster. We hypothesize that large scale deployment of such haptic-based simulators in science laboratories is now possible due to the advancements in haptic software and hardware technology.',\n",
       " 'Drones are a versatile platform for both amateur and professional photographers, enabling them to capture photos that are impossible to shoot with ground-based cameras. However, when guided by inexperienced pilots, they have a high incidence of collisions, crashes, and poorly framed photographs. This paper presents an intelligent user interface for photographing objects that is robust against navigation errors and reliably collects high quality photographs. By retaining the human in the loop, our system is faster and more selective than purely autonomous UAVs that employ simple coverage algorithms. The intelligent user interface operates in multiple modes, allowing the user to either directly control the quadcopter or fly in a semi-autonomous mode around a target object in the environment. To evaluate the interface, users completed a data set collection task in which they were asked to photograph objects from multiple views. Our sketchbased control paradigm facilitated task completion, reduced crashes, and was favorably reviewed by the participants.',\n",
       " 'In conditionally automated vehicles, drivers can engage in secondary activities while traveling to their destination. However, drivers are required to appropriately respond, in a limited amount of time, to a take-over request when the system reaches its functional boundaries. In this context, Virtual Reality systems represent a promising training and learning tool to properly familiarize drivers with the automated vehicle and allow them to interact with the novel equipment involved. In this study, the effectiveness of an Head-Mounted display (HMD)-based training program for acquiring interaction skills in automated cars was compared to a user manual and a fixed-base simulator. Results show that the training system affects the take-over performances evaluated in a test drive in a high-end driving simulator. Moreover, self-reported measures indicate that the HMD-based training is preferred with respect to the other systems.',\n",
       " 'We study the performance and user experience of two popular mainstream text entry devices, desktop keyboards and touchscreen keyboards, for use in Virtual Reality (VR) applications. We discuss the limitations arising from limited visual feedback, and examine the efficiency of different strategies of use. We analyze a total of 24 hours of typing data in VR from 24 participants and find that novice users are able to retain about 60% of their typing speed on a desktop keyboard and about 40-45\\\\% of their typing speed on a touchscreen keyboard. We also find no significant learning effects, indicating that users can transfer their typing skills fast into VR. Besides investigating baseline performances, we study the position in which keyboards and hands are rendered in space. We find that this does not adversely affect performance for desktop keyboard typing and results in a performance trade-off for touchscreen keyboard typing.',\n",
       " \"Although the live music entertainment sector does not directly fuel the current debate on automation, it might harbor positions that resonate with it. In this paper we study a prototype software application helping DJs and VJs to accurately manage and even automate the synchronization of visuals with music during amateur or professional live performance. The goal of the study was to unravel VJs' and DJs' ambivalent positions about this software. We preliminarily investigated VJs' and DJs' perception of their sector of activity with seven face-to-face interviews and an online survey (N = 102); then, we asked DJs and VJs (N = 25) for their opinions about our prototype software application. Four core controversies were identified in their answers, along with a set of arguments mobilized to take side on them. The advantages of focusing on ambivalence and argumentation when studying users' response to new media are discussed.\",\n",
       " 'The advent of abundant on-board sensors and electronic devices in vehicles populates the paradigm of participatory sensing to harness crowd-sourced data gathering for intelligent transportation applications, such as distance-to-empty prediction and eco-routing. While participatory sensing can provide diverse driving data, there lacks a systematic study of effective utilization of the data for personalized prediction. There are considerable challenges on how to interpolate the missing data from a sparse dataset, which often arises from participatory sensing. This paper presents and compares various approaches for personalized vehicle energy consumption prediction, including a blackbox framework that identifies driver/vehicle/environment-dependent factors and a collaborative filtering approach based on matrix factorization. Furthermore, a case study of distance-to-empty prediction for electric vehicles by participatory sensing data is conducted and evaluated empirically, which shows that our approaches can significantly improve the prediction accuracy.',\n",
       " 'Cartograms are maps in which areas of geographic regions (countries, states) appear in proportion to some variable of interest (population, income). Cartograms are popular visualizations for geo-referenced data that have been used for over a century and that make it possible to gain insight into patterns and trends in the world around us. Despite the popularity of cartograms and the large number of cartogram types, there are few studies evaluating the effectiveness of cartograms in conveying information. Based on a recent task taxonomy for cartograms, we evaluate four major different types of cartograms: contiguous, non-contiguous, rectangular, and Dorling cartograms. Specifically, we evaluate the effectiveness of these cartograms by quantitative performance analysis, as well as by subjective preferences. We analyze the results of our study in the context of some prevailing assumptions in the literature of cartography and cognitive science. Finally, we make recommendations for the use of different types of cartograms for different tasks and settings.',\n",
       " 'Tactile enhanced multimedia is generated by synchronizing traditional multimedia clips, to generate hot and cold air effect, with an electric heater and a fan. This objective is to give viewers a more realistic and immersing feel of the multimedia content. The response to this enhanced multimedia content (mulsemedia) is evaluated in terms of the appreciation/emotion by using human brain signals. We observe and record electroencephalography (EEG) data using a commercially available four channel MUSE headband. A total of 21 participants voluntarily participated in this study for EEG recordings. We extract frequency domain features from five different bands of each EEG channel. Four emotions namely: happy, relaxed, sad, and angry are classified using a support vector machine in response to the tactile enhanced multimedia. An increased accuracy of 76:19% is achieved when compared to 63:41% by using the time domain features. Our results show that the selected frequency domain features could be better suited for emotion classification in mulsemedia studies.',\n",
       " 'Many people struggle to control their use of digital devices. However, our understanding of the design mechanisms that support user self-control remains limited. In this paper, we make two contributions to HCI research in this space: first, we analyse 367 apps and browser extensions from the Google Play, Chrome Web, and Apple App stores to identify common core design features and intervention strategies afforded by current tools for digital self-control. Second, we adapt and apply an integrative dual systems model of self-regulation as a framework for organising and evaluating the design features found. Our analysis aims to help the design of better tools in two ways: (i) by identifying how, through a well-established model of self-regulation, current tools overlap and differ in how they support self-control; and (ii) by using the model to reveal underexplored cognitive mechanisms that could aid the design of new tools.',\n",
       " \"The purpose of this research is to see the application of modified TAM model by entering the experiential variable as a moderation variable to see one's intention in the use of technology especially internet banking. Data obtained through the distribution of questionnaires to customers. The study population is bank customers registered as users of internet banking services. The sample selection used a simple random sampling technique. Hypothesis testing using Partial Least Square (PLS) method through AMOS program. The results showed that the proposed five hypotheses, two significant and three insignificant. Perceived ease of use is significantly related to perceived usefulness. Per-ceived usefulness is not significantly related to intention to use. Perceived ease of use is significantly related to intention to use moderated by experience and not significantly correlated with intention to use moderated by the Experience.\",\n",
       " 'An important problem for HCI researchers is to estimate the parameter values of a cognitive model from behavioral data. This is a difficult problem, because of the substantial complexity and variety in human behavioral strategies. We report an investigation into a new approach using approximate Bayesian computation (ABC) to condition model parameters to data and prior knowledge. As the case study we examine menu interaction, where we have click time data only to infer a cognitive model that implements a search behaviour with parameters such as fixation duration and recall probability. Our results demonstrate that ABC (i) improves estimates of model parameter values, (ii) enables meaningful comparisons between model variants, and (iii) supports fitting models to individual users. ABC provides ample opportunities for theoretical HCI research by allowing principled inference of model parameter values and their uncertainty.',\n",
       " \"Multiplayer online battle arena games provide an excellent opportunity to study team performance. When designing a team, players must negotiate a \\\\textit{proficiency-congruency dilemma} between selecting roles that best match their experience and roles that best complement the existing roles on the team. We adopt a mixed-methods approach to explore how users negotiate this dilemma. Using data from \\\\textit{League of Legends}, we define a similarity space to operationalize team design constructs about role proficiency, generality, and congruency. We collect publicly available data from 3.36 million users to test the influence of these constructs on team performance. We also conduct focus groups with novice and elite players to understand how players' team design practices vary with expertise. We find that player proficiency increases team performance more than team congruency. These findings have implications for players, designers, and theorists about how to recommend team designs that jointly prioritize individuals' expertise and teams' compatibility.\",\n",
       " \"Today's competition between the professional eSports teams is so strong that in-depth analysis of players' performance literally crucial for creating a powerful team. There are two main approaches to such an estimation: obtaining features and metrics directly from the in-game data or collecting detailed information about the player including data on his/her physical training. While the correlation between the player's skill and in-game data has already been covered in many papers, there are very few works related to analysis of eSports athlete's skill through his/her physical behavior. We propose the smart chair platform which is to collect data on the person's behavior on the chair using an integrated accelerometer, a gyroscope and a magnetometer. We extract the important game events to define the players' physical reactions to them. The obtained data are used for training machine learning models in order to distinguish between the low-skilled and high-skilled players. We extract and figure out the key features during the game and discuss the results.\",\n",
       " \"The usability of small devices such as smartphones or interactive watches is often hampered by the limited size of command vocabularies. This paper is an attempt at better understanding how finger identification may help users invoke commands on touch screens, even without recourse to multi-touch input. We describe how finger identification can increase the size of input vocabularies under the constraint of limited real estate, and we discuss some visual cues to communicate this novel modality to novice users. We report a controlled experiment that evaluated, over a large range of input-vocabulary sizes, the efficiency of single-touch command selections with vs. without finger identification. We analyzed the data not only in terms of traditional time and error metrics, but also in terms of a throughput measure based on Shannon's theory, which we show offers a synthetic and parsimonious account of users' performance. The results show that the larger the input vocabulary needed by the designer, the more promising the identification of individual fingers.\",\n",
       " 'The relation between performance and stress is described by the Yerkes-Dodson Law but varies significantly between individuals. This paper describes a method for determining the individual optimal performance as a function of physiological signals. The method is based on attention and reasoning tests of increasing complexity under monitoring of three physiological signals: Galvanic Skin Response (GSR), Heart Rate (HR), and Electromyogram (EMG). Based on the test results with 15 different individuals, we first show that two of the signals, GSR and HR, have enough discriminative power to distinguish between relax and stress periods. We then show a positive correlation between the complexity level of the tests and the GSR and HR signals, and we finally determine the optimal performance point as the signal level just before a performance decrease. We also discuss the differences among signals depending on the type of test.',\n",
       " \"Using highly interactive systems like computer games requires a lot of visual activity and eye movements. Eye movements are best characterized by visual fixation - periods of time when the eyes stay relatively still over an object. We analyzed the distributions of fixation duration of professional athletes, amateur and newbie players. We show that the analysis of fixation durations can be used to deduce the skill level in computer game players. Highly skilled gaming performance is characterized by more variability in fixation durations and by bimodal fixation duration distributions suggesting the presence of two fixation types in high skill gamers. These fixation types were identified as ambient (automatic spatial processing) and focal (conscious visual processing). The analysis of computer gamers' skill level via the analysis of fixation durations may be used in developing adaptive interfaces and in interface design.\",\n",
       " 'User interfaces provide an interactive window between physical and virtual environments. A new concept in the field of human-computer interaction is a soft user interface; a compliant surface that facilitates touch interaction through deformation. Despite the potential of these interfaces, they currently lack a signal processing framework that can efficiently extract information from their deformation. Here we present OrbTouch, a device that uses statistical learning algorithms, based on convolutional neural networks, to map deformations from human touch to categorical labels (i.e., gestures) and touch location using stretchable capacitor signals as inputs. We demonstrate this approach by using the device to control the popular game Tetris. OrbTouch provides a modular, robust framework to interpret deformation in soft media, laying a foundation for new modes of human computer interaction through shape changing solids.',\n",
       " \"Researchers and experts are taking efforts in delivering an optimal user experience from a long time. Computer interfaces are being developed to keep user 'in the flow' as well as for making users more connected to the real world wile using virtual environment. Developing ubiquitous user interfaces for novices and experts at the same time is crucial work for interaction designers. This paper molds the designing approach of user interfaces in bit different parameters by reviewing the existing literature and proposing a different way to develop a smart user interface to make user more familiar with the design and to keep user 'in the flow'. Contextually proximate approach (CPA) will help users to minimize their feeling of insecurity as designing process includes local resources of users to develop the user interfaces. These various resources and parameters are explained further in the paper by giving different examples.\",\n",
       " \"The rise of increasingly more powerful chatbots offers a new way to collect information through conversational surveys, where a chatbot asks open-ended questions, interprets a user's free-text responses, and probes answers when needed. To investigate the effectiveness and limitations of such a chatbot in conducting surveys, we conducted a field study involving about 600 participants. In this study, half of the participants took a typical online survey on Qualtrics and the other half interacted with an AI-powered chatbot to complete a conversational survey. Our detailed analysis of over 5200 free-text responses revealed that the chatbot drove a significantly higher level of participant engagement and elicited significantly better quality responses in terms of relevance, depth, and readability. Based on our results, we discuss design implications for creating AI-powered chatbots to conduct effective surveys and beyond.\",\n",
       " \"We propose, in this paper, a model of continuous use of corporate collaborative KMS. Companies do not always have the guaranty that their KMS will be continuously used. This statement can constitute an important obstacle for knowledge management processes. Our work is based on the analysis of classical models for initial and continuous use of technologies. We also analyse the regulation concept and explain how it is valuable to support a continuous use of KMS. We observed that awareness may be a regulation means that allows taking this problem into account. Awareness is a concept, which has been profusely used to improve user experience in collaborative environments. It is an important element for regulation of activity. In our model, we assume that one can integrate awareness in information systems to positively influence beliefs about them. The final objective of our work is to refine some concepts to fit the particularities of collaborative KMS and to propose an awareness regulation process using the traces of the users' interactions with the systems.\",\n",
       " \"Many personal devices have transitioned from visual-controlled interfaces to speech-controlled interfaces to reduce costs and interactive friction, supported by the rapid growth in capabilities of speech-controlled interfaces, e.g., Amazon Echo or Apple's Siri. A consequence is that people who are deaf or hard of hearing (DHH) may be unable to use these speech-controlled devices. We show that deaf speech has a high error rate compared to hearing speech, in commercial speech-controlled interfaces. Deaf speech had approximately a 78% word error rate (WER) compared to a hearing speech 18% WER. Our findings show that current speech-controlled interfaces are not usable by DHH people. Based on our findings, significant advances in speech recognition software or alternative approaches will be needed for deaf use of speech-controlled interfaces. We show that current speech-controlled interfaces are not usable by DHH people.\",\n",
       " 'Multi-modality is an important feature of sensor based activity recognition. In this work, we consider two inherent characteristics of human activities, the spatially-temporally varying salience of features and the relations between activities and corresponding body part motions. Based on these, we propose a multi-agent spatial-temporal attention model. The spatial-temporal attention mechanism helps intelligently select informative modalities and their active periods. And the multiple agents in the proposed model represent activities with collective motions across body parts by independently selecting modalities associated with single motions. With a joint recognition goal, the agents share gained information and coordinate their selection policies to learn the optimal recognition model. The experimental results on four real-world datasets demonstrate that the proposed model outperforms the state-of-the-art methods.',\n",
       " 'This paper presents an autoethnography of my experiences living without a mobile phone. What started as an experiment motivated by a personal need to reduce stress, has resulted in two voluntary mobile phone breaks spread over nine years (i.e., 2002-2008 and 2014-2017). Conducting this autoethnography is the means to assess if the lack of having a phone has had any real impact in my life. Based on formative and summative analyses, four meaningful units or themes were identified (i.e., social relationships, everyday work, research career, and location and security), and judged using seven criteria for successful ethnography from existing literature. Furthermore, I discuss factors that allow me to make the choice of not having a mobile phone, as well as the relevance that the lessons gained from not having a mobile phone have on the lives of people who are involuntarily disconnected from communication infrastructures.',\n",
       " 'This research proposes Finger Based Technique (FBT) for non-visual touch screen device interaction designed for blind users. Based on the proposed technique, the blind user can access virtual keys based on finger holding positions. Three different models have been proposed. They are Single Digit Finger-Digit Input (FDI), Double Digit FDI for digital text entry, and Finger-Text Input (FTI) for normal text entry. All the proposed models were implemented with voice feedback while enabling touch as the input gesture. The models were evaluated with 7 blind participants with Samsung Galaxy S2 apparatus. The results show that Single Digit FDI is substantially faster and more accurate than Double Digit FDI and iPhone voice-over. FTI also looks promising for text entry. Our study also reveals 11 accessible regions to place widgets for quick access by blind users in flat touch screen based smartphones. Identification of these accessible regions will promote dynamic interactions for blind users and serve as a usability design framework for touch screen applications.',\n",
       " \"Entry-level crowd work is often reported to pay less than minimum wage. While this may be appropriate or even necessary, due to various legal, economic, and pragmatic factors, some Requesters and workers continue to question this status quo. To promote further discussion on the issue, we survey Requesters and workers whether they would support restricting tasks to require minimum wage pay. As a form of design activism, we confronted workers with this dilemma directly by posting a dummy Mechanical Turk task which told them that they could not work on it because it paid less than their local minimum wage, and we invited their feedback. Strikingly, for those workers expressing an opinion, two-thirds of Indians favored the policy while two-thirds of Americans opposed it. Though a majority of Requesters supported minimum wage pay, only 20\\\\% would enforce it. To further empower Requesters, and to ensure that effort or ignorance are not barriers to change, we provide a simple public API to make it easy to find a worker's local minimum wage by his/her IP address.\",\n",
       " \"Currently, adaptive voice applications supported by voice assistants (VA) are very popular (i.e., Alexa skills and Google Home Actions). Under this circumstance, how to design and evaluate these voice interactions well is very important. In our study, we developed a voice crawler to collect responses from 100 most popular Alexa skills under 10 different categories and evaluated these responses to find out how they comply with 8 selected design guidelines published by Amazon. Our findings show that basic commands support are the most followed ones while those related to personalised interaction are relatively less. There also exists variation in design guidelines compliance across different skill categories. Based on our findings and real skill examples, we offer suggestions for new guidelines to complement the existing ones and propose agendas for future HCI research to improve voice applications' user experiences.\",\n",
       " \"Reproducibility should be a cornerstone of scientific research and is a growing concern among the scientific community and the public. Understanding how to design services and tools that support documentation, preservation and sharing is required to maximize the positive impact of scientific research. We conducted a study of user attitudes towards systems that support data preservation in High Energy Physics, one of science's most data-intensive branches. We report on our interview study with 12 experimental physicists, studying requirements and opportunities in designing for research preservation and reproducibility. Our findings suggest that we need to design for motivation and benefits in order to stimulate contributions and to address the observed scalability challenge. Therefore, researchers' attitudes towards communication, uncertainty, collaboration and automation need to be reflected in design. Based on our findings, we present a systematic view of user needs and constraints that define the design space of systems supporting reproducible practices.\",\n",
       " 'This study proposes middleware, GameControllerizer, that allows users to combine the processes of Internet of Things (IoT) devices, Web services, and applications of Artificial Intelligence (AI), and to convert them into game control operations to augment existing digital games. The system facilitates easy trial-and-error development of new forms of entertainment and the configuration of gamification by enabling the use of diverse devices and sources of information as inputs to games. GameControllerizer consists of a visual programming element that uses the Node-RED tool to allow users to program easily to convert diverse formats of information into inputs to games, and contains a game input emulation element whereby hardware- and software-based emulation generates inputs for gaming devices. Evidence of the usefulness of the system was provided by a performance assessment and the proposal of a variety of use cases.',\n",
       " \"Dynamic Difficulty Adjustment (DDA) is a mechanism used in video games that automatically tailors the individual gaming experience to match an appropriate difficulty setting. This is generally achieved by removing pre-defined difficulty tiers such as Easy, Medium and Hard; and instead concentrates on balancing the gameplay to match the challenge to the individual's abilities. The work presented in this paper examines the implementation of DDA in a custom survival game developed by the author, namely Colwell's Castle Defence. The premise of this arcade-style game is to defend a castle from hordes of oncoming enemies. The AI system that we developed adjusts the enemy spawn rate based on the current performance of the player. Specifically, we read the Player Health and Gate Health at the end of each level and then assign the player with an appropriate difficulty tier for the proceeding level. We tested the impact of our technique on thirty human players and concluded, based on questionnaire feedback, that enabling the technique led to more enjoyable gameplay.\",\n",
       " \"Even though many approaches have been proposed for entity resolution (ER), it remains very challenging to find one with quality guarantees. To this end, we proposea risk-aware HUman-Machine cOoperation framework for ER, denoted by r-HUMO. Built on the existing HUMO framework, r-HUMO similarly enforces both precision and recall levels by partitioning an ER workload between the human and the machine. However, r-HUMO is the first solution to optimize the process of human workload selection from a risk perspective. It iteratively selects human workload based on real-time risk analysis on human-labeled results as well as prespecified machine metrics. In this paper,we first introduce the r-HUMO framework and then present the risk analysis technique to prioritize the instances for manual labeling. Finally,we empirically evaluate r-HUMO's performance on real data. Our extensive experiments show that r-HUMO is effective in enforcing quality guarantees,and compared with the state-of-the-art alternatives, it can achieve better quality control with reduced human cost.\",\n",
       " \"Machine learning is the capacity of a computational system to learn structures from datasets in order to make predictions on newly seen data. Such an approach offers a significant advantage in music scenarios in which musicians can teach the system to learn an idiosyncratic style, or can break the rules to explore the system's capacity in unexpected ways. In this chapter we draw on music, machine learning, and human-computer interaction to elucidate an understanding of machine learning algorithms as creative tools for music and the sonic arts. We motivate a new understanding of learning algorithms as human-computer interfaces. We show that, like other interfaces, learning algorithms can be characterised by the ways their affordances intersect with goals of human users. We also argue that the nature of interaction between users and algorithms impacts the usability and usefulness of those algorithms in profound ways. This human-centred view of machine learning motivates our concluding discussion of what it means to employ machine learning as a creative tool.\",\n",
       " \"In recent years, configuration problems have drawn tremendous attention because of their increasing prevalence and their big impact on system availability. We believe that many of these problems are attributable to today's configuration interfaces that have not evolved to accommodate the enormous shift of the system administrator group. Plain text files, as the de facto configuration interfaces, assume administrators' understanding of the system under configuration. They ask administrators to directly edit the corresponding entries with little guidance or assistance. However, this assumption no longer holds for todays administrator group which has expanded greatly to include non- and semi-professional administrators. In this paper, we provide an HCI view of today's configuration problems, and articulate system configuration as a new HCI problem. Moreover, we present the top obstacles to correctly and efficiently configuring software systems, and most importantly their implications on the design and implementation of new-generation configuration interfaces.\",\n",
       " 'Design Thinking workshops are used by companies to help generate new ideas for technologies and products by engaging subjects in exercises to understand their users\\' wants and become more empathetic towards their needs. The \"aha moment\" experienced during these thought-provoking, step outside the yourself activities occurs when a group of persons iterate over several problems and converge upon a solution that will fit seamlessly everyday life. With the increasing use and cost of Design workshops being offered, it is important that technology be developed that can help identify empathy and its onset in humans. This position paper presents an approach to modeling empathy using Gaussian mixture models and heart rate and skin conductance. This paper also presents an updated approach to Design Thinking that helps to ensure participants are thinking outside of their own race\\'s, culture\\'s, or other affiliations\\' motives.',\n",
       " \"This work addresses our research on driving skill modeling using artificial neural networks for haptic assistance. In this paper, we present a haptic driving training simulator with performance-based, error-corrective haptic feedback. One key component of our simulator is the ability to learn an optimized driving skill model from the driving data of expert drivers. To this end, we obtain a model utilizing artificial neural networks to extract a desired movement of a steering wheel and an accelerator pedal based on the experts' prediction. Then, we can deliver haptic assistance based on a driver's performance error which is a difference between a current and the desired movement. We validate the performance of our framework in two respective user experiments recruiting expert/novice drivers to show the feasibility and applicability of facilitating neural networks for performance-based haptic driving skill transfer.\",\n",
       " 'The uncertainty and variability of underwater environment propose the request to control underwater robots in real time and dynamically, especially in the scenarios where human and robots need to work collaboratively in the field. However, the underwater environment imposes harsh restrictions on the application of typical control and communication methods. Considering that gestures are a natural and efficient interactive way for human, we, utilizing convolution neural network, implement a real-time gesture-based recognition system, who can recognize 50 kinds of gestures from images captured by one normal monocular camera, and apply this recognition system in human and underwater robot interaction. We design A Flexible and Extendable Interaction Scheme (AFEIS) through which underwater robots can be programmed in situ underwater by human operators using customized gesture-based sign language. This paper elaborates the design of gesture recognition system and AFEIS, and presents our field trial results when applying this system and scheme on underwater robots.',\n",
       " 'The F-layout was introduced in 1955 and eventually enforced as a national standard as a replacement to the popular QWERTY keyboard layout in Turkey. In a more recent work, another alternative (E-layout) was developed for Turkish language and argued to be faster and more comfortable than the F-layout. However, there has not been any empirical evidence favouring any of these layouts so far. To fill this research gap in the literature, we have employed a hybrid model and conducted both between-subjects and within-subjects user experiments with twelve freshmen majoring in computer engineering. The experimental results show that there is no significant difference between learning percentage of these two layouts but the completion time of typing a trial passage with the F-layout is significantly lower than the E-layout. The F-layout has also a significantly lower physical demand score, as revealed by the subjective assessments of participants. Based on our user survey data, we also discuss some possible reasons of F-keyboard limited prevalence among Turkish users.',\n",
       " \"Mental Imagery based Brain-Computer Interfaces (MI-BCI) are a mean to control digital technologies by performing MI tasks alone. Throughout MI-BCI use, human supervision (e.g., experimenter or caregiver) plays a central role. While providing emotional and social feedback, people present BCIs to users and ensure smooth users' progress with BCI use. Though, very little is known about the influence experimenters might have on the results obtained. Such influence is to be expected as social and emotional feedback were shown to influence MI-BCI performances. Furthermore, literature from different fields showed an experimenter effect, and specifically of their gender, on experimental outcome. We assessed the impact of the interaction between experi-menter and participant gender on MI-BCI performances and progress throughout a session. Our results revealed an interaction between participants gender, experimenter gender and progress over runs. It seems to suggest that women experimenters may positively influence partici-pants' progress compared to men experimenters.\",\n",
       " 'Driving simulators can be used to test vehicle designs earlier, prior to building physical prototypes. One area of particular interest is winter testing since testing is limited to specific times of year and specific regions in the world. To ensure that the simulator is fit for purpose, an objective assessment is required. In this study a simulator and real world comparison was performed with three simulator configurations (standard, no steering torque, no motion) to assess the ability of a utility triplet of analyses to be able to quantify the differences between the real world and the different simulator configurations. The results suggest that the utility triplet is effective in measuring the differences in simulator configurations and that the developed Virtual Sweden environment achieved rather good behavioural fidelity in the sense of preserving absolute levels of many measures of behaviour. The main limitation in the simulated environment seemed to be the poor match of the dynamic lateral friction limit on snow and ice when compared to the real world.',\n",
       " 'Human thermal comfort measurement plays a critical role in giving feedback signals for building energy efficiency. A non-invasive measuring method based on subtleness magnification and deep learning (NIDL) was designed to achieve a comfortable, energy efficient built environment. The method relies on skin feature data, e.g., subtle motion and texture variation, and a 315-layer deep neural network for constructing the relationship between skin features and skin temperature. A physiological experiment was conducted for collecting feature data (1.44 million) and algorithm validation. The non-invasive measurement algorithm based on a partly-personalized saturation temperature model (NIPST) was used for algorithm performance comparisons. The results show that the mean error and median error of the NIDL are 0.4834 Celsius and 0.3464 Celsius which is equivalent to accuracy improvements of 16.28% and 4.28%, respectively.',\n",
       " 'Semi-supervised learning is crucial for alleviating labelling burdens in people-centric sensing. However, human-generated data inherently suffer from distribution shift in semi-supervised learning due to the diverse biological conditions and behavior patterns of humans. To address this problem, we propose a generic distributionally robust model for semi-supervised learning on distributionally shifted data. Considering both the discrepancy and the consistency between the labeled data and the unlabeled data, we learn the latent features that reduce person-specific discrepancy and preserve task-specific consistency. We evaluate our model in a variety of people-centric recognition tasks on real-world datasets, including intention recognition, activity recognition, muscular movement recognition and gesture recognition. The experiment results demonstrate that the proposed model outperforms the state-of-the-art methods.',\n",
       " 'Intelligent agents such as Alexa, Siri, and Google Assistant are now built into streaming TV systems, allowing people to use voice input to navigate the increasingly complex set of apps available on a TV. However, these systems typically support a narrow range of control- and search-oriented commands, and do not support deeper recommendation or exploration queries. To learn about how people interact with a recommendation-oriented voice-controlled TV, we use research through design methods to explore an early prototype movie recommendation system where the only input modality is voice. We describe in-depth qualitative research sessions with 11 participants. We contribute implications for designers of voice-controlled TV: mitigating the drawbacks of voice-only interactions, navigating the tension between expressiveness and efficiency, and building voice-driven recommendation interfaces that facilitate exploration.',\n",
       " 'We present a novel multi-modal bio-sensing platform capable of integrating multiple data streams for use in real-time applications. The system is composed of a central compute module and a companion headset. The compute node collects, time-stamps and transmits the data while also providing an interface for a wide range of sensors including electroencephalogram, photoplethysmogram, electrocardiogram, and eye gaze among others. The companion headset contains the gaze tracking cameras. By integrating many of the measurements systems into an accessible package, we are able to explore previously unanswerable questions ranging from open-environment interactions to emotional response studies. Though some of the integrated sensors are designed from the ground-up to fit into a compact form factor, we validate the accuracy of the sensors and find that they perform similarly to, and in some cases better than, alternatives.',\n",
       " \"Distributed, parallel crowd workers can accomplish simple tasks through workflows, but teams of collaborating crowd workers are necessary for complex goals. Unfortunately, a fundamental condition for effective teams - familiarity with other members - stands in contrast to crowd work's flexible, on-demand nature. We enable effective crowd teams with Huddler, a system for workers to assemble familiar teams even under unpredictable availability and strict time constraints. Huddler utilizes a dynamic programming algorithm to optimize for highly familiar teammates when individual availability is unknown. We first present a field experiment that demonstrates the value of familiarity for crowd teams: familiar crowd teams doubled the performance of ad-hoc (unfamiliar) teams on a collaborative task. We then report a two-week field deployment wherein Huddler enabled crowd workers to convene highly familiar teams in 18 minutes on average. This research advances the goal of supporting long-term, team-based collaborations without sacrificing the flexibility of crowd work.\",\n",
       " 'Human-computer interaction (HCI) studies the design and use of interfaces and interactive systems. HCI has been adopted successfully in modern commercial products. Recently, its use for promoting social good and pursuing sustainability, known as sustainable HCI, has begun to receive wide attention. Conventionally, scientists and decision-makers apply top-down approaches to lead research activities that engage lay people in facilitating sustainability, such as saving energy. We introduce an alternative framework, Community Citizen Science (CCS), to closely connect research and social issues by empowering communities to produce scientific knowledge, represent their needs, address their concerns, and advocate for impact. CCS advances the current science-oriented concept to a deeper level that aims to sustain community engagement when researchers are no longer involved after the intervention of interactive systems.',\n",
       " \"Knowing who is in one's vicinity is key to managing privacy in everyday environments, but is challenging for people with visual impairments. Wearable cameras and other sensors may be able to detect such information, but how should this complex visually-derived information be conveyed in a way that is discreet, intuitive, and unobtrusive? Motivated by previous studies on the specific information that visually impaired people would like to have about their surroundings, we created three medium-fidelity prototypes: 1) a 3D printed model of a watch to convey tactile information; 2) a smartwatch app for haptic feedback; and 3) a smartphone app for audio feedback. A usability study with 14 participants with visual impairments identified a range of practical issues (e.g., speed of conveying information) and design considerations (e.g., configurable privacy bubble) for conveying privacy feedback in real-world contexts.\",\n",
       " 'The application of mobile computing is currently altering patterns of our behavior to a greater degree than perhaps any other invention. In combination with the introduction of BLE (Bluetooth Low Energy) and similar technologies enabling context-awareness, designers are today finding themselves empowered to build experiences and facilitate interactions with our physical surroundings in ways not possible before. The aim of this thesis is to present a research project, currently underway at the University of Cambridge, which is dealing with implementation of a BLE system into a museum environment. By assessing the technology, describing the design decisions as well as presenting a qualitative evaluation, this paper seeks to provide insight into some of the challenges and possible solutions connected to the process of developing ubiquitous BLE computing systems for public spaces. The project outcome revealed the potential use of BLE to engage whole new groups of audiences as well as made me argue in favor of a more seamful approach to the design of these systems.',\n",
       " 'Making decisions about what clinical tasks to prepare for is multi-factored, and especially challenging in intensive care environments where resources must be balanced with patient needs. Electronic health records (EHRs) are a rich data source, but are task-agnostic and can be difficult to use as summarizations of patient needs for a specific task, such as \"could this patient need a ventilator tomorrow?\" In this paper, we introduce ClinicalVis, an open-source EHR visualization-based prototype system for task-focused design evaluation of interactions between healthcare providers (HCPs) and EHRs. We situate ClinicalVis in a task-focused proof-of-concept design study targeting these interactions with real patient data. We conduct an empirical study of 14 HCPs, and discuss our findings on usability, accuracy, preference, and confidence in treatment decisions. We also present design implications that our findings suggest for future EHR interfaces, the presentation of clinical data for task-based planning, and evaluating task-focused HCP/EHR interactions in practice.',\n",
       " 'Invasive species are a major cause of ecological damage and commercial losses. A current problem spreading in North America and Europe is the vinegar fly Drosophila suzukii. Unlike other Drosophila, it infests non-rotting and healthy fruits and is therefore of concern to fruit growers, such as vintners. Consequently, large amounts of data about infestations have been collected in recent years. However, there is a lack of interactive methods to investigate this data. We employ ensemble-based classification to predict areas susceptible to infestation by D. suzukii and bring them into a spatio-temporal context using maps and glyph-based visualizations. Following the information-seeking mantra, we provide a visual analysis system Drosophigator for spatio-temporal event prediction, enabling the investigation of the spread dynamics of invasive species. We demonstrate the usefulness of this approach in two use cases.',\n",
       " 'In this paper we focus on the International Journal of Human-Computer Studies (IJHCS) as a domain of analysis, to gain insights about its evolution in the past 50 years and what this evolution tells us about the research landscape associated with the journal. To this purpose we use techniques from the field of Science of Science and analyse the relevant scholarly data to identify a variety of phenomena, including significant geopolitical patterns, the key trends that emerge from a topic-centric analysis, and the insights that can be drawn from an analysis of citation data. Because the area of Human-Computer Interaction (HCI) has always been a central focus for IJHCS, we also include in the analysis the CHI conference, which is the premiere scientific venue in HCI. Analysing both venues provides more data points to our study and allows us to consider two alternative viewpoints on the evolution of HCI research.',\n",
       " 'Internet of Things (IoT) systems are bundles of networked sensors and actuators that are deployed in an environment and act upon the sensory data that they receive. These systems, especially consumer electronics, have two main cooperating components: a device and a mobile app. The unique combination of hardware and software in IoT systems presents challenges that are lesser known to mainstream software developers. They might require innovative solutions to support the development and integration of such systems. In this paper, we analyze more than 90,000 reviews of ten IoT devices and their corresponding apps and extract the issues that users encountered while using these systems. Our results indicate that issues with connectivity, timing, and updates are particularly prevalent in the reviews. Our results call for a new software-hardware development framework to assist the development of reliable IoT systems.',\n",
       " \"Navigation applications are becoming ubiquitous in our daily navigation experiences. With the intention to circumnavigate congested roads, their route guidance always follows the basic assumption that drivers always want the fastest route. However, it is unclear how their recommendations are followed and what factors affect their adoption. We present the results of a semi-structured qualitative study with 17 drivers, mostly from the Philippines and Japan. We recorded their daily commutes and occasional trips, and inquired into their navigation practices, route choices and on-the-fly decision-making. We found that while drivers choose a recommended route in urgent situations, many still preferred to follow familiar routes. Drivers deviated because of a recommendation's use of unfamiliar roads, lack of local context, perceived driving unsuitability, and inconsistencies with realized navigation experiences. Our findings and implications emphasize their personalization needs, and how the right amount of algorithmic sophistication can encourage behavioral adaptation.\",\n",
       " 'Effective data analysis ideally requires the analyst to have high expertise as well as high knowledge of the data. Even with such familiarity, manually pursuing all potential hypotheses and exploring all possible views is impractical. We present DataSite, a proactive visual analytics system where the burden of selecting and executing appropriate computations is shared by an automatic server-side computation engine. Salient features identified by these automatic background processes are surfaced as notifications in a feed timeline. DataSite effectively turns data analysis into a conversation between analyst and computer, thereby reducing the cognitive load and domain knowledge requirements. We validate the system with a user study comparing it to a recent visualization recommendation system, yielding significant improvement, particularly for complex analyses that existing analytics systems do not support well.',\n",
       " 'This paper describes \"ARbis Pictus\" --a novel system for immersive language learning through dynamic labeling of real-world objects in augmented reality. We describe a within-subjects lab-based study (N=52) that explores the effect of our system on participants learning nouns in an unfamiliar foreign language, compared to a traditional flashcard-based approach. Our results show that the immersive experience of learning with virtual labels on real-world objects is both more effective and more enjoyable for the majority of participants, compared to flashcards. Specifically, when participants learned through augmented reality, they scored significantly better by 7% (p=0.011) on productive recall tests performed same-day, and significantly better by 21% (p=0.001) on 4-day delayed productive recall post tests than when they learned using the flashcard method. We believe this result is an indication of the strong potential for language learning in augmented reality, particularly because of the improvement shown in sustained recall compared to the traditional approach.',\n",
       " 'One of the most crucial issues in data mining is to model human behaviour in order to provide personalisation, adaptation and recommendation. This usually involves implicit or explicit knowledge, either by observing user interactions, or by asking users directly. But these sources of information are always subject to the volatility of human decisions, making utilised data uncertain to a particular extent. In this contribution, we elaborate on the impact of this human uncertainty when it comes to comparative assessments of different data mining approaches. In particular, we reveal two problems: (1) biasing effects on various metrics of model-based prediction and (2) the propagation of uncertainty and its thus induced error probabilities for algorithm rankings. For this purpose, we introduce a probabilistic view and prove the existence of those problems mathematically, as well as provide possible solution strategies. We exemplify our theory mainly in the context of recommender systems along with the metric RMSE as a prominent example of precision quality measures.',\n",
       " 'This paper outlines the development and testing of a novel, feedback-enabled attention allocation aid (AAAD), which uses real-time physiological data to improve human performance in a realistic sequential visual search task. Indeed, by optimizing over search duration, the aid improves efficiency, while preserving decision accuracy, as the operator identifies and classifies targets within simulated aerial imagery. Specifically, using experimental eye-tracking data and measurements about target detectability across the human visual field, we develop functional models of detection accuracy as a function of search time, number of eye movements, scan path, and image clutter. These models are then used by the AAAD in conjunction with real time eye position data to make probabilistic estimations of attained search accuracy and to recommend that the observer either move on to the next image or continue exploring the present image. An experimental evaluation in a scenario motivated from human supervisory control in surveillance missions confirms the benefits of the AAAD.',\n",
       " \"Automatic detection of emotion has the potential to revolutionize mental health and wellbeing. Recent work has been successful in predicting affect from unimodal electrocardiogram (ECG) data. However, to be immediately relevant for real-world applications, physiology-based emotion detection must make use of ubiquitous photoplethysmogram (PPG) data collected by affordable consumer fitness trackers. Additionally, applications of emotion detection in healthcare settings will require some measure of uncertainty over model predictions. We present here a Bayesian deep learning model for end-to-end classification of emotional valence, using only the unimodal heartbeat time series collected by a consumer fitness tracker (Garmin V\\\\'ivosmart 3). We collected a new dataset for this task, and report a peak F1 score of 0.7. This demonstrates a practical relevance of physiology-based emotion detection `in the wild' today.\",\n",
       " \"Pedestrian's road crossing behaviour is one of the important aspects of urban dynamics that will be affected by the introduction of autonomous vehicles. In this study we introduce DeepSurvival, a novel framework for estimating pedestrian's waiting time at unsignalized mid-block crosswalks in mixed traffic conditions. We exploit the strengths of deep learning in capturing the nonlinearities in the data and develop a cox proportional hazard model with a deep neural network as the log-risk function. An embedded feature selection algorithm for reducing data dimensionality and enhancing the interpretability of the network is also developed. We test our framework on a dataset collected from 160 participants using an immersive virtual reality environment. Validation results showed that with a C-index of 0.64 our proposed framework outperformed the standard cox proportional hazard-based model with a C-index of 0.58.\",\n",
       " \"Personality affect the way someone feels or acts. This paper examines the effect of personality traits, as operationalized by the Big-five questionnaire, on the number, type, and severity of the identified usability issues, physiological signals (skin conductance), and subjective emotional ratings (valence-arousal).Twenty-four users interacted with a web service and then participated in a retrospective thinking aloud session. Results revealed that the number of usability issues is significantly affected by the Openness trait. Emotional Stability significantly affects the type of reported usability issues. Problem severity is not affected by any trait. Valence ratings are significantly affected by Conscientiousness, whereas Agreeableness, Emotional Stability and Openness significantly affect arousal ratings. Finally, Openness has a significant effect on the number of detected peaks in user's skin conductance.\",\n",
       " 'Effective task management is essential to successful team collaboration. While the past decade has seen considerable innovation in systems that track and manage group tasks, these innovations have typically been outside of the principal communication channels: email, instant messenger, and group chat. Teams formulate, discuss, refine, assign, and track the progress of their collaborative tasks over electronic communication channels, yet they must leave these channels to update their task-tracking tools, creating a source of friction and inefficiency. To address this problem, we explore how bots might be used to mediate task management for individuals and teams. We deploy a prototype bot to eight different teams of information workers to help them create, assign, and keep track of tasks, all within their main communication channel. We derived seven insights for the design of future bots for coordinating work.',\n",
       " 'Proactive decision support (PDS) helps in improving the decision making experience of human decision makers in human-in-the-loop planning environments. Here both the quality of the decisions and the ease of making them are enhanced. In this regard, we propose a PDS framework, named RADAR, based on the research in Automated Planning in AI, that aids the human decision maker with her plan to achieve her goals by providing alerts on: whether such a plan can succeed at all, whether there exist any resource constraints that may foil her plan, etc. This is achieved by generating and analyzing the landmarks that must be accomplished by any successful plan on the way to achieving the goals. Note that, this approach also supports naturalistic decision making which is being acknowledged as a necessary element in proactive decision support, since it only aids the human decision maker through suggestions and alerts rather than enforcing fixed plans or decisions. We demonstrate the utility of the proposed framework through search-and-rescue examples in a fire-fighting domain.',\n",
       " 'Objective: Effective collaboration between machines and clinicians requires flexible data structures to represent medical processes and clinical practice guidelines. Such a data structure could enable effective turn-taking between human and automated components of a complex treatment, accurate on-line monitoring of clinical treatments (for example to detect medical errors), or automated treatment systems (such as future medical robots) whose overall treatment plan is understandable and auditable by human experts.\\n  Materials and Methods: Behavior trees (BTs) emerged from video game development as a graphical language for modeling intelligent agent behavior. BTs have several properties which are attractive for modeling medical procedures including human-readability, authoring tools, and composability.\\n  Results: This paper will illustrate construction of BTs for exemplary medical procedures and clinical protocols.\\n  Discussion and Conclusion: Behavior Trees thus form a useful, and human authorable/readable bridge between clinical practice guidelines and AI systems.',\n",
       " \"Android is Google's latest open source software platform for mobile devices which has already attained enormous popularity. The purpose of this paper is to describe the development of mobile application for shopping mall using Android platform. A prototype was developed for the shoppers of Bashundhara Shopping Mall of Bangladesh. This prototype will serve as a framework for any such applications (apps). The paper presents a practical demonstration of how to integrate shops' information, such as names, categories, locations, descriptions, floor layout and so forth, with map module via an android application. A summary of survey results of the related literature and projects have also been included. Critical Evaluation of the prototype along with future research and development plan has been revealed. The paper will serve as a guideline for the researchers and developers to introduce and develop similar apps.\",\n",
       " \"The increasing prevalence of Virtual Reality technologies as a platform for gaming and video playback warrants research into how to best apply the current state of the art to challenges in data visualization. Many current VR systems are noncollaborative, while data analysis and visualization is often a multi-person process. Our goal in this paper is to address the technical and user experience challenges that arise when creating VR environments for collaborative data visualization. We focus on the integration of multiple tracking systems and the new interaction paradigms that this integration can enable, along with visual design considerations that apply specifically to collaborative network visualization in virtual reality. We demonstrate a system for collaborative interaction with large 3D layouts of Twitter friend/follow networks. The system is built by combining a 'Holojam' architecture (multiple GearVR Headsets within an OptiTrack motion capture stage) and Perception Neuron motion suits, to offer an untethered, full-room multi-person visualization experience.\",\n",
       " \"In this work, we address the symptoms of cognitive depletion as they relate to generalized knowledge workers. We unify previous findings within a single analytical model of cognitive depletion. Our purpose is to develop a model that will help us predict when a person has reached a sufficient state of cognitive depletion such that taking a break or some other restorative action will benefit both his or her own wellbeing and the quality of his or her performance. We provide a definition of each symptom in our model as well as the effect it would have on a knowledge worker's ability to work productively. We discuss methods to detect each symptom that do not require self assessment. Understanding symptoms of cognitive depletion provides the ability to support human knowledge workers by reducing the stress involved with cognitive and work overload while maintaining or improving the quality of their performance.\",\n",
       " 'Pluridisciplinar convergence is a major problem that had emerged with Human-Artefact Systems and so-called \" Augmented Humanity \" as academical fields and even more as technical fields. Problems come mainly from the juxtaposition of two very different types of system, a biological one and an artificial one. Thus, conceiving and designing the multiple couplings between them has become a major difficulty. Some came with reductionnist solutions to answer these problems but since we know that a biological system and a technical system are different, this approach is limited from its beginning. Using a specifically designed questionnaire and statistical analysis we determined how specialists (medical practitioners, ergonomists and engineers) in the domain conceive themselves what is a Human-Artifact System and how they relate to existent traditions and showed that some of them relate to the integrativist views.',\n",
       " 'In this paper, we present an abstract model of visualization and inference processes and describe an information-theoretic measure for optimizing such processes. In order to obtain such an abstraction, we first examined six classes of workflows in data analysis and visualization, and identified four levels of typical visualization components, namely disseminative, observational, analytical and model-developmental visualization. We noticed a common phenomenon at different levels of visualization, that is, the transformation of data spaces (referred to as alphabets) usually corresponds to the reduction of maximal entropy along a workflow. Based on this observation, we establish an information-theoretic measure of cost-benefit ratio that may be used as a cost function for optimizing a data visualization process. To demonstrate the validity of this measure, we examined a number of successful visualization processes in the literature, and showed that the information-theoretic measure can mathematically explain the advantages of such processes over possible alternatives.',\n",
       " 'Conversational agents promise conversational interaction but fail to deliver. Efforts often emulate functional rules from human speech, without considering key characteristics that conversation must encapsulate. Given its potential in supporting long-term human-agent relationships, it is paramount that HCI focuses efforts on delivering this promise. We aim to understand what people value in conversation and how this should manifest in agents. Findings from a series of semi-structured interviews show people make a clear dichotomy between social and functional roles of conversation, emphasising the long-term dynamics of bond and trust along with the importance of context and relationship stage in the types of conversations they have. People fundamentally questioned the need for bond and common ground in agent communication, shifting to more utilitarian definitions of conversational qualities. Drawing on these findings we discuss key challenges for conversational agent design, most notably the need to redefine the design parameters for conversational agent interaction.',\n",
       " 'This paper studies the concept of color semantics by modeling a dataset of magazine cover designs, evaluating the model via crowdsourcing, and demonstrating several prototypes that facilitate color-related design tasks. We investigate a probabilistic generative modeling framework that expresses semantic concepts as a combination of color and word distributions $-$color-word topics. We adopt an extension to Latent Dirichlet Allocation (LDA) topic modeling called LDA-dual to infer a set of color-word topics over a corpus of 2,654 magazine covers spanning 71 distinct titles and 12 genres. While LDA models text documents as distributions over word topics, we model magazine covers as distributions over color-word topics. The results of our crowdsourced experiments confirm that the model is able to successfully discover the associations between colors and linguistic concepts. Finally, we demonstrate several simple prototypes that apply the learned model to color palette recommendation, design example retrieval, image retrieval, image color selection, and image recoloring.',\n",
       " 'In this study, we developed a method to estimate the relationship between stimulation current and volatility during isometric contraction. In functional electrical stimulation (FES), joints are driven by applying voltage to muscles. This technology has been used for a long time in the field of rehabilitation, and recently application oriented research has been reported. However, estimation of the relationship between stimulus value and exercise capacity has not been discussed to a great extent. Therefore, in this study, a human muscle model was estimated using the transfer function estimation method with fast Fourier transform. It was found that the relationship between stimulation current and force exerted could be expressed by a first-order lag system. In verification of the force estimate, the ability of the proposed model to estimate the exerted force under steady state response was found to be good.',\n",
       " 'Result diversification is an important aspect in query events, web-based search, facility location and other applications. To satisfy more users in event-based social networks(EBSNs), search result diversification in an event that covers as many user intents as possible. Most existing result diversification algorithms recognize an user may search for information by issuing the different query as much as possible. In this paper, we leverage many different users in a same event such that satisfy the maximum benefit of users, where users want to participate in an event that s/he did not know any users, for example, blind date, Greek and other activities. To solve this problem, we devise an effective greedy heuristic method and integrate simulated annealing techniques to optimize the algorithm performance. In particular, the Greedy algorithm is more effective but less efficient than Integrate Simulated Annealing in most cases. Finally, we conduct extensive experiments on real and synthetic datasets which verify the efficiency and effectiveness of our proposed algorithms.',\n",
       " \"Room-level air conditioners (also referred as ACs) consume a significant proportion of total energy in residential and small-scale commercial buildings. In a typical AC, occupants specify their comfort requirements by manually setting the desired temperature on the thermostat. Though commercial thermostats (such as Tado) provide basic energy-saving features, they neither consider the influence of external factors (such as weather) to set the thermostat temperature nor offer advanced features such as monitoring the fitness level of AC. In this paper, we discuss grey-box modeling techniques to enhance existing thermostats for energy-efficient control of the ACs and provide actionable and corrective feedback to the users. Our study indicates that the enhancements can reduce occupants' discomfort by 23% when maximising the user experience, and reduce AC energy consumption by 26% during the power-saving mode.\",\n",
       " 'Though detection systems have been developed to identify obscene content such as pornography and violence, artificial intelligence is simply not good enough to fully automate this task yet. Due to the need for manual verification, social media companies may hire internal reviewers, contract specialized workers from third parties, or outsource to online labor markets for the purpose of commercial content moderation. These content moderators are often fully exposed to extreme content and may suffer lasting psychological and emotional damage. In this work, we aim to alleviate this problem by investigating the following question: How can we reveal the minimum amount of information to a human reviewer such that an objectionable image can still be correctly identified? We design and conduct experiments in which blurred graphic and non-graphic images are filtered by human moderators on Amazon Mechanical Turk (AMT). We observe how obfuscation affects the moderation experience with respect to image classification accuracy, interface usability, and worker emotional well-being.',\n",
       " 'Crowd-powered conversational assistants have been shown to be more robust than automated systems, but do so at the cost of higher response latency and monetary costs. A promising direction is to combine the two approaches for high quality, low latency, and low cost solutions. In this paper, we introduce Evorus, a crowd-powered conversational assistant built to automate itself over time by (i) allowing new chatbots to be easily integrated to automate more scenarios, (ii) reusing prior crowd answers, and (iii) learning to automatically approve response candidates. Our 5-month-long deployment with 80 participants and 281 conversations shows that Evorus can automate itself without compromising conversation quality. Crowd-AI architectures have long been proposed as a way to reduce cost and latency for crowd-powered systems; Evorus demonstrates how automation can be introduced successfully in a deployed system. Its architecture allows future researchers to make further innovation on the underlying automated components in the context of a deployed open domain dialog system.',\n",
       " 'Many different approaches for estimating the Interaction Quality (IQ) of Spoken Dialogue Systems have been investigated. While dialogues clearly have a sequential nature, statistical classification approaches designed for sequential problems do not seem to work better on automatic IQ estimation than static approaches, i.e., regarding each turn as being independent of the corresponding dialogue. Hence, we analyse this effect by investigating the subset of temporal features used as input for statistical classification of IQ. We extend the set of temporal features to contain the system and the user view. We determine the contribution of each feature sub-group showing that temporal features contribute most to the classification performance. Furthermore, for the feature sub-group modeling the temporal effects with a window, we modify the window size increasing the overall performance significantly by +15.69%.',\n",
       " 'The performance of brain-computer interfaces (BCIs) improves with the amount of available training data, the statistical distribution of this data, however, varies across subjects as well as across sessions within individual subjects, limiting the transferability of training data or trained models between them. In this article, we review current transfer learning techniques in BCIs that exploit shared structure between training data of multiple subjects and/or sessions to increase performance. We then present a framework for transfer learning in the context of BCIs that can be applied to any arbitrary feature space, as well as a novel regression estimation method that is specifically designed for the structure of a system based on the electroencephalogram (EEG). We demonstrate the utility of our framework and method on subject-to-subject transfer in a motor-imagery paradigm as well as on session-to-session transfer in one patient diagnosed with amyotrophic lateral sclerosis (ALS), showing that it is able to outperform other comparable methods on an identical dataset.',\n",
       " 'The number of applications on online mobile application stores is increasing at a rapid rate. Smart-phones are used by a wide range of people varying in age, and also in the ability to use a smart phone. With the increasing dependency on smart-phones, the paper aims to determine whether the popular applications on Google Play, the official store for Android applications, can be used by people with vision impairment. The accessibility of the applications was tested using an external keyboard, and TalkBack, an accessibility tool developed by Google. It was found that several popular applications on the store were not designed keeping accessibility in mind. It was observed that there exists a weak positive relationship between the popularity of the application and its accessibility. A framework is proposed that can be used by developers to improve the accessibility of an application. The paper also discusses the programming aspects to be considered while developing an Android application, so that the application can be used by sighted as well as visually impaired users.',\n",
       " 'Natural language programming is a promising approach to enable end users to instruct new tasks for intelligent agents. However, our formative study found that end users would often use unclear, ambiguous or vague concepts when naturally instructing tasks in natural language, especially when specifying conditionals. Existing systems have limited support for letting the user teach agents new concepts or explaining unclear concepts. In this paper, we describe a new multi-modal domain-independent approach that combines natural language programming and programming-by-demonstration to allow users to first naturally describe tasks and associated conditions at a high level, and then collaborate with the agent to recursively resolve any ambiguities or vagueness through conversations and demonstrations. Users can also define new procedures and concepts by demonstrating and referring to contents within GUIs of existing mobile apps. We demonstrate this approach in PUMICE, an end-user programmable agent that implements this approach. A lab study with 10 users showed its usability.',\n",
       " \"Children under 11 are often regarded as too young to comprehend the implications of online privacy. Perhaps as a result, little research has focused on younger kids' risk recognition and coping. Such knowledge is, however, critical for designing efficient safeguarding mechanisms for this age group. Through 12 focus group studies with 29 children aged 6-10 from UK schools, we examined how children described privacy risks related to their use of tablet computers and what information was used by them to identify threats. We found that children could identify and articulate certain privacy risks well, such as information oversharing or revealing real identities online; however, they had less awareness with respect to other risks, such as online tracking or game promotions. Our findings offer promising directions for supporting children's awareness of cyber risks and the ability to protect themselves online.\",\n",
       " 'In this paper we describe the requirements and early system design for a smart conversational agent that can assist older adults in the reminiscence process. The practice of reminiscence has well documented benefits for the mental, social and emotional well-being of older adults. However, the technology support, valuable in many different ways, is still limited in terms of need of co-located human presence, data collection capabilities, and ability to support sustained engagement, thus missing key opportunities to improve care practices, facilitate social interactions, and bring the reminiscence practice closer to those with less opportunities to engage in co-located sessions with a (trained) companion. We discuss conversational agents and cognitive services as the platform for building the next generation of reminiscence applications, and introduce the concept application of a smart reminiscence agent.',\n",
       " 'We investigate crowdsourcing algorithms for finding the top-quality item within a large collection of objects with unknown intrinsic quality values. This is an important problem with many relevant applications, for example in networked recommendation systems. The core of the algorithms is that objects are distributed to crowd workers, who return a noisy and biased evaluation. All received evaluations are then combined, to identify the top-quality object. We first present a simple probabilistic model for the system under investigation. Then, we devise and study a class of efficient adaptive algorithms to assign in an effective way objects to workers. We compare the performance of several algorithms, which correspond to different choices of the design parameters/metrics. In the simulations we show that some of the algorithms achieve near optimal performance for a suitable setting of the system parameters.',\n",
       " 'Big data often has emergent structure that exists at multiple levels of abstraction, which are useful for characterizing complex interactions and dynamics of the observations. Here, we consider multiple levels of abstraction via a multiresolution geometry of data points at different granularities. To construct this geometry we define a time-inhomogeneous diffusion process that effectively condenses data points together to uncover nested groupings at larger and larger granularities. This inhomogeneous process creates a deep cascade of intrinsic low pass filters in the data that are applied in sequence to gradually eliminate local variability while adjusting the learned data geometry to increasingly coarser resolutions. We provide visualizations to exhibit our method as a \"continuously-hierarchical\" clustering with directions of eliminated variation highlighted at each step. The utility of our algorithm is demonstrated via neuronal data condensation, where the constructed multiresolution data geometry uncovers the organization, grouping, and connectivity between neurons.',\n",
       " 'Taller and sleeker smartphone devices are becoming the new norm. More screen space and very responsive touchscreens have made for enjoyable experiences available to us at all times. However, after years of interacting with smaller, portable devices, we still try to use these large smartphones on the go, and do not want to change how, where, and when we interact with them. The older devices were easier to use with one hand, when mobile. Now, with bigger devices, users have trouble accessing all parts of the screen with one hand. We need to recognize the limitations in usability due to these large screens. We must start designing user interfaces that are more conducive to one hand usage, which is the preferred way of interacting with the phone. This paper introduces Adaptive App Design, a design methodology that promotes dynamic and adaptive interfaces for one handed usage. We present a novel method of recognizing which hand the user is interacting with and suggest how to design friendlier interfaces for them by presenting a set of design guidelines for this methodology.',\n",
       " 'Users of Virtual Reality (VR) systems often experience vection, the perception of self-motion in the absence of any physical movement. While vection helps to improve presence in VR, it often leads to a form of motion sickness called cybersickness. Cybersickness is a major deterrent to large scale adoption of VR.\\n  Prior work has discovered that changing vection (changing the perceived speed or moving direction) causes more severe cybersickness than steady vection (walking at a constant speed or in a constant direction). Based on this idea, we try to reduce the cybersickness caused by character movements in a First Person Shooter (FPS) game in VR. We propose Rotation Blurring (RB), uniformly blurring the screen during rotational movements to reduce cybersickness. We performed a user study to evaluate the impact of RB in reducing cybersickness. We found that the blurring technique led to an overall reduction in sickness levels of the participants and delayed its onset. Participants who experienced acute levels of cybersickness benefited significantly from this technique.',\n",
       " \"We describe a case study with the participation of a Danish veteran suffering from post-traumatic stress disorder (PTSD). As part of psychotherapeutic treatment the participant and therapist have used our novel technique for instrumenting self-tracking of select aspects of subjective experience using a one-button wearable device. The instrumentation system is described along with the specific self-track- ing protocol which defined the participant's self-tracking of a single symptom, namely the occurrences of a bodily experienced precursor to hyperarousal. Results from the case study demonstrate how self-tracking data on a single symptom collected by a patient can provide valuable input to the therapeutic process. Specifically, it facilitated identification of crucial details otherwise unavailable from the clinical assessment and even became decisive in disentangling different symptoms and their causes.\",\n",
       " 'Urban air pollution has been linked to various human health considerations, including cardiopulmonary diseases. Communities who suffer from poor air quality often rely on experts to identify pollution sources due to the lack of accessible tools. Taking this into account, we developed Smell Pittsburgh, a system that enables community members to report odors and track where these odors are frequently concentrated. All smell report data are publicly accessible online. These reports are also sent to the local health department and visualized on a map along with air quality data from monitoring stations. This visualization provides a comprehensive overview of the local pollution landscape. Additionally, with these reports and air quality data, we developed a model to predict upcoming smell events and send push notifications to inform communities. Our evaluation of this system demonstrates that engaging residents in documenting their experiences with pollution odors can help identify local air pollution patterns, and can empower communities to advocate for better air quality.',\n",
       " 'Mobile users today interact with a variety of mobile device types including smartphones, tablets, smartwatches, and others. However research on mobile device type substitution has been limited in several respects including a lack of detailed and robust analyses. Therefore, in this work we study mobile device type substitution through analysis of multidevice usage data from a large US-based user panel. Specifically, we use regression analysis over paired user groups to test five device type substitution hypotheses. We find that both tablets and PCs are partial substitutes for smartphones with tablet and PC ownership decreasing smartphone usage by about 12.5 and 13 hours/month respectively. Additionally, we find that tablets and PCs also prompt about 20 and 57 hours/month respectively of additional (non-substituted) usage. We also illustrate significant inter-user diversity in substituted and additional usage. Overall, our results can help in understanding the relative positioning of different mobile device types and in parameterizing higher level mobile ecosystem models.',\n",
       " \"The increasing application of social and human-enabled systems in people's daily life from one side and from the other side the fast growth of mobile and smart phones technologies have resulted in generating tremendous amount of data, also referred to as big data, and a need for analyzing these data, i.e., big data analytics. Recently a trend has emerged to incorporate human computing power into big data analytics to solve some shortcomings of existing big data analytics such as dealing with semi or unstructured data. Including crowd into big data analytics creates some new challenges such as security, privacy and availability issues.\\n  In this paper study hybrid human-machine big data analytics and propose a framework to study these systems from crowd involvement point of view. We identify some open issues in the area and propose a set of research directions for the future of big data analytics area.\",\n",
       " 'Usability and user experience (UX) issues are often not well emphasized and addressed in open source software (OSS) development. There is an imperative need for supporting OSS communities to collaboratively identify, understand, and fix UX design issues in a distributed environment. In this paper, we provide an initial step towards this effort and report on an exploratory study that investigated how the OSS communities currently reported, discussed, negotiated, and eventually addressed usability and UX issues. We conducted in-depth qualitative analysis of selected issue tracking threads from three OSS projects hosted on GitHub. Our findings indicated that discussions about usability and UX issues in OSS communities were largely influenced by the personal opinions and experiences of the participants. Moreover, the characteristics of the community may have greatly affected the focus of such discussion.',\n",
       " 'Tactile perception plays an important role in medical simulation and training, specifically in surgery. The surgeon must feel organic tissue hardness, evaluate anatomical structures, measure tissue properties, and apply appropriate force control actions for safe tissue manipulation. Development of novel cost effective haptic-based simulators and their introduction in the minimally invasive surgery learning cycle can absorb the learning curve for residents. Receiving pre-training in a core set of surgical skills can reduce skill acquisition time and risks. We present the development of a cost-effective visuo-haptic simulator for the liver tissue, designed to improve practice-based education in minimally invasive surgery. Such systems can positively affect the next generations of learners by enhancing their knowledge in connection with real-life situations while they train in mandatory safe conditions.',\n",
       " \"In brain machine interfaces (BMI) that are used to control motor rehabilitation devices there is the need to process the monitored brain signals with the purpose of recognizing patient's intentions to move his hands or limbs and reject artifact and noise superimposed on these signals. This kind of processing has to take place within time limits imposed by the on-line control requirements of such devices. A widely-used algorithm is the Second Order Blind Identification (SOBI) independent component analysis (ICA) algorithm. This algorithm, however, presents long processing time and therefor it not suitable for use in the brain-based control of rehabilitation devices. A rework of this algorithm that is presented in this paper and based on SCHUR decomposition results to significantly reduced processing time. This new algorithm is quite appropriate for use in brain-based control of rehabilitation devices.\",\n",
       " \"Our work aims to generate visualizations to enable meta-analysis of analytic provenance and aid better understanding of analysts' strategies during exploratory text analysis. We introduce ProvThreads, a visual analytics approach that incorporates interactive topic modeling outcomes to illustrate relationships between user interactions and the data topics under investigation. ProvThreads uses a series of continuous analysis paths called topic threads to demonstrate both topic coverage and the progression of an investigation over time. As an analyst interacts with different pieces of data during the analysis, interactions are logged and used to track user interests in topics over time. A line chart shows different amounts of interest in multiple topics over the duration of the analysis. We discuss how different configurations of ProvThreads can be used to reveal changes in focus throughout an analysis.\",\n",
       " 'We present AirCode, a technique that allows the user to tag physically fabricated objects with given information. An AirCode tag consists of a group of carefully designed air pockets placed beneath the object surface. These air pockets are easily produced during the fabrication process of the object, without any additional material or postprocessing. Meanwhile, the air pockets affect only the scattering light transport under the surface, and thus are hard to notice to our naked eyes. But, by using a computational imaging method, the tags become detectable. We present a tool that automates the design of air pockets for the user to encode information. AirCode system also allows the user to retrieve the information from captured images via a robust decoding algorithm. We demonstrate our tagging technique with applications for metadata embedding, robotic grasping, as well as conveying object affordances.',\n",
       " 'Processes of urban planning, urban design and architecture are inherently tangible, iterative and collaborative. Nevertheless, the majority of tools in these fields offer virtual environments and single user experience. This paper presents CityScopeAR: a computational-tangible mixed-reality platform designed for collaborative urban design processes. It portrays the evolution of the tool and presents an overview of the history and limitations of notable CAD and TUI platforms. As well, it depicts the development of a distributed networking system between TUIs and CityScopeAR, as a key in design collaboration. It shares the potential advantage of broad and decentralized community-engagement process using such tools. Finally, this paper demonstrates several real-world tests and deployments of CityScopeAR and proposes a path to future integration of AR/MR devices in urban design and public participation.',\n",
       " \"Recently, great progress has been made in virtual reality(VR) research and application. However, virtual reality faces a big problem since its appearance, i.e. discomfort (nausea, stomach awareness, etc). Discomfort can be relieved by increasing hardware (sensor, cpu and display) speed. But this will increase cost. This paper gives another low cost solution. The phenomenon of cybersickness is explained with the control theory: discomfort arises if feedback scene differs from expectation, so it can be relieved by disturbing feedback loop in human brain. A hardware platform is build to test this explanation. The VR display on a Samsung S6 is blurred while head movement is detected. The effect is evaluated by comparing responses to the Simulated Sickness Questionnaire (SSQ) between a control and experimental condition. Experimental results show that the new method can ease discomfort remarkably with little extra cost. As a result, VR may be used more widely in teaching (like foreign language, medicine). It's also reasonable to expect likewise merits in other VR applications.\",\n",
       " \"The increasing availability and diversity of virtual reality (VR) applications highlighted the importance of their usability. Function-oriented VR applications posed new challenges that are not well studied in the literature. Moreover, user feedback becomes readily available thanks to modern software engineering tools, such as app stores and open source platforms. Using Firefox Reality as a case study, we explored the major types of VR usability issues raised in these platforms. We found that 77% of usability feedbacks can be mapped to Nielsen's heuristics while few were mappable to VR-specific heuristics. This result indicates that Nielsen's heuristics could potentially help developers address the usability of this VR application in its early development stage. This work paves the road for exploring tools leveraging the community effort to promote the usability of function-oriented VR applications.\",\n",
       " \"This workshop invites researchers and practitioners to participate in exploring behavioral change support intelligent transportation applications. We welcome submissions that explore intelligent transportation systems (ITS), which interact with travelers in order to persuade them or nudge them towards sustainable transportation behaviors and decisions. Emerging opportunities including the use of data and information generated by ITS and users' mobile devices in order to render personalized, contextualized and timely transport behavioral change interventions are in our focus. We invite submissions and ideas from domains of ITS including, but not limited to, multi-modal journey planners, advanced traveler information systems and in-vehicle systems. The expected outcome will be a deeper understanding of the challenges and future research directions with respect to behavioral change support through ITS.\",\n",
       " \"Augmented Reality (AR) and Mobile Augmented Reality (MAR) applications have gained much research and industry attention these days. The mobile nature of MAR applications limits users' interaction capabilities such as inputs, and haptic feedbacks. This survey reviews current research issues in the area of human computer interaction for MAR and haptic devices. The survey first presents human sensing capabilities and their applicability in AR applications. We classify haptic devices into two groups according to the triggered sense: cutaneous/tactile: touch, active surfaces, and mid-air, kinesthetic: manipulandum, grasp, and exoskeleton. Due to the mobile capabilities of MAR applications, we mainly focus our study on wearable haptic devices for each category and their AR possibilities. To conclude, we discuss the future paths that haptic feedbacks should follow for MAR applications and their challenges.\",\n",
       " \"Mobile applications (a.k.a., apps), which facilitate a large variety of tasks on mobile devices, have become indispensable in our everyday lives. Accomplishing a task may require the user to navigate among various apps. Unlike Web pages that are inherently interconnected through hyperlinks, mobile apps are usually isolated building blocks, and the lack of direct links between apps has largely compromised the efficiency of task completion. In this paper, we present the first in-depth empirical study of inter-app navigation behaviors of smartphone users based on a comprehensive dataset collected through a sizable user study over three months. We propose a model to distinguish informational pages and transitional pages, based on which a large number of inter-app navigation are identified. We reveal that developing 'tunnels' between of isolated apps has a huge potential to reduce the cost of navigation. Our analysis provides various practical implications on how to improve app-navigation experiences from both the operating system's perspective and the developer's perspective.\",\n",
       " 'On Kickstarter only 36% of crowdfunding campaigns successfully raise sufficient funds for their projects. In this paper, we explore the possibility of redistribution of crowdfunding donations to increase the chances of success. We define several intuitive redistribution policies and, using data from a real crowdfunding platform, LaunchGood, we assess the potential improvement in campaign fundraising success rates. We find that an aggressive redistribution scheme can boost campaign success rates from 37% to 79%, but such choice-agnostic redistribution schemes come at the cost of disregarding donor preferences. Taking inspiration from offline giving societies and donor clubs, we build a case for choice preserving redistribution schemes that strike a balance between increasing the number of successful campaigns and respecting giving preference. We find that choice-preserving redistribution can easily achieve campaign success rates of 48%. Finally, we discuss the implications of these different redistribution schemes for the various stakeholders in the crowdfunding ecosystem.',\n",
       " \"To choose restaurants and coffee shops, people are increasingly relying on social-networking sites. In a popular site such as Foursquare or Yelp, a place comes with descriptions and reviews, and with profile pictures of people who frequent them. Descriptions and reviews have been widely explored in the research area of data mining. By contrast, profile pictures have received little attention. Previous work showed that people are able to partly guess a place's ambiance, clientele, and activities not only by observing the place itself but also by observing the profile pictures of its visitors. Here we further that work by determining which visual cues people may have relied upon to make their guesses; showing that a state-of-the-art algorithm could make predictions more accurately than humans at times; and demonstrating that the visual cues people relied upon partly differ from those of the algorithm.\",\n",
       " 'The lack of certain types of geographic data prevents the development of location-aware technologies in a number of important domains. One such type of \"unmapped\" geographic data is space usage rules (SURs), which are defined as geographically-bound activity restrictions (e.g. \"no dogs\", \"no smoking\", \"no fishing\", \"no skateboarding\"). Researchers in the area of human-computer interaction have recently begun to develop techniques for the automated mapping of SURs with the aim of supporting activity planning systems (e.g. one-touch \"Can I Smoke Here?\" apps, SUR-aware vacation planning tools). In this paper, we present a novel SUR mapping technique - SPtP - that outperforms state-of-the-art approaches by 30% for one of the most important components of the SUR mapping pipeline: associating a point observation of a SUR (e.g. a \\'no smoking\\' sign) with the corresponding polygon in which the SUR applies (e.g. the nearby park or the entire campus on which the sign is located). This paper also contributes a series of new SUR benchmark datasets to help further research in this area.',\n",
       " 'Most window management systems support multitasking by allowing users to open, resize, position, and switch between application windows. Although multitasking has become a way of life for most knowledge workers, our current understanding of how users use window management features to switch between multiple tasks---which may comprise multiple application windows---is limited. In this paper, we present a study providing an in-depth analysis of how task switching is supported in Windows 7. As part of analysis, we developed an interface-agnostic classification of common task switching operations supported by window managers which can be used to quantify the time spent on each constituting action. Our study shows that task switching is a time intensive activity and highlights the dominant actions that contribute to task switch time. Furthermore, our classification highlights the specific operations that are optimized by more recent and experimental window managers and allows identifying opportunities for design that could further reduce the overhead of switching between tasks.',\n",
       " 'Visualizations of tabular data are widely used; understanding their effectiveness in different task and data contexts is fundamental to scaling their impact. However, little is known about how basic tabular data visualizations perform across varying data analysis tasks and data attribute types. In this paper, we report results from a crowdsourced experiment to evaluate the effectiveness of five visualization types --- Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart --- across ten common data analysis tasks and three data attribute types using two real-world datasets. We found the effectiveness of these visualization types significantly varies across task and data attribute types, suggesting that visualization design would benefit from considering context dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different tasks. We finally train a decision tree on the data we collected to drive a recommender, showcasing how to effectively engineer experimental user data into practical visualization systems.',\n",
       " 'Advertisements (ads) often include strongly emotional content to leave a lasting impression on the viewer. This work (i) compiles an affective ad dataset capable of evoking coherent emotions across users, as determined from the affective opinions of five experts and 14 annotators; (ii) explores the efficacy of convolutional neural network (CNN) features for encoding emotions, and observes that CNN features outperform low-level audio-visual emotion descriptors upon extensive experimentation; and (iii) demonstrates how enhanced affect prediction facilitates computational advertising, and leads to better viewing experience while watching an online video stream embedded with ads based on a study involving 17 users. We model ad emotions based on subjective human opinions as well as objective multimodal features, and show how effectively modeling ad emotions can positively impact a real-life application.',\n",
       " \"While smartphones are widely used for web browsing, also other novel devices like Smart TVs become increasingly popular. Yet, current interfaces do not cater for the newly available devices beyond touch and small screens, if at all for the latter. Particularly search engines--today's entry points of the WWW--must ensure their interfaces are easy to use on any web-enabled device. We report on a survey that investigated (1) users' perception and usage of current search interfaces, and (2) their expectations towards current and future search interfaces. Users are mostly satisfied with desktop and mobile search, but seem to be skeptical towards web search with novel devices and input modalities. Hence, we derive REFOCUS--a novel set of requirements for current and future search interfaces, which shall address the demand for improvement of novel web search and has been validated by 12 dedicated experts.\",\n",
       " 'Understanding the needs of a variety of distinct user groups is vital in designing effective, desirable dialogue systems that will be adopted by the largest possible segment of the population. Despite the increasing popularity of dialogue systems in both mobile and home formats, user studies remain relatively infrequent and often sample a segment of the user population that is not representative of the needs of the potential user population as a whole. This is especially the case for users who may be more reluctant adopters, such as older adults.\\n  In this paper we discuss the results of a recent user study performed over a large population of age 50 and over adults in the Midwestern United States that have experience using a variety of commercial dialogue systems. We show the common preferences, use cases, and feature gaps identified by older adult users in interacting with these systems. Based on these results, we propose a new, robust user modeling framework that addresses common issues facing older adult users, which can then be generalized to the wider user population.',\n",
       " 'The past, present and future are not fundamental properties of Minkowski spacetime. It has been suggested that they are properties of a class of information gathering and utilizing systems (IGUSs).The past, present and future are psychologically created phenomena not actually properties of spacetime. A human is a model IGUS robot. We develop a way to establish that the past, present, and future do not follow from the laws of physics by constructing robots that process information differently and therefore experience different nows (presents). We construct a customized virtual reality (VR) system which allows an observer to switch between present and past. This robot (human with VR system) can experience immersion in the immediate past ad libitum. Being able to actually construct an IGUS that has the same present at two different coordinates along the worldline lends support to the IGUS hypothesis.',\n",
       " \"Parallel Coordinates are a popular data visualization technique for multivariate data. Dating back to as early as 1880 PC are nearly as old as John Snow's famous cholera outbreak map of 1855, which is frequently regarded as a historic landmark for modern data visualization. Numerous extensions have been proposed to address integrity, scalability and readability. We make a new case to employ PC on conditional data, where additional dimensions are only unfolded if certain criteria are met in an observation. Compared to standard PC which operate on a flat set of dimensions the ontology of our input to Conditional Parallel Coordinates is of hierarchical nature. We therefore briefly review related work around hierarchical PC using aggregation or nesting techniques. Our contribution is a visualization to seamlessly adapt PC for conditional data under preservation of intuitive interaction patterns to select or highlight polylines. We conclude with intuitions on how to operate CPC on two data sets: an AutoML hyperparameter search log, and session results from a conversational agent.\",\n",
       " 'The purpose of this study is to provide an accessibility measure of web-pages, in order to draw disabled users to the pages that have been designed to be ac-cessible to them. Our approach is based on the theory of belief functions, using data which are supplied by reports produced by automatic web content assessors that test the validity of criteria defined by the WCAG 2.0 guidelines proposed by the World Wide Web Consortium (W3C) organization. These tools detect errors with gradual degrees of certainty and their results do not always converge. For these reasons, to fuse information coming from the reports, we choose to use an information fusion framework which can take into account the uncertainty and imprecision of infor-mation as well as divergences between sources. Our accessibility indicator covers four categories of deficiencies. To validate the theoretical approach in this context, we propose an evaluation completed on a corpus of 100 most visited French news websites, and 2 evaluation tools. The results obtained illustrate the interest of our accessibility indicator.',\n",
       " \"Crowdsourcing is the primary means to generate training data at scale, and when combined with sophisticated machine learning algorithms, crowdsourcing is an enabler for a variety of emergent automated applications impacting all spheres of our lives. This paper surveys the emerging field of formally reasoning about and optimizing open-ended crowdsourcing, a popular and crucially important, but severely understudied class of crowdsourcing---the next frontier in crowdsourced data management. The underlying challenges include distilling the right answer when none of the workers agree with each other, teasing apart the various perspectives adopted by workers when answering tasks, and effectively selecting between the many open-ended operators appropriate for a problem. We describe the approaches that we've found to be effective for open-ended crowdsourcing, drawing from our experiences in this space.\",\n",
       " 'Mobile is taking center stage and becoming the device of preference for all aspects of communication because of our increasingly on the go lifestyles. With this the demands on mobile capability to execute increasingly complex operations are also on the rise. However, despite improvements in device computing power in the last couple of years a mobile device continues to have limitations. Mobile driven everyday use cases are increasingly raising expectations that rest on mobile technologies that are still evolving. A number of fragmented approaches and solutions have been created that address various requirements unique to mobility, however there is a lack of a single framework that serves as a unifying reference for industry and solution architectures. The paper addresses this concern through the specification of a comprehensive reference framework for mobility that is generic and vendor neutral.',\n",
       " 'Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.',\n",
       " \"Advanced driver assistance systems have successfully reduced drivers' workloads and increased safety. On the other hand, the excessive use of such systems can impede the development of driving skills. However, there exist collaborative driver assistance systems, including shared and cooperative controls, which can promote effective collaboration between an assistance system and a human operator under appropriate system settings. Given an effective collaboration setup, we address the goal of simultaneously developing or maintaining driving skills while reducing workload. As there has been a paucity of research on such systems and their methodologies, we discuss a methodology applying shared and cooperative controls by considering related concepts in the skill training field. Reverse parking assisted by haptic shared control is presented as a means of increasing performance during assistance, while skill improvement following assistance is used to demonstrate the possibility of simultaneous achievement of driver assistance through the reduction of workload and skill improvement.\",\n",
       " 'In this paper we propose a computational design tool that al-lows end-users to create advanced quadrotor trajectories witha variety of application scenarios in mind. Our algorithm al-lows novice users to create quadrotor based use-cases withoutrequiring deep knowledge in either quadrotor control or theunderlying constraints of the target domain. To achieve thisgoal we propose an optimization-based method that gener-ates feasible trajectories which can be flown in the real world.Furthermore, the method incorporates high-level human ob-jectives into the planning of flight trajectories. An easy touse 3D design tool allows for quick specification and edit-ing of trajectories as well as for intuitive exploration of theresulting solution space. We demonstrate the utility of our ap-proach in several real-world application scenarios, includingaerial-videography, robotic light-painting and drone racing.',\n",
       " \"Executive coaching has been drawing more and more attention for developing corporate managers. While conversing with managers, coach practitioners are also required to understand internal states of coachees through objective observations. In this paper, we present REsCUE, an automated system to aid coach practitioners in detecting unconscious behaviors of their clients. Using an unsupervised anomaly detection algorithm applied to multimodal behavior data such as the subject's posture and gaze, REsCUE notifies behavioral cues for coaches via intuitive and interpretive feedback in real-time. Our evaluation with actual coaching scenes confirms that REsCUE provides the informative cues to understand internal states of coachees. Since REsCUE is based on the unsupervised method and does not assume any prior knowledge, further applications beside executive coaching are conceivable using our framework.\",\n",
       " 'Current exergaming sensors and inertial systems attached to sports equipment or the human body can provide quantitative information about the movement or impact e.g. with the ball. However, the scope of these technologies is not to qualitatively assess sports technique at a personalised level, similar to a coach during training or replay analysis. The aim of this paper is to demonstrate a novel approach to automate identification of tennis swings executed with erroneous technique without recorded ball impact. The presented spatiotemporal transformations relying on motion gradient vector flow and polynomial regression with RBF classifier, can identify previously unseen erroneous swings (84.5-94.6%). The presented solution is able to learn from a small dataset and capture two subjective swing-technique assessment criteria from a coach. Personalised and flexible assessment criteria required for players of diverse skill levels and various coaching scenarios were demonstrated by assigning different labelling criteria for identifying similar spatiotemporal patterns of tennis swings.',\n",
       " \"A multitude of web and desktop applications are now widely available in diverse human languages. This paper explores the design issues that are specifically relevant for multilingual users. It reports on the continued studies of Information System (IS) issues and users' behaviour across cross-cultural and transnational boundaries. Taking the BBC website as a model that is internationally recognised, usability tests were conducted to compare different versions of the website. The dependant variables derived from the questionnaire were analysed (via descriptive statistics) to elucidate the multilingual UI design issues. Using Principal Component Analysis (PCA), five de-correlated variables were identified which were then used for hypotheses tests. A modified version of Herzberg's Hygiene-motivational Theory about the Workplace was applied to assess the components used in the website. Overall, it was concluded that the English versions of the website gave superior usability results and this implies the need for deeper study of the problems in usability of the translated versions.\",\n",
       " 'This paper introduces Wisture, a new online machine learning solution for recognizing touch-less dynamic hand gestures on a smartphone. Wisture relies on the standard Wi-Fi Received Signal Strength (RSS) using a Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN), thresholding filters and traffic induction. Unlike other Wi-Fi based gesture recognition methods, the proposed method does not require a modification of the smartphone hardware or the operating system, and performs the gesture recognition without interfering with the normal operation of other smartphone applications.\\n  We discuss the characteristics of Wisture, and conduct extensive experiments to compare its performance against state-of-the-art machine learning solutions in terms of both accuracy and time efficiency. The experiments include a set of different scenarios in terms of both spatial setup and traffic between the smartphone and Wi-Fi access points (AP). The results show that Wisture achieves an online recognition accuracy of up to 94% (average 78%) in detecting and classifying three hand gestures.',\n",
       " 'Conversion optimization means designing a web interface so that as many users as possible take a desired action on it, such as register or purchase. Such design is usually done by hand, testing one change at a time through A/B testing, or a limited number of combinations through multivariate testing, making it possible to evaluate only a small fraction of designs in a vast design space. This paper describes Sentient Ascend, an automatic conversion optimization system that uses evolutionary optimization to create effective web interface designs. Ascend makes it possible to discover and utilize interactions between the design elements that are difficult to identify otherwise. Moreover, evaluation of design candidates is done in parallel online, i.e. with a large number of real users interacting with the system. A case study on an existing media site shows that significant improvements (i.e. over 43%) are possible beyond human design. Ascend can therefore be seen as an approach to massively multivariate conversion optimization, based on a massively parallel interactive evolution.',\n",
       " \"Traditionally, evaluation studies in information visualization have measured effectiveness by assessing performance time and accuracy. More recently, there has been a concerted effort to understand aspects beyond time and errors. In this paper we study enjoyment, which, while arguably not the primary goal of visualization, has been shown to impact performance and memorability. Different models of enjoyment have been proposed in psychology, education and gaming; yet there is no standard approach to evaluate and measure enjoyment in visualization. In this paper we relate the flow model of Csikszentmihalyi to Munzner's nested model of visualization evaluation and previous work in the area. We suggest that, even though previous papers tackled individual elements of flow, in order to understand what specifically makes a visualization enjoyable, it might be necessary to measure all specific elements.\",\n",
       " 'An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed -- Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.',\n",
       " 'We present Lemotif. Lemotif generates a motif for your emotional life. You tell Lemotif a little bit about your day -- what were salient events or aspects and how they made you feel. Lemotif will generate a lemotif -- a creative abstract visual depiction of your emotions and their sources. Over time, Lemotif can create visual motifs to capture a summary of your emotional states over arbitrary periods of time -- making patterns in your emotions and their sources apparent, presenting opportunities to take actions, and measure their effectiveness. The underlying principles in Lemotif are that the lemotif should (1) separate out the sources of the emotions, (2) depict these sources visually, (3) depict the emotions visually, and (4) have a creative aspect to them. We verify via human studies that each of these factors contributes to the proposed lemotifs being favored over corresponding baselines.',\n",
       " 'The electroencephalogram (EEG) is the most widely used input for brain computer interfaces (BCIs), and common spatial pattern (CSP) is frequently used to spatially filter it to increase its signal-to-noise ratio. However, CSP is a supervised filter, which needs some subject-specific calibration data to design. This is time-consuming and not user-friendly. A promising approach for shortening or even completely eliminating this calibration session is transfer learning, which leverages relevant data or knowledge from other subjects or tasks. This paper reviews three existing approaches for incorporating transfer learning into CSP, and also proposes a new transfer learning enhanced CSP approach. Experiments on motor imagery classification demonstrate their effectiveness. Particularly, our proposed approach achieves the best performance when the number of target domain calibration samples is small.',\n",
       " 'Traditional instrument learning is time-consuming. It begins with learning music notation and necessitates layers of sophistication and abstraction. Haptic interfaces open another door to the music world for the vast majority of beginners when traditional training methods are not effective. However, existing haptic interfaces can only deal with specially designed pieces with great restrictions on performance duration and pitch range due to the fact that not all performance motions could be guided haptically for most instruments. Our system breaks such restrictions using a semi-haptic interface. For the first time, the pitch range of the haptically learned pieces goes beyond an octave (with the fingering motion covers most of the possible choices) and the duration of learned pieces cover a whole phrase. This significant change leads to a more realistic instrument learning process. Experiments show that our semi-haptic interface is effective as long as learners are not \"tone deaf.\" Using our prototype device, the learning rate is about 30% faster compared to learning from videos.',\n",
       " 'Developing applications for interactive space is different from developing cross-platform applications for personal computing. Input, output, and architectural variations in each interactive space introduce big overhead in terms of cost and time for developing, deploying and maintaining applications for interactive spaces. Often, these applications become on-off experience tied to the deployed spaces. To alleviate this problem and enable rapid responsive space design applications similar to responsive web design, we present CELIO application development framework for interactive spaces. The framework is micro services based and neatly decouples application and design specifications from hardware and architecture specifications of an interactive space. In this paper, we describe this framework and its implementation details. Also, we briefly discuss the use cases developed using this framework.',\n",
       " 'The ever evolving informatics technology has gradually bounded human and computer in a compact way. Understanding user behavior becomes a key enabler in many fields such as sedentary-related healthcare, human-computer interaction (HCI) and affective computing. Traditional sensor-based and vision-based user behavior analysis approaches are obtrusive in general, hindering their usage in realworld. Therefore, in this article, we first introduce WiFi signal as a new source instead of sensor and vision for unobtrusive user behaviors analysis. Then we design BeSense, a contactless behavior analysis system leveraging signal processing and computational intelligence over WiFi channel state information (CSI). We prototype BeSense on commodity low-cost WiFi devices and evaluate its performance in realworld environments. Experimental results have verified its effectiveness in recognizing user behaviors.',\n",
       " \"The purpose of this paper is to investigate the applicability of applying gamification approach on the physiotherapy rehabilitation. A new developing game called JCave was designed and developed for the prove of concept. The propose game target the children from six to twelve years of age who need physical therapy in their upper limbs. JCave is a finite and multilevel single-player 3D video game. The player's role is to collect jewels from a cave and increase his/her score by performing physical therapy exercises. The game uses Xbox360 Kinect as a motion capture camera to observe gestures and track the child. Automatic gesture recognition algorithms are implemented for elbow flexion-extension exercises and shoulder flexion, which are the active range of motion (AROM) exercises for both the right and left arms. The JCave game is implemented using Unity3D and Blender to design 3D model objects.\",\n",
       " \"Designing a user interface for military situation awareness presents challenges for managing information in a useful and usable manner. We present an integrated set of functions for the presentation of and interaction with information for a mobile augmented reality application for military applications. Our research has concentrated on four areas. We filter information based on relevance to the user (in turn based on location), evaluate methods for presenting information that represents entities occluded from the user's view, enable interaction through a top-down map view metaphor akin to current techniques used in the military, and facilitate collaboration with other mobile users and/or a command center. In addition, we refined the user interface architecture to conform to requirements from subject matter experts. We discuss the lessons learned in our work and directions for future research.\",\n",
       " 'Security incidents such as targeted distributed denial of service (DDoS) attacks on power grids and hacking of factory industrial control systems (ICS) are on the increase. This paper unpacks where emerging security risks lie for the industrial internet of things, drawing on both technical and regulatory perspectives. Legal changes are being ushered by the European Union (EU) Network and Information Security (NIS) Directive 2016 and the General Data Protection Regulation 2016 (GDPR) (both to be enforced from May 2018). We use the case study of the emergent smart energy supply chain to frame, scope out and consolidate the breadth of security concerns at play, and the regulatory responses. We argue the industrial IoT brings four security concerns to the fore, namely: appreciating the shift from offline to online infrastructure; managing temporal dimensions of security; addressing the implementation gap for best practice; and engaging with infrastructural complexity. Our goal is to surface risks and foster dialogue to avoid the emergence of an Internet of Insecure Industrial Things',\n",
       " \"Visual attention is highly fragmented during mobile interactions, but the erratic nature of attention shifts currently limits attentive user interfaces to adapting after the fact, i.e. after shifts have already happened. We instead study attention forecasting -- the challenging task of predicting users' gaze behaviour (overt visual attention) in the near future. We present a novel long-term dataset of everyday mobile phone interactions, continuously recorded from 20 participants engaged in common activities on a university campus over 4.5 hours each (more than 90 hours in total). We propose a proof-of-concept method that uses device-integrated sensors and body-worn cameras to encode rich information on device usage and users' visual scene. We demonstrate that our method can forecast bidirectional attention shifts and predict whether the primary attentional focus is on the handheld mobile device. We study the impact of different feature sets on performance and discuss the significant potential but also remaining challenges of forecasting user attention during mobile interactions.\",\n",
       " \"Nowadays Digital Personal Assistants (DPA) become more and more popular. DPAs help to increase quality of life especially for elderly or disabled people. In this paper we develop an open source DPA and smart home system as a 3-rd party extension to show the functionality of the assistant. The system is designed to use the DPA as a learning platform for engineers to provide them with the opportunity to create and test their own hypothesis. The DPA is able to recognize users' commands in natural language and transform it to the set of machine commands that can be used to control different 3rd-party application. We use smart home system as an example of such 3rd-party. We demonstrate that the system is able to control home appliances, like lights, or to display information about the current state of the home, like temperature, through a dialogue between a user and the Digital Personal Assistant.\",\n",
       " 'The combination of clinical and personal health and wellbeing data can tell us much about our behaviors, risks and overall status. The way this data is visualized may affect our understanding of our own health. To study this effect, we conducted a small experiment with 30 participants in which we presented a holistic overview of the health and wellbeing of two modeled individuals, one of them with metabolic syndrome. We used an insight-based methodology to assess the effectiveness of the visualizations. The results show that adequate visualization of holistic health data helps users without medical background to better understand the overall health situation and possible health risks related to lifestyles. Furthermore, we found that the application of insight-based methodology in the health and wellbeing domain remains unexplored and additional research and methodology development are needed.',\n",
       " 'Fully immersive virtual reality (VR) has the potential to improve neurosurgical planning. For example, it may offer 3D visualizations of relevant anatomical structures with complex shapes, such as blood vessels and tumors. However, there is a lack of research tools specifically tailored for this area. We present a research framework for VR neurosurgery based on open-source tools and preliminary evaluation results. We showcase the potential of such a framework using clinical data of two patients and research data of one subject. As a first step toward practical evaluations, two certified senior neurosurgeons positively assessed the usefulness of the VR visualizations using head-mounted displays. The methods and findings described in our study thus provide a foundation for research and development aiming at versatile and user-friendly VR tools for improving neurosurgical planning and training.',\n",
       " 'Humanness is core to speech interface design. Yet little is known about how users conceptualise perceptions of humanness and how people define their interaction with speech interfaces through this. To map these perceptions n=21 participants held dialogues with a human and two speech interface based intelligent personal assistants, and then reflected and compared their experiences using the repertory grid technique. Analysis of the constructs show that perceptions of humanness are multidimensional, focusing on eight key themes: partner knowledge set, interpersonal connection, linguistic content, partner performance and capabilities, conversational interaction, partner identity and role, vocal qualities and behavioral affordances. Through these themes, it is clear that users define the capabilities of speech interfaces differently to humans, seeing them as more formal, fact based, impersonal and less authentic. Based on the findings, we discuss how the themes help to scaffold, categorise and target research and design efforts, considering the appropriateness of emulating humanness.',\n",
       " \"The smartification of industries is marked by the development of cyber-physical systems, interfaces, and intelligent software featuring knowledge models, empirical real-time data, and feedback-loops. This brings up new requirements and challenges for HMI design and industrial labor. Social sciences can contribute to such engineering projects with their perspectives, concepts and knowledge. Hence, we claim that, in addition to following their own intellectual curiosities, the social sciences can and should contribute to such projects in terms of an 'applied' science, helping to foster interdisciplinary collaboration and providing toolkits and devices for what we call 'interdisciplinary diplomacy'. We illustrate the benefits of such an approach, support them with selected examples of our involvement in such an engineering project and propose using methods as diplomatic devices and concepts as social theory plug-ins. The article ends with an outlook and reflection on the remaining issue of whether and in how far such 'applied' and critical social science can or should be integrated.\",\n",
       " 'This paper is concerned with the design and implementation of an innovative user support system in the frame of an open educational environment. The environment adapted is ModelsCreator (MC), an educational system supporting learning through modelling activities. The pupils typical interaction with the system was modelled us-ing Bayesian Belief Networks (BBN). This model has been used in ModelsCreator to build an adaptive help system providing the most useful guidelines according to the current state of interaction. A brief description of the system and an overview of application of Bayesian techniques to educational systems is presented together with discussion about the process of building of the Bayesian Network derived from actual student interaction data. A preliminary evaluation of the developed prototype indicates that the proposed approach produces systems with promising performance.',\n",
       " 'In this position paper, we present ideas about creating a next generation framework towards an adaptive interface for data communication and visualisation systems. Our objective is to develop a system that accepts large data sets as inputs and provides user-centric, meaningful visual information to assist owners to make sense of their data collection. The proposed framework comprises four stages: (i) the knowledge base compilation, where we search and collect existing state-ofthe-art visualisation techniques per domain and user preferences; (ii) the development of the learning and inference system, where we apply artificial intelligence techniques to learn, predict and recommend new graphic interpretations (iii) results evaluation; and (iv) reinforcement and adaptation, where valid outputs are stored in our knowledge base and the system is iteratively tuned to address new demands. These stages, as well as our overall vision, limitations and possible challenges are introduced in this article. We also discuss further extensions of this framework for other knowledge discovery tasks.',\n",
       " 'Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization systems that utilizes state-of-the-art natural language processing techniques to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and special utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram special utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem and a formal user study.',\n",
       " 'Sensory feedback is the fundamental driving force behind motor control and learning. However, the technology for low-cost and efficient sensory feedback remains a big challenge during stroke rehabilitation, and for prosthetic designs. Here we show that a low-cost accelerometer mounted on the finger can provide accurate decoding of many daily life materials during touch. We first designed a customized touch analysis system that allowed us to present different materials for touch by human participants, while controlling for the contact force and touch speed. Then, we collected data from six participants, who touched seven daily life materials-plastic, cork, wool, aluminum, paper, denim, cotton. We use linear sparse logistic regression and show that the materials can be classified from accelerometer recordings with an accuracy of 88% across materials and participants within 7 seconds of touch.',\n",
       " 'The course description provided by instructors is an important piece of information as it defines what is expected from the instructor and what he/she is going to deliver during a particular course. One of the key components of a course description is the Learning Outcomes section. The contents of this section are used by program managers who are tasked to compare and match two different courses during the development of Transfer Agreements between different institutions. This research introduces the development of visual tools for understanding the two different courses and making comparisons. We designed methods to extract the text from a course description document, developed an algorithm to perform semantic analysis, and displayed the results in a web interface. We are able to achieve the intermediate results of the research which includes extracting, analyzing and visualizing the data.',\n",
       " 'Critical human-machine interfaces are present in many systems including avionics systems and medical devices. Use error is a concern in these systems both in terms of hardware panels and input devices, and the software that drives the interfaces. Guaranteeing safe usability, in terms of  buttons, knobs and displays is now a key element in the overall safety of the system. New integrated development environments (IDEs) based on formal methods technologies have been developed by the research community to support the design and analysis of high-confidence human-machine interfaces. To date, little work has focused on the comparison of these particular types of formal IDEs. This paper compares and evaluates two state-of-the-art toolkits: CIRCUS, a model-based development and analysis tool based on Petri net extensions, and PVSio-web, a prototyping toolkit based on the PVS theorem proving system.',\n",
       " 'The use of complex machine learning models can make systems opaque to users. Machine learning research proposes the use of post-hoc explanations. However, it is unclear if they give users insights into otherwise uninterpretable models. One minimalistic way of explaining image classifications by a deep neural network is to show only the areas that were decisive for the assignment of a label. In a pilot study, 20 participants looked at 14 of such explanations generated either by a human or the LIME algorithm. For explanations of correct decisions, they identified the explained object with significantly higher accuracy (75.64% vs. 18.52%). We argue that this shows that explanations can be very minimalistic while retaining the essence of a decision, but the decision-making contexts that can be conveyed in this manner is limited. Finally, we found that explanations are unique to the explainer and human-generated explanations were assigned 79% higher trust ratings. As a starting point for further studies, this work shares our first insights into quality criteria of post-hoc explanations.',\n",
       " '\"Social sensing\" is a form of crowd-sourcing that involves systematic analysis of digital communications to detect real-world events. Here we consider the use of social sensing for observing natural hazards. In particular, we present a case study that uses data from a popular social media platform (Twitter) to detect and locate flood events in the UK. In order to improve data quality we apply a number of filters (timezone, simple text filters and a naive Bayes `relevance\\' filter) to the data. We then use place names in the user profile and message text to infer the location of the tweets. These two steps remove most of the irrelevant tweets and yield orders of magnitude more located tweets than we have by relying on geo-tagged data. We demonstrate that high resolution social sensing of floods is feasible and we can produce high-quality historical and real-time maps of floods using Twitter.',\n",
       " 'This paper proposes the usage of \\\\emph{visualisation widgets} for exploratory search with \\\\emph{sentiment} as a facet. Starting from specific design goals for depiction of ambivalence in sentiment, two visualization widgets were implemented: \\\\emph{scatter plot} and \\\\emph{parallel coordinates}. Those widgets were evaluated against a text baseline in a small-scale usability study with exploratory tasks using Wikipedia as dataset. The study results indicate that users spend more time browsing with scatter plots in a positive way. A post-hoc analysis of individual differences in behavior revealed that when considering two types of users, \\\\emph{explorers} and \\\\emph{achievers}, engagement with scatter plots is positive and significantly greater \\\\textit{when users are explorers}. We discuss the implications of these findings for sentiment-based exploratory search and personalised user interfaces.',\n",
       " 'Gestalt theory has provided perceptual science with a conceptual framework which has inspired researchers ever since, taking the field of perceptual organization into the 21st century. This opinion article discusses the importance of rules of perceptual organization for the testing and design of visual interface technology. It is argued that major Gestalt principles, such as the law of good continuation or the principle of Praegnanz (suggested translation: salience), taken as examples here, are important to our understanding of visual image processing by a human observer. Perceptual integration of contrast information across collinear space, and the organization of objects in the 2D image plane into figure and ground are of a particular importance here. Visual interfaces for image-guided surgery illustrate the criticality of these two types of perceptual processes for reliable decision making and action. It is concluded that Gestalt theory continues to generate powerful concepts and insights for perceptual science placed within the context of major technological challenges of today.',\n",
       " 'This paper presents a pilot study on developing an instrument to predict the quality of e-commerce websites. The 8C model was adopted as the reference model of the heuristic evaluation. Each dimension of the 8C was mapped into a set of quantitative website elements, selected websites were scraped to get the quantitative website elements, and the score of each dimension was calculated. A software was developed in PHP for the experiments. In the training process, 10 experiments were conducted and quantitative analyses were regressively conducted between the experiments. The conversion rate was used to verify the heuristic evaluation of an e-commerce website after each experiment. The results showed that the mapping revisions between the experiments improved the performance of the evaluation instrument, therefore the experiment process and the quantitative mapping revision guideline proposed was on the right track. The software resulted from the experiment 10 can serve as the aimed e-commerce website evaluation instrument. The experiment results and the future work have been discussed.',\n",
       " \"The exponential growth in smartphone adoption is contributing to the availability of vast amounts of human behavioral data. This data enables the development of increasingly accurate data-driven user models that facilitate the delivery of personalized services which are often free in exchange for the use of its customers' data. Although such usage conventions have raised many privacy concerns, the increasing value of personal data is motivating diverse entities to aggressively collect and exploit the data. In this paper, we unfold profiling scenarios around mobile HTTP(S) traffic, focusing on those that have limited but meaningful segments of the data. The capability of the scenarios to profile personal information is examined with real user data, collected in-the-wild from 61 mobile phone users for a minimum of 30 days. Our study attempts to model heterogeneous user traits and interests, including personality, boredom proneness, demographics, and shopping interests. Based on our modeling results, we discuss various implications to personalization, privacy, and personal data rights.\",\n",
       " 'We examine a class of techniques for 3D object manipulation on mobile devices, in which the device\\'s physical motion is applied to 3D objects displayed on the device itself. This \"local coupling\" between input and display creates specific challenges compared to manipulation techniques designed for monitor-based or immersive virtual environments. Our work focuses specifically on the mapping between device motion and object motion. We review existing manipulation techniques and introduce a formal description of the main mappings under a common notation. Based on this notation, we analyze these mappings and their properties in order to answer crucial usability questions. We first investigate how the 3D objects should move on the screen, since the screen also moves with the mobile device during manipulation. We then investigate the effects of a limited range of manipulation and present a number of solutions to overcome this constraint. This work provides a theoretical framework to better understand the properties of locally-coupled 3D manipulation mappings based on mobile device motion.',\n",
       " \"We present a new approach for improving the friendliness and warmth of a virtual agent in an AR environment by generating appropriate movement characteristics. Our algorithm is based on a novel data-driven friendliness model that is computed using a user-study and psychological characteristics. We use our model to control the movements corresponding to the gaits, gestures, and gazing of friendly virtual agents (FVAs) as they interact with the user's avatar and other agents in the environment. We have integrated FVA agents with an AR environment using with a Microsoft HoloLens. Our algorithm can generate plausible movements at interactive rates to increase the social presence. We also investigate the perception of a user in an AR setting and observe that an FVA has a statistically significant improvement in terms of the perceived friendliness and social presence of a user compared to an agent without the friendliness modeling. We observe an increment of 5.71% in the mean responses to a friendliness measure and an improvement of 4.03% in the mean responses to a social presence measure.\",\n",
       " \"To allow non-designers' involvement in design projects new methods are needed. Co-design gives the same opportunity to all the multidisciplinary participants to co-create ideas simultaneously. Nevertheless, current co-design processes involving such users tend to limit their contribution to the proposal of basic design ideas only through brainstorming. The co-design approach needs to be enhanced by a properly suited representational ecosystem supporting active participation and by conscious use of structured verbal exchanges giving awareness of the creative process. In this respect, we developed two social virtual reality co-design systems, and a co-design verbal exchange methodology to favour participants' awareness of the co-creative process. By using such representations and verbal exchanges, participants could co-create with more ease by benefiting from being informed of the process and from the collective immersion, empowering their participation. This paper presents the rationale behind this approach of using Social VR in co-design and the feedback of three co-design workshops.\",\n",
       " \"This demo takes the form of a challenge to the IJCAI community. A physical vault, secured by a 4-digit code, will be placed in the demo area. The author will publicly open the vault by entering the code on a touch-based interface, and as many times as requested. The challenge to the IJCAI participants will be to crack the code, open the vault, and collect its content. The interface is based on previous work on calibration-free interactive systems that enables a user to start instructing a machine without the machine knowing how to interpret the user's actions beforehand. The intent and the behavior of the human are simultaneously learned by the machine. An online demo and videos are available for readers to participate in the challenge. An additional interface using vocal commands will be revealed on the demo day, demonstrating the scalability of our approach to continuous input signals.\",\n",
       " 'The development and design of visualization solutions that are truly usable is essential for ensuring both their adoption and effectiveness. User-centered design principles, which focus on involving users throughout the entire development process, are well suited for visualization and have been shown to be effective in numerous information visualization endeavors. In this paper, we report a two year long collaboration with combustion scientists that, by applying these design principles, generated multiple results including an in situ visualization technique and a post hoc probability distribution function (PDF) exploration tool. Furthermore, we examine the importance of user-centered design principles and describe lessons learned over the design process in an effort to aid others who also seek to work with scientists for developing effective and usable scientific visualization solutions.',\n",
       " \"As pre-diagnostic technologies are becoming increasingly accessible, using them to improve the quality of care available to dementia patients and their caregivers is of increasing interest. Specifically, we aim to develop a tool for non-invasively assessing task performance in a simple gaming application. To address this, we have developed Caregiver Assessment using Smart Gaming Technology (CAST), a mobile application that personalizes a traditional word scramble game. Its core functionality uses a Fuzzy Inference System (FIS) optimized via a Genetic Algorithm (GA) to provide customized performance measures for each user of the system. With CAST, we match the relative level of difficulty of play using the individual's ability to solve the word scramble tasks. We provide an analysis of the preliminary results for determining task difficulty, with respect to our current participant cohort.\",\n",
       " 'We present an analytic provenance data repository that can be used to study human analysis activity, thought processes, and software interaction with visual analysis tools during exploratory data analysis. We conducted a series of user studies involving exploratory data analysis scenario with textual and cyber security data. Interactions logs, think-alouds, videos and all coded data in this study are available online for research purposes. Analysis sessions are segmented in multiple sub-task steps based on user think-alouds, video and audios captured during the studies. These analytic provenance datasets can be used for research involving tools and techniques for analyzing interaction logs and analysis history. By providing high-quality coded data along with interaction logs, it is possible to compare algorithmic data processing techniques to the ground-truth records of analysis history.',\n",
       " 'Brain computer interfaces (BCI) provide a direct communication link between the brain and a computer or other external devices. They offer an extended degree of freedom either by strengthening or by substituting human peripheral working capacity and have potential applications in various fields such as rehabilitation, affective computing, robotics, gaming and artificial intelligence. Significant research efforts on a global scale have delivered common platforms for technology standardization and help tackle highly complex and nonlinear brain dynamics and related feature extraction and classification challenges. Psycho-neurophysiological phenomena and their impact on brain signals impose another challenge for BCI researchers to transform the technology from laboratory experiments to plug-and-play daily life. This review summarizes progress in BCI field and highlights critical challenges.',\n",
       " 'This paper outlines ongoing dissertation research located in the intersection of science fiction, human-computer interaction and computer science. Through an interdisciplinary perspective, drawing from fields such as human-computer interaction, film theory and studies of science and technology, qualitative and quantitative content analysis techniques are used to contextually analyze expressions of science fiction in peer-reviewed computer science research repositories, such as the ACM or IEEE Xplore Digital Libraries. This paper concisely summarizes and introduces the relationship of science fiction and computer science research and presents the research questions, aims and implications in addition to prior work and study methodology. In the latter part of this work-in-progress report, preliminary results, current limitations, future work and a post-dissertation trajectory are outlined.',\n",
       " 'We introduce and evaluate a novel approach for detecting smooth pursuit eye movements that increases the number of distinguishable targets and is more robust against false positives. Being natural and calibration-free, Pursuits has been gaining popularity in the past years. At the same time, current implementations show poor performance when more than eight on-screen targets are being used, thus limiting its applicability. Our approach (1) leverages the slope of a regression line, and (2) introduces a minimum signal duration that improves both the new and the traditional detection method. After introducing the approach as well as the implementation, we compare it to the traditional correlation-based Pursuits detection method. We tested the approach up to 24 targets and show that, if accepting a similar error rate, nearly twice as many targets can be distinguished compared to state of the art. For fewer targets, accuracy increases significantly. We believe our approach will enable more robust pursuit-based user interfaces, thus making it valuable for both researchers and practitioners.',\n",
       " \"In this exploratory study we assessed how attitudes of children with autism spectrum disorder (ASD) towards robots together with children's autism-related social impairments are linked to indicators of children's preference of an interaction with a robot over an interaction with a person. We found that children with ASD have overall positive attitudes towards robots and that they often prefer interacting with a robot than with a person. Several of children's attitudes were linked to children's longer gazes towards a robot compared to a person. Autism-related social impairments were linked to more repetitive and stereotyped behaviors and to a shorter gaze duration in the interaction with the robot compared to the person. These preliminary results contribute to better understand factors that might help determine sub-groups of children with ASD for whom robots could be particularly useful.\",\n",
       " \"With the increasing complexity of modern industrial automatic and robotic systems, an increasing burden is put on the operators, who are requested to supervise and interact with such complex systems, typically under challenging and stressful conditions. To overcome this issue, it is necessary to adopt a responsible approach based on the anthropocentric design methodology, such that machines adapt to the humans capabilities. Moving along these lines, a methodological approach called MATE was introduced in [1], which consists in devising complex automatic or robotic solutions that measure current operator's status, adapting the interaction accordingly, and providing her/him with proper training to improve the interaction and learn lacking skills and expertise. In this paper we propose an evaluation and validation procedure to guarantee the achievement of the requirements of a MATE system.\",\n",
       " 'People from all over the world use social media to share thoughts and opinions about events, and understanding what people say through these channels has been of increasing interest to researchers, journalists, and marketers alike. However, while automatically generated summaries enable people to consume large amounts of data efficiently, they do not provide the context needed for a viewer to fully understand an event. Narrative structure can provide templates for the order and manner in which this data is presented to create stories that are oriented around narrative elements rather than summaries made up of facts. In this paper, we use narrative theory as a framework for identifying the links between social media content. To do this, we designed crowdsourcing tasks to generate summaries of events based on commonly used narrative templates. In a controlled study, for certain types of events, people were more emotionally engaged with stories created with narrative structure and were also more likely to recommend them to others compared to summaries created without narrative structure.',\n",
       " 'AI and advanced automation are involved in almost all aspects of our life. In such systems, human responsibility for outcomes becomes equivocal. We analyze the descriptive abilities of a newly developed responsibility quantification model (ResQu) to predict actual human responsibility and perceptions of responsibility in HCI. In two laboratory experiments, participants performed an aided decision task. We compared the theoretical responsibility values to the actual responsibility a person took on and to the subjectively perceived responsibility. The ResQu model predictions were strongly correlated with the measured and the subjective responsibility. However, observed values differed from the model predictions, as less knowledgeable participants overestimated their own capabilities and assumed greater-than-optimal responsibility. The results demonstrate the value of the ResQu model as a descriptive model, considering some systematic deviations. It can be used to aid system design and guide policy and legal decisions regarding human responsibility in events involving intelligent systems.',\n",
       " 'Understanding validity of user behaviour in Virtual Environments (VEs) is critical as they are increasingly being used for serious Health and Safety applications such as predicting human behaviour and training in hazardous situations. This paper presents a comparative study exploring user behaviour in VE-based fire evacuation and investigates whether this is affected by the addition of thermal and olfactory simulation. Participants (N=43) were exposed to a virtual fire in an office building. Quantitative and qualitative analyses of participant attitudes and behaviours found deviations from those we would expect in real life (e.g. pre-evacuation actions), but also valid behaviours like fire avoidance. Potentially important differences were found between multisensory and audiovisual-only conditions (e.g. perceived urgency). We conclude VEs have significant potential in safety-related applications, and that multimodality may afford additional uses in this context, but the identified limitations of behavioural validity must be carefully considered to avoid misapplication of the technology.',\n",
       " 'Social participation is known to bring great benefits to the health and well-being of people as they age. From being in contact with others to engaging in group activities, keeping socially active can help slow down the effects of age-related declines, reduce risks of loneliness and social isolation and even mortality in old age. There are unfortunately a variety of barriers that make it difficult for older adults to engage in social activities in a regular basis. In this chapter, we give an overview of the challenges to social participation and discuss how technology can help overcome these barriers and promote participation in social activities. We examine two particular research threads and designs, exploring ways in which technology can support co-located and virtual participation: i) an application that motivates the virtual participation in group training programs, and ii) a location-based game that supports co-located intergenerational ICT training classes. We discuss the effectiveness and limitations of various design choices in the two use cases and outline the lessons learned',\n",
       " 'Single-switch scanning systems allow nonspeaking individuals with motor disabilities to communicate by triggering a single switch (e.g., raising an eye brow). A problem with current single-switch scanning systems is that while they result in reasonable performance in noiseless conditions, for instance via simulation or tests with able-bodied users, they fail to accurately model the noise sources that are introduced when a non-speaking individual with motor disabilities is triggering the switch in a realistic use context. To help assist the development of more noise-resilient single-switch scanning systems we have developed a mathematical model of scanning systems which incorporates extensive noise modelling. Our model includes an improvement to the standard scanning method, which we call fast-scan, which we show via simulation can be more suitable for certain users of scanning systems.',\n",
       " 'Multimodal features play a key role in wearable sensor based Human Activity Recognition (HAR). Selecting the most salient features adaptively is a promising way to maximize the effectiveness of multimodal sensor data. In this regard, we propose a \"collect fully and select wisely (Fullie and Wiselie)\" principle as well as a dual-stream recurrent convolutional attention model, Recurrent Attention and Activity Frame (RAAF), to improve the recognition performance. We first collect modality features and the relations between each pair of features to generate activity frames, and then introduce an attention mechanism to select the most prominent regions from activity frames precisely. The selected frames not only maximize the utilization of valid features but also reduce the number of features to be computed effectively. We further analyze the hyper-parameters, accuracy, interpretability, and annotation dependency of the proposed model based on extensive experiments. The results show that RAAF achieves competitive performance on two benchmarked datasets and works well in real life scenarios.',\n",
       " \"The movement of a user's face, easily detected by a smartphone's front camera, is an underexploited input modality for mobile interactions. We introduce three sets of face-engaged interaction techniques for augmenting the traditional mobile inputs, which leverages the combination of the head movements with touch gestures and device motions, all sensed via the phone's built-in sensors. We systematically present the space of design considerations for mobile interactions using one or more of the three input modalities (i.e., touch, motion, and head). The additional affordances of the proposed techniques expand the mobile interaction vocabulary, and can facilitate unique usage scenarios such as one-hand or touch-free interaction. An initial evaluation was conducted and users had positive reactions to the new techniques, indicating the promise of an intuitive and convenient user experience.\",\n",
       " 'The main aim of this paper is to discuss how the combination of Web 2.0, social media and geographic technologies can provide opportunities for learning and new forms of participation in an urban design studio. This discussion is mainly based on our recent findings from two experimental urban design studio setups as well as former research and literature studies. In brief, the web platform enabled us to extend the learning that took place in the design studio beyond the studio hours, to represent the design information in novel ways and allocate multiple communication forms. We found that the student activity in the introduced web platform was related to their progress up to a certain extent. Moreover, the students perceived the platform as a convenient medium and addressed it as a valuable resource for learning. This study should be conceived as a continuation of a series of our Design Studio 2.0 experiments which involve the exploitation of opportunities provided by novel socio-geographic information and communication technologies for the improvement of the design learning processes.',\n",
       " 'Sketching and natural languages are effective communication media for interactive applications. We introduce Sketchforme, the first neural-network-based system that can generate sketches based on text descriptions specified by users. Sketchforme is capable of gaining high-level and low-level understanding of multi-object sketched scenes without being trained on sketched scene datasets annotated with text descriptions. The sketches composed by Sketchforme are expressive and realistic: we show in our user study that these sketches convey descriptions better than human-generated sketches in multiple cases, and 36.5% of those sketches are considered to be human-generated. We develop multiple interactive applications using these generated sketches, and show that Sketchforme can significantly improve language learning applications and support intelligent language-based sketching assistants.',\n",
       " 'Existing research highlight the myriad of benefits realized when technology is sufficiently democratized and made accessible to non-technical or novice users. However, democratizing complex technologies such as artificial intelligence (AI) remains hard. In this work, we draw on theoretical underpinnings from the democratization of innovation, in exploring the design of maker kits that help introduce novice users to complex technologies. We report on our work designing TJBot: an open source cardboard robot that can be programmed using pre-built AI services. We highlight principles we adopted in this process (approachable design, simplicity, extensibility and accessibility), insights we learned from showing the kit at workshops (66 participants) and how users interacted with the project on GitHub over a 12-month period (Nov 2016 - Nov 2017). We find that the project succeeds in attracting novice users (40% of users who forked the project are new to GitHub) and a variety of demographics are interested in prototyping use cases such as home automation, task delegation, teaching and learning.',\n",
       " \"The blocks editor, such as the editor in Scratch, is widely applied for visual programming languages (VPL) nowadays. Despite it's friendly for non-programmers, it exists three main limitations while displaying block codes: (1) the readability, (2) the program structure, and (3) the re-use. To cope with these issues, we introduce a novel formatting tool, block shelves, into the editor for organizing blocks. A user could utilize shelves to constitute a user-defined structure for the VPL projects. Based on the experiment results, block shelves improves the block code navigating and searching significantly. Besides, for achieving code re-use, users could use shelf export/import to share/re-use their block codes between projects in the file format of eXtensible Markup Language (xml.) All functions were demonstrated on MIT App inventor 2, while all modifications were made in Google Blockly.\",\n",
       " 'Many user studies of home automation, as the most familiar representative of the Internet of Things, have shown the difficulty of developing technology that users understand and like. It helps to state requirements as largely-independent features, but features are not truly independent, so this incurs the cost of managing and explaining feature interactions. We propose to compose features at runtime, resolving their interactions by means of priority. Although the basic idea is simple, its details must be designed to make users comfortable by balancing manual and automatic control. On the technical side, its details must be designed to allow meaningful separation of features and maximum generality. As evidence that our composition mechanism achieves its goals, we present three substantive examples of home automation, and the results of a user study to investigate comprehension of feature interactions. A survey of related work shows that this proposal occupies a sensible place in a design space whose dimensions include actuator type, detection versus resolution strategies, and modularity.',\n",
       " \"This paper presents a novel game prototype that uses music and motion detection as preventive medicine for the elderly. Given the aging populations around the globe, and the limited resources and staff able to care for these populations, eHealth solutions are becoming increasingly important, if not crucial, additions to modern healthcare and preventive medicine. Furthermore, because compliance rates for performing physical exercises are often quite low in the elderly, systems able to motivate and engage this population are a necessity. Our prototype uses music not only to engage listeners, but also to leverage the efficacy of music to improve mental and physical wellness. The game is based on a memory task to stimulate cognitive function, and requires users to perform physical gestures to mimic the playing of different musical instruments. To this end, the Microsoft Kinect sensor is used together with a newly developed gesture detection module in order to process users' gestures. The resulting prototype system supports both cognitive functioning and physical strengthening in the elderly.\",\n",
       " 'A graphical user interface (GUI) represents the most common option for interacting with computer systems. However, according to the literature system administrators often favor command line interfaces (CLIs). The goal of our work is to investigate which interfaces system administrators prefer, and which they actually utilize in their daily tasks. We collected experiences and opinions from 300 system administrators with the help of an online survey. All our respondents are system administrators, who work or have worked with firewalls. Our results show that only 32% of the respondents prefer CLIs for managing firewalls, while the corresponding figure is 60% for GUIs. We report the mentioned strengths and limitations of each interface and the tasks for which they are utilized by the system administrators. Based on these results, we provide design recommendations for firewall interfaces.',\n",
       " 'In this paper we consider the neuroscientific theory of the Bayesian brain in the light of adaptive web systems and content personalisation. In particular, we elaborate on neural mechanisms of human decision-making and the origin of lacking reliability of user feedback, often denoted as noise or human uncertainty. To this end, we first introduce an adaptive model of cognitive agency in which populations of neurons provide an estimation for states of the world. Subsequently, we present various so-called decoder functions with which neuronal activity can be translated into quantitative decisions. The interplay of the underlying cognition model and the chosen decoder function leads to different model-based properties of decision processes. The goal of this paper is to promote novel user models and exploit them to naturally associate users to different clusters on the basis of their individual neural characteristics and thinking patterns. These user models might be able to turn the variability of user behaviour into additional information for improving web personalisation and its experience.',\n",
       " \"Sharkzor is a web application for machine-learning assisted image sort and summary. Deep learning algorithms are leveraged to infer, augment, and automate the user's mental model. Initially, images uploaded by the user are spread out on a canvas. The user then interacts with the images to impute their mental model into the application's algorithmic underpinnings. Methods of interaction within Sharkzor's user interface and user experience support three primary user tasks; triage, organize and automate. The user triages the large pile of overlapping images by moving images of interest into proximity. The user then organizes said images into meaningful groups. After interacting with the images and groups, deep learning helps to automate the user's interactions. The loop of interaction, automation, and response by the user allows the system to quickly make sense of large amounts of data.\",\n",
       " \"Traditional relative pointing devices such as mice and trackpads are unsuitable for pointing at distant displays, because they encumber the users by requiring either a flat surface to operate on or being held by two hands. Past research has examined many new pointing methods, but few could surpass the speed and accuracy of mice and trackpads. This paper introduces a new pointing system that is developed based on HTC Vive, a relatively low-cost virtual reality system, and proposes two methods of combining absolute and relative pointing. The proposed methods were compared against single-mode pointing methods (i.e., pure absolute pointing and pure relative pointing) in a Fitts' law study. The results show that with only a short period of practice, one hybrid pointing technique enabled faster and more accurate pointing than both single-mode pointing techniques, which included a trackpad.\",\n",
       " 'Distant pointing is still not efficient, accurate or flexible enough for many applications, although many researchers have focused on it. To improve upon distant pointing, we propose MPP3D, which is especially suitable for high-resolution displays. MPP3D uses two dimensions of hand positioning to move a pointer, and it also uses the third dimension to adjust the precision of the movement. Based on the idea of MPP3D, we propose four techniques which combine two ways of mapping and two techniques for precision adjustment. We further provide three types of mapping scheme and visual feedback for each technique. The potential of the proposed techniques was investigated through experimentation. The results show that these techniques were competent for usual computer operations with a cursor, and the adjustment for pointing precision was beneficial for both pointing efficiency and accuracy.',\n",
       " 'In recent times, there have been significant advancements in utilizing the sensing capabilities of mobile devices for developing applications. The primary objective has been to enhance the way a user interacts with the application by making it effortless and convenient. This paper explores the capabilities of using Brain Computer Interfaces (BCI), an evolving subset of Human Computer Interaction (HCI) paradigms, to control mobile devices. We present a comprehensive survey of the state-of-the-art in this area, discussing the challenges and limitations in using BCI for mobile applications. Further we propose possible modalities that in future can benefit with BCI applications. This paper consolidates research directions being pursued in this domain, and draws conclusions on feasibility and benefits of using BCI systems effectively augmented to the mobile application development domain.',\n",
       " \"Software tools for generating digital sound often present users with high-dimensional, parametric interfaces, that may not facilitate exploration of diverse sound designs. In this paper, we propose to investigate artificial agents using deep reinforcement learning to explore parameter spaces in partnership with users for sound design. We describe a series of user-centred studies to probe the creative benefits of these agents and adapting their design to exploration. Preliminary studies observing users' exploration strategies with parametric interfaces and testing different agent exploration behaviours led to the design of a fully-functioning prototype, called Co-Explorer, that we evaluated in a workshop with professional sound designers. We found that the Co-Explorer enables a novel creative workflow centred on human-machine partnership, which has been positively received by practitioners. We also highlight varied user exploration behaviors throughout partnering with our system. Finally, we frame design guidelines for enabling such co-exploration workflow in creative digital applications.\",\n",
       " 'In this paper, we discuss the generation of symbols (and alphabets) based on specific user requirements (medium, priorities, type of information that needs to be conveyed). A framework for the generation of alphabets is proposed, and its use for the generation of a shorthand writing system is explored. We discuss the possible use of machine learning and genetic algorithms to gather inputs for generation of such alphabets and for optimization of already generated ones. The alphabets generated using such methods may be used in very different fields, from the creation of synthetic languages and constructed scripts to the creation of sensible commands for multimodal interaction through Human-Computer Interfaces, such as mouse gestures, touchpads, body gestures, eye-tracking cameras, and brain-computing Interfaces, especially in applications for elderly care and people with disabilities.',\n",
       " 'Collaborative sensemaking requires that analysts share their information and insights with each other, but this process of sharing runs the risks of prematurely focusing the investigation on specific suspects. To address this tension, we propose and test an interface for collaborative crime analysis that aims to make analysts more aware of their sensemaking processes. We compare our sensemaking translucence interface to a standard interface without special sensemaking features in a controlled laboratory study. We found that the sensemaking translucence interface significantly improved clue finding and crime solving performance, but that analysts rated the interface lower on subjective measures than the standard interface. We conclude that designing for distributed sensemaking requires balancing task performance vs. user experience and real-time information sharing vs. data accuracy.',\n",
       " 'Engagement is a vital metric in the advertising industry and its automatic estimation has huge commercial implications. This work presents a basic and simple framework for engagement estimation using EEG (electroencephalography) data specifically recorded while watching advertisement videos, and is meant to be a first step in a promising line of research. The system combines recent advances in low cost commercial Brain-Computer Interfaces with modeling user engagement in response to advertisement videos. We achieve an F1 score of nearly 0.7 for a binary classification of high and low values of self-reported engagement from multiple users. This study illustrates the possibility of seamless engagement measurement in the wild when interacting with media using a non invasive and readily available commercial EEG device. Performing engagement measurement via implicit tagging in this manner with a direct feedback from physiological signals, thus requiring no additional human effort, demonstrates a novel and potentially commercially relevant application in the area of advertisement video analysis.',\n",
       " 'The \"gig economy\" has transformed the ways in which people work, but in many ways these markets stifle the growth of workers and the autonomy and protections that workers have grown to expect. We explored the viability of a \"worker centric peer economy\"--a system wherein workers benefit as well as consumers-- and conducted ethnographic field work across fields ranging from domestic labor to home health care. We discovered seven facets that system designers ought to consider when designing a labor market for \"gig workers,\" consisting principally of the following: constructive feedback, assigning work fairly, managing customer expectations, protecting vulnerable workers, reconciling worker identities, assessing worker qualifications, & communicating worker quality. We discuss these considerations and provide guidance toward the design of a mutually beneficial market for gig workers.',\n",
       " 'Fall is one of the major health threats and obstacles to independent living for elders, timely and reliable fall detection is crucial for mitigating the effects of falls. In this paper, leveraging the fine-grained Channel State Information (CSI) and multi-antenna setting in commodity WiFi devices, we design and implement a real-time, non-intrusive, and low-cost indoor fall detector, called Anti-Fall. For the first time, the CSI phase difference over two antennas is identified as the salient feature to reliably segment the fall and fall-like activities, both phase and amplitude information of CSI is then exploited to accurately separate the fall from other fall-like activities. Experimental results in two indoor scenarios demonstrate that Anti-Fall consistently outperforms the state-of-the-art approach WiFall, with 10% higher detection rate and 10% less false alarm rate on average.',\n",
       " 'Opportunistic affect sensing offers unprecedented potential for capturing spontaneous affect ubiquitously, obviating biases inherent in the laboratory setting. Facial expression and voice are two major affective displays, however most affect sensing systems on smartphone avoid them due to extensive power requirement. Encouragingly, due to the recent advent of low-power DSP (Digital Signal Processing) co-processor and GPU (Graphics Processing Unit) technology, audio and video sensing are becoming more feasible. To properly evaluate opportunistically captured facial expression and voice, contextual information about the dynamic audio-visual stimuli needs to be inferred. This paper discusses recent advances of affect sensing on the smartphone and identifies the key barriers and potential solutions of implementing opportunistic and context-aware affect sensing on smartphone platforms.',\n",
       " 'Recently, we developed a dynamic distributed end-to-end vehicle routing system (E2ECAV) using a network of intelligent intersections and level 5 CAVs (Djavadian & Farooq, 2018). The case study of the downtown Toronto Network showed that E2ECAV has the ability to maximize throughput and reduce travel time up to 40%. However, the efficiency of these new technologies relies on the acceptance of users in adapting to them and their willingness to give control fully or partially to CAVs. In this study a stated preference laboratory experiment is designed employing Virtual Reality Immersive Environment (VIRE) driving simulator to evaluate the behavioral response of drivers to E2ECAV. The aim is to investigate under what conditions drivers are more willing to adapt. The results show that factors such as locus of control, congestion level and ability to multi-task have significant impact.',\n",
       " 'Medical non-adherence increasingly is recognized as a major medical health problem. Approximately 50% of patients do not take their medications as prescribed and such poor adherence has been shown to result in complications, death, and increased health care costs. This problem becomes even more significant for patients with chronic illness and those who need to take medications lifetime, like transplant patients. Studies show that one-half of rejection episodes and 15% of graft losses happen due to immunosuppression medications non-adherence. This article explores factors that have an impact on non-compliant behavior among transplant patients: patient factors, illness factor, therapeutic regimen factors. Using user-centered design thinking approach a set of hypotheses are defined and discussed strategies to enhance adherence by using mobile technology and gamification techniques.',\n",
       " 'The main challenges in large-scale people tracking are the recognition of people density in a specific area and tracking the people flow path. To address these challenges, we present SenseFlow, a lightweight people tracking system. SenseFlow utilises off-the-shelf devices which sniff probe requests periodically polled by user\\'s smartphones in a passive manner. We demonstrate the feasibility of SenseFlow by building a proof-of-concept prototype and undertaking extensive evaluations in real-world settings. We deploy the system in one laboratory to study office hours of researchers, a crowded public area in city to evaluate the scalability and performance \"in the wild\", and four classrooms in the university to monitor the number of students. We also evaluate SenseFlow with varying walking speeds and different models of smartphones to investigate the people flow tracking performance.',\n",
       " \"An implicit association test is a human psychological test used to measure subconscious associations. While widely recognized by psychologists as an effective tool in measuring attitudes and biases, the validity of the results can be compromised if a subject does not follow the instructions or attempts to manipulate the outcome. Compared to previous work, we collect training data using a more generalized methodology. We train a variety of different classifiers to identify a participant's first attempt versus a second possibly compromised attempt. To compromise the second attempt, participants are shown their score and are instructed to change it using one of five randomly selected deception methods. Compared to previous work, our methodology demonstrates a more robust and practical framework for accurately identifying a wide variety of deception techniques applicable to the IAT.\",\n",
       " 'When people browse online news, small thumbnail images accompanying links to articles attract their attention and help them to decide which articles to read. As an increasing proportion of online news can be construed as data journalism, we have witnessed a corresponding increase in the incorporation of visualization in article thumbnails. However, there is little research to support alternative design choices for visualization thumbnails, which include resizing, cropping, simplifying, and embellishing charts appearing within the body of the associated article. We therefore sought to better understand these design choices and determine what makes a visualization thumbnail inviting and interpretable. This paper presents our findings from a survey of visualization thumbnails collected online and from conversations with data journalists and news graphics designers. Our study reveals that there exists an uncharted design space, one that is in need of further empirical study. Our work can thus be seen as a first step toward providing structured guidance on how to design thumbnails for data stories.',\n",
       " 'Teaching and advocating data visualization are among the most important activities in the visualization community. With growing interest in data analysis from business and science professionals, data visualization courses attract students across different disciplines. However, comprehensive visualization training requires students to have a certain level of proficiency in programming, a requirement that imposes challenges on both teachers and students. With recent developments in visualization tools, we have managed to overcome these obstacles by teaching a wide range of visualization and supporting tools. Starting with GUI-based visualization tools and data analysis with Python, students put visualization knowledge into practice with increasing amounts of programming. At the end of the course, students can design and implement visualizations with D3 and other programming-based visualization tools. Throughout the course, we continuously collect student feedback and refine the teaching materials. This paper documents our teaching methods and considerations when designing the teaching materials.',\n",
       " \"When studying human-technology interaction systems, researchers thrive to achieve intuitiveness and facilitate the people's life through a thoughtful and in-depth study of several components of the application system that supports some particular business communication with customers. Particularly in the healthcare field, some requirements such as clarity, transparency, efficiency, and speed in transmitting information to patients and or healthcare professionals might mean an important increase in the well-being of the patient and productivity of the healthcare professional. In this work, the authors study the difficulties patients frequently have when communicating with pharmacists. In addition to a statistical study of a survey conducted with more than two hundred frequent pharmacy customers, we propose an IT solution for better communication between patients and pharmacists.\",\n",
       " 'Data visualization and interaction with large data sets is known to be essential and critical in many businesses today, and the same applies to research and teaching, in this case, when exploring large and complex mathematical objects. GAP is a computer algebra system for computational discrete algebra with an emphasis on computational group theory. The existing XGAP package for GAP works exclusively on the X Window System. It lacks abstraction between its mathematical and graphical cores, making it difficult to extend, maintain, or port. In this paper, we present Francy, a graphical semantics package for GAP. Francy is responsible for creating a representational structure that can be rendered using many GUI frameworks independent from any particular programming language or operating system. Building on this, we use state of the art web technologies that take advantage of an improved REPL environment, which is currently under development for GAP. The integration of this project with Jupyter provides a rich graphical environment full of features enhancing the usability and accessibility of GAP.',\n",
       " \"The advent of the digital pathology has introduced new avenues of diagnostic medicine. Among them, crowdsourcing has attracted researchers' attention in the recent years, allowing them to engage thousands of untrained individuals in research and diagnosis. While there exist several articles in this regard, prior works have not collectively documented them. We, therefore, aim to review the applications of crowdsourcing in human pathology in a semi-systematic manner. We firstly, introduce a novel method to do a systematic search of the literature. Utilizing this method, we, then, collect hundreds of articles and screen them against a pre-defined set of criteria. Furthermore, we crowdsource part of the screening process, to examine another potential application of crowdsourcing. Finally, we review the selected articles and characterize the prior uses of crowdsourcing in pathology.\",\n",
       " \"While visual comparison of directed acyclic graphs (DAGs) is commonly encountered in various disciplines (e.g., finance, biology), knowledge about humans' perception of graph similarity is currently quite limited. By graph similarity perception we mean how humans perceive commonalities and differences in graphs and herewith come to a similarity judgment. As a step toward filling this gap the study reported in this paper strives to identify factors which influence the similarity perception of DAGs. In particular, we conducted a card-sorting study employing a qualitative and quantitative analysis approach to identify 1) groups of DAGs that are perceived as similar by the participants and 2) the reasons behind their choice of groups. Our results suggest that similarity is mainly influenced by the number of levels, the number of nodes on a level, and the overall shape of the graph.\",\n",
       " \"Smart devices have become common place in many homes, and these devices can be utilized to provide support for people with mental or physical deficits. Voice-controlled assistants are a class of smart device that collect a large amount of data in the home. In this work we present Echo SCraper and ClAssifier of Persons (ESCAPE), an open source software for the extraction of Amazon Echo interaction data, and speaker recognition on that data. We show that ESCAPE is able to extract data from a voice-controlled assistant and classify with accuracy who is talking, based on a small number of labeled audio data. Using ESCAPE to extract interactions recorded over 3 months in the first author's home yields a rich dataset of transcribed audio recordings. Our results demonstrate that using this software the Amazon Echo can be used to study participants in a naturalistic setting with minimal intrusion. We also discuss the potential for usage of voice-controlled devices together with ESCAPE to understand how diseases affect individuals, and how these data can be used to monitor disease processes in general.\",\n",
       " 'The human visual color response is driven by specialized cells called cones, which exist in three types, viz. R, G, and B. Software is developed to simulate how color images are displayed for different types of color blindness. Specified the default color deficiency associated with a user, it generates a preview of the rainbow (in the visible range, from red to violet) and shows up, side by side with a colorful image provided as input, the display correspondent colorblind. The idea is to provide an image processing after image acquisition to enable a better perception ofcolors by the color blind. Examples of pseudo-correction are shown for the case of Protanopia (red blindness). The system is adapted into a screen of an i-pad or a cellphone in which the colorblind observe the camera, the image processed with color detail previously imperceptible by his naked eye. As prospecting, wearable computer glasses could be manufactured to provide a corrected image playback. The approach can also provide augmented reality for human vision by adding the UV or IR responses as a new feature of Google Glass.',\n",
       " \"In the last decade, the effects of interruptions through mobile notifications have been extensively researched in the field of Human-Computer Interaction. Breakpoints in tasks and activities, cognitive load, and personality traits have all been shown to correlate with individuals' interruptibility. However, concepts that explain interruptibility in a broader sense are needed to provide a holistic understanding of its characteristics. In this paper, we build upon the theory of social roles to conceptualize and investigate the correlation between individuals' private and work-related smartphone usage and their interruptibility. Through our preliminary study with four participants over 11 weeks, we found that application sequences on smartphones correlate with individuals' private and work roles. We observed that participants engaged in these roles tend to follow specific interruptibility strategies - integrating, combining, or segmenting private and work-related engagements. Understanding these strategies breaks new ground for attention and interruption management systems in ubiquitous computing.\",\n",
       " 'Intelligent systems and advanced automation are involved in information collection and evaluation, in decision-making and in the implementation of chosen actions. In such systems, human responsibility becomes equivocal. Understanding human responsibility is particularly important when intelligent autonomous systems can harm people, as with autonomous vehicles or, most notably, with Advanced Weapon Systems (AWS). Using Information Theory, we develop a responsibility quantification (ResQu) model of human involvement in intelligent automated systems and demonstrate its applications on decisions regarding AWS. The analysis reveals that human comparative responsibility is often low, even when major functions are allocated to the human. Thus, broadly stated policies of keeping humans in the loop and having meaningful human control are misleading and cannot truly direct decisions on how to involve humans in intelligent systems and advanced automation. Our responsibility model can guide system design decisions and can aid policy and legal decisions regarding human responsibility in intelligent systems.',\n",
       " \"Interactive visual analytic systems enable users to discover insights from complex data. Users can express and test hypotheses via user interaction, leveraging their domain expertise and prior knowledge to guide and steer the analytic models in the system. For example, semantic interaction techniques enable systems to learn from the user's interactions and steer the underlying analytic models based on the user's analytical reasoning. However, an open challenge is how to not only steer models based on the dimensions or features of the data, but how to add dimensions or attributes to the data based on the domain expertise of the user. In this paper, we present a technique for inferring and appending dimensions onto the dataset based on the prior expertise of the user expressed via user interactions. Our technique enables users to directly manipulate a spatial organization of data, from which both the dimensions of the data are weighted, and also dimensions created to represent the prior knowledge the user brings to the system. We describe this technique and demonstrate its utility via a use case.\",\n",
       " 'The present paper reports the results of testing first year students of Informatics on their algorithmic skills and knowledge transfer abilities in spreadsheet environments. The selection of students plays a crucial role in the project. On the one hand, they have officially finished their spreadsheet training - they know everything - while on the other hand, they do not need any training, since they are digital natives, to whom digital skills are assigned by birth. However, we found that the students had serious difficulties in solving the spreadsheet problems presented: so low were their results that it allowed us to form broad tendencies. Considering computational thinking, algorithmic skills, and knowledge transfer abilities, it is clear that those students performed better who used algorithm-based, multilevel array formulas instead of problem specific, unconnected built-in functions. Furthermore, we can conclude that students, regardless of their birth date and digital generation assigned to them, are in great need of official, high-mathability, algorithm-based training with expert teachers.',\n",
       " \"We present an end-to-end voice-based conversational agent that is able to engage in naturalistic multi-turn dialogue and align with the interlocutor's conversational style. The system uses a series of deep neural network components for speech recognition, dialogue generation, prosodic analysis and speech synthesis to generate language and prosodic expression with qualities that match those of the user. We conducted a user study (N=30) in which participants talked with the agent for 15 to 20 minutes, resulting in over 8 hours of natural interaction data. Users with high consideration conversational styles reported the agent to be more trustworthy when it matched their conversational style. Whereas, users with high involvement conversational styles were indifferent. Finally, we provide design guidelines for multi-turn dialogue interactions using conversational style adaptation.\",\n",
       " 'A significant body of research in Artificial Intelligence (AI) has focused on generating stories automatically, either based on prior story plots or input images. However, literature has little to say about how users would receive and use these stories. Given the quality of stories generated by modern AI algorithms, users will nearly inevitably have to edit these stories before putting them to real use. In this paper, we present the first analysis of how human users edit machine-generated stories. We obtained 962 short stories generated by one of the state-of-the-art visual storytelling models. For each story, we recruited five crowd workers from Amazon Mechanical Turk to edit it. Our analysis of these edits shows that, on average, users (i) slightly shortened machine-generated stories, (ii) increased lexical diversity in these stories, and (iii) often replaced nouns and their determiners/articles with pronouns. Our study provides a better understanding on how users receive and edit machine-generated stories,informing future researchers to create more usable and helpful story generation systems.',\n",
       " \"Traditional approaches for ensuring high quality crowdwork have failed to achieve high-accuracy on difficult problems. Aggregating redundant answers often fails on the hardest problems when the majority is confused. Argumentation has been shown to be effective in mitigating these drawbacks. However, existing argumentation systems only support limited interactions and show workers general justifications, not context-specific arguments targeted to their reasoning.\\n  This paper presents Cicero, a new workflow that improves crowd accuracy on difficult tasks by engaging workers in multi-turn, contextual discussions through real-time, synchronous argumentation. Our experiments show that compared to previous argumentation systems which only improve the average individual worker accuracy by 6.8 percentage points on the Relation Extraction domain, our workflow achieves 16.7 percentage point improvement. Furthermore, previous argumentation approaches don't apply to tasks with many possible answers; in contrast, Cicero works well in these cases, raising accuracy from 66.7% to 98.8% on the Codenames domain.\",\n",
       " 'Web portals have served as an excellent medium to facilitate user centric services for organizations irrespective of the type, size, and domain of operation. The objective of these portals has been to deliver a plethora of services such as information dissemination, transactional services, and customer feedback. Therefore, the design of a web portal is crucial in order that it is accessible to a wide range of user community irrespective of age group, physical abilities, and level of literacy. In this paper, we have studied the compliance of WCAG 2.0 by three different categories of Indian web sites which are most frequently accessed by a large section of user community. We have provided a quantitative evaluation of different aspects of accessibility which we believe can pave the way for better design of web sites by taking care of the deficiencies inherent in the web portals.',\n",
       " \"Recently, researchers started using cognitive load in various settings, e.g., educational psychology, cognitive load theory, or human-computer interaction. Cognitive load characterizes a tasks' demand on the limited information processing capacity of the brain. The widespread adoption of eye-tracking devices led to increased attention for objectively measuring cognitive load via pupil dilation. However, this approach requires a standardized data processing routine to reliably measure cognitive load. This technical report presents CEP-Web, an open source platform to providing state of the art data processing routines for cleaning pupillary data combined with a graphical user interface, enabling the management of studies and subjects. Future developments will include the support for analyzing the cleaned data as well as support for Task-Evoked Pupillary Response (TEPR) studies.\",\n",
       " 'Automatic recognition of the quality of movement in human beings is a challenging task, given the difficulty both in defining the constraints that make a movement correct, and the difficulty in using noisy data to determine if these constraints were satisfied. This paper presents a method for the detection of deviations from the correct form in movements from physical therapy routines based on Hidden Markov Models, which is compared to Dynamic Time Warping. The activities studied include upper an lower limbs movements, the data used comes from a Kinect sensor. Correct repetitions of the activities of interest were recorded, as well as deviations from these correct forms. The ability of the proposed approach to detect these deviations was studied. Results show that a system based on HMM is much more likely to determine if a certain movement has deviated from the specification.',\n",
       " 'Due to the prevalence of online services in modern society, such as internet banking and social media, it is important for users to have an understanding of basic security measures in order to keep themselves safe online. However, users often do not know how to make their online interactions secure, which demonstrates an educational need in this area. Gamification has grown in popularity in recent years and has been used to teach people about a range of subjects. This paper presents an exploratory study investigating the use of gamification techniques to educate average users about password security, with the aim of raising overall security awareness. To explore the impact of such techniques, a role-playing quiz application (RPG) was developed for the Android platform to educate users about password security. Results gained from the work highlighted that users enjoyed learning via the use of the password application, and felt they benefitted from the inclusion of gamification techniques. Future work seeks to expand the prototype into a full solution, covering a range of security awareness issues.',\n",
       " 'Mobile money can facilitate financial inclusion in developing countries, which usually have high mobile phone use and steady remittance activity. Many countries in Latin America meet the minimum technological requirements to use mobile money, however, the adoption in this region is relatively low. This paper investigates the different factors that lead people in Latin America to distrust and therefore not adopt mobile money. For this purpose, we analyzed 27 mobile money applications on the market and investigated the perceptions that people in Latin America have of such interfaces. From our study, we singled out the interface features that have the greatest influence in user adoption in developing countries. We identified that for the Latin America market it is crucial to create mobile applications that allow the user to visualize and understand the workflow through which their money is traveling to recipients. We examined the significance of these findings in the design of future mobile money applications that can effectively improve the use of electronic financial transactions in Latin America.',\n",
       " \"Today, eye trackers are extensively used in user interface evaluations. However, it's still hard to analyze and interpret eye tracking data from the aesthetic point of view. To find quantitative links between eye movements and aesthetic experience, we tracked 30 observers' initial landings for 40 web pages (each displayed for 3 seconds). The web pages were also rated based on the observers' subjective aesthetic judgments. Shannon entropy was introduced to analyze the eye-tracking data. The result shows that the heatmap entropy (visual attention entropy, VAE) is highly correlated with the observers' aesthetic judgements of the web pages. Its improved version, relative VAE (rVAE), has a more significant correlation with the perceived aesthetics. (r=-0.65, F= 26.84, P$<$0.0001). This single metric alone can distinguish between good- and bad-looking pages with an approximate 85\\\\% accuracy. Further investigation reveals that the performance of both VAE and rVAE became stable after 1 second. The curves indicate that their performances could be better, if the tracking time was extended beyond 3 seconds.\",\n",
       " 'Virtual Learning Environments (VLEs) are spaces designed to educate students remotely via online platforms. Although traditional VLEs such as iSocial have shown promise in educating students, they offer limited immersion that diminishes learning effectiveness. This paper outlines a virtual reality learning environment (VRLE) over a high-speed network, which promotes educational effectiveness and efficiency via our creation of flexible content and infrastructure which meet established VLE standards with improved immersion. This paper further describes our implementation of multiple learning modules developed in High Fidelity, a \"social VR\" platform. Our experiment results show that the VR mode of content delivery better stimulates the generalization of lessons to the real world than non-VR lessons and provides improved immersion when compared to an equivalent desktop version.',\n",
       " 'In the early stages of designing graphical user interfaces (GUIs), the look (appearance) can be easily presented by sketching, but the feel (interactive behaviors) cannot, and often requires an accompanying description of how it works (Myers et al. 2008). We propose to use crowdsourcing to augment early sketches with interactive behaviors generated, used, and reused by collective \"wizards-of-oz\" as opposed to a single wizard as in prior work (Davis et al. 2007). This demo presents an extension of Apparition (Lasecki et al. 2015), a crowd-powered prototyping tool that allows end users to create functional GUIs using speech and sketch. In Apparition, crowd workers collaborate in real-time on a shared canvas to refine the user-requested sketch interactively, and with the assistance of the end users. Our demo extends this functionality to let crowd workers \"demonstrate\" the canvas changes that are needed for a behavior and refine their demonstrations to improve the fidelity of interactive behaviors. The system then lets workers \"remix\" these behaviors to make creating future behaviors more efficient.',\n",
       " 'Today, more and more open data statistics are published by governments, statistical offices and organizations like the United Nations, The World Bank or Eurostat. This data is freely available and can be consumed by end users in interactive visualizations. However, additional information is needed to enable laymen to interpret these statistics in order to make sense of the raw data. In this paper, we present an approach to combine open data statistics with historical events. In a user interface we have integrated interactive visualizations of open data statistics with a timeline of thematically appropriate historical events from Wikipedia. This can help users to explore statistical data in several views and to get related events for certain trends in the timeline. Events include links to Wikipedia articles, where details can be found and the search process can be continued. We have conducted a user study to evaluate if users can use the interface intuitively, if relations between trends in statistics and historical events can be found and if users like this approach for their exploration process.',\n",
       " \"Based on the previously proposed concept Understanding Tree, this paper introduces two concepts: Understanding Graph and Understanding Map, and explores their potential applications. Understanding Graph and Understanding Map can be deemed as special cases of mind map, semantic network, or concept map. The two main differences are: Firstly, the data sources for constructing Understanding Map and Understanding Graph are distinctive and simple. Secondly, the relations between concepts in Understanding Graph and Understanding Map are monotonous. Based on their characteristics, applications of them include quantitatively measuring a concept's complexity degree, quantitatively measuring a concept's importance degree in a domain, and computing an optimized learning sequence for comprehending a concept etc. Further study involves evaluating their performances in these applications.\",\n",
       " 'Promotion of healthy habits help maintain and improve people health, reduce disease risks, and manage chronic illness. Regular healthy activities like walking, exercising, healthy eating, drinking water or taking medication on time require forming the new habits. Gamification techniques are promising in promoting healthy behaviors and delivering health promotion information. However, using gaming elements such as badges, leader boards, health-related challenges in mobile applications to motivate and engage people to change health behavior is quite new. In this exploratory study, we aimed to assess how game mechanics and dynamics influence formation of a habit through the mobile application. Results indicate the different level of user engagement depending on the presence of gamification elements and suggest that there is value in adding game elements to the user experience.',\n",
       " 'Texture is an essential property of physical objects that affects aesthetics, usability, and functionality. However, designing and applying textures to 3D objects with existing tools remains difficult and time-consuming; it requires proficient 3D modeling skills. To address this, we investigated an auto-completion approach for efficient texture creation that automates the tedious, repetitive process of applying texture while allowing flexible customization. We developed techniques for users to select a target surface, sketch and manipulate a texture with 2D drawings, and then generate 3D printable textures onto an arbitrary curved surface. In a controlled experiment our tool sped texture creation by 80% over conventional tools, a performance gain that is higher with more complex target surfaces. This result confirms that auto-completion is powerful for creating 3D textures.',\n",
       " \"In this paper we introduce a paradigm for completing complex tasks from wearable devices by leveraging crowdsourcing, and demonstrate its validity for academic writing. We explore this paradigm using a collaborative authoring system, called WearWrite, which is designed to enable authors and crowd workers to work together using an Android smartwatch and Google Docs to produce academic papers, including this one. WearWrite allows expert authors who do not have access to large devices to contribute bits of expertise and big picture direction from their watch, while freeing them of the obligation of integrating their contributions into the overall document. Crowd workers on desktop computers actually write the document. We used this approach to write several simple papers, and found it was effective at producing reasonable drafts. However, the workers often needed more structure and the authors more context. WearWrite addresses these issues by focusing workers on specific tasks and providing select context to authors on the watch. We demonstrate the system's feasibility by writing this paper using it.\",\n",
       " 'We present a Virtual Reality (VR) application for labeling and handling point cloud data sets. A series of room-scale point clouds are recorded as a video sequence using a Microsoft Kinect. The data can be played and paused, and frames can be skipped just like in a video player. The user can walk around and inspect the data while it is playing or paused. Using the tracked hand-held controller, the user can select and label individual parts of the point cloud. The points are highlighted with a color when they are labeled. With a tracking algorithm, the labeled points can be tracked from frame to frame to ease the labeling process. Our sample data is an RGB point cloud recording of two people juggling with pins. Here, the user can select and label, for example, the juggler pins as shown in Figure 1. Each juggler pin is labeled with various colors to indicate di erent labels.',\n",
       " 'Human computation games (HCGs) are a crowdsourcing approach to solving computationally-intractable tasks using games. In this paper, we describe the need for generalizable HCG design knowledge that accommodates the needs of both players and tasks. We propose a formal representation of the mechanics in HCGs, providing a structural breakdown to visualize, compare, and explore the space of HCG mechanics. We present a methodology based on small-scale design experiments using fixed tasks while varying game elements to observe effects on both the player experience and the human computation task completion. Finally we discuss applications of our framework using comparisons of prior HCGs and recent design experiments. Ultimately, we wish to enable easier exploration and development of HCGs, helping these games provide meaningful player experiences while solving difficult problems.',\n",
       " \"We present Eventful, a system for producing news reports of local events using remote and locative crowd workers. The system recruits and guides novice crowd workers as they perform the roles of field reporter, curator, or writer. Field reporters attend the events in person, and use Eventful's mobile web app to get a personalized mission, submit content, and receive feedback. Missions include tasks such as taking a photo, and asking a question to an attendee. In parallel, remote curators approve, reject, and give real-time feedback on the content collected by field reporters. Finally, writers put together a report by mashing up and tweaking the content approved by the curators. We used Eventful to produce a news report for each of the six local events we decided to cover as we piloted the system. The process was typically completed under an hour and costing under $150 USD.\",\n",
       " 'Task-based, rather than vehicle-based, control architectures have been shown to provide superior performance in certain human supervisory control missions. These results motivate the need for the development of robust, reliable usability metrics to aid in creating interfaces for use in this domain. To this end, we conduct a pilot usability study of a particular task-based supervisory control interface called the Research Environment for Supervisory Control of Heterogenous Unmanned Vehicles (RESCHU). In particular, we explore the use of eye-tracking metrics as an objective means of evaluating the RESCHU interface and providing guidance in improving usability. Our main goals for this study are to 1) better understand how eye-tracking can augment standard usability metrics, 2) formulate initial models of operator behavior, and 3) identify interesting areas of future research.',\n",
       " \"Here, we introduce a new data visualization and exploration method, TMAP (tree-map), which exploits locality sensitive hashing, Kruskal's minimum-spanning-tree algorithm, and a multilevel multipole-based graph layout algorithm to represent large and high dimensional data sets as a tree structure, which is readily understandable and explorable. Compared to other data visualization methods such as t-SNE or UMAP, TMAP increases the size of data sets that can be visualized due to its significantly lower memory requirements and running time and should find broad applicability in the age of big data. We exemplify TMAP in the area of cheminformatics with interactive maps for 1.16 million drug-like molecules from ChEMBL, 10.1 million small molecule fragments from FDB17, and 131 thousand 3D- structures of biomolecules from the PDB Databank, and to visualize data from literature (GUTENBERG data set), cancer biology (PANSCAN data set) and particle physics (MiniBooNE data set). TMAP is available as a Python package. Installation, usage instructions and application examples can be found at http://tmap.gdb.tools.\",\n",
       " 'Humans use a host of signals to infer the emotional state of others. In general, computer systems that leverage signals from multiple modalities will be more robust and accurate in the same task. We present a multimodal affect and context sensing platform. The system is composed of video, audio and application analysis pipelines that leverage ubiquitous sensors (camera and microphone) to log and broadcast emotion data in real-time. The platform is designed to enable easy prototyping of novel computer interfaces that sense, respond and adapt to human emotion. This paper describes the different audio, visual and application processing components and explains how the data is stored and/or broadcast for other applications to consume. We hope that this platform helps advance the state-of-the-art in affective computing by enabling development of novel human-computer interfaces.',\n",
       " \"In virtual reality games, players dive into fictional environments and can experience a compelling and immersive world. State-of-the-art VR systems allow for natural and intuitive navigation through physical walking. However, the tracking space is still limited, and viable alternatives \\\\comm{or extensions }are required to reach further virtual destinations. Our work focuses on the exploration of vast open worlds -- an area where existing local navigation approaches such as the arc-based teleport are not ideally suited and world-in-miniature techniques potentially reduce presence. We present a novel alternative for open environments: Our idea is to equip players with the ability to switch from first-person to a third-person bird's eye perspective on demand. From above, players can command their avatar and initiate travels over large distance. Our evaluation reveals a significant increase in spatial orientation while avoiding cybersickness and preserving presence, enjoyment, and competence. We summarize our findings in a set of comprehensive design guidelines to help developers integrate our technique.\",\n",
       " \"Collaboration is built on trust, and establishing trust with a creative Artificial Intelligence is difficult when the decision process or internal state driving its behaviour isn't exposed. When human musicians improvise together, a number of extra-musical cues are used to augment musical communication and expose mental or emotional states which affect musical decisions and the effectiveness of the collaboration. We developed a collaborative improvising AI drummer that communicates its confidence through an emoticon-based visualisation. The AI was trained on musical performance data, as well as real-time skin conductance, of musicians improvising with professional drummers, exposing both musical and extra-musical cues to inform its generative process. Uni- and bi-directional extra-musical communication with real and false values were tested by experienced improvising musicians. Each condition was evaluated using the FSS-2 questionnaire, as a proxy for musical engagement. The results show a positive correlation between extra-musical communication of machine internal state and human musical engagement.\",\n",
       " 'The way that design is being taught is continuously changing under the pressure of the transition from analogical to digital environments. This becomes even more important as the novelty and the alleged superiority of the digital world is used as a marketing tool by competing universities. Even though in some fields of application this approach is desirable, some particular aspects of teaching design and architecture make this transition debatable. The advantages of drawing on blackboards over drawing on whiteboard surfaces in regards of line aesthetic and expression possibilities were previously identified, along with the complementary necessary features for improvement. This study showcases a proof of concept in digitally augmenting a blackboard surface. The system allows the capturing, processing and making real time projections of images over the blackboard surface as trace references. Such a hybrid system, along with providing support for design and architecture related presentations and discussions could also mediate the contradictory relation towards technology that students and teachers have.',\n",
       " \"In a contemporary world, people become dependent on electronic devices. Technologies help to clarification and structure life in many ways to meet the need of the children oriented requirements. The children suffering from disabilities (e.g. autism) has desperate needs for elucidation and structures their life. MumIES is a research based system facilitates to support and manage their living. This paper works on MumIES system to evaluate usability of the system in extraordinary environment for extraordinary people. The paper shows from the survey observation users need supporting tools to access the children's potential and challenges and to give the full support to overcome disabilities. Usability evaluation has been considered one of the key challenges to MumIES system. The paper represents analysis, design of usability studies for the extraordinary user in environment.\",\n",
       " '\"Does placing workers together based on their personality give better performance results in cooperative crowdsourcing settings, compared to non-personality based crowd team formation?\" In this work we examine the impact of personality compatibility on the effectiveness of crowdsourced team work. Using a personality-based group dynamics approach, we examine two main types of personality combinations (matching and crashing) on two main types of tasks (collaborative and competitive). Our experimental results show that personality compatibility significantly affects the quality of the team\\'s final outcome, the quality of interactions and the emotions experienced by the team members. The present study is the first to examine the effect of personality over team result in crowdsourcing settings, and it has practical implications for the better design of crowdsourced team work.',\n",
       " \"The assumptions we make about a dialogue partner's knowledge and communicative ability (i.e. our partner models) can influence our language choices. Although similar processes may operate in human-machine dialogue, the role of design in shaping these models, and their subsequent effects on interaction are not clearly understood. Focusing on synthesis design, we conduct a referential communication experiment to identify the impact of accented speech on lexical choice. In particular, we focus on whether accented speech may encourage the use of lexical alternatives that are relevant to a partner's accent, and how this is may vary when in dialogue with a human or machine. We find that people are more likely to use American English terms when speaking with a US accented partner than an Irish accented partner in both human and machine conditions. This lends support to the proposal that synthesis design can influence partner perception of lexical knowledge, which in turn guide user's lexical choices. We discuss the findings with relation to the nature and dynamics of partner models in human machine dialogue.\",\n",
       " 'Around-device interaction promises to extend the input space of mobile and wearable devices beyond the common but restricted touchscreen. So far, most around-device interaction approaches rely on instrumenting the device or the environment with additional sensors. We believe, that the full potential of ordinary cameras, specifically user-facing cameras, which are integrated in most mobile devices today, are not used to their full potential, yet. We To this end, we present a novel approach for extending the input space around unmodified mobile devices using built-in front-facing cameras of unmodified handheld devices. Our approach estimates hand poses and gestures through reflections in sunglasses, ski goggles or visors. Thereby, GlassHands creates an enlarged input space, rivaling input reach on large touch displays. We discuss the idea, its limitations and future work.',\n",
       " 'With the fast development of network information technology, more and more people are immersed in the virtual community environment brought by the network, ignoring the social interaction in real life. The consequent urban autism problem has become more and more serious. Promoting offline communication between people \" and \"eliminating loneliness through emotional communication between pet robots and breeders\" to solve this problem, and has developed a design called \"Tom\". \"Tom\" is a smart pet robot with a pet robot-based social mechanism Called \"Tom-Talker\". The main contribution of this paper is to propose a social mechanism called \"Tom-Talker\" that encourages users to socialize offline. And \"Tom-Talker\" also has a corresponding reward mechanism and a friend recommendation algorithm. It also proposes a pet robot named \"Tom\" with an emotional interaction algorithm to recognize users\\' emotions, simulate animal emotions and communicate emotionally with use s. This paper designs experiments and analyzes the results. The results show that our pet robots have a good effect on solving urban autism problems.',\n",
       " 'The Q-method has been utilized over time in various areas, including information systems. In this study, we used a systematic mapping to illustrate how the Q-method was applied within Information Systems (IS) community and proposing towards the integration of Q-method into the Design Sciences Research (DSR) process as a tool for future research DSR-based IS studies. In this mapping study, we collected peer-reviewed journals from Basket-of-Eight journals and the digital library of the Association for Information Systems (AIS). Then we grouped the publications according to the process of DSR, and different variables for preparing Q-method from IS publications. We found that the potential of the Q-methodology can be used to support each main research stage of DSR processes and can serve as the useful tool to evaluate a system in the IS topic of system analysis and design',\n",
       " 'The creation and support of Embodied Conversational Agents (ECAs) has been quite challenging, as features required might not be straight-forward to implement and to integrate in a single application. Furthermore, ECAs as desktop applications present drawbacks for both developers and users; the former have to develop for each device and operating system and the latter must install additional software, limiting their widespread use. In this paper we demonstrate how recent advances in web technologies show promising steps towards capable web-based ECAs, through some off-the-shelf technologies, in particular, the Web Speech API, Web Audio API, WebGL and Web Workers. We describe their integration into a simple fully functional web-based 3D ECA accessible from any modern device, with special attention to our novel work in the creation and support of the embodiment aspects.',\n",
       " 'In traditional usability studies, researchers talk to users of tools to understand their needs and challenges. Insights gained via such interviews offer context, detail, and background. Due to costs in time and money, we are beginning to see a new form of tool interrogation that prioritizes scale, cost, and breadth by utilizing existing data from online forums. In this case study, we set out to apply this method of using online forum data to a specific issue---challenges that users face with Excel spreadsheets. Spreadsheets are a versatile and powerful processing tool if used properly. However, with versatility and power come errors, from both users and the software, which make using spreadsheets less effective. By scraping posts from the website Reddit, we collected a dataset of questions and complaints about Excel. Specifically, we explored and characterized the issues users were facing with spreadsheet software in general, and in particular, as resulting from a large amount of data in their spreadsheets. We discuss the implications of our findings on the design of next-generation spreadsheet software.',\n",
       " 'Knowledge workers, such as scientists, journalists, or consultants, adaptively seek, gather, and consume information. These processes are often inefficient as existing user interfaces provide limited possibilities to combine information from various sources and different formats into a common knowledge representation. In this paper, we present the concept of an information collage (IC) -- a web browser extension combining manual spatial organization of gathered information fragments and automatic text analysis for interactive content exploration and expressive visual summaries. We used IC for case studies with knowledge workers from different domains and longer-term field studies over a period of one month. We identified three different ways how users collect and structure information and provide design recommendations how to support these observed usage strategies.',\n",
       " 'This work compares user collaboration with conversational personal assistants vs. teams of expert chatbots. Two studies were performed to investigate whether each approach affects accomplishment of tasks and collaboration costs. Participants interacted with two equivalent financial advice chatbot systems, one composed of a single conversational adviser and the other based on a team of four experts chatbots. Results indicated that users had different forms of experiences but were equally able to achieve their goals. Contrary to the expected, there were evidences that in the teamwork situation that users were more able to predict agent behavior better and did not have an overhead to maintain common ground, indicating similar collaboration costs. The results point towards the feasibility of either of the two approaches for user collaboration with conversational agents.',\n",
       " 'The automated detection of corrosion from images (i.e., photographs) or video (i.e., drone footage) presents significant advantages in terms of corrosion monitoring. Such advantages include access to remote locations, mitigation of risk to inspectors, cost savings and monitoring speed. The automated detection of corrosion requires deep learning to approach human level artificial intelligence (A.I.). The training of a deep learning model requires intensive image labelling, and in order to generate a large database of labelled images, crowd sourced labelling via a dedicated website was sought. The website (corrosiondetector.com) permits any user to label images, with such labelling then contributing to the training of a cloud based A.I. model - with such a cloud-based model then capable of assessing any fresh (or uploaded) image for the presence of corrosion. In other words, the website includes both the crowd sourced training process, but also the end use of the evolving model. Herein, the results and findings from the website (corrosiondetector.com) over the period of approximately one month, are reported.',\n",
       " 'Even though speech-emotion recognition (SER) has been receiving much attention as research topic, there are still some disputes about which vocal features can identify certain emotion. Emotion expression is also known to be differed according to the cultural backgrounds that make it important to study SER specific to the culture where the language belongs to. Furthermore, only a few studies addresses the SER in Indonesian which what this study attempts to explore. In this study, we extract simple features from 3420 voice data gathered from 38 participants. The features are compared by means of linear mixed effect model which shows that people who are in emotional and non-emotional state can be differentiated by their speech duration. Using SVM and speech duration as input feature, we achieve 76.84% average accuracy in classifying emotional and non-emotional speech.',\n",
       " 'Low-quality results have been a long-standing problem on microtask crowdsourcing platforms, driving away requesters and justifying low wages for workers. To date, workers have been blamed for low-quality results: they are said to make as little effort as possible, do not pay attention to detail, and lack expertise. In this paper, we hypothesize that requesters may also be responsible for low-quality work: they launch unclear task designs that confuse even earnest workers, under-specify edge cases, and neglect to include examples. We introduce prototype tasks, a crowdsourcing strategy requiring all new task designs to launch a small number of sample tasks. Workers attempt these tasks and leave feedback, enabling the re- quester to iterate on the design before publishing it. We report a field experiment in which tasks that underwent prototype task iteration produced higher-quality work results than the original task designs. With this research, we suggest that a simple and rapid iteration cycle can improve crowd work, and we provide empirical evidence that requester \"quality\" directly impacts result quality.',\n",
       " \"Like sighted people, visually impaired people want to share photographs on social networking services, but find it difficult to identify and select photos from their albums. We aimed to address this problem by incorporating state-of-the-art computer-generated descriptions into Facebook's photo-sharing feature. We interviewed 12 visually impaired participants to understand their photo-sharing experiences and designed a photo description feature for the Facebook mobile application. We evaluated this feature with six participants in a seven-day diary study. We found that participants used the descriptions to recall and organize their photos, but they hesitated to upload photos without a sighted person's input. In addition to basic information about photo content, participants wanted to know more details about salient objects and people, and whether the photos reflected their personal aesthetics. We discuss these findings from the lens of self-disclosure and self-presentation theories and propose new computer vision research directions that will better support visual content sharing by visually impaired people.\",\n",
       " 'In building on theories of Computer-Mediated Communication (CMC), Human-Robot Interaction, and Media Psychology (i.e. Theory of Affective Bonding), the current paper proposes an explanation of how over time, people experience the mediated or simulated aspects of the interaction with a social robot. In two simultaneously running loops, a more reflective process is balanced with a more affective process. If human interference is detected behind the machine, Robot-Mediated Communication commences, which basically follows CMC assumptions; if human interference remains undetected, Human-Robot Communication comes into play, holding the robot for an autonomous social actor. The more emotionally aroused a robot user is, the more likely they develop an affective relationship with what actually is a machine. The main contribution of this paper is an integration of Computer-Mediated Communication, Human-Robot Communication, and Media Psychology, outlining a full-blown theory of robot communication connected to friendship formation, accounting for communicative features, modes of processing, as well as psychophysiology.',\n",
       " 'We consider a smart home or smart office environment with a number of IoT devices connected and passing data between one another. The footprints of the data transferred can provide valuable information about the devices, which can be used to (a) identify the IoT devices and (b) in case of failure, to identify the correct replacements for these devices. In this paper, we generate the embeddings for IoT devices in a smart home using Word2Vec, and explore the possibility of having a similar concept for IoT devices, aka IoT2Vec. These embeddings can be used in a number of ways, such as to find similar devices in an IoT device store, or as a signature of each type of IoT device. We show results of a feasibility study on the CASAS dataset of IoT device activity logs, using our method to identify the patterns in embeddings of various types of IoT devices in a household.',\n",
       " 'This paper describes the development of a real-time Human-Robot Interaction (HRI) system for a service robot based on 3D human activity recognition and human-like decision mechanism. The Human-Robot Interactive (HRI) system, which allows one person to interact with a service robot using natural body language, collects sequences of 3D skeleton joints comprising rich human movement information about the user via Microsoft Kinect. This information is used to train a three-layer Long-Short-Term Memory (LSTM) network for human action recognition. The robot understands user intent based on an online LSTM network test, and responds to the user via movements of the robotic arm or chassis. Furthermore, the human-like decision mechanism is also fused into this process, which allows the robot to instinctively decide whether to interrupt the current task according to task priority. The framework of the overall system is established on the Robot Operating System (ROS) platform. The real-life activity interaction between our service robot and the user was conducted to demonstrate the effectiveness of developed HRI system.',\n",
       " 'The identification of intentionally delivered commands is a challenge in Brain Computer Interfaces (BCIs) based on Sensory-Motor Rhythms (SMR). It is of fundamental importance that BCI systems controlling a robotic device (i.e., upper limb prosthesis) are capable of detecting if the user is in the so called Intentional Non-Control (INC) state (i.e., holding the prosthesis in a given position). In this work, we propose a novel approach based on the entropy of the Electroencephalogram (EEG) signals to provide a continuous identification of motion intention. Results from ten healthy subjects suggest that the proposed system can be used for reliably predicting motion in real-time at a framerate of 8 Hz with $80\\\\% \\\\pm 5\\\\%$ of accuracy. Moreover, motion intention can be detected more than 1 second before muscular activation with an average accuracy of $76\\\\% \\\\pm 11\\\\%$.',\n",
       " 'Sleep deprivation is a public health issue. A lack of sleep not only harms our body immune systems but also degrades their capacity to maintain cognitive skills. Awareness of sleep deprivation has not been widely investigated in work-based wellness programmes. In the study, the project carried out with nine participants from a local manufacture company to raise that awareness. The common causes of sleep deprivation have been identified the through the deployment of probes and the interviews. The research generated design concepts of smart IoT workplace to track and share daytime sleep-related activities. Through the bottom up and co-design methods, participants give the points of considering the use of sleep data from different power relationship perspective, includes the unexpected use of sleep for fatigue risk management and evaluation of employee performance.',\n",
       " 'Human behavior recognition has been considered as a core technology that can facilitate variety of applications. However, accurate detection and recognition of human behavior is still a big challenge that attracts a lot of research efforts. Recent advances in the wireless technology (e.g., Wi-Fi Channel State Information, i.e., CSI) enable a new behavior recognition paradigm, which is able to recognize behaviors in a device-free and non-intrusive manner. In this article, we first provide an overview of the basics of Wi-Fi CSI based behavior recognition. Afterwards, we classify related applications into three-granularity: signals, actions and activities, and then provide some insights for designing new schemes. Finally, we conclude by discussing the challenges, possible solutions to these challenges and some open issues involved in CSI based behavior recognition.',\n",
       " 'The Python--elsA user interface of the elsA cfd (Computational Fluid Dynamics) software has been developed to allow users to specify simulations with confidence, through a global context of description objects grouped inside scripts. The software main features are generated documentation, context checking and completion, and helpful error management. Further developments have used this foundation as a coupling framework, allowing (thanks to the descriptive approach) the coupling of external algorithms with the cfd solver in a simple and abstract way, leading to more success in complex simulations. Along with the description of the technical part of the interface, we try to gather the salient points pertaining to the psychological viewpoint of user experience (ux). We point out the differences between user interfaces and pure data management systems such as cgns.',\n",
       " 'Graph exploration and editing are still mostly considered independently and systems to work with are not designed for todays interactive surfaces like smartphones, tablets or tabletops. When developing a system for those modern devices that supports both graph exploration and graph editing, it is necessary to 1) identify what basic tasks need to be supported, 2) what interactions can be used, and 3) how to map these tasks and interactions. This technical report provides a list of basic interaction tasks for graph exploration and editing as a result of an extensive system review. Moreover, different interaction modalities of interactive surfaces are reviewed according to their interaction vocabulary and further degrees of freedom that can be used to make interactions distinguishable are discussed. Beyond the scope of graph exploration and editing, we provide an approach for finding and evaluating a mapping from tasks to interactions, that is generally applicable. Thus, this work acts as a guideline for developing a system for graph exploration and editing that is specifically designed for interactive surfaces.',\n",
       " \"Design for Voice User Interfaces (VUIs) has become more relevant in recent years due to the enormous advances of speech technologies and their growing presence in our everyday lives. Although modern VUIs still present interaction issues, reports indicate they are being adopted by people with different disabilities and having a positive impact. For the first author's PhD research project, an ethnographic study is currently being carried out in a local charity that provides support and services to people with visual impairments. The purpose is to understand people's competencies and practices, and how these are, or could be, related to voice technologies (assistive technology and mainstream VUIs). Through direct observation and contextual interviews, we aim to investigate the problems and solutions they encounter and the ways they cope with particular situations.\",\n",
       " \"The exponential growth of popularity of multimedia has led to needs for user-centric adaptive applications that manage multimedia content more effectively. Implicit analysis, which examines users' perceptual experience of multimedia by monitoring physiological or behavioral cues, has potential to satisfy such demands. Particularly, physiological signals categorized into cerebral physiological signals (electroencephalography, functional magnetic resonance imaging, and functional near-infrared spectroscopy) and peripheral physiological signals (heart rate, respiration, skin temperature, etc.) have recently received attention along with notable development of wearable physiological sensors. In this paper, we review existing studies on physiological signal analysis exploring perceptual experience of multimedia. Furthermore, we discuss current trends and challenges.\",\n",
       " 'Human-machine systems required a deep understanding of human behaviors. Most existing research on action recognition has focused on discriminating between different actions, however, the quality of executing an action has received little attention thus far. In this paper, we study the quality assessment of driving behaviors and present WiQ, a system to assess the quality of actions based on radio signals. This system includes three key components, a deep neural network based learning engine to extract the quality information from the changes of signal strength, a gradient based method to detect the signal boundary for an individual action, and an activitybased fusion policy to improve the recognition performance in a noisy environment. By using the quality information, WiQ can differentiate a triple body status with an accuracy of 97%, while for identification among 15 drivers, the average accuracy is 88%. Our results show that, via dedicated analysis of radio signals, a fine-grained action characterization can be achieved, which can facilitate a large variety of applications, such as smart driving assistants.',\n",
       " \"Bias is a common problem in today's media, appearing frequently in text and in visual imagery. Users on social media websites such as Twitter need better methods for identifying bias. Additionally, activists --those who are motivated to effect change related to some topic, need better methods to identify and counteract bias that is contrary to their mission. With both of these use cases in mind, in this paper we propose a novel tool called UnbiasedCrowd that supports identification of, and action on bias in visual news media. In particular, it addresses the following key challenges (1) identification of bias; (2) aggregation and presentation of evidence to users; (3) enabling activists to inform the public of bias and take action by engaging people in conversation with bots. We describe a preliminary study on the Twitter platform that explores the impressions that activists had of our tool, and how people reacted and engaged with online bots that exposed visual bias. We conclude by discussing design and implication of our findings for creating future systems to identify and counteract the effects of news bias.\",\n",
       " 'In mixed reality, real objects can be used to interact with virtual objects. However, unlike in the real world, real objects do not encounter any opposite reaction force when pushing against virtual objects. The lack of reaction force during manipulation prevents users from perceiving the mass of virtual objects. Although this could be addressed by equipping real objects with force-feedback devices, such a solution remains complex and impractical.In this work, we present a technique to produce an illusion of mass without any active force-feedback mechanism. This is achieved by simulating the effects of this reaction force in a purely visual way. A first study demonstrates that our technique indeed allows users to differentiate light virtual objects from heavy virtual objects. In addition, it shows that the illusion is immediately effective, with no prior training. In a second study, we measure the lowest mass difference (JND) that can be perceived with this technique. The effectiveness and ease of implementation of our solution provides an opportunity to enhance mixed reality interaction at no additional cost.',\n",
       " \"The paper describes a novel social network-based open educational resource for learning foreign languages in real time from native speakers, based on the predefined teaching materials. This virtual learning platform, named i2istudy, eliminates misunderstanding by providing prepared and predefined scenarios, enabling the participants to understand each other and, as a consequence, to communicate freely. The system allows communication through the real time video and audio feed. In addition to establishing the communication, it tracks the student progress and allows rating the instructor, based on the learner's experience. The system went live in April 2014, and had over six thousand active daily users, with over 40,000 total registered users. Currently monetization is being added to the system, and time will show how popular the system will become in the future.\",\n",
       " \"This study investigates the use of accelerometer data from a smart watch to infer an individual's emotional state. We present our preliminary findings on a user study with 50 participants. Participants were primed either with audio-visual (movie clips) or audio (classical music) to elicit emotional responses. Participants then walked while wearing a smart watch on one wrist and a heart rate strap on their chest. Our hypothesis is that the accelerometer signal will exhibit different patterns for participants in response to different emotion priming. We divided the accelerometer data using sliding windows, extracted features from each window, and used the features to train supervised machine learning algorithms to infer an individual's emotion from their walking pattern. Our discussion includes a description of the methodology, data collected, and early results.\",\n",
       " 'The paper describes creation and development of the educational online communication platform for teaching and learning foreign languages. The system is based on the time bank principle, allowing users to teach others their native tongue along with taking foreign language lessons. The system is based on the WebRTC technology, allowing users to access synchronized teaching materials along with seeing and hearing each other. The platform is free for the users with implemented gamification mechanics to motivate them. It is based on the freemium model, where the main functions are provided free of charged with some premium features. The paper describes studies associated with user involvement in the learning/teaching process. The hypothesis whether two previously unfamiliar individuals could communicate with each other using a foreign language, based on the developed system algorithms, was tested. System virality, where new users are attracted by the existing users was also studied, along with user motivation for viral behavior. Relationships between monetization, virality and user involvement were also considered.',\n",
       " 'Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.',\n",
       " 'Driver assistance systems, also called automated driving systems, allow drivers to immerse themselves in non-driving-related tasks. Unfortunately, drivers may not trust the automated driving system, which prevents either handing over the driving task or fully focusing on the secondary task. We assert that enhancing situational awareness can increase trust in automation. Situational awareness should increase trust and lead to better secondary task performance. This study manipulated situational awareness by providing them with different types of information: the control condition provided no information to the driver, the low condition provided a status update, while the high condition provided a status update and a suggested course of action. Data collected included measures of trust, trusting behavior, and task performance through surveys, eye-tracking, and heart rate data. Results show that situational awareness both promoted and moderated the impact of trust in the automated vehicle, leading to better secondary task performance. This result was evident in measures of self-reported trust and trusting behavior.',\n",
       " 'The domestic environment is a key area for the design and deployment of autonomous systems. Yet research indicates their adoption is already being hampered by a variety of critical issues including trust, privacy and security. This paper explores how potential users relate to the concept of autonomous systems in the home and elaborates further points of friction. It makes two contributions. One methodological, focusing on the use of provocative utopian and dystopian scenarios of future autonomous systems in the home. These are used to drive an innovative workshop-based approach to breaching experiments, which surfaces the usually tacit and unspoken background expectancies implicated in the organisation of everyday life that have a powerful impact on the acceptability of future and emerging technologies. The other contribution is substantive, produced through participants efforts to repair the incongruity or \"reality disjuncture\" created by utopian and dystopian visions, and highlights the need to build social as well as computational accountability into autonomous systems, and to enable coordination and control.',\n",
       " 'Providing reinforcement learning agents with informationally rich human knowledge can dramatically improve various aspects of learning. Prior work has developed different kinds of shaping methods that enable agents to learn efficiently in complex environments. All these methods, however, tailor human guidance to agents in specialized shaping procedures, thus embodying various characteristics and advantages in different domains. In this paper, we investigate the interplay between different shaping methods for more robust learning performance. We propose an adaptive shaping algorithm which is capable of learning the most suitable shaping method in an on-line manner. Results in two classic domains verify its effectiveness from both simulated and real human studies, shedding some light on the role and impact of human factors in human-robot collaborative learning.',\n",
       " 'Ubiquitous technology platforms have been created to track and improve health and fitness; similar technologies can help individuals monitor and reduce their carbon footprints. This paper proposes CarbonKit, a platform combining technology, markets, and incentives to empower and reward people for reducing their carbon footprint. We argue that a goal-and-reward behavioral feedback loop can be combined with the Big Data available from tracked activities, apps, and social media to make CarbonKit an integral part of individuals daily lives. CarbonKit comprises five modules that link personal carbon tracking, health and fitness, social media, and economic incentives. Protocols for safeguarding security, privacy and individuals control over their own data are essential to the design of the CarbonKit. Initially CarbonKit would operate on a voluntary basis, but such a system can also serve as part of a mandatory region-wide initiative. We use the example of the British Columbia to illustrate the regulatory framework and participating stakeholders that would be required to support the CarbonKit in specific jurisdictions.',\n",
       " 'Smart phone and tablet usage has sharply increased for the last decade. While entering test on these devices, virtual keyboards are generally used instead of conventional hardware keyboards. In this study, a new problem which is two-finger keyboard layout problem and solution approach is presented for increasing user test entrance performance, especially on virtual keyboards. Defined two-finger keyboard layout problem is modeled as Quadratic Assignment Problem. Because of combinatorial structure of the problem a genetic algorithm is developed. Its result is given to mathematical model as initial solution for finding better solutions with mathematical model. Proposed approach is applied on Turkish language. The new two finger keyboard layout for Turkish language is compared with F and QWERTY keyboard layouts based on certain performance measurement techniques.',\n",
       " 'We describe a new visualization tool, dubbed HCMapper, that visually helps to compare a pair of dendrograms computed on the same dataset by displaying multiscale partition-based layered structures. The dendrograms are obtained by hierarchical clustering techniques whose output reflects some hypothesis on the data and HCMapper is specifically designed to grasp at first glance both whether the two compared hypotheses broadly agree and the data points on which they do not concur. Leveraging juxtaposition and explicit encodings, HCMapper focus on two selected partitions while displaying coarser ones in context areas for understanding multiscale structure and eventually switching the selected partitions. HCMapper utility is shown through the example of testing whether the prices of credit default swap financial time series only undergo correlation. This use case is detailed in the supplementary material as well as experiments with code on toy-datasets for reproducible research. HCMapper is currently released as a visualization tool on the DataGrapple time series and clustering analysis platorm at www.datagrapple.com.',\n",
       " 'Accurate hand pose estimation at joint level has several uses on human-robot interaction, user interfacing and virtual reality applications. Yet, it currently is not a solved problem. The novel deep learning techniques could make a great improvement on this matter but they need a huge amount of annotated data. The hand pose datasets released so far present some issues that make them impossible to use on deep learning methods such as the few number of samples, high-level abstraction annotations or samples consisting in depth maps. In this work, we introduce a multiview hand pose dataset in which we provide color images of hands and different kind of annotations for each, i.e the bounding box and the 2D and 3D location on the joints in the hand. Besides, we introduce a simple yet accurate deep learning architecture for real-time robust 2D hand pose estimation.',\n",
       " 'In presence of multiple clustering solutions for the same dataset, a clustering ensemble approach aims to yield a single clustering of the dataset by achieving a consensus among the input clustering solutions. The goal of this consensus is to improve the quality of clustering. It has been seen that there are some image clustering tasks that cannot be easily solved by computer. But if these images can be outsourced to the general people (crowd workers) to group them based on some similar features, and opinions are collected from them, then this task can be managed in an efficient manner and time effective way. In this work, the power of crowd has been used to annotate the images so that multiple clustering solutions can be obtained from them and thereafter a Markov chain based ensemble method is introduced to make a consensus of multiple clustering solutions.',\n",
       " 'Taking a picture has been traditionally a one-persons task. In this paper we present a novel system that allows multiple mobile devices to work collaboratively in a synchronized fashion to capture a panorama of a highly dynamic scene, creating an entirely new photography experience that encourages social interactions and teamwork. Our system contains two components: a client app that runs on all participating devices, and a server program that monitors and communicates with each device. In a capturing session, the server collects in realtime the viewfinder images of all devices and stitches them on-the-fly to create a panorama preview, which is then streamed to all devices as visual guidance. The system also allows one camera to be the host and to send direct visual instructions to others to guide camera adjustment. When ready, all devices take pictures at the same time for panorama stitching. Our preliminary study suggests that the proposed system can help users capture high quality panoramas with an enjoyable teamwork experience.\\n  A demo video of the system in action is provided at http://youtu.be/PwQ6k_ZEQSs.',\n",
       " \"Student learning activity in MOOCs can be viewed from multiple perspectives. We present a new organization of MOOC learner activity data at a resolution that is in between the fine granularity of the clickstream and coarse organizations that count activities, aggregate students or use long duration time units. A detailed access trajectory (DAT) consists of binary values and is two dimensional with one axis that is a time series, e.g. days and the other that is a chronologically ordered list of a MOOC component type's instances, e.g. videos in instructional order. Most popular MOOC platforms generate data that can be organized as detailed access trajectories (DATs).We explore the value of DATs by conducting four empirical mini-studies. Our studies suggest DATs contain rich information about students' learning behaviors and facilitate MOOC learning analyses.\",\n",
       " \"In this demo paper, we introduce LogCanvas, a platform for user search history visualisation. Different from the existing visualisation tools, LogCanvas focuses on helping users re-construct the semantic relationship among their search activities. LogCanvas segments a user's search history into different sessions and generates a knowledge graph to represent the information exploration process in each session. A knowledge graph is composed of the most important concepts or entities discovered by each search query as well as their relationships. It thus captures the semantic relationship among the queries. LogCanvas offers a session timeline viewer and a snippets viewer to enable users to re-find their previous search results efficiently. LogCanvas also provides a collaborative perspective to support a group of users in sharing search results and experience.\",\n",
       " 'The monthly Bureau of Labor Statistics Employment Situation Report is widely anticipated by economists, journalists, and politicians as it is used to forecast the economic condition of the United States. The report has broad impact on public and corporate economic confidence; however, the online access to this data employs outdated techniques, using a PDF format containing solely text and fixed tabular information. Creating an interactive interface for dynamic discovery on the BLS website could elicit more dialogue between the public and government spheres, drawing more traffic to government websites and triggering greater civic engagement. Our work suggests that the implementation of interactive visual analysis techniques to enable dynamic discovery leads to rapid interpretation of data as well as provides the means to explore the data for further insights. This paper presents two inspirational prototypes: a dashboard of interactive visualizations and an interactive time series explorer, allowing for temporal and spatial analyses and enabling users to combine data sets to create their own customized visualization.',\n",
       " 'Studies have shown that children can be exposed to smart devices at a very early age. This has important implications on research in children-computer interaction, children online safety and early education. Many systems have been built based on such research. In this work, we present multiple techniques to automatically detect the presence of a child on a smart device, which could be used as the first step on such systems. Our methods distinguish children from adults based on behavioral differences while operating a touch-enabled modern computing device. Behavioral differences are extracted from data recorded by the touchscreen and built-in sensors. To evaluate the effectiveness of the proposed methods, a new data set has been created from 50 children and adults who interacted with off-the-shelf applications on smart phones. Results show that it is possible to achieve 99% accuracy and less than 0.5% error rate after 8 consecutive touch gestures using only touch information or 5 seconds of sensor reading. If information is used from multiple sensors, then only after 3 gestures, similar performance could be achieved.',\n",
       " 'Buses are the primary means of public transportation in the city of Rio de Janeiro, carrying around 100 million passengers every month. Recently, real-time GPS coordinates of all operating public buses has been made publicly available - roughly 1 million GPS entries each captured each day. In an initial study, we observed that a substantial number of buses follow trajectories that do not follow the expected behavior. In this paper, we present RioBusData, a tool that helps users identify and explore, through different visualizations, the behavior of outlier trajectories. We describe how the system automatically detects these outliers using a Convolutional Neural Network (CNN) and we also discuss a series of case studies which show how RioBusData helps users better understand not only the flow and service of outlier buses but also the bus system as a whole.',\n",
       " 'In this position paper, we frame the field of Visual Musicology by providing an overview of well-established musicological sub-domains and their corresponding analytic and visualization tasks. To foster collaborative, interdisciplinary research, we discuss relevant data and domain characteristics. We give a description of the problem space, as well as the design space of musicology and discuss how existing problem-design mappings or solutions from other fields can be transferred to musicology. We argue that, through methodology transfer, established methods can be exploited to solve current musicological problems and show exemplary mappings from analytics fields related to text, geospatial, time-series, and other high-dimensional data to musicology. Finally, we point out open challenges, discuss research gaps, and highlight future research opportunities.',\n",
       " \"This paper draws from literature and our experience of conducting Wizard-of-Oz (WoZ) studies using natural language, conversational user interfaces (CUIs) in the automotive domain. These studies have revealed positive effects of using in-vehicle CUIs on issues such as: cognitive demand/workload, passive task-related fatigue, trust, acceptance and environment engagement. A nascent set of human-centred design guidelines that have emerged is presented. These are based on the analysis of users' behaviour and the positive benefits observed, and aim to make interactions with an in-vehicle agent interlocutor safe, effective, engaging and enjoyable, while confirming with users' expectations. The guidelines can be used to inform the design of future in-vehicle CUIs or applied experimentally using WoZ methodology, and will be evaluated and refined in ongoing work.\",\n",
       " \"We report from the Do Not Disturb Challenge where 30 volunteers disabled notification alerts for 24 hours across all devices. The effect of the absence of notifications on the participants was isolated through an experimental study design: we compared self-reported feedback from the day without notifications against a baseline day. The evidence indicates that notifications have locked us in a dilemma: without notifications, participants felt less distracted and more productive. But, they also felt no longer able to be as responsive as expected, which made some participants anxious. And, they felt less connected with one's social group. In contrast to previous reports, about two third of the participants expressed the intention to change how they manage notifications. Two years later, half of the participants are still following through with their plans.\",\n",
       " 'This paper positions and explores the topic of image-based personality test. Instead of responding to text-based questions, the subjects will be provided a set of \"choose-your-favorite-image\" visual questions. With the image options of each question belonging to the same concept, the subjects\\' personality traits are estimated by observing their preferences of images under several unique concepts. The solution to design such an image-based personality test consists of concept-question identification and image-option selection. We have presented a preliminary framework to regularize these two steps in this exploratory study. A demo version of the designed image-based personality test is available at http://www.visualbfi.org/. Subjective as well as objective evaluations have demonstrated the feasibility of image-based personality test in limited questions.',\n",
       " 'Cursor tracking data contains information about website visitors which may provide new ways to understand visitors and their needs. This paper presents an Amazon Mechanical Turk study where participants were tracked as they used modified variants of the Wikipedia and BBC News websites. Participants were asked to complete reading and information-finding tasks. The results showed that it was possible to differentiate between users reading content and users looking for information based on cursor data. The effects of website aesthetics, user interest and cursor hardware were also analysed which showed it was possible to identify hardware from cursor data, but no relationship between cursor data and engagement was found. The implications of these results, from the impact on web analytics to the design of experiments to assess user engagement, are discussed.',\n",
       " \"Modern security operations centers (SOCs) employ a variety of tools for intrusion detection, prevention, and widespread log aggregation and analysis. While research efforts are quickly proposing novel algorithms and technologies for cyber security, access to actual security personnel, their data, and their problems are necessarily limited by security concerns and time constraints. To help bridge the gap between researchers and security centers, this paper reports results of semi-structured interviews of 13 professionals from five different SOCs including at least one large academic, research, and government organization. The interviews focused on the current practices and future desires of SOC operators about host-based data collection capabilities, what is learned from the data, what tools are used, and how tools are evaluated. Questions and the responses are organized and reported by topic. Then broader themes are discussed. Forest-level takeaways from the interviews center on problems stemming from size of data, correlation of heterogeneous but related data sources, signal-to-noise ratio of data, and analysts' time.\",\n",
       " 'In research and practice into the accessibility of digital games, much of the work has focused on how to make games accessible to people with disa- bilities. With an increasing number of people with disabilities playing main- stream commercial games, it is important that we understand who they are and how they play in order to take a more user-centered approach as this field grows. We conducted a demographic survey of 230 players with disabilities and found that they play mainstream digital games using a variety of assistive tech- nologies, use accessibility options such as key remapping and subtitles, and they identify themselves as gamers who play digital games as their primary hobby. This gives us a richer picture of players with disabilities and indicates that there are opportunities to begin to look at accessible player experiences (APX) in games.',\n",
       " 'We investigate grasping of rigid objects in unilateral robot-assisted minimally invasive surgery (RAMIS) in this paper. We define a human-centered transparency that quantifies natural action and perception in RAMIS. We demonstrate this human-centered transparency analysis for different values of gripper scaling - the scaling between the grasp aperture of the surgeon-side manipulator and the aperture of the surgical instrument grasper. Thirty-one participants performed teleoperated grasping and perceptual assessment of rigid objects in one of three gripper scaling conditions (fine, normal, and quick, trading off precision and responsiveness). Psychophysical analysis of the variability of maximal grasping aperture during prehension and of the reported size of the object revealed that in normal and quick (but not in the fine) gripper scaling conditions, teleoperated grasping with our system was similar to natural grasping, and therefore, human-centered transparent. We anticipate that using motor control and psychophysics for human-centered optimizing of teleoperation control will eventually improve the usability of RAMIS.',\n",
       " \"One of the main benefits of a wrist-worn computer is its ability to collect a variety of physiological data in a minimally intrusive manner. Among these data, electrodermal activity (EDA) is readily collected and provides a window into a person's emotional and sympathetic responses. EDA data collected using a wearable wristband are easily influenced by motion artifacts (MAs) that may significantly distort the data and degrade the quality of analyses performed on the data if not identified and removed. Prior work has demonstrated that MAs can be successfully detected using supervised machine learning algorithms on a small data set collected in a lab setting. In this paper, we demonstrate that unsupervised learning algorithms perform competitively with supervised algorithms for detecting MAs on EDA data collected in both a lab-based setting and a real-world setting comprising about 23 hours of data. We also find, somewhat surprisingly, that incorporating accelerometer data as well as EDA improves detection accuracy only slightly for supervised algorithms and significantly degrades the accuracy of unsupervised algorithms.\",\n",
       " 'We consider worker skill estimation for the single-coin Dawid-Skene crowdsourcing model. In practice, skill-estimation is challenging because worker assignments are sparse and irregular due to the arbitrary and uncontrolled availability of workers. We formulate skill estimation as a rank-one correlation-matrix completion problem, where the observed components correspond to observed label correlations between workers. We show that the correlation matrix can be successfully recovered and skills are identifiable if and only if the sampling matrix (observed components) does not have a bipartite connected component. We then propose a projected gradient descent scheme and show that skill estimates converge to the desired global optima for such sampling matrices. Our proof is original and the results are surprising in light of the fact that even the weighted rank-one matrix factorization problem is NP-hard in general. Next, we derive sample complexity bounds in terms of spectral properties of the signless Laplacian of the sampling matrix. Our proposed scheme achieves state-of-art performance on a number of real-world datasets.',\n",
       " \"The SmartAbility Android Application recommends Assistive Technology (AT) for people with reduced physical ability, by focusing on the actions (abilities) that can be performed independently. The Application utilises built-in sensor technologies in Android devices to detect user abilities, including head and limb movements, speech and blowing. The Application was evaluated by 18 participants with varying physical conditions and assessed through the System Usability Scale (SUS) and NASA Task Load Index (TLX). The Application achieved a SUS score of 72.5 (indicating 'Good Usability') with low levels of Temporal Demand and Frustration and medium levels of Mental Demand, Physical Demand and Effort. It is anticipated that the SmartAbility Application will be disseminated to the AT domain, to improve quality of life for people with reduced physical ability.\",\n",
       " 'Extensible 3D (X3D) modeling language is one of the leading Web3D technologies. Despite the rich functionality, the language does not currently provide tools for rapid development of conventional graphical user interfaces (GUIs). Every X3D author is responsible for building from primitives a purpose specific set of required interface components, often for a single use. We address the challenge of creating consistent, efficient, interactive, and visually appealing GUIs by proposing the X3D User Interface (X3DUI) library. This library includes a wide range of cross-compatible X3D widgets, equipped with configurable appearance and behavior. With this library, we attempt to standardize the GUI construction across various X3D-driven projects, and improve the usability, compatibility, adaptability, readability, and flexibility of many existing applications.',\n",
       " \"The adoption of intelligent systems creates opportunities as well as challenges for medical work. On the positive side, intelligent systems have the potential to compute complex data from patients and generate automated diagnosis recommendations for doctors. However, medical professionals often perceive such systems as black boxes and, therefore, feel concerned about relying on system generated results to make decisions. In this paper, we contribute to the ongoing discussion of explainable artificial intelligence (XAI) by exploring the concept of explanation from a human-centered perspective. We hypothesize that medical professionals would perceive a system as explainable if the system was designed to think and act like doctors. We report a preliminary interview study that collected six medical professionals' reflection of how they interact with data for diagnosis and treatment purposes. Our data reveals when and how doctors prioritize among various types of data as a central part of their diagnosis process. Based on these findings, we outline future directions regarding the design of XAI systems in the medical context.\",\n",
       " \"With a number of cheap commercial dry EEG kits available today, it is possible to look at user attention driven scenarios for interaction with the web browser. Using EEG to determine the user's attention level is preferable to using methods such as gaze tracking or time spent on the webpage. In this paper we use the attention level in three different ways. First, as a control mechanism, to control user interface elements such as menus or buttons. Second, to make the web browser responsive to the current attention level. Third, as a means for the web developer to control the user experience based on the level of attention paid by the user, thus creating attention sensitive websites. We present implementation details for each of these, using the NeuroSky MindWave sensor. We also explore issues in the system, and possibility of an EEG based web standard.\",\n",
       " \"This paper brings into discussion some of the most relevant technological challenges involving haptic systems in medical education. One of these challenges is choosing the suitable haptic hardware, API or framework for developing a visuo-haptic e-Learning system. The decision is based on several criteria such as the multimodal resources needed by the software system, compatibility with haptic devices and the dynamic configuration of the scene. Another challenge is related to the software system reactivity in conjunction with the user's actions. The immediate haptic feedback from the virtual models, together with the synchronization of the rendered haptic and visual cues seen by the users are essential for enhancing the user's learning ability. Visuo-haptic simulation facilitates accurate training scenarios of medical protocols and surgical processes.\",\n",
       " \"The sizes of compressed images depend on their spatial resolution (number of pixels) and on their color resolution (number of color quantization levels). We introduce DaltonQuant, a new color quantization technique for image compression that cloud services can apply to images destined for a specific user with known color vision deficiencies. DaltonQuant improves compression in a user-specific but reversible manner thereby improving a user's network bandwidth and data storage efficiency. DaltonQuant quantizes image data to account for user-specific color perception anomalies, using a new method for incremental color quantization based on a large corpus of color vision acuity data obtained from a popular mobile game. Servers that host images can revert DaltonQuant's image requantization and compression when those images must be transmitted to a different user, making the technique practical to deploy on a large scale. We evaluate DaltonQuant's compression performance on the Kodak PC reference image set and show that it improves compression by an additional 22%-29% over the state-of-the-art compressors TinyPNG and pngquant.\",\n",
       " 'StreamBED is an embodied VR training for citizen scientists to make qualitative stream assessments. Early findings garnered positive feedback about training qualitative assessment using a virtual representation of different stream spaces, but presented field-specific challenges; novice biologists had trouble interpreting qualitative protocols, and needed substantive guidance to look for and interpret environment cues. In order to address these issues in the redesign, this work uses research through design (RTD) methods to consider feedback from expert stream biologists, firsthand stream monitoring experience, discussions with education and game designers, and feedback from a low fidelity prototype. The qualitative findings found that training should facilitate personal narratives, maximize realism, and should use social dynamics to scaffold learning.',\n",
       " 'Technology has become an essential part in every aspect of our lives. However the key to a successful implementation of a technology depends on the acceptance by the general public. In order to increase the acceptance various approaches can be applied. In this paper, we will examine the human-robot emotional interaction by investigating the capabilities of a developed low-resolution RGB-LED display in the context of artificial emotions. We are focusing on four of the most representative human emotions which include happiness, anger, sadness and fear. We will work with colors and dynamic light patterns which are supposed to evoke various associations. In an experiment, the use these patterns as expressions of emotions are validated. The results of the conducted study show that some of the considered basic emotions can be recognized by human observers.',\n",
       " 'These days mobile devices like phones or tablets are very common among people of all age. They are connected with network and provide seamless communications through internet or cellular services. These devices can be a big help for the people who are not able to communicate properly and even in emergency conditions. A disabled person who is not able to speak or a person who speak a different language, these devices can be a boon for them as understanding, translating and speaking systems for these people. This chapter discusses a portable android based hand sign recognition system which can be used by disabled people. This chapter shows a part of on-going project. Computer Vision based techniques were used for image analysis and PCA was used after image tokenizer for recognition. This method was tested with webcam results to make system more robust.',\n",
       " 'We examine the utility of implicit user behavioral signals captured using low-cost, off-the-shelf devices for anonymous gender and emotion recognition. A user study designed to examine male and female sensitivity to facial emotions confirms that females recognize (especially negative) emotions quicker and more accurately than men, mirroring prior findings. Implicit viewer responses in the form of EEG brain signals and eye movements are then examined for existence of (a) emotion and gender-specific patterns from event-related potentials (ERPs) and fixation distributions and (b) emotion and gender discriminability. Experiments reveal that (i) Gender and emotion-specific differences are observable from ERPs, (ii) multiple similarities exist between explicit responses gathered from users and their implicit behavioral signals, and (iii) Significantly above-chance ($\\\\approx$70%) gender recognition is achievable on comparing emotion-specific EEG responses-- gender differences are encoded best for anger and disgust. Also, fairly modest valence (positive vs negative emotion) recognition is achieved with EEG and eye-based features.',\n",
       " 'Today\\'s conversational agents are restricted to simple standalone commands. In this paper, we present Iris, an agent that draws on human conversational strategies to combine commands, allowing it to perform more complex tasks that it has not been explicitly designed to support: for example, composing one command to \"plot a histogram\" with another to first \"log-transform the data\". To enable this complexity, we introduce a domain specific language that transforms commands into automata that Iris can compose, sequence, and execute dynamically by interacting with a user through natural language, as well as a conversational type system that manages what kinds of commands can be combined. We have designed Iris to help users with data science tasks, a domain that requires support for command combination. In evaluation, we find that data scientists complete a predictive modeling task significantly faster (2.6 times speedup) with Iris than a modern non-conversational programming environment. Iris supports the same kinds of commands as today\\'s agents, but empowers users to weave together these commands to accomplish complex goals.',\n",
       " 'Recent years have seen a sharp increase in the use of open source projects by common novice users; Open Source Software (OSS) is thus no longer a reserved arena for software developers and computer gurus. Although user-centered designs are gaining popularity in OSS, usability is still not considered as one of the prime objectives in many design scenarios. In this paper, we analyze industry users perception of usability factors, including understandability, learnability, operability and attractiveness, on OSS usability. The research model of this empirical study establishes the relationship between the key usability factors and OSS usability from industrial perspective. In order to conduct the study, a data set of 105 industry users is included. The results of the empirical investigation indicate the significance of the key factors for OSS usability.',\n",
       " \"Information architecture forms the foundation of users' navigation experience. Open card sorting is a widely-used method to create information architectures based on users' groupings of the content. However, little is known about the method's cross-study reliability: Does it produce consistent content groupings for similar profile participants involved in different card sort studies? This paper presents an empirical evaluation of the method's cross-study reliability. Six card sorts involving 140 participants were conducted: three open sorts for a travel website, and three for an eshop. Results showed that participants provided highly similar card sorting data for the same content. A rather high agreement of the produced navigation schemes was also found. These findings provide support for the cross-study reliability of the open card sorting method.\",\n",
       " 'Older users population is rapidly increasing all over the World. Presently, we observe efforts in the human-computer interaction domain aiming to improve life quality of age 65 and over through the use of mobile apps. Nonetheless, these efforts focus primary on interface and interaction de- sign. Little work has focused on the study of motivation to use and adherence to, of elderly to technology. Developing specific design guidelines for this population is relevant, however it should be parallel to the study of desire of elderly to embrace specific technology in their life. Designers should not be limited to technology design but consider as well how to fully convey the value that technology can bring to the lives of the users and motivate adoption. This position paper discusses techniques that might nudge elderly towards the use of new technology.',\n",
       " 'In this paper we present the evaluation process for Barbarossa, a pervasive role playing game. Barbarossa involves an invitational (preparatory) and a main execution phase. The former is freely available though Google Play store and may be played anytime/ anywhere. The latter defines three inter-dependent player roles acted by players who need to collaborate in a treasure hunting game. The eligibility of players for participating in the main game phase is restricted among those ranked relatively high in the invitational phase. Herein, we investigate the impact of the invitational game mode on the players overall game experience. The main hypothesis tested is that game awareness (gained from participating in a preliminary game phase) may serve as a means for recruiting the most suitable subjects for user trials on pervasive game research prototypes.',\n",
       " 'Crowdsourcing platforms enable companies to propose tasks to a large crowd of users. The workers receive a compensation for their work according to the serious of the tasks they managed to accomplish. The evaluation of the quality of responses obtained from the crowd remains one of the most important problems in this context. Several methods have been proposed to estimate the expertise level of crowd workers. We propose an innovative measure of expertise assuming that we possess a dataset with an objective comparison of the items concerned. Our method is based on the definition of four factors with the theory of belief functions. We compare our method to the Fagin distance on a dataset from a real experiment, where users have to assess the quality of some audio recordings. Then, we propose to fuse both the Fagin distance and our expertise measure.',\n",
       " 'We compiled a demo application and collected a motion database of more than 10,000 smartphone users to produce a health risk model trained on physical activity streams. We turned to adversarial domain adaptation and employed the UK Biobank dataset of motion data, augmented by a rich set of clinical information as the source domain to train the model using a deep residual convolutional neuron network (ResNet). The model risk score is a biomarker of ageing, since it was predictive of lifespan and healthspan (as defined by the onset of specified diseases), and was elevated in groups associated with life-shortening lifestyles, such as smoking. We ascertained the target domain performance in a smaller cohort of the mobile application that included users who were willing to share answers to a short questionnaire related to their disease and smoking status. We thus conclude that the proposed pipeline combining deep convolutional and Domain Adversarial neuron networks (DANN) is a powerful tool for disease risk and lifestyle-associated hazard assessment from mobile motion sensors that are transferable across devices and populations.',\n",
       " 'Social Virtual Reality based Learning Environments (VRLEs) such as vSocial render instructional content in a three-dimensional immersive computer experience for training youth with learning impediments. There are limited prior works that explored attack vulnerability in VR technology, and hence there is a need for systematic frameworks to quantify risks corresponding to security, privacy, and safety (SPS) threats. The SPS threats can adversely impact the educational user experience and hinder delivery of VRLE content. In this paper, we propose a novel risk assessment framework that utilizes attack trees to calculate a risk score for varied VRLE threats with rate and duration of threats as inputs. We compare the impact of a well-constructed attack tree with an adhoc attack tree to study the trade-offs between overheads in managing attack trees, and the cost of risk mitigation when vulnerabilities are identified. We use a vSocial VRLE testbed in a case study to showcase the effectiveness of our framework and demonstrate how a suitable attack tree formalism can result in a more safer, privacy-preserving and secure VRLE system.',\n",
       " \"Exploration has been one of the greatest challenges in reinforcement learning (RL), which is a large obstacle in the application of RL to robotics. Even with state-of-the-art RL algorithms, building a well-learned agent often requires too many trials, mainly due to the difficulty of matching its actions with rewards in the distant future. A remedy for this is to train an agent with real-time feedback from a human observer who immediately gives rewards for some actions. This study tackles a series of challenges for introducing such a human-in-the-loop RL scheme. The first contribution of this work is our experiments with a precisely modeled human observer: binary, delay, stochasticity, unsustainability, and natural reaction. We also propose an RL method called DQN-TAMER, which efficiently uses both human feedback and distant rewards. We find that DQN-TAMER agents outperform their baselines in Maze and Taxi simulated environments. Furthermore, we demonstrate a real-world human-in-the-loop RL application where a camera automatically recognizes a user's facial expressions as feedback to the agent while the agent explores a maze.\",\n",
       " 'Powered wheelchair users encounter barriers to their mobility everyday. Entering a building with non barrier-free areas can massively impact the user mobility related activities. There are a few commercial devices and some experimental that can climb stairs using for instance adaptive wheels with joints or caterpillar drive. These systems rely on the use for sensing and control. For safe automated obstacle crossing, a robust and environment invariant detection of the surrounding is necessary. Radar may prove to be a suitable sensor for its capability to handle harsh outdoor environmental conditions. In this paper, we introduce a mirror based two dimensional Frequency-Modulated Continuous-Wave (FMCW) radar scanner for stair detection. A radar image based stair dimensioning approach is presented and tested under laboratory and realistic conditions.',\n",
       " 'The emergence of mobile eye trackers embedded in next generation smartphones or VR displays will make it possible to trace not only what objects we look at but also the level of attention in a given situation. Exploring whether we can quantify the engagement of a user interacting with a laptop, we apply mobile eye tracking in an in-depth study over 2 weeks with nearly 10.000 observations to assess pupil size changes, related to attentional aspects of alertness, orientation and conflict resolution. Visually presenting conflicting cues and targets we hypothesize that it\\'s feasible to measure the allocated effort when responding to confusing stimuli. Although such experiments are normally carried out in a lab, we are able to differentiate between sustained alertness and complex decision making even with low cost eye tracking \"in the wild\". From a quantified self perspective of individual behavioral adaptation, the correlations between the pupil size and the task dependent reaction time and error rates may longer term provide a foundation for modifying smartphone content and interaction to the users perceived level of attention.',\n",
       " 'As aging societies grow, researchers are actively studying care systems concerning the life and diseases of the elderly. Among these diseases, dementia makes it difficult to maintain daily life due to the degradation of cognitive functioning, memory, and reasoning, as well as the ability to perform actions. Moreover, dementia does not have a perfect cure, though therapy and care can slow its onset and provide patients with physical and mental support. In this paper, we developed a projection-based augmented reality system robot that can cover 360 degrees of space. We also propose an application that supports continuous monitoring of dementia patients to address the difficulties they face in daily life. The system is also designed to provide therapy applications, such as entertainment and spatial art, to provide mental care aids for the patients.',\n",
       " 'This paper describes the conception, development and deployment of a novel HCI system for public participation and decision making. This system was applied for the process of allocating refugee accommodation in the City of Hamburg within the FindingPlaces project in 2016. The CityScope a rapid prototyping platform for urban planning and decision making offered a technical solution which was complemented by a workshop process to facilitate effective interaction of multiple participants and stakeholder groups. This paper presents the origins of CS and the evolution of the tangible user interface approach to urban planning and public participation. It further outlines technical features of the system, including custom hardware and software in use, utilization in real time as well as technical constraints and limitations. Special focus is on the adaptation of the CS technology to the specific demands of Hamburg FP project, whose procedures, processes, and results are reflected. The final section analyzes success factors as well as shortcomings of the approach, and indicates further R&D as well as application scenarios for the CS.',\n",
       " 'Science fiction literature, comics, cartoons and, in particular, audio-visual materials, such as science fiction movies and shows, can be a valuable addition in Human-computer interaction (HCI) Education. In this paper, we present an overview of research relative to future directions in HCI Education, distinct crossings of science fiction in HCI and Computer Science teaching and the Framework for 21st Century Learning. Next, we provide examples where science fiction can add to the future of HCI Education. In particular, we argue herein first that science fiction, as tangible and intangible cultural artifact, can serve as a trigger for creativity and innovation and thus, support us in exploring the design space. Second, science fiction, as a means to analyze yet-to-come HCI technologies, can assist us in developing an open-minded and reflective dialogue about technological futures, thus creating a singular base for critical thinking and problem solving. Provided that one is cognizant of its potential and limitations, we reason that science fiction can be a meaningful extension of selected aspects of HCI curricula and research.',\n",
       " 'We present an interactive visualisation tool for recommending travel trajectories. This system is based on new machine learning formulations and algorithms for the sequence recommendation problem. The system starts from a map-based overview, taking an interactive query as starting point. It then breaks down contributions from different geographical and user behavior features, and those from individual points-of-interest versus pairs of consecutive points on a route. The system also supports detailed quantitative interrogation by comparing a large number of features for multiple points. Effective trajectory visualisations can potentially benefit a large cohort of online map users and assist their decision-making. More broadly, the design of this system can inform visualisations of other structured prediction tasks, such as for sequences or trees.',\n",
       " 'Visually impaired people face numerous challenges when it comes to transportation. Not only must they circumvent obstacles while navigating, but they also need access to essential information related to available public transport, up-to-date weather forecast, and convenient method for booking private taxis. In this paper we introduce Transport Assistant - a voice based assistive technology prototype, built with a goal of leveling the playing field for the visually impaired to solve these problems that they face in their day to day life. Being voice enabled makes it seamlessly integrate into the environment, and can be invoked by saying a hotword - hello assistant. The paper explores this research question, followed by investigating existing technologies, explains the methodology and design, then concludes by presenting the prototype and results.',\n",
       " 'We investigate to what extent mobile use patterns can predict -- at the moment it is posted -- whether a notification will be clicked within the next 10 minutes. We use a data set containing the detailed mobile phone usage logs of 279 users, who over the course of 5 weeks received 446,268 notifications from a variety of apps. Besides using classical gradient-boosted trees, we demonstrate how to make continual predictions using a recurrent neural network (RNN). The two approaches achieve a similar AUC of ca. 0.7 on unseen users, with a possible operation point of 50% sensitivity and 80% specificity considering all notification types (an increase of 40% with respect to a probabilistic baseline). These results enable automatic, intelligent handling of mobile phone notifications without the need for user feedback or personalization. Furthermore, they showcase how forego feature-extraction by using RNNs for continual predictions directly on mobile usage logs. To the best of our knowledge, this is the first work that leverages mobile sensor data for continual, context-aware predictions of interruptibility using deep neural networks.',\n",
       " 'The distinct abilities of older adults to interact with computers has motivated a wide range of contributions in the the form of design guidelines for making technologies usable and accessible for the elderly population. However, despite the growing effort by the research community, the adoption of guidelines by developers and designers has been scant or not properly translated into more accessible interaction systems. In this paper we explore this issue by reporting on a qualitative outcomes of a systematic review of 204 research-derived design guidelines for touchscreen applications. We report first on the different definitions of \"elderly\" and assess the reliability, organization and accessibility of the guidelines. Then we present our early attempt at facilitating the reporting and access of such guidelines to researchers and practitioners.',\n",
       " 'Email is one of the most successful computer applications yet devised. Communication features in email, however, have remained relatively static in years. We investigate one way of expanding email functionality without modifying the existing email infrastructure. We introduce email late bound content, a simple and generalizable technique that defers message content binding through image lazy-loading. Parts of an email are converted into external images embedded in HTML code snippets, making it so that email clients will defer the image download (i.e. content binding) until the moment users open the email. This late bound content allows email senders and third party services to update delivered emails. To illustrate the utilities of late bound content, we present four new example features and discuss the tradeoffs of email content late binding.',\n",
       " 'We model Human-Robot-Interaction (HRI) scenarios as linear dynamical systems and use Model Predictive Control (MPC) with mixed integer constraints to generate human-aware control policies. We motivate the approach by presenting two scenarios. The first involves an assistive robot that aims to maximize productivity while minimizing the human\\'s workload, and the second involves a listening humanoid robot that manages its eye contact behavior to maximize \"connection\" and minimize social \"awkwardness\" with the human during the interaction. Our simulation results show that the robot generates useful behaviors as it finds control policies to minimize the specified cost function. Further, we implement the second scenario on a humanoid robot and test the eye contact scenario with 48 human participants to demonstrate and evaluate the desired controller behavior. The humanoid generated 25% more eye contact when it was told to maximize connection over when it was told to maximize awkwardness. However, despite showing the desired behavior, there was no statistical difference between the participant\\'s perceived connection with the humanoid.',\n",
       " 'Designing 3D User Interfaces (UI) requires adequate evaluation tools to ensure good usability and user experience. While many evaluation tools are already available and widely used, existing approaches generally cannot provide continuous and objective measures of usa-bility qualities during interaction without interrupting the user. In this paper, we propose to use brain (with ElectroEncephaloGraphy) and physiological (ElectroCardioGraphy, Galvanic Skin Response) signals to continuously assess the mental effort made by the user to perform 3D object manipulation tasks. We first show how this mental effort (a.k.a., mental workload) can be estimated from such signals, and then measure it on 8 participants during an actual 3D object manipulation task with an input device known as the CubTile. Our results suggest that monitoring workload enables us to continuously assess the 3DUI and/or interaction technique ease-of-use. Overall, this suggests that this new measure could become a useful addition to the repertoire of available evaluation tools, enabling a finer grain assessment of the ergonomic qualities of a given 3D user interface.',\n",
       " 'Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.',\n",
       " 'This research investigates the variety and complexity of situational impairment events (SIE) that are being experienced by users of smartphone technology of all abilities. The authors have created a classification system to help describe the different types of SIE as well as differentiate a certain subgroup of events that were identified as severely constraining. Continuing research examined workarounds that users deploy when attempting to complete a mobile I/O transaction in the presence of an SIE, as well as social/cultural barriers to attempting mobile interaction that users recognize but do not always follow. The ultimate goal of this research arc would be the creation of guidelines to assist mobile designers and researchers in the accounting of SIE and perhaps different design considerations for those events deemed severely constraining.',\n",
       " 'Data credibility is a crucial issue in mobile crowd sensing (MCS) and, more generally, people-centric Internet of Things (IoT). Prior work takes approaches such as incentive mechanism design and data mining to address this issue, while overlooking the power of crowds itself, which we exploit in this paper. In particular, we propose a cross validation approach which seeks a validating crowd to verify the data credibility of the original sensing crowd, and uses the verification result to reshape the original sensing dataset into a more credible posterior belief of the ground truth. Following this approach, we design a specific cross validation mechanism, which integrates four sampling techniques with a privacy-aware competency-adaptive push (PACAP) algorithm and is applicable to time-sensitive and quality-critical MCS applications. It does not require redesigning a new MCS system but rather functions as a lightweight \"plug-in\", making it easier for practical adoption. Our results demonstrate that the proposed mechanism substantially improves data credibility in terms of both reinforcing obscure truths and scavenging hidden truths.',\n",
       " 'Despite the enormous interest in emotion classification from speech, the impact of noise on emotion classification is not well understood. This is important because, due to the tremendous advancement of the smartphone technology, it can be a powerful medium for speech emotion recognition in the outside laboratory natural environment, which is likely to incorporate background noise in the speech. We capitalize on the current breakthrough of Recurrent Neural Network (RNN) and seek to investigate its performance for emotion classification from noisy speech. We particularly focus on the recently proposed Gated Recurrent Unit (GRU), which is yet to be explored for emotion recognition from speech. Experiments conducted with speech compounded with eight different types of noises reveal that GRU incurs an 18.16% smaller run-time while performing quite comparably to the Long Short-Term Memory (LSTM), which is the most popular Recurrent Neural Network proposed to date. This result is promising for any embedded platform in general and will initiate further studies to utilize GRU to its full potential for emotion recognition on smartphones.',\n",
       " \"This study explores the use of natural language to give instructions that might be interpreted by Internet of Things (IoT) devices in a domestic `smart home' environment. We start from the proposition that reminders can be considered as a type of end-user programming, in which the executed actions might be performed either by an automated agent or by the author of the reminder. We conducted an experiment in which people wrote sticky notes specifying future actions in their home. In different conditions, these notes were addressed to themselves, to others, or to a computer agent.We analyse the linguistic features and strategies that are used to achieve these tasks, including the use of graphical resources as an informal visual language. The findings provide a basis for design guidance related to end-user development for the Internet of Things.\",\n",
       " 'In recent years, the rapid development of neuroimaging technology has been providing many powerful tools for cognitive neuroscience research. Among them, the functional magnetic resonance imaging (fMRI), which has high spatial resolution, acceptable temporal resolution, simple calibration, and short preparation time, has been widely used in brain research. Compared with the electroencephalogram (EEG), real-time fMRI-based brain computer interface (rtfMRI-BCI) not only can perform decoding analysis across the whole brain to control external devices, but also allows a subject to voluntarily self-regulate specific brain regions. This paper reviews the basic architecture of rtfMRI-BCI, the emerging machine learning based data analysis approaches (also known as multi-voxel pattern analysis), and the applications and recent advances of rtfMRI-BCI.',\n",
       " \"Explanations given by automation are often used to promote automation adoption. However, it remains unclear whether explanations promote acceptance of automated vehicles (AVs). In this study, we conducted a within-subject experiment in a driving simulator with 32 participants, using four different conditions. The four conditions included: (1) no explanation, (2) explanation given before or (3) after the AV acted and (4) the option for the driver to approve or disapprove the AV's action after hearing the explanation. We examined four AV outcomes: trust, preference for AV, anxiety and mental workload. Results suggest that explanations provided before an AV acted were associated with higher trust in and preference for the AV, but there was no difference in anxiety and workload. These results have important implications for the adoption of AVs.\",\n",
       " 'Mobile proactive tourist recommender systems can support tourists by recommending the best choice depending on different contexts related to herself and the environment. In this paper, we propose to utilize wearable sensors to gather health information about a tourist and use them for recommending tourist activities. We discuss a range of wearable devices, sensors to infer physiological conditions of the users, and exemplify the feasibility using a popular self-quantification mobile app. Our main contribution then comprises a data model to derive relations between the parameters measured by the wearable sensors, such as heart rate, body temperature, blood pressure, and use them to infer the physiological condition of a user. This model can then be used to derive classes of tourist activities that determine which items should be recommended.',\n",
       " 'We present a novel, real-time algorithm, EVA, for generating virtual agents with various perceived emotions. Our approach is based on using Expressive Features of gaze and gait to convey emotions corresponding to happy, sad, angry, or neutral. We precompute a data-driven mapping between gaits and their perceived emotions. EVA uses this gait emotion association at runtime to generate appropriate walking styles in terms of gaits and gaze. Using the EVA algorithm, we can simulate gaits and gazing behaviors of hundreds of virtual agents in real-time with known emotional characteristics. We have evaluated the benefits in different multi-agent VR simulation environments. Our studies suggest that the use of expressive features corresponding to gait and gaze can considerably increase the sense of presence in scenarios with multiple virtual agents.',\n",
       " 'The analysis of the collaborative learning process is one of the growing fields of education research, which has many different analytic solutions. In this paper, we provided a new solution to improve automated collaborative learning analyses using deep neural networks. Instead of using self-reported questionnaires, which are subject to bias and noise, we automatically extract group-working information by object recognition results using Mask R-CNN method. This process is based on detecting the people and other objects from pictures and video clips of the collaborative learning process, then evaluate the mobile learning performance using the collaborative indicators. We tested our approach to automatically evaluate the group-work collaboration in a controlled study of thirty-three dyads while performing an anatomy body painting intervention. The results indicate that our approach recognizes the differences of collaborations among teams of treatment and control groups in the case study. This work introduces new methods for automated quality prediction of collaborations among human-human interactions using computer vision techniques.',\n",
       " 'Insights into social phenomenon can be gleaned from trends and patterns in corpora of documents associated with that phenomenon. Recent years have witnessed the use of computational techniques, mostly based on keywords, to analyze large corpora for these purposes. In this paper, we extend these techniques to incorporate semantic features. We introduce Doris, an interactive exploration tool that combines semantic features with information retrieval techniques to enable exploration of document corpora corresponding to the social phenomenon. We discuss the semantic techniques and describe an implementation on a corpus of United States (US) presidential speeches. We illustrate, with examples, how the ability to combine syntactic and semantic features in a visualization helps researchers more easily gain insights into the underlying phenomenon.',\n",
       " 'The wide spread of mobile devices in the consumer market has posed a number of new issues in the design of internet applications and their user interfaces. In particular, applications need to adapt their interaction modalities to different portable devices. In this paper we address the problem of defining models and techniques for designing internet based applications that automatically adapt to different mobile devices. First, we define a formal model that allows for specifying the interaction in a way that is abstract enough to be decoupled from the presentation layer, which is to be adapted to different contexts. The model is mainly based on the idea of describing the user interaction in terms of elementary actions. Then, we provide a formal device characterization showing how to effectively implements the AIUs in a multidevice context.',\n",
       " 'We propose a system to deliver dynamic guidance in drawing, sketching and handwriting tasks via an electromagnet moving underneath a high refresh rate pressure sensitive tablet. The system allows the user to move the pen at their own pace and style and does not take away control. The system continously and iteratively measures the pen motion and adjusts magnet position and power according to the user input in real-time via a receding horizon optimal control formulation. The optimization is based on a novel approximate electromagnet model that is fast enough for use in real-time methods, yet provides very good fit to experimental data. Using a closed-loop time-free approach allows for error-correcting behavior, gently pulling the user back to the desired trajectory rather than pushing or pulling the pen to a continuously advancing setpoint. Our experimental results show that the system can control the pen position with a very low dispersion of 2.8mm (+/-0.8mm). An initial user study indicates that it significantly increases accuracy of users drawing a variety of shapes and that this improvement increases with complexity of the shape.',\n",
       " \"AI systems are being deployed to support human decision making in high-stakes domains. In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI's inferences. A successful partnership requires that the human develops insights into the performance of the AI system, including its failures. We study the influence of updates to an AI system in this setting. While updates can increase the AI's predictive performance, they may also lead to changes that are at odds with the user's prior experiences and confidence in the AI's inferences, hurting therefore the overall team performance. We introduce the notion of the compatibility of an AI update with prior user experience and present methods for studying the role of compatibility in human-AI teams. Empirical results on three high-stakes domains show that current machine learning algorithms do not produce compatible updates. We propose a re-training objective to improve the compatibility of an update by penalizing new errors. The objective offers full leverage of the performance/compatibility tradeoff, enabling more compatible yet accurate updates.\",\n",
       " 'Virtual reality (VR) is an important new technology that is fun-damentally changing the way people experience entertainment and education content. Due to the fact that most currently available VR products are one size fits all, the accessibility of the content design and user interface design, even for healthy children is not well understood. It requires more research to ensure that children can have equally good user compared to adults in VR. In our study, we seek to explore accessibility of locomotion in VR between healthy adults and minors along both objective and subjective dimensions. We performed a user experience experiment where subjects completed a simple task of moving and touching underwater animals in VR using one of four different locomotion modalities, as well as real-world walking without wearing VR headsets as the baseline. Our results show that physical body movement that mirrors real-world movement exclusively is the least preferred by both adults and minors. However, within the different modalities of controller assisted locomotion there are variations between adults and minors for preference and challenge levels.',\n",
       " 'Development of the majority of the leading web services and software products today is generally guided by data-driven decisions based on evaluation that ensures a steady stream of updates, both in terms of quality and quantity. Large internet companies use online evaluation on a day-to-day basis and at a large scale. The number of smaller companies using A/B testing in their development cycle is also growing. Web development across the board strongly depends on quality of experimentation platforms. In this tutorial, we overview state-of-the-art methods underlying everyday evaluation pipelines at some of the leading Internet companies. Software engineers, designers, analysts, service or product managers --- beginners, advanced specialists, and researchers --- can learn how to make web service development data-driven and do it effectively.',\n",
       " 'Cartograms are maps in which areas of geographic regions (countries, states) appear in proportion to some variable of interest (population, income). Cartograms are popular visualizations for geo-referenced data that have been around for over a century. Newspapers, magazines, textbooks, blogs, and presentations frequently employ cartograms to show voting results, popularity, and in general, geographic patterns. Despite the popularity of cartograms and the large number of cartogram variants, there are very few studies evaluating the effectiveness of cartograms in conveying information. In order to design cartograms as a useful visualization tool and to be able to compare the effectiveness of cartograms generated by different methods, we need to study the nature of information conveyed and the specific tasks that can be performed on cartograms. In this paper we consider a set of cartogram visualization tasks, based on standard taxonomies from cartography and information visualization. We then propose a cartogram task taxonomy that can be used to organize not only the tasks considered here but also other tasks that might be added later.',\n",
       " 'The future scenarios often associated with Internet of Things (IoT) oscillate between the peril of IoT for the future of humanity and the promises for an ever-connected and efficient future. Such a dichotomous positioning creates problems not only for expanding the field of application of the technology, but also ensuring ethical and responsible design and production. As part of VirtEU (Values and Ethics in Innovation for Responsible Technology in Europe) (EU Horizon 2020 FP7), we have conducted ethnographic research into the main hubs of IoT in Europe, such as London, Amsterdam, Barcelona and Belgrade, with developers and designers of IoT to identify the challenges they face in their day-to-day work. In this paper, we focus on the IoT and the ethical imaginaries explore the practical challenges IoT developers face when they are designing, producing and marketing IoT technologies. We argue that top-down ethical frameworks that overlook the situated capabilities of developers or the solutionist approaches that treat ethical issues as technical problems are unlikely to provide an alternative to the dichotomous imaginary for the future.',\n",
       " 'Context-aware applications process context information to support users in their daily tasks and routines. These applications can adapt their functionalities by aggregating context information through machine-learning and data processing algorithms, supporting users with recommendations or services based on their current needs. In the last years, smartphones have been used in the field of context-awareness due to their embedded sensors and various communication interfaces such as Bluetooth, WiFi, NFC or cellular. However, building context-aware applications for smartphones can be a challenging and time-consuming task. In this paper, we describe an ontology-based reasoning framework to create context-aware applications. The framework is based on an ontology as well as micro-services to aggregate, process and represent context information.',\n",
       " 'Aerial robots are becoming popular among general public, and with the development of artificial intelligence (AI), there is a trend to equip aerial robots with a natural user interface (NUI). Hand/arm gestures are an intuitive way to communicate for humans, and various research works have focused on controlling an aerial robot with natural gestures. However, the techniques in this area are still far from mature. Many issues in this area have been poorly addressed, such as the principles of choosing gestures from the design point of view, hardware requirements from an economic point of view, considerations of data availability, and algorithm complexity from a practical perspective. Our work focuses on building an economical monocular system particularly designed for gesture-based piloting of an aerial robot. Natural arm gestures are mapped to rich target directions and convenient fine adjustment is achieved. Practical piloting scenarios, hardware cost and algorithm applicability are jointly considered in our system design. The entire system is successfully implemented in an aerial robot and various properties of the system are tested.',\n",
       " 'Recent trend of touch-screen devices produces an accessibility barrier for visually impaired people. On the other hand, these devices come with sensors such as accelerometer. This calls for new approaches to human computer interface (HCI). In this study, our aim is to find an alternative approach to classify 20 different hand gestures captured by iPhone 3GS\\'s built-in accelerometer and make high accuracy on user-independent classifications using Dynamic Time Warping (DTW) with dynamic warping window sizes. 20 gestures with 1,100 gesture data are collected from 15 normal-visioned people. This data set is used for training. Experiment-1 based on this data set produced an accuracy rate of 96.7~\\\\%. In order for visually impaired people to use the system, a gesture recognition based \"talking\" calculator is implemented. In Experiment-2, 4 visually impaired end-users used the calculator and obtained 95.5~\\\\% accuracy rate among 17 gestures with 720 gesture data totally. Contributions of the techniques to the end result is also investigated. Dynamic warping window size is found to be the most effective one. The data and the code is available.',\n",
       " 'Mobile agents that can leverage help from humans can potentially accomplish more complex tasks than they could entirely on their own. We develop \"Help, Anna!\" (HANNA), an interactive photo-realistic simulator in which an agent fulfills object-finding tasks by requesting and interpreting natural languageand-vision assistance. An agent solving tasks in a HANNA environment can leverage simulated human assistants, called ANNA (Automatic Natural Navigation Assistants), which, upon request, provide natural language and visual instructions to direct the agent towards the goals. To address the HANNA problem, we develop a memory-augmented neural agent that hierarchically models multiple levels of decision-making, and an imitation learning algorithm that teaches the agent to avoid repeating past mistakes while simultaneously predicting its own chances of making future progress. Empirically, our approach is able to ask for help more effectively than competitive baselines and, thus, attains higher task success rate on both previously seen and previously unseen environments. We publicly release code and data at https://github.com/khanhptnk/hanna .',\n",
       " 'This paper explores a design study of a smartphone enabled meet-up app meant to inspire engagement in community innovation. Community hubs such as co-working spaces, incubators, and maker spaces attract community members with diverse interests. This paper presents these spaces as a design opportunity for an application that helps host community-centered meet-ups in smart and connected communities. Our design study explores three scenarios of use, inspired by previous literature, for organizing meet-ups and compares them by surveying potential users. Based on the results of our survey, we propose several design implications and implement them in the Community Animator geosocial networking application, which identifies nearby individuals that are willing to chat or perform community-centered activities. We present the results of both our survey and our prototype, discuss our design goals, and provide design implications for civic-minded, geosocial networking applications. Our contribution in this work is the development process, proposed design of a mobile application to support community-centered meet-ups, and insights for future work.',\n",
       " \"Traditional employment usually provides mechanisms for workers to improve their skills to access better opportunities. However, crowd work platforms like Amazon Mechanical Turk (AMT) generally do not support skill development (i.e., becoming faster and better at work). While researchers have started to tackle this problem, most solutions are dependent on experts or requesters willing to help. However, requesters generally lack the necessary knowledge, and experts are rare and expensive. To further facilitate crowd workers' skill growth, we present Crowd Coach, a system that enables workers to receive peer coaching while on the job. We conduct a field experiment and real world deployment to study Crowd Coach in the wild. Hundreds of workers used Crowd Coach in a variety of tasks, including writing, doing surveys, and labeling images. We find that Crowd Coach enhances workers' speed without sacrificing their work quality, especially in audio transcription tasks. We posit that peer coaching systems hold potential for better supporting crowd workers' skill development while on the job. We finish with design implications from our research.\",\n",
       " 'In this paper we describe the design and validation of a virtual fitness environment aiming at keeping older adults physically and socially active. We target particularly older adults who are socially more isolated, physically less active, and with less chances of training in a gym. The virtual fitness environment, namely Gymcentral, was designed to enable and motivate older adults to follow personalised exercises from home, with a (heterogeneous) group of remote friends and under the remote supervision of a Coach. We take the training activity as an opportunity to create social interactions, by complementing training features with social instruments. Finally, we report on the feasibility and effectiveness of the virtual environment, as well as its effects on the usage and social interactions, from an intervention study in Trento, Italy',\n",
       " \"The work presented in this paper focuses on the improvement of corporate knowledge management systems. For the implementation of such systems, companies deploy can important means for small gains. Indeed, management services often notice very limited use compared to what they actually expect. We present a five-step redesigning approach which takes into account different factors to increase the use of these systems. We use as an example the knowledge sharing platform implemented for the employees of Soci{\\\\'e}t{\\\\'e} du Canal de Provence (SCP). This system was taken into production but very occasionally used. We describe the reasons for this limited use and we propose a design methodology adapted to the context. Promoting the effective use of the system, our approach has been experimented and evaluated with a panel of users working at SCP.\",\n",
       " 'We propose VRGym, a virtual reality testbed for realistic human-robot interaction. Different from existing toolkits and virtual reality environments, the VRGym emphasizes on building and training both physical and interactive agents for robotics, machine learning, and cognitive science. VRGym leverages mechanisms that can generate diverse 3D scenes with high realism through physics-based simulation. We demonstrate that VRGym is able to (i) collect human interactions and fine manipulations, (ii) accommodate various robots with a ROS bridge, (iii) support experiments for human-robot interaction, and (iv) provide toolkits for training the state-of-the-art machine learning algorithms. We hope VRGym can help to advance general-purpose robotics and machine learning agents, as well as assisting human studies in the field of cognitive science.',\n",
       " 'Globally distributed groups require collaborative systems to support their work. Besides being able to support the teamwork, these systems also should promote well-being and maximize the human potential that leads to an engaging system and joyful experience. Designing such system is a significant challenge and requires a thorough understanding of group work. We used the field theory as a lens to view the essential aspects of group motivation and then utilized collaboration personas to analyze the elements of group work. We integrated well-being determinants as engagement factors to develop a group-centered framework for digital collaboration in a global setting. Based on the outcomes, we proposed a conceptual framework to design an engaging collaborative system and recommend system values that can be used to evaluate the system further',\n",
       " 'This position paper takes the first step to attempt to present the initial characterization of HCI research in China. We discuss the current streams and methodologies of Chinese HCI research based on two well-known HCI theories: Micro/Marco-HCI and the Three Paradigms of HCI. We evaluate the discussion with a survey of Chinese publications at CHI 2019, which shows HCI research in China has less attention to Macro-HCI topics and the third paradigms of HCI (Phenomenologically situated Interaction). We then propose future HCI research directions such as paying more attention to Macro-HCI topics and third paradigm of HCI, combining research methodologies from multiple HCI paradigms, including emergent users who have less access to technology, and addressing the cultural dimensions in order to provide better technical solutions and support.',\n",
       " 'From a computational viewpoint, emotions continue to be intriguingly hard to understand. In research, direct, real-time inspection in realistic settings is not possible. Discrete, indirect, post-hoc recordings are therefore the norm. As a result, proper emotion assessment remains a problematic issue. The Continuously Annotated Signals of Emotion (CASE) dataset provides a solution as it focusses on real-time continuous annotation of emotions, as experienced by the participants, while watching various videos. For this purpose, a novel, intuitive joystick-based annotation interface was developed, that allowed for simultaneous reporting of valence and arousal, that are instead often annotated independently. In parallel, eight high quality, synchronized physiological recordings (1000 Hz, 16-bit ADC) were made of ECG, BVP, EMG (3x), GSR (or EDA), respiration and skin temperature. The dataset consists of the physiological and annotation data from 30 participants, 15 male and 15 female, who watched several validated video-stimuli. The validity of the emotion induction, as exemplified by the annotation and physiological data, is also presented.',\n",
       " \"Online experimentation is at the core of Booking.com's customer-centric product development. While randomised controlled trials are a powerful tool for estimating the overall effects of product changes on business metrics, they often fall short in explaining the mechanism of change. This becomes problematic when decision-making depends on being able to distinguish between the direct effect of a treatment on some outcome variable and its indirect effect via a mediator variable. In this paper, we demonstrate the need for mediation analyses in online experimentation, and use simulated data to show how these methods help identify and estimate direct causal effect. Failing to take into account all confounders can lead to biased estimates, so we include sensitivity analyses to help gauge the robustness of estimates to missing causal factors.\",\n",
       " '3D gaze information is important for scene-centric attention analysis but accurate estimation and analysis of 3D gaze in real-world environments remains challenging. We present a novel 3D gaze estimation method for monocular head-mounted eye trackers. In contrast to previous work, our method does not aim to infer 3D eyeball poses but directly maps 2D pupil positions to 3D gaze directions in scene camera coordinate space. We first provide a detailed discussion of the 3D gaze estimation task and summarize different methods, including our own. We then evaluate the performance of different 3D gaze estimation approaches using both simulated and real data. Through experimental validation, we demonstrate the effectiveness of our method in reducing parallax error, and we identify research challenges for the design of 3D calibration procedures.',\n",
       " 'The health effects of air pollution have been subject to intense study in recent decades. Exposure to pollutants such as airborne particulate matter and ozone has been associated with increases in morbidity and mortality, especially with regards to respiratory and cardiovascular diseases. Unfortunately, individuals do not have readily accessible methods by which to track their exposure to pollution. This paper proposes how pollution parameters like CO, NO2, O3, PM2.5, PM10 and SO2 can be monitored for respiratory and cardiovascular personalized health during outdoor exercise events. Using location tracked activities, we synchronize them to public data sets of pollution sensors. For improved accuracy in estimation, we use heart rate data to understand breathing volume mapped with the local air quality sensors via constant GPS tracking.',\n",
       " 'The increasing accessibility of data provides substantial opportunities for understanding user behaviors. Unearthing anomalies in user behaviors is of particular importance as it helps signal harmful incidents such as network intrusions, terrorist activities, and financial frauds. Many visual analytics methods have been proposed to help understand user behavior-related data in various application domains. In this work, we survey the state of art in visual analytics of anomalous user behaviors and classify them into four categories including social interaction, travel, network communication, and transaction. We further examine the research works in each category in terms of data types, anomaly detection techniques, and visualization techniques, and interaction methods. Finally, we discuss the findings and potential research directions.',\n",
       " \"In 2013, scholars laid out a framework for a sustainable, ethical future of crowd work, recommending career ladders so that crowd work can lead to career advancement and more economic mobility. Five years later, we consider this vision in the context of Amazon Mechanical Turk (AMT). To understand how workers currently view their experiences on AMT, and how they publicly present and share these experiences in their professional lives, we conducted a survey study with workers on AMT (n=98). The survey we administered included a combination of multiple choice, binary, and open-ended (short paragraph) items gauging Turkers' perceptions of their experiences on AMT within the context of their broader work experience and career goals. This work extends existing understandings of who crowd workers are and why they crowd work by seeking to better understand how crowd work factors into Turkers' professional profiles, and how we can subsequently better support crowd workers in their career advancement. Our survey results can inform the design of better tools to empower crowd workers in their professional development both inside and outside of AMT.\",\n",
       " 'We live in a world where data generation is omnipresent. Innovations in computer hardware in the last few decades coupled with increasingly reliable connectivity among them have fueled this phenomenon. We are constantly creating and consuming data across digital devices of varying form factors. Leveraging huge quantities of data involves making interpretations from it. However, interpreting data is still a difficult task. We need data analysts to help make decisions. These experts apply their domain knowledge, understanding of the problem space and numerical analysis to draw inferences from the data in order to support decision making. Existing tools and techniques for interference serve users making decisions with hard constraints. Consumer systems are often built to support exploratory data analysis in mind rather than sense making.',\n",
       " 'The 9th Semantic Ambient Media Experience (SAME) proceedings where based on the academic contributions to a two day workshop that was held at Curtin University, Perth, WA, Australia. The symposium was held to discuss visualisation, emerging media, and user-experience from various angles. The papers of this workshop are freely available through http://www.ambientmediaassociation.org/Journal under open access as provided by the International Ambient Media Association (iAMEA) Ry. iAMEA is hosting the international open access journal entitled \"International Journal on Information Systems and Management in Creative eMedia\", and the series entitled \"International Series on Information Systems and Management in Creative eMedia\". For any further information, please visit the website of the Association: http://www.ambientmediaassociation.org.',\n",
       " \"The virtuality continuum describes the degrees of positive virtuality under the umbrella term mixed reality. Besides adding virtual information within a mixed environment, diminished reality aims at reducing real world information. Mann defined the term mediated reality (MR), which also considered diminished reality, but without the possibility to describe different degrees of fusion between a mixed and a diminished reality. That is why this work defines the new term blending entropy that captures the relations between a mixed and a diminished reality. The blending entropy is based on the information density of the mediated reality and the actual area the user has to comprehend, which is named perceptual frustum. We describe the blending entropy's twodimensional dependencies and detail important points in the blending entropy's space.\",\n",
       " 'With rapid development of computer techniques, the human computer interaction scenarios are becoming more and more frequent. The development history of the human-computer interaction is from a person to adapt to the computer to the computer and continually adapt to the rapid development. Facing the process of human-computer interaction, information system daily operation to produce huge amounts of data, how to ensure human-computer interaction interface clear, generated data safe and reliable, has become a problem to be solved in the world of information. To deal with the challenging, we propose the information security enhancement approaches and the core applications on HCI systems. Through reviewing the other state-of-the-art methods, we propose the data encryption system to deal with the issues that uses mixed encryption system to make full use of the symmetric cipher algorithm encryption speed and encryption intensity is high while the encryption of large amounts of data efficiently. Our method could enhance the general safety of the HCI system, the experimental result verities the feasibility and general robustness of our approach.',\n",
       " \"With the proliferation of social networking sites (SNSs) such as Facebook and Google+, investigating the impact of SNSs on our lives has become an important research area in recent years. Though SNS usage plays a key role in connecting people with friends and families from distant places, SNSs also bring concern for families. We focus on imbalance SNS usage, i.e., an individual remains busy in using SNSs when her family member is expecting to spend time with her. More specifically, we investigate the cause and pattern of imbalance SNS usage and how the emotion of family members may become affected, if they use SNSs in an imbalanced way in a regular manner. This paper is the first attempt to identify the relationship between an individual's imbalance SNS usage and the emotion of her family member in the context of a developing country.\",\n",
       " \"Many personal devices have transitioned from visual-controlled interfaces to speech-controlled interfaces to reduce device costs and interactive friction. This transition has been hastened by the increasing capabilities of speech-controlled interfaces, e.g., Amazon Echo or Apple's Siri. A consequence is that people who are deaf or hard of hearing (DHH) may be unable to use these speech-controlled devices. We show that deaf speech has a high error rate compared to hearing speech, in commercial speech-controlled interfaces. Deaf speech had approximately a 78% word error rate (WER) compared to a hearing speech 18% WER. Our findings show that current speech-controlled interfaces are not usable by deaf and hard of hearing people. Therefore, it might be wise to pursue other methods for deaf persons to deliver natural commands to computers.\",\n",
       " 'Good quality sleep is essential for good health and sleep monitoring becomes a vital research topic. This paper provides a low cost, continuous and contactless WiFi-based vital signs (breathing and heart rate) monitoring method. In particular, we set up the antennas based on Fresnel diffraction model and signal propagation theory, which enhances the detection of weak breathing/heartbeat motion. We implement a prototype system using the off-shelf devices and a real-time processing system to monitor vital signs in real time. The experimental results indicate the accurate breathing rate and heart rate detection performance. To the best of our knowledge, this is the first work to use a pair of WiFi devices and omnidirectional antennas to achieve real-time individual breathing rate and heart rate monitoring in different sleeping postures.',\n",
       " 'Fallback authentication is the backup authentication method used when the primary authentication method (e.g., passwords, fingerprints, etc.) fails. Currently, widely-deployed fallback authentication methods (e.g., security questions, email resets, and SMS resets) suffer from documented security and usability flaws that threaten the security of accounts. These flaws motivate us to design and study Geographical Security Questions (GeoSQ), a system for fallback authentication. GeoSQ is an Android application that utilizes autobiographical location data for fallback authentication. We performed security and usability analyses of GeoSQ through an in-person two-session lab study (n=36,18 pairs). Our results indicate that GeoSQ exceeds the security of its counterparts, while its usability (specifically login time) has room for improvement.',\n",
       " 'Immigrants usually are pro-social towards their hometowns and try to improve them. However, the lack of trust in their government can drive immigrants to work individually. As a result, their pro-social activities are usually limited in impact and scope. This paper studies the interface factors that ease collaborations between immigrants and their home governments. We specifically focus on Mexican immigrants in the US who want to improve their rural communities. We identify that for Mexican immigrants having clear workflows of how their money flows and a sense of control over this workflow is important for collaborating with their government. Based on these findings, we create a blockchain based system for building trust between governments and immigrants. We finish by discussing design implications of our work and future directions.',\n",
       " 'Assessing and understanding intelligent agents is a difficult task for users that lack an AI background. A relatively new area, called \"Explainable AI,\" is emerging to help address this problem, but little is known about how users would forage through information an explanation system might offer. To inform the development of Explainable AI systems, we conducted a formative study, using the lens of Information Foraging Theory, into how experienced users foraged in the domain of StarCraft to assess an agent. Our results showed that participants faced difficult foraging problems. These foraging problems caused participants to entirely miss events that were important to them, reluctantly choose to ignore actions they did not want to ignore, and bear high cognitive, navigation, and information costs to access the information they needed.',\n",
       " 'Many data mining approaches aim at modelling and predicting human behaviour. An important quantity of interest is the quality of model-based predictions, e.g. for finding a competition winner with best prediction performance.\\n  In real life, human beings meet their decisions with considerable uncertainty. Its assessment and resulting implications for statistically evident evaluation of predictive models are in the main focus of this contribution. We identify relevant sources of uncertainty as well as the limited ability of its accurate measurement, propose an uncertainty-aware methodology for more evident evaluations of data mining approaches, and discuss its implications for existing quality assessment strategies. Specifically, our approach switches from common point-paradigm to more appropriate distribution-paradigm.\\n  This is exemplified in the context of recommender systems and their established metrics of prediction quality. The discussion is substantiated by comprehensive experiments with real users, large-scale simulations, and discussion of prior evaluation campaigns (i.a. Netflix Prize) in the light of human uncertainty aspects.',\n",
       " 'We present a qualitative analysis of interviews with participants in the NoSleep community within Reddit where millions of fans and writers of horror fiction congregate. We explore how the community handled a massive, sudden, and sustained increase in new members. Although existing theory and stories like Usenet\\'s infamous \"Eternal September\" suggest that large influxes of newcomers can hurt online communities, our interviews suggest that NoSleep survived without major incident. We propose that three features of NoSleep allowed it to manage the rapid influx of newcomers gracefully: (1) an active and well-coordinated group of administrators, (2) a shared sense of community which facilitated community moderation, and (3) technological systems that mitigated norm violations. We also point to several important trade-offs and limitations.',\n",
       " \"Bar charts with y-axes that don't begin at zero can visually exaggerate effect sizes, and in turn lead to unjustified or erroneous judgments. However, advice for whether or not to truncate the y-axis can be equivocal for other visualization types, and there is little existing empirical work on how axis truncation impacts judgments. In this paper we present examples of visualizations where this y-axis truncation can be beneficial as well as harmful, depending on the communicative and analytical intent. We also present the results of a series of crowd-sourced experiments in which we examine how y-axis truncation impacts subjective effect size across visualization types, and we explore alternative designs that more directly alert viewers to this truncation. We find that the subjective impact of axis truncation is persistent across visualizations designs, even for viewers that are aware of the presence of truncated axes. We therefore advise against ironclad rules about when y-axes are appropriate, but ask designers to consider the scale of the meaningful effect sizes and variation they intend to communicate, regardless of the visual encoding.\",\n",
       " 'This article presents \"John\", an open-source software designed to help collective free improvisation. It provides generated screen-scores running on distributed, reactive web-browsers. The musicians can then concurrently edit the scores in their own browser. John is used by ONE, a septet playing improvised electro-acoustic music with digital musical instruments (DMI). One of the original features of John is that its design takes care of leaving the musician\\'s attention as free as possible. Firstly, a quick review of the context of screen-based scores will help situate this research in the history of contemporary music notation. Then I will trace back how improvisation sessions led to John\\'s particular \"notational perspective\". A brief description of the software will precede a discussion about the various aspects guiding its design.',\n",
       " \"Cluster analysis has become one of the most exercised research areas over the past few decades in computer science. As a consequence, numerous clustering algorithms have already been developed to find appropriate partitions of a set of objects. Given multiple such clustering solutions, it is a challenging task to obtain an ensemble of these solutions. This becomes more challenging when the ground truth about the number of clusters is unavailable. In this paper, we introduce a crowd-powered model to collect solutions of image clustering from the general crowd and pose it as a clustering ensemble problem with variable number of clusters. The varying number of clusters basically reflects the crowd workers' perspective toward a particular set of objects. We allow a set of crowd workers to independently cluster the images as per their perceptions. We address the problem by finding out centroid of the clusters using an appropriate distance measure and prioritize the likelihood of similarity of the individual cluster sets. The effectiveness of the proposed method is demonstrated by applying it on multiple artificial datasets obtained from crowd.\",\n",
       " 'Recently, with the impact of AJAX a new way of web development techniques have been emerged. Hence, with the help of this model, single-page web application was introduced which can be updated/replaced independently. Today we have a new challenge of building a powerful single-page application using the currently emerged technologies. Gaining an understanding of navigational model and user interface structure of the source application is the first step to successfully build a single- page application. In this paper, it explores not only building powerful single-page application but also Two Dimensional (2D) drawings on images and videos. Moreover, in this research it clearly express the findings on 2D multi-points polygon drawing concepts on client side; real-time data binding in between drawing module on image, video and view pages.',\n",
       " \"The ability to focus one's attention underlies success in many everyday tasks, but voluntary attention cannot be sustained for a long period of time. Several studies indicate that attention training using computer-based exercises can lead to improved attention in children and adults. a major goal of recent research is to create a short (10 minutes) and effective VR Mindfulness meditation particularly designed for regaining or improving sustained attention. In this study, we have created a custom virtually relaxing environment including an archery game with multiple targets. In the experiment, the attention span of 12 adults are tested before and after the virtual reality session by a non-action video game ([19]) score and Muse headband EEG-signals. After the 10-minute virtual reality session participants' game scores increased (according to game experience): for the beginner by 275%, for intermediate by 107%, and for an expert by 17%. For Muse headband data, calm points increased by 250% irrespective of the participants gaming experiences. After the experiment, all participants reported feeling recharged to continue their daily activities.\",\n",
       " \"Virtual Reality (VR) can cause an unprecedented immersion and feeling of presence yet a lot of users experience motion sickness when moving through a virtual environment. Rollercoaster rides are popular in Virtual Reality but have to be well designed to limit the amount of nausea the user may feel. This paper describes a novel framework to get automated ratings on motion sickness using Neural Networks. An application that lets users create rollercoasters directly in VR, share them with other users and ride and rate them is used to gather real-time data related to the in-game behaviour of the player, the track itself and users' ratings based on a Simulator Sickness Questionnaire (SSQ) integrated into the application. Machine learning architectures based on deep neural networks are trained using this data aiming to predict motion sickness levels. While this paper focuses on rollercoasters this framework could help to rate any VR application on motion sickness and intensity that involves camera movement. A new well defined dataset is provided in this paper and the performance of the proposed architectures are evaluated in a comparative study.\",\n",
       " 'Driving under the influence of alcohol is a widespread phenomenon in the US where it is considered a major cause of fatal accidents. In this research we present a novel approach and concept for detecting intoxication from motion differences obtained by the sensors of wearable devices. We formalize the problem of drunkenness detection as a supervised machine learning task, both as a binary classification problem (drunk or sober) and a regression problem (the breath alcohol content level).\\n  In order to test our approach, we collected data from 30 different subjects (patrons at three bars) using Google Glass and the LG G-watch, Microsoft Band, and Samsung Galaxy S4. We validated our results against an admissible breathalyzer used by the police.\\n  A system based on this concept, successfully detected intoxication and achieved the following results: 0.95 AUC and 0.05 FPR, given a fixed TPR of 1.0. Applications based on our system can be used to analyze the free gait of drinkers when they walk from the car to the bar and vice-versa, in order to alert people, or even a connected car and prevent people from driving under the influence of alcohol.',\n",
       " \"We present magnetic fingerprints; an input technique for mobile touchscreen devices that uses a small magnet attached to a user's fingernail in order to differentiate between a normal touch and a magnetic touch. The polarity of the magnet can be used to create different magnetic fingerprints where this technique takes advantage of the rich vocabulary offered by the use of multitouch input. User studies investigate the accuracy of magnetic fingerprint recognition in relation to magnet size, number of magnetic fingerprints used; and size of the touchscreen. Studies found our technique to be limited to using up to two fingerprints non-simultaneously, while achieving a high classification accuracy (95%) but it nearly triples the number of distinguishable multi touch events. Potential useful applications of this technique are presented.\",\n",
       " \"The interactive machine learning (IML) community aims to augment humans' ability to learn and make decisions over time through the development of automated decision-making systems. This interaction represents a collaboration between multiple intelligent systems---humans and machines. A lack of appropriate consideration for the humans involved can lead to problematic system behaviour, and issues of fairness, accountability, and transparency. This work presents a human-centred thinking approach to applying IML methods. This guide is intended to be used by AI practitioners who incorporate human factors in their work. These practitioners are responsible for the health, safety, and well-being of interacting humans. An obligation of responsibility for public interaction means acting with integrity, honesty, fairness, and abiding by applicable legal statutes. With these values and principles in mind, we as a research community can better achieve the collective goal of augmenting human ability. This practical guide aims to support many of the responsible decisions necessary throughout iterative design, development, and dissemination of IML systems.\",\n",
       " 'The Vim text editor is very rich in capabilities and thus complex. This article is a description of Vim and a set of considerations about its usage and design. It results from more than ten years of experience in using Vim for writing and editing various types of documents, e.g. Python, C++, JavaScript, ChucK programs; \\\\LaTeX, Markdown, HTML, RDF, Make and other markup files; % TTM binary files. It is commonplace, in the Vim users and developers communities, to say that it takes about ten years to master (or start mastering) this text editor, and I find that other experienced users have a different view of Vim and that they use a different set of features. Therefore, this document exposes my understandings in order to confront my usage with that of other Vim users. Another goal is to make available a reference document with which new users can grasp a sound overview by reading it and the discussions that it might generate. Also, it should be useful for users of any degree of experience, including me, as a compendium of commands, namespaces and tweaks. Upon feedback, and maturing of my Vim usage, this document might be enhanced and expanded.',\n",
       " 'We present MedCATTrainer an interface for building, improving and customising a given Named Entity Recognition and Linking (NER+L) model for biomedical domain text. NER+L is often used as a first step in deriving value from clinical text. Collecting labelled data for training models is difficult due to the need for specialist domain knowledge. MedCATTrainer offers an interactive web-interface to inspect and improve recognised entities from an underlying NER+L model via active learning. Secondary use of data for clinical research often has task and context specific criteria. MedCATTrainer provides a further interface to define and collect supervised learning training data for researcher specific use cases. Initial results suggest our approach allows for efficient and accurate collection of research use case specific training data.',\n",
       " 'Dark patterns are user interface design choices that benefit an online service by coercing, steering, or deceiving users into making unintended and potentially harmful decisions. We present automated techniques that enable experts to identify dark patterns on a large set of websites. Using these techniques, we study shopping websites, which often use dark patterns to influence users into making more purchases or disclosing more information than they would otherwise. Analyzing ~53K product pages from ~11K shopping websites, we discover 1,818 dark pattern instances, together representing 15 types and 7 broader categories. We examine these dark patterns for deceptive practices, and find 183 websites that engage in such practices. We also uncover 22 third-party entities that offer dark patterns as a turnkey solution. Finally, we develop a taxonomy of dark pattern characteristics that describes the underlying influence of the dark patterns and their potential harm on user decision-making. Based on our findings, we make recommendations for stakeholders including researchers and regulators to study, mitigate, and minimize the use of these patterns.',\n",
       " \"While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance- and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.\",\n",
       " \"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\",\n",
       " 'The migration of robots from the laboratory into sensitive home settings as commercially available therapeutic agents represents a significant transition for information privacy and ethical imperatives. We present new privacy paradigms and apply the Fair Information Practices (FIPs) to investigate concerns unique to the placement of therapeutic robots in private home contexts. We then explore the importance and utility of research ethics as operationalized by existing human subjects research frameworks to guide the consideration of therapeutic robotic users -- a step vital to the continued research and development of these platforms. Together, privacy and research ethics frameworks provide two complementary approaches to protect users and ensure responsible yet robust information sharing for technology development. We make recommendations for the implementation of these principles -- paying particular attention to specific principles that apply to vulnerable individuals (i.e., children, disabled, or elderly persons)--to promote the adoption and continued improvement of long-term, responsible, and research-enabled robotics in private settings.',\n",
       " 'The fact that emotions play a vital role in social interactions, along with the demand for novel human-computer interaction applications, have led to the development of a number of automatic emotion classification systems. However, it is still debatable whether the performance of such systems can compare with human coders. To address this issue, in this study, we present a comprehensive comparison in a speech-based emotion classification task between 138 Amazon Mechanical Turk workers (Turkers) and a state-of-the-art automatic computer system. The comparison includes classifying speech utterances into six emotions (happy, neutral, sad, anger, disgust and fear), into three arousal classes (active, passive, and neutral), and into three valence classes (positive, negative, and neutral). The results show that the computer system outperforms the naive Turkers in almost all cases. Furthermore, the computer system can increase the classification accuracy by rejecting to classify utterances for which it is not confident, while the Turkers do not show a significantly higher classification accuracy on their confident utterances versus unconfident ones.',\n",
       " 'Event data is present in a variety of domains such as electronic health records, daily living activities and web clickstream records. Current visualization methods to explore event data focus on discovering sequential patterns but present limitations when studying time attributes in event sequences. Time attributes are especially important when studying waiting times or lengths of visit in patient flow analysis. We propose a visual analytics methodology that allows the identification of trends and outliers in respect of duration and time of occurrence in event sequences. The proposed method presents event data using a single Sequential and Time Patterns overview. User-driven alignment by multiple events, sorting by sequence similarity and a novel visual encoding of events allows the comparison of time trends across and within sequences. The proposed visualization allows the derivation of findings that otherwise could not be obtained using traditional visualizations. The proposed methodology has been applied to a real-world dataset provided by Sheffield Teaching Hospitals NHS Foundation Trust, for which four classes of conclusions were derived.',\n",
       " \"Personalized Active Learner (PAL) is a wearable system for real-time, personalized, and context-aware health and cognition support. PAL's system consists of a wearable device, mobile app, cloud database, data visualization web app, and machine learning server. PAL's wearable device uses multi-modal sensors (camera, microphone, heart-rate) with on-device machine learning and open-ear audio output to provide real-time and context-aware cognitive, behavioral and psychological interventions. PAL also allows users to track the long-term correlations between their activities and physiological states to make well-informed lifestyle decisions. In this paper, we present and open-source PAL's system so that people can use it for health and cognition support applications. We also open-source three fully-developed example applications using PAL for face-based memory augmentation, contextual language learning, and heart-rate-based psychological support. PAL's flexible, modular and extensible platform combines trends in data-driven medicine, mobile psychology, and cognitive enhancement to support data-driven and empowering health and cognition applications.\",\n",
       " \"Surgeons must accomplish complex technical and intellectual tasks that can generate unexpected and serious challenges with little or no room for error. In the last decade, computer simulations have played an increasing role in surgical training, pre-operative planning, and biomedical research. Specifically, visuo-haptic simulations have been the focus of research to develop advanced e-Learning systems facilitating surgical training. The cost of haptic hardware was reduced through mass scale production and as haptics gained popularity in the gaming industry. Visuo-haptic simulations combine the tactile sense with visual information and provide training scenarios with a high degree of reality. For surgical training, such scenarios can be used as ways to gain, improve, and assess resident and expert surgeons' skills and knowledge.\",\n",
       " \"People naturally bring their prior beliefs to bear on how they interpret the new information, yet few formal models exist for accounting for the influence of users' prior beliefs in interactions with data presentations like visualizations. We demonstrate a Bayesian cognitive model for understanding how people interpret visualizations in light of prior beliefs and show how this model provides a guide for improving visualization evaluation. In a first study, we show how applying a Bayesian cognition model to a simple visualization scenario indicates that people's judgments are consistent with a hypothesis that they are doing approximate Bayesian inference. In a second study, we evaluate how sensitive our observations of Bayesian behavior are to different techniques for eliciting people subjective distributions, and to different datasets. We find that people don't behave consistently with Bayesian predictions for large sample size datasets, and this difference cannot be explained by elicitation technique. In a final study, we show how normative Bayesian inference can be used as an evaluation framework for visualizations, including of uncertainty.\",\n",
       " \"In today's age of pervasive computing and social media people make extensive use of technology for communicating, sharing media and learning. Yet while in the outdoors, on a hike or a trail we find ourselves inept of information about the natural world surrounding us. In this paper I present in detail the design and technological considerations required to build a location based mobile application for learning about the avian taxonomy present in the user's surroundings. It is designed to be a game for better engagement and learning. The application makes suggestions for birds likely to be sighted in the vicinity of the user and requires the user to spot those birds and upload a photograph to the system. If spotted correctly the user scores points. I also discuss some design methods and evaluation approaches for the application.\",\n",
       " 'This paper presents Living Globe, an application for visualization of demo- graphic data supporting the temporal comparison of data from several countries represented on a 3D globe. Living Globe allows the visual exploration of the following demographic data: total population, population density and growth, crude birth and death rates, life expectancy, net migration and population per- centage of different age groups. While offering unexperienced users a default mapping of these data variables into visual variables, Living Globe allows more advanced users to select the mapping, increasing its flexibility. The main aspects of the Living Globe model and prototype are described as well as the evaluation results obtained using heuristic evaluation and usability testing. Some conclusions and ideas for future work are also presented.',\n",
       " 'We present a new approach to achieve tangible object manipulation with a single, fully portable and self-contained device. Our solution is based on the concept of a \"tangible volume\". We turn a tangible object into a handheld fish-tank display. The tangible volume represents a volume of space that can be freely manipulated within a virtual scene. This volume can be positioned onto virtual objects to directly grasp them, and to manipulate them in 3D space. We investigate this concept through two user studies. The first study evaluates the intuitiveness of using a tangible volume for grasping and manipulating virtual objects. The second study evaluates the effects of the limited field of view on spatial awareness. Finally, we present a generalization of this concept to other forms of interaction through the surface of the volume.',\n",
       " \"Both offline and online human behaviors are affected by personality. Of special interests are online games, where players have to impersonate specific roles and their behaviors are extensively tracked by the game. In this paper, we propose to study the relationship between players' personality and game behavior in League of Legends (LoL), one of the most popular Multiplayer Online Battle Arena (MOBA) games. We use linear mixed effects (LME) models to describe relationships between players' personality traits (measured by the Five Factor Model) and two major aspects of the game: the impersonated roles and in-game actions. On the one hand, we study relationships within the game environment by modeling role attributes from match behaviors and vice versa. On the other hand, we analyze the relationship between a player's five personality traits and their game behavior by showing significant correlations between each personality trait and the set of corresponding behaviors. Our findings suggest that personality and behavior are highly entangled and provide a new perspective to understand how personality can affect behavior in role-based online games.\",\n",
       " 'Virtual and augmented reality communication platforms are seen as promising modalities for next-generation remote face-to-face interactions. Our study attempts to explore non-verbal communication features in relation to their conversation context for virtual and augmented reality mediated communication settings. We perform a series of user experiments, triggering nine conversation tasks in 4 settings, each containing corresponding non-verbal communication features. Our results indicate that conversation types which involve less emotional engagement are more likely to be acceptable in virtual reality and augmented reality settings with low-fidelity avatar representation, compared to scenarios that involve high emotional engagement or intellectually difficult discussions. We further systematically analyze and rank the impact of low-fidelity representation of micro-expressions, body scale, head pose, and hand gesture in affecting the user experience in one-on-one conversations, and validate that preserving micro-expression cues plays the most effective role in improving bi-directional conversations in future virtual and augmented reality settings.',\n",
       " \"The increasing use of electronic forms of communication presents new opportunities in the study of mental health, including the ability to investigate the manifestations of psychiatric diseases unobtrusively and in the setting of patients' daily lives. A pilot study to explore the possible connections between bipolar affective disorder and mobile phone usage was conducted. In this study, participants were provided a mobile phone to use as their primary phone. This phone was loaded with a custom keyboard that collected metadata consisting of keypress entry time and accelerometer movement. Individual character data with the exceptions of the backspace key and space bar were not collected due to privacy concerns. We propose an end-to-end deep architecture based on late fusion, named DeepMood, to model the multi-view metadata for the prediction of mood scores. Experimental results show that 90.31% prediction accuracy on the depression score can be achieved based on session-level mobile phone typing dynamics which is typically less than one minute. It demonstrates the feasibility of using mobile phone metadata to infer mood disturbance and severity.\",\n",
       " 'As the uni-cultural studies of website usability have matured, the paucity of cross-cultural studies of usability become increasingly apparent. Moving toward these cross-cultural studies will require the development of a new tool to assess website usability in the context of cultural dimensions. This paper introduces the preliminary results from the first phase of this project and then presents the proposed method for the research in progress that specifically is directed to the development and quantitative evaluation of a measurement scale of a culture sensitive measurement of website usability. The recognition of the need to develop this scale resulted from the identification of culture-related shortcomings of previous measurement tools that have been used widely within the Management of Information Systems (MIS) literature.',\n",
       " 'We present encube $-$ a qualitative, quantitative and comparative visualisation and analysis system, with application to high-resolution, immersive three-dimensional environments and desktop displays. encube extends previous comparative visualisation systems by considering: 1) the integration of comparative visualisation and analysis into a unified system; 2) the documentation of the discovery process; and 3) an approach that enables scientists to continue the research process once back at their desktop. Our solution enables tablets, smartphones or laptops to be used as interaction units for manipulating, organising, and querying data. We highlight the modularity of encube, allowing additional functionalities to be included as required. Additionally, our approach supports a high level of collaboration within the physical environment. We show how our implementation of encube operates in a large-scale, hybrid visualisation and supercomputing environment using the CAVE2 at Monash University, and on a local desktop, making it a versatile solution. We discuss how our approach can help accelerate the discovery rate in a variety of research scenarios.',\n",
       " 'In image-guided surgical tasks, the precision and timing of hand movements depend on the effectiveness of visual cues relative to specific target areas in the surgeons peri-personal space. Two-dimensional (2D) image views of real-world movements are known to negatively affect both constrained (with tool) and unconstrained(no tool) hand movements compared with direct action viewing. Task conditions where virtual 3D would generate and advantage for surgical eye-hand coordination are unclear. Here, we compared effects of 2D and 3D image views on the precision and timing of surgical hand movement trajectories in a simulator environment. Eight novices had to pick and place a small cube on target areas across different trajectory segments in the surgeons peri-personal space, with the dominant hand, with and without a tool, under conditions of: (1) direct (2) 2D fisheye camera and (3) virtual 3D viewing (headmounted). Significant effects of the location of trajectories in the surgeons peri-personal space on movement times and precision were found. Subjects were faster and more precise across specific target locations, depending on the viewing modality.',\n",
       " 'Incorporating accurate physics-based simulation into interactive design tools is challenging. However, adding the physics accurately becomes crucial to several emerging technologies. For example, in virtual/augmented reality (VR/AR) videos, the faithful reproduction of surrounding audios is required to bring the immersion to the next level. Similarly, as personal fabrication is made possible with accessible 3D printers, more intuitive tools that respect the physical constraints can help artists to prototype designs. One main hurdle is the sheer amount of computation complexity to accurately reproduce the real-world phenomena through physics-based simulation. In my thesis research, I develop interactive tools that implement efficient physics-based simulation algorithms for automatic optimization and intuitive user interaction.',\n",
       " 'Since the launch of Google Glass in 2014, smart glasses have mainly been designed to support micro-interactions. The ultimate goal for them to become an augmented reality interface has not yet been attained due to an encumbrance of controls. Augmented reality involves superimposing interactive computer graphics images onto physical objects in the real world. This survey reviews current research issues in the area of human computer interaction for smart glasses. The survey first studies the smart glasses available in the market and afterwards investigates the interaction methods proposed in the wide body of literature. The interaction methods can be classified into hand-held, touch, and touchless input. This paper mainly focuses on the touch and touchless input. Touch input can be further divided into on-device and on-body, while touchless input can be classified into hands-free and freehand. Next, we summarize the existing research efforts and trends, in which touch and touchless input are evaluated by a total of eight interaction goals. Finally, we discuss several key design challenges and the possibility of multi-modal input for smart glasses.',\n",
       " 'In this paper, we present MVC-3D design pattern to develop virtual and augmented (or mixed) reality interfaces that use new types of sensors, modalities and implement specific algorithms and simulation models. The proposed pattern represents the extension of classic MVC pattern by enriching the View component (interactive View) and adding a specific component (Library). The results obtained on the development of augmented reality interfaces showed that the complexity of M, iV and C components is reduced. The complexity increases only on the Library component (L). This helps the programmers to well structure their models even if the interface complexity increases. The proposed design pattern is also used in a design process called MVC-3D in the loop that enables a seamless evolution from initial prototype to the final system.',\n",
       " 'We describe the design and implementation of a vision based interactive entertainment system that makes use of both involuntary and voluntary control paradigms. Unintentional input to the system from a potential viewer is used to drive attention-getting output and encourage the transition to voluntary interactive behaviour. The iMime system consists of a character animation engine based on the interaction metaphor of a mime performer that simulates non-verbal communication strategies, without spoken dialogue, to capture and hold the attention of a viewer. The system was developed in the context of a project studying care of dementia sufferers. Care for a dementia sufferer can place unreasonable demands on the time and attentional resources of their caregivers or family members. Our study contributes to the eventual development of a system aimed at providing relief to dementia caregivers, while at the same time serving as a source of pleasant interactive entertainment for viewers. The work reported here is also aimed at a more general study of the design of interactive entertainment systems involving a mixture of voluntary and involuntary control.',\n",
       " \"Video watching had emerged as one of the most frequent media activities on the Internet. Yet, little is known about how users watch online video. Using two distinct YouTube datasets, a set of random YouTube videos crawled from the Web and a set of videos watched by participants tracked by a Chrome extension, we examine whether and how indicators of collective preferences and reactions are associated with view duration of videos. We show that video view duration is positively associated with the video's view count, the number of likes per view, and the negative sentiment in the comments. These metrics and reactions have a significant predictive power over the duration the video is watched by individuals. Our findings provide a more precise understandings of user engagement with video content in social media beyond view count.\",\n",
       " 'Expertise of annotators has a major role in crowdsourcing based opinion aggregation models. In such frameworks, accuracy and biasness of annotators are occasionally taken as important features and based on them priority of the annotators are assigned. But instead of relying on a single feature, multiple features can be considered and separate rankings can be produced to judge the annotators properly. Finally, the aggregation of those rankings with perfect weightage can be done with an aim to produce better ground truth prediction. Here, we propose a novel weighted rank aggregation method and its efficacy with respect to other existing approaches is shown on artificial dataset. The effectiveness of weighted rank aggregation to enhance quality prediction is also shown by applying it on an Amazon Mechanical Turk (AMT) dataset.',\n",
       " 'Our paper contributes to the literature recommending approaches to make online reviews more credible and representative. We analyze data from four diverse major online retailers and find that verified customers who are prompted (by an email) to write a review, submit, on average, up to 0.5 star higher ratings than self-motivated web reviewers. Moreover, these email-prompted reviews remain stable over time, whereas web reviews exhibit a downward trend. This finding provides support for the existence of social influence and selection biases during the submission of a web review, when social signals are being displayed. In contrast, no information about the current state of the reviews is displayed in the email promptings. Moreover, we find that when a retailer decides to start sending email promptings, the existing population of web reviewers is unaffected both in their volume as well as the characteristics of their submitted reviews. We explore how our combined findings can suggest ways to mitigate various biases that govern online review submissions and help practitioners provide more credible, representative and higher ratings to their customers.',\n",
       " 'When creating 3D city models, selecting relevant visualization techniques is a particularly difficult user interface design task. A first obstacle is that current geodata-oriented tools, e.g. ArcGIS, have limited 3D capabilities and limited sets of visualization techniques. Another important obstacle is the lack of unified description of information visualization techniques for 3D city models. If many techniques have been devised for different types of data or information (wind flows, air quality fields, historic or legal texts, etc.) they are generally described in articles, and not really formalized. In this paper we address the problem of visualizing information in (rich) 3D city models by presenting a model-based approach for the rapid prototyping of visualization techniques. We propose to represent visualization techniques as the composition of graph transformations. We show that these transformations can be specified with SPARQL construction operations over RDF graphs. These specifications can then be used in a prototype generator to produce 3D scenes that contain the 3D city model augmented with data represented using the desired technique.',\n",
       " 'Autism Spectrum Disorder (ASD) is an umbrella term for a wide range of developmental disorders. For the past two decades, researchers proposed the use of various technologies in order to tackle specific symptoms of the disorder. Although there exist many literature reviews about screening, assessment, and rehabilitation of ASD, no comprehensive survey of types of technologies in all defined symptoms of ASD has been presented. Therefore, in this paper a comprehensive survey of previous studies has been presented in which the studies are classified into three main categories, and several sub-categories, and three main technologies. An analysis of the number of studies in each category and sub-category is given to help researchers decide on areas which need further investigation. The analysis show that the majority of studies fall into the software-based systems technology category. Finally, a brief review of studies in each category of ASD is presented for each type of technology. As a result, this paper also helps researchers to obtain an overview of the typical methods of using a specific technology in ASD screening, assessment, and rehabilitation.',\n",
       " 'Non-linear dimensionality reduction (NDR) methods such as LLE and t-SNE are popular with visualization researchers and experienced data analysts, but present serious problems of interpretation. In this paper, we present DimReader, a technique that recovers readable axes from such techniques. DimReader is based on analyzing infinitesimal perturbations of the dataset with respect to variables of interest. The perturbations define exactly how we want to change each point in the original dataset and we measure the effect that these changes have on the projection. The recovered axes are in direct analogy with the axis lines (grid lines) of traditional scatterplots. We also present methods for discovering perturbations on the input data that change the projection the most. The calculation of the perturbations is efficient and easily integrated into programs written in modern programming languages. We present results of DimReader on a variety of NDR methods and datasets both synthetic and real-life, and show how it can be used to compare different NDR methods. Finally, we discuss limitations of our proposal and situations where further research is needed.',\n",
       " 'Maps --- specifically floor plans --- are useful for a variety of tasks from arranging furniture to designating conceptual or functional spaces (e.g., kitchen, walkway). We present a simple algorithm for quickly laying a floor plan (or other conceptual map) onto a SLAM map, creating a one-to-one mapping between them. Our goal was to enable using a floor plan (or other hand-drawn or annotated map) in robotic applications instead of the typical SLAM map created by the robot. We look at two use cases, specifying \"no-go\" regions within a room and locating objects within a scanned room. Although a user study showed no statistical difference between the two types of maps in terms of performance on this spatial memory task, we argue that floor plans are closer to the mental maps people would naturally draw to characterize spaces.',\n",
       " \"Spreadsheet users regularly deal with uncertainty in their data, for example due to errors and estimates. While an insight into data uncertainty can help in making better informed decisions, prior research suggests that people often use informal heuristics to reason with probabilities, which leads to incorrect conclusions. Moreover, people often ignore or simplify uncertainty. To understand how people currently encounter and deal with uncertainty in spreadsheets, we conducted an interview study with 11 spreadsheet users from a range of domains. We found that how people deal with uncertainty is influenced by the role the spreadsheet plays in people's work and the user's aims. Spreadsheets are used as a database, template, calculation tool, notepad and exploration tool. In doing so, participants' aims were to compute and compare different scenarios, understand something about the nature of the uncertainty in their situation, and translate the complexity of data uncertainty into simplified presentations to other people, usually decision-makers. Spreadsheets currently provide limited tools to support these aims, and participants had various workarounds.\",\n",
       " 'Event-related potentials (ERPs) are very small voltage produced by the brain in response to external stimulation. In order to detect and evaluate an ERP in an ongoing electroencephalogram (EEG), it is necessary to tag the EEG with the exact onset time of the stimulus. We define the latency as the delay between the time the tagging command is sent and the detection of the stimulus on the screen. Failing to control sequencing in the tagging pipeline causes problems when interpreting latency, in particular when comparing ERPs generated from stimuli displayed by different systems. In this work, we present number of technical aspects which can influence latency such as the refresh rate of the screen or the display of a stimulus at different screen location. A few propositions are suggested to estimate and correct this latency.',\n",
       " \"In this paper we describe and evaluate a mixed reality system that aims to augment users in task guidance applications by combining automated and unsupervised information collection with minimally invasive video guides. The result is a self-contained system that we call GlaciAR (Glass-enabled Contextual Interactions for Augmented Reality), that operates by extracting contextual interactions from observing users performing actions. GlaciAR is able to i) automatically determine moments of relevance based on a head motion attention model, ii) automatically produce video guidance information, iii) trigger these video guides based on an object detection method, iv) learn without supervision from observing multiple users and v) operate fully on-board a current eyewear computer (Google Glass). We describe the components of GlaciAR together with evaluations on how users are able to use the system to achieve three tasks. We see this work as a first step toward the development of systems that aim to scale up the notoriously difficult authoring problem in guidance systems and where people's natural abilities are enhanced via minimally invasive visual guidance.\",\n",
       " 'Automatic optimization of spoken dialog management policies that are robust to environmental noise has long been the goal for both academia and industry. Approaches based on reinforcement learning have been proved to be effective. However, the numerical representation of dialog policy is human-incomprehensible and difficult for dialog system designers to verify or modify, which limits its practical application. In this paper we propose a novel framework for optimizing dialog policies specified in domain language using genetic algorithm. The human-interpretable representation of policy makes the method suitable for practical employment. We present learning algorithms using user simulation and real human-machine dialogs respectively.Empirical experimental results are given to show the effectiveness of the proposed approach.',\n",
       " \"Game-based technologies and mobile learning aids open up many opportunities for learners; however, evidence-based decisions on their appropriate use are necessary. This explorative study (N = 100) examines the role of game elements in university education using a game-based learning app for mobile devices. The educational goal of the app is to support students in the field of engineering to memorize factual knowledge. The study investigates how the game-based app affects learners' motivation. It analyses the perceived impact and appeal as well as the game elements as an incentive in learners' perception. To realize this aim, the study combines structured methods like questionnaires with semi-structured methods like thinking aloud, game diaries, and interviews. The results indicate that flexible tem-poral and spatial use of the app was an important factor of learners' motivation. The app allowed more spontaneous involvement with the subject matter and the learners took advantage of an improved attitude toward the subject matter. However, only a low impact on intrinsic motivation could be observed. We discuss reasons and present practical implications.\",\n",
       " 'Human perception of surrounding events is strongly dependent on audio cues. Thus, acoustic insulation can seriously impact situational awareness. We present an exploratory study in the domain of assistive computing, eliciting requirements and presenting solutions to problems found in the development of an environmental sound recognition system, which aims to assist deaf and hard of hearing people in the perception of sounds. To take advantage of smartphones computational ubiquity, we propose a system that executes all processing on the device itself, from audio features extraction to recognition and visual presentation of results. Our application also presents the confidence level of the classification to the user. A test of the system conducted with deaf users provided important and inspiring feedback from participants.',\n",
       " 'This research aims to quantify human walking patterns through depth cameras to (1) detect walking pattern changes of a person with and without a motion-restricting device or a walking aid, and to (2) identify distinct walking patterns from different persons of similar physical attributes. Microsoft Kinect devices, often used for video games, were used to provide and track coordinates of 25 different joints of people over time to form a human skeleton. Then multiple machine learning (ML) models were applied to the SE datasets from ten college-age subjects - five males and five females. In particular, ML models were applied to classify subjects into two categories: normal walking and abnormal walking (i.e. with motion-restricting devices). The best ML model (K-nearest neighborhood) was able to predict 97.3% accuracy using 10-fold cross-validation. Finally, ML models were applied to classify five gait conditions: walking normally, walking while wearing the ankle brace, walking while wearing the ACL brace, walking while using a cane, and walking while using a walker. The best ML model was again the K-nearest neighborhood performing at 98.7% accuracy rate.',\n",
       " 'Three-dimension will be a characteristic of future user interfaces, although we are just starting to gain an understanding of how users can navigate and share information within a virtual 3D environment. Three-dimensional graphical user interfaces (3D-GUI) raise many issues of design, metaphor and usability. This research is devoted to designing a 3D-GUI as a front-end tool for a file management system, in this case, for Microsoft Windows\\\\c{opyright} Explorer; as well as evaluating the efficiency of a 3D application. The software design was implemented by extending the Half-Life 3D engine. This extension provides a directory traversal and basic file management functions, like cut, copy, paste, delete, and so on. This paper shows the design and implementation of a real-world application that contains an efficient 3D-GUI.',\n",
       " 'Developers are usually unaware of the impact of code changes to the performance of software systems. Although developers can analyze the performance of a system by executing, for instance, a performance test to compare the performance of two consecutive versions of the system, changing from a programming task to a testing task would disrupt the development flow. In this paper, we propose the use of a city visualization that dynamically provides developers with a pervasive view of the continuous performance of a system. We use an immersive augmented reality device (Microsoft HoloLens) to display our visualization and extend the integrated development environment on a computer screen to use the physical space. We report on technical details of the design and implementation of our visualization tool, and discuss early feedback that we collected of its usability. Our investigation explores a new visual metaphor to support the exploration and analysis of possibly very large and multidimensional performance data. Our initial result indicates that the city metaphor can be adequate to analyze dynamic performance data on a large and non-trivial software system.',\n",
       " 'In situ self-report is widely used in human-computer interaction, ubiquitous computing, and for assessment and intervention in health and wellness. Unfortunately, it remains limited by high burdens. We examine unlock journaling as an alternative. Specifically, we build upon recent work to introduce single slide unlock journaling gestures appropriate for health and wellness measures. We then present the first field study comparing unlock journaling with traditional diaries and notification based reminders in self report of health and wellness measures. We find unlock journaling is less intrusive than reminders, dramatically improves frequency of journaling, and can provide equal or better timeliness. Where appropriate to broader design needs, unlock journaling is thus an overall promising method for in situ self report.',\n",
       " 'Building a deployable PhysiComp that merges form and function typically involves a significant investment of time and skill in digital electronics, 3D modeling and mechanical design. We aim to help designers quickly create prototypes by removing technical barriers in that process. Other methods for constructing PhysiComp prototypes either lack fidelity in representing shape and function or are confined to use in the studio next to a workstation, camera or projector system. Software 3D CAD tools can be used to design the shape but do not provide immediate tactile feedback on fit and feel. In this work, sculpting around 3D printed replicas of electronics combines electronics and form in a fluid design environment. The sculptures are scanned, modified for assembly and then printed on a 3D printer. Using this process, functional prototypes can be created with about 4 hours of focused effort over a day and a half with most of that time spent waiting for the 3D printer. The process lends itself to concurrent exploration of several designs and to rapid iteration. This allows the design process to converge quickly to a PhysiComp that is comfortable and useful.',\n",
       " 'Objectives: This paper presents an up-to-date overview of research performed in the Virtual Reality (VR) environment ranging from definitions, its presence in the various fields, and existing market players and their projects in the VR technology. Further an attempt is made to gain an insight on the psychological mechanism underlying experience in using VR device. Methods: Our literature survey is based on the research articles, analysis of the projects of various companies and their findings for different areas of interest. Findings: In our literature survey we observed that the recent advances in virtual reality enabling technologies have led to variety of virtual devices that facilitate people to interact with the digital world. In fact in the past two decades researchers have tried to integrate reality and VR in the form of intuitive computer interface. Improvements: This has led to variety of potential benefits of VR in many applications such as News, Healthcare, Entertainment, Tourism, Military and Defence etc. However despite the extensive research efforts in creating virtual system environments it is yet to become apparent in normal daily life.',\n",
       " 'Based on a large data set of emoji using behavior collected from smartphone users over the world, this paper investigates gender-specific usage of emojis. We present various interesting findings that evidence a considerable difference in emoji usage by female and male users. Such a difference is significant not just in a statistical sense; it is sufficient for a machine learning algorithm to accurately infer the gender of a user purely based on the emojis used in their messages. In real world scenarios where gender inference is a necessity, models based on emojis have unique advantages over existing models that are based on textual or contextual information. Emojis not only provide language-independent indicators, but also alleviate the risk of leaking private user information through the analysis of text and metadata.',\n",
       " 'Wearable cameras allow people to record their daily activities from a user-centered (First Person Vision) perspective. Due to their favorable location, wearable cameras frequently capture the hands of the user, and may thus represent a promising user-machine interaction tool for different applications. Existent First Person Vision methods handle hand segmentation as a background-foreground problem, ignoring two important facts: i) hands are not a single \"skin-like\" moving element, but a pair of interacting cooperative entities, ii) close hand interactions may lead to hand-to-hand occlusions and, as a consequence, create a single hand-like segment. These facts complicate a proper understanding of hand movements and interactions. Our approach extends traditional background-foreground strategies, by including a hand-identification step (left-right) based on a Maxwell distribution of angle and position. Hand-to-hand occlusions are addressed by exploiting temporal superpixels. The experimental results show that, in addition to a reliable left/right hand-segmentation, our approach considerably improves the traditional background-foreground hand-segmentation.',\n",
       " 'Despite significant research that was influenced by Situationally-Induced Impairments and Disabilities (SIIDs) to improve the accessibility of mobile technology, there is still lack of awareness on how to design for SIIDs. Designing for situational impairments does not only affect usability for people who have temporary or long-term disabilities, but also for the \"ideal\" users who get impacted. Limited resources on how to design for situational impairments overlook inclusive interactions and hinder the creation of accessible technology. Thus, I am going to create method cards that can be used during the design process to figure out how to get designers to design for SIIDs. These method cards help us better understand how to improve the design process by addressing the subject of how to design in order to reduce SIIDs.',\n",
       " \"Recent research has exposed disagreements over the nature and usefulness of what may (or may not) be Human-Computer Interaction's fundamental phenomenon: 'interaction'. For some, HCI's theorising about interaction has been deficient, impacting its capacity to inform decisions in design, suggesting the need either to perform first-principles definition work or broader administrative clarification and formalisation of the multitude of formulations of the concepts of interaction and their particular uses. For others, there remain open questions over the continued relevance of certain 'versions' of interaction as a useful concept in HCI at all. We pursue a different perspective in this paper, reviewing how HCI treats interaction through examining its 'conceptual pragmatics' within HCI's discourse. We argue that articulations of the concepts of interaction can be a site of productive conflict for HCI that for many reasons may resist attempts of formalisation as well as attempts to dispense with them. The main contribution of this paper is in specifying how we might go about talking of interaction and the value of interaction language as promiscuous concepts.\",\n",
       " \"This paper presents a game based on storytelling, in which the players are faced with ethical dilemmas related to software engineering specific issues. The players' choices have consequences on how the story unfolds and could lead to various alternative endings. This Ethics Game was used as a tool to mediate the learning activity and it was evaluated by 144 students during a Software Engineering Course on the 2017-2018 academic year. This evaluation was based on a within-subject pre-post design methodology and provided insights on the students learning gain (academic performance), as well as on the students' perceived educational experience. In addition, it provided the results of the students' usability evaluation of the Ethics Game. The results indicated that the students did improve their knowledge about software engineering ethics by playing this game. Also, they considered this game to be a useful educational tool and of high usability. Female students had statistically significant higher knowledge gain and higher evaluation scores than male students, while no statistically significant differences were measured in groups based on the year of study.\",\n",
       " 'Activity tracking devices have found its way in the world of cycling. With its projected market demand and increasing popularity of cycling in the Philippines, cyclists are slowly adopting this technology in their daily cycling routines. Activity trackers demonstrate real-time data which allow cyclists to adjust physical efforts to achieve their personal goals. Six common features of activity trackers were formed as constructs to explore its influence on health empowerment in the context of cycling using Partial Least Squares Structural Equation Model. A total of 393 cyclists in the Philippines participated in the study. Some features demonstrated strong evidence of positive influence in achieving health empowerment and commitment. Implications for future design and development of this technology device are discussed.',\n",
       " \"In this paper, we present a tool to assess users ability to change tasks. To do this, we use a variation of the Box and Blocks Test. In this version, a humanoid robot instructs a user to perform a task involving the movement of certain colored blocks. The robot changes randomly change the color of blocks that the user is supposed to move. Canny Edge Detection and Hough Transformation are used to assess user perform the robot's built-in camera. This will allow the robot to inform the user and keep a log of their progress. We present this method for monitoring user progress by describing how the moved blocks are detected. We also present the results of a pilot study where users used this system to perform the task. Preliminary results show that users do not perform differently when the task is changed in this scenario.\",\n",
       " 'In this paper, we investigate the effectiveness of two distinct techniques (Special Moment Approach & Spatial Frequency Approach) for reviewing the lifelogs, which were collected by lifeloggers who were willing to use a wearable camera and a bracelet simultaneously for two days. Generally, Special moment approach is a technique for extracting episodic events and Spatial frequency approach is a technique for associating visual with temporal and location information, especially heat map is applied as the spatial data for expressing frequency awareness. Based on that, the participants were asked to fill in two post-study questionnaires for evaluating the effectiveness of those two techniques and their combination. The preliminary result showed the positive potential of exploring individual lifelogs using our approaches.',\n",
       " \"Online learners spend millions of hours per year testing their new skills on assignments with known answers. This paper explores whether framing research questions as assignments with unknown answers helps learners generate novel, useful, and difficult-to-find knowledge while increasing their motivation by contributing to a larger goal. Collaborating with the American Gut Project, the world's largest crowdfunded citizen science project, we deploy Gut Instinct to allow novices to generate hypotheses about the constitution of the human gut microbiome. The tool enables online learners to explore learning material about the microbiome and create their own theories around causal variances for microbiome. Building on crowdsourcing or serious games that use people as replaceable units, this work-in-progress lays our plans for how people (a) use their personal knowledge (b) towards solving a larger real-world goal (c) that can provide potential benefits to them. We hope to demonstrate that Gut Instinct citizen scientists generate useful hypotheses, perform better on learning tasks than traditional MOOC learners, and are better engaged with the learning material.\",\n",
       " 'The number of user reviews of tourist attractions, restaurants, mobile apps, etc. is increasing for all languages; yet, research is lacking on how reviews in multiple languages should be aggregated and displayed. Speakers of different languages may have consistently different experiences, e.g., different information available in different languages at tourist attractions or different user experiences with software due to internationalization/localization choices. This paper assesses the similarity in the ratings given by speakers of different languages to London tourist attractions on TripAdvisor. The correlations between different languages are generally high, but some language pairs are more correlated than others. The results question the common practice of computing average ratings from reviews in many languages.',\n",
       " 'While emerging deep-learning systems have outclassed knowledge-based approaches in many tasks, their application to detection tasks for autonomous technologies remains an open field for scientific exploration. Broadly, there are two major developmental bottlenecks: the unavailability of comprehensively labeled datasets and of expressive evaluation strategies. Approaches for labeling datasets have relied on intensive hand-engineering, and strategies for evaluating learning systems have been unable to identify failure-case scenarios. Human intelligence offers an untapped approach for breaking through these bottlenecks. This paper introduces Driverseat, a technology for embedding crowds around learning systems for autonomous driving. Driverseat utilizes crowd contributions for (a) collecting complex 3D labels and (b) tagging diverse scenarios for ready evaluation of learning systems. We demonstrate how Driverseat can crowdstrap a convolutional neural network on the lane-detection task. More generally, crowdstrapping introduces a valuable paradigm for any technology that can benefit from leveraging the powerful combination of human and computer intelligence.',\n",
       " 'The efficient and timely access to patient data is essential for successful patient treatment in clinical wards. Even though, most of the patient data is stored electronically, e.g. in the electronic health record (EHR), relevant patient information is frequently captured, processed and passed in non-electronic form in day-to-day ward routines, e.g. during ward rounds or at shift handover. Following a disruptive design mode, we present a design and development approach comprising a fundamental visualization concept that is refined in a feedback-loop between computer scientists, sociologists and physicians in order to support peri-operative collaborative workflows on a neurosurgical hospital ward. The resulting prototype realizes on-patient visualization methods using an anatomical avatar. It allows for the visual access of medical data with spatial reference relevant for spinal disc herniation and it handles various quantitative and qualitative medical data related to the same anatomical structure as well as referring to hidden and/or small anatomical structures with limited detectability. Furthermore, temporal changes of medical data are made accessible.',\n",
       " 'Today, many of the home automation systems deployed are mostly controlled by humans. This control by humans restricts the automation of home appliances to an extent. Also, most of the deployed home automation systems use the Internet of Things technology to control the appliances. In this paper, we propose a system developed using action recognition to fully automate the home appliances. We recognize the three actions of a person (sitting, standing and lying) along with the recognition of an empty room. The accuracy of the system was 90% in the real-life test experiments. With this system, we remove the human intervention in home automation systems for controlling the home appliances and at the same time we ensure the data privacy and reduce the energy consumption by efficiently and optimally using home appliances.',\n",
       " 'Differential privacy is a promising framework for addressing the privacy concerns in sharing sensitive datasets for others to analyze. However differential privacy is a highly technical area and current deployments often require experts to write code, tune parameters, and optimize the trade-off between the privacy and accuracy of statistical releases. For differential privacy to achieve its potential for wide impact, it is important to design usable systems that enable differential privacy to be used by ordinary data owners and analysts. PSI is a tool that was designed for this purpose, allowing researchers to release useful differentially private statistical information about their datasets without being experts in computer science, statistics, or privacy. We conducted a thorough usability study of PSI to test whether it accomplishes its goal of usability by non-experts. The usability test illuminated which features of PSI are most user-friendly and prompted us to improve aspects of the tool that caused confusion. The test also highlighted some general principles and lessons for designing usable systems for differential privacy, which we discuss in depth.',\n",
       " 'Analyzing large, multivariate graphs is an important problem in many domains, yet such graphs are challenging to visualize. In this paper, we introduce a novel, scalable, tree+table multivariate graph visualization technique, which makes many tasks related to multivariate graph analysis easier to achieve. The core principle we follow is to selectively query for nodes or subgraphs of interest and visualize these subgraphs as a spanning tree of the graph. The tree is laid out linearly, which enables us to juxtapose the nodes with a table visualization where diverse attributes can be shown. We also use this table as an adjacency matrix, so that the resulting technique is a hybrid node-link/adjacency matrix technique. We implement this concept in Juniper and complement it with a set of interaction techniques that enable analysts to dynamically grow, restructure, and aggregate the tree, as well as change the layout or show paths between nodes. We demonstrate the utility of our tool in usage scenarios for different multivariate networks: a bipartite network of scholars, papers, and citation metrics and a multitype network of story characters, places, books, etc.',\n",
       " 'We build on the increasing availability of Virtual Reality (VR) devices and Web technologies to conduct behavioral experiments in VR using crowdsourcing techniques. A new recruiting and validation method allows us to create a panel of eligible experiment participants recruited from Amazon Mechanical Turk. Using this panel, we ran three different crowdsourced VR experiments, each reproducing one of three VR illusions: place illusion, embodiment illusion, and plausibility illusion. Our experience and worker feedback on these experiments show that conducting Web-based VR experiments using crowdsourcing is already feasible, though some challenges---including scale---remain. Such crowdsourced VR experiments on the Web have the potential to finally support replicable VR experiments with diverse populations at a low cost.',\n",
       " 'Visualizing a complex network is computationally intensive process and depends heavily on the number of components in the network. One way to solve this problem is not to render the network in real time. PRE-render Content Using Tiles (PRECUT) is a process to convert any complex network into a pre-rendered network. Tiles are generated from pre-rendered images at different zoom levels, and navigating the network simply becomes delivering relevant tiles. PRECUT is exemplified by performing large-scale compound-target relationship analyses. Matched molecular pair (MMP) networks were created using compounds and the target class description found in the ChEMBL database. To visualize MMP networks, the MMP network viewer has been implemented in COMBINE and as a web application, hosted at http://cheminformatic.com/mmpnet/.',\n",
       " 'In this paper, we present CrowdTone, a system designed to help people set the appropriate tone in their email communication. CrowdTone utilizes the context and content of an email message to identify and set the appropriate tone through a consensus-building process executed by crowd workers. We evaluated CrowdTone with 22 participants, who provided a total of 29 emails that they had received in the past, and ran them through CrowdTone. Participants and professional writers assessed the quality of improvements finding a substantial increase in the percentage of emails deemed \"appropriate\" or \"very appropriate\" - from 25% to more than 90% by recipients, and from 45% to 90% by professional writers. Additionally, the recipients\\' feedback indicated that more than 90% of the CrowdTone processed emails showed improvement.',\n",
       " 'Good code quality is a prerequisite for efficiently developing maintainable software. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that report code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept could generalize to multivariate data and report lessons learned in a broader scope.',\n",
       " 'PeyeDF is a Portable Document Format (PDF) reader with eye tracking support, available as free and open source software. It is especially useful to researchers investigating reading and learning phenomena, as it integrates PDF reading-related behavioural data with gaze-related data. It is suitable for short and long-term research and supports multiple eye tracking systems. We utilised it to conduct an experiment which demonstrated that features obtained from both gaze and reading data collected in the past can predict reading comprehension which takes place in the future. PeyeDF also provides an integrated means for data collection and indexing using the DiMe personal data storage system. It is designed to collect data in the background without interfering with the reading experience, behaving like a modern lightweight PDF reader. Moreover, it supports annotations, tagging and collaborative work. A modular design allows the application to be easily modified in order to support additional eye tracking protocols and run controlled experiments. We discuss the implementation of the software and report on the results of the experiment which we conducted with it.',\n",
       " 'We present a framework to identify whether a public speaker\\'s body movements are meaningful or non-meaningful (\"Mannerisms\") in the context of their speeches. In a dataset of 84 public speaking videos from 28 individuals, we extract 314 unique body movement patterns (e.g. pacing, gesturing, shifting body weights, etc.). Online workers and the speakers themselves annotated the meaningfulness of the patterns. We extracted five types of features from the audio-video recordings: disfluency, prosody, body movements, facial, and lexical. We use linear classifiers to predict the annotations with AUC up to 0.82. Analysis of the classifier weights reveals that it puts larger weights on the lexical features while predicting self-annotations. Contrastingly, it puts a larger weight on prosody features while predicting audience annotations. This analysis might provide subtle hint that public speakers tend to focus more on the verbal features while evaluating self-performances. The audience, on the other hand, tends to focus more on the non-verbal aspects of the speech. The dataset and code associated with this work has been released for peer review and further analysis.',\n",
       " 'Data visualizations typically show retrospective views of an existing dataset with little or no focus on repeatability. However, consumers of these tools often use insights gleaned from retrospective visualizations as the basis for decisions about future events. In this way, visualizations often serve as visual predictive models despite the fact that they are typically designed to present historical views of the data. This \"visual predictive model\" approach, however, can lead to invalid inferences. In this paper, we describe an approach to visual model validation called Inline Replication (IR) which, similar to the cross-validation technique used widely in machine learning, provides a nonparametric and broadly applicable technique for visual model assessment and repeatability. This paper describes the overall IR process and outlines how it can be integrated into both traditional and emerging \"big data\" visualization pipelines. Examples are provided showing IR integrated within common visualization techniques (such as bar charts and linear regression lines) as well as a more fully-featured visualization system designed for complex exploratory analysis tasks.',\n",
       " 'Feedback has been shown to affect performance when using a Brain-Computer Interface (BCI) based on sensorimotor rhythms. In contrast, little is known about the influence of feedback on P300-based BCIs. There is still an open question whether feedback affects the regulation of P300 and consequently the operation of P300-based BCIs. In this paper, for the first time, the influence of feedback on the P300-based BCI speller task is systematically assessed. For this purpose, 24 healthy participants performed the classic P300-based BCI speller task, while only half of them received feedback. Importantly, the number of flashes per letter was reduced on a regular basis in order to increase the frequency of providing feedback. Experimental results showed that feedback could significantly improve the P300-based BCI speller performance, if it was provided in short time intervals (e.g. in sequences as short as 4 to 6 flashes per row/column). Moreover, our offline analysis showed that providing feedback remarkably enhanced the relevant ERP patterns and attenuated the irrelevant ERP patterns, such that the discrimination between target and nontarget EEG trials increased.',\n",
       " 'In this paper we present a method which aims to improve the spelling of children with dyslexia through playful and targeted exercises. In contrast to previous approaches, our method does not use correct words or positive examples to follow, but presents the child a misspelled word as an exercise to solve. We created these training exercises on the basis of the linguistic knowledge extracted from the errors found in texts written by children with dyslexia. To test the effectiveness of this method in Spanish, we integrated the exercises in a game for iPad, DysEggxia (Piruletras in Spanish), and carried out a within-subject experiment. During eight weeks, 48 children played either DysEggxia or Word Search, which is another word game. We conducted tests and questionnaires at the beginning of the study, after four weeks when the games were switched, and at the end of the study. The children who played DysEggxia for four weeks in a row had significantly less writing errors in the tests that after playing Word Search for the same time. This provides evidence that error-based exercises presented in a tablet help children with dyslexia improve their spelling skills.',\n",
       " 'P300 is an electric signal emitted by brain about 300 milliseconds after a rare, but relevant-for-the-user event. One of the applications of this signal is sentence spelling that enables subjects who lost the control of their motor pathways to communicate by selecting characters in a matrix containing all the alphabet symbols. Although this technology has made considerable progress in the last years, it still suffers from both low communication rate and high error rate. This article presents a P300 speller, named PolyMorph, that introduces two major novelties in the field: the selection matrix polymorphism, that reduces the size of the selection matrix itself by removing useless symbols, and sentence-based predictions, that exploit all the spelt characters of a sentence to determine the probability of a word. In order to measure the effectiveness of the presented speller, we describe two sets of tests: the first one in vivo and the second one in silico. The results of these experiments suggest that the use of PolyMorph in place of the naive character-by-character speller both increases the number of spelt characters per time unit and reduces the error rate.',\n",
       " \"Digital prototyping and evaluation using 3D modeling and digital human models are becoming more practical for customizing products to the preference of a user. However, the 3D modeling is less accessible to casual users, and digital human models suffer from insufficient body data and less intuitive illustration on how people use the product or how it accommodates to their body. Recently, VR-supported 'Do It Yourself' design has achieved real-time ergonomic evaluation with users themselves by capturing their poses, however, it lacks reliability and quality of design. In this paper, we explore a multi-person interactive design approach that enables designer, user, and even ergonomist to collaborate to achieve the effective and reliable design and prototyping tasks. Mixed Reality that utilizes Hololens and motion tracking devices had been developed to provide instant design feedback and evaluation, and to experience prototyping in physical space. We evaluate the system based on the usability study, where casual users and designers are engaged in the interactive process of designing items with respect to the body information, the preference, and the environment.\",\n",
       " \"Consider the following problem faced by an online voting platform: A user is provided with a list of alternatives, and is asked to rank them in order of preference using only drag-and-drop operations. The platform's goal is to recommend an initial ranking that minimizes the time spent by the user in arriving at her desired ranking. We develop the first optimization framework to address this problem, and make theoretical as well as practical contributions. On the practical side, our experiments on Amazon Mechanical Turk provide two interesting insights about user behavior: First, that users' ranking strategies closely resemble selection or insertion sort, and second, that the time taken for a drag-and-drop operation depends linearly on the number of positions moved. These insights directly motivate our theoretical model of the optimization problem. We show that computing an optimal recommendation is NP-hard, and provide exact and approximation algorithms for a variety of special cases of the problem. Experimental evaluation on MTurk shows that, compared to a random recommendation strategy, the proposed approach reduces the (average) time-to-rank by up to 50%.\",\n",
       " 'We describe the experimental procedures for a dataset that we have made publicly available at https://doi.org/10.5281/zenodo.2649006 in mat and csv formats. This dataset contains electroencephalographic (EEG) recordings of 25 subjects testing the Brain Invaders (Congedo, 2011), a visual P300 Brain-Computer Interface inspired by the famous vintage video game Space Invaders (Taito, Tokyo, Japan). The visual P300 is an event-related potential elicited by a visual stimulation, peaking 240-600 ms after stimulus onset. EEG data were recorded by 16 electrodes in an experiment that took place in the GIPSA-lab, Grenoble, France, in 2012 (Van Veen, 2013 and Congedo, 2013). Python code for manipulating the data is available at https://github.com/plcrodrigues/py.BI.EEG.2012-GIPSA. The ID of this dataset is BI.EEG.2012-GIPSA.',\n",
       " 'Security products often create more problems than they solve, drowning users in alerts without providing the context required to remediate threats. This challenge is compounded by a lack of experienced personnel and security tools with complex interfaces. These interfaces require users to become domain experts or rely on repetitive, time consuming tasks to turn this data deluge into actionable intelligence. In this paper we present Artemis, a conversational interface to endpoint detection and response (EDR) event data. Artemis leverages dialog to drive the automation of complex tasks and reduce the need to learn a structured query language. Designed to empower inexperienced and junior security workers to better understand their security environment, Artemis provides an intuitive platform to ask questions of alert data as users are guided through triage and hunt workflows. In this paper, we will discuss our user-centric design methodology, feedback from user interviews, and the design requirements generated upon completion of our study. We will also present core functionality, findings from scenario-based testing, and future research for the Artemis platform.',\n",
       " \"As autonomous vehicles have benefited the society, understanding the dynamic change of humans' trust during human-autonomous vehicle interaction can help to improve the safety and performance of autonomous driving. We designed and conducted a human subjects study involving 19 participants. Each participant was asked to enter their trust level in a Likert scale in real-time during experiments on a driving simulator. We also collected physiological data (e.g., heart rate, pupil size) of participants as complementary indicators of trust. We used analysis of variance (ANOVA) and Signal Temporal Logic (STL) to analyze the experimental data. Our results show the influence of different factors (e.g., automation alarms, weather conditions) on trust, and the individual variability in human reaction time and trust change.\",\n",
       " 'Artificial objects often subjectively look eerie when their appearance to some extent resembles a human, which is known as the uncanny valley phenomenon. From a cognitive psychology perspective, several explanations of the phenomenon have been put forth, two of which are object categorization and realism inconsistency. Recently, MacDorman and Chattopadhyay (2016) reported experimental data as evidence in support of the latter. In our estimation, however, their results are still consistent with categorization-based stranger avoidance. In this Discussions paper, we try to describe why categorization-based stranger avoidance remains a viable explanation, despite the evidence of MacDorman and Chattopadhyay, and how it offers a more inclusive explanation of the impression of eeriness in the uncanny valley phenomenon.',\n",
       " 'Classifying human cognitive states from behavioral and physiological signals is a challenging problem with important applications in robotics. The problem is challenging due to the data variability among individual users, and sensor artefacts. In this work, we propose an end-to-end framework for real-time cognitive workload classification with mixture Hyper Long Short Term Memory Networks, a novel variant of HyperNetworks. Evaluating the proposed approach on an eye-gaze pattern dataset collected from simulated driving scenarios of different cognitive demands, we show that the proposed framework outperforms previous baseline methods and achieves 83.9\\\\% precision and 87.8\\\\% recall during test. We also demonstrate the merit of our proposed architecture by showing improved performance over other LSTM-based methods.',\n",
       " 'Human Body Communication (HBC) has recently emerged as an alternative to radio frequency transmission for connecting devices on and in the human body with order(s) of magnitude lower energy. The communication between these devices can give rise to different scenarios, which can be classified as wearable-wearable, wearable-machine, machine-machine interactions. In this paper, for the first time, the human body channel characteristics is measured for a wide range of such possible scenarios (14 vs. a few in previous literature) and classified according to the form-factor of the transmitter and receiver. The effect of excitation/termination configurations on the channel loss is also explored, which helps explain the previously unexplained wide variation in HBC Channel measurements. Measurement results show that wearable-wearable interaction has the maximum loss (upto -50 dB) followed by wearable-machine and machinemachine interaction (min loss of 0.5 dB), primarily due to the small ground size of the wearable devices. Among the excitation configurations, differential excitation is suitable for small channel length whereas single ended is better for longer channel.',\n",
       " 'This article investigates graph analysis for intelligent marketing in smart cities, where metatrails are crowdsourced by mobile sensing for marketing strategies. Unlike most works that focused on client sides, this study is intended for market planning, from the perspective of enterprises. Several novel crowdsourced features based on metatrails, including hotspot networks, crowd transitions, affinity subnetworks, and sequential visiting patterns, are discussed in the article. These smart footprints can reflect crowd preferences and the topology of a site of interest. Marketers can utilize such information for commercial resource planning and deployment. Simulations were conducted to demonstrate the performance. At the end, this study also discusses different scenarios for practical geo-conquesting applications.',\n",
       " 'We propose the concept of Speculative Execution for Visual Analytics and discuss its effectiveness for model exploration and optimization. Speculative Execution enables the automatic generation of alternative, competing model configurations that do not alter the current model state unless explicitly confirmed by the user. These alternatives are computed based on either user interactions or model quality measures and can be explored using delta-visualizations. By automatically proposing modeling alternatives, systems employing Speculative Execution can shorten the gap between users and models, reduce the confirmation bias and speed up optimization processes. In this paper, we have assembled five application scenarios showcasing the potential of Speculative Execution, as well as a potential for further research.',\n",
       " \"An emotion orientated intelligent interface consists of Emotion Generating Calculations (EGC) and Mental State Transition Network (MSTN). We have developed the Android EGC application software which the agent works to evaluate the feelings in the conversation. In this paper, we develop the tourist information system which can estimate the user's feelings at the sightseeing spot. The system can recommend the sightseeing spot and the local food corresponded to the user's feeling. The system calculates the recommendation list by the estimate function which consists of Google search results, the important degree of a term at the sightseeing website, and the the aroused emotion by EGC. In order to show the effectiveness, this paper describes the experimental results for some situations during Hiroshima sightseeing.\",\n",
       " \"Several techniques for visualization of dynamic graphs are based on different spatial arrangements of a temporal sequence of node-link diagrams. Many studies in the literature have investigated the importance of maintaining the user's mental map across this temporal sequence, but usually each layout is considered as a static graph drawing and the effect of user interaction is disregarded. We conducted a task-based controlled experiment to assess the effectiveness of two basic interaction techniques: the adjustment of the layout stability and the highlighting of adjacent nodes and edges. We found that generally both interaction techniques increase accuracy, sometimes at the cost of longer completion times, and that the highlighting outclasses the stability adjustment for many tasks except the most complex ones.\",\n",
       " \"Research investigating cognitive aspects of information systems is often dependent on detail-rich data. Eye-trackers promise to provide respective data, but the associated costs are often beyond the researchers' budget. Recently, eye-trackers have entered the market that promise eye-tracking support at a reasonable price. In this work, we explore whether such eye-trackers are of use for information systems research and explore the accuracy of a low-cost eye-tracker (Gazepoint GP3) in an empirical study. The results show that Gazepoint GP3 is well suited for respective research, given that experimental material acknowledges the limits of the eye-tracker. To foster replication and comparison of results, all data, experimental material as well as the source code developed for this study are made available online.\",\n",
       " 'This paper explores the potential for using Brain Computer Interfaces (BCI) as a relevance feedback mechanism in content-based image retrieval. We investigate if it is possible to capture useful EEG signals to detect if relevant objects are present in a dataset of realistic and complex images. We perform several experiments using a rapid serial visual presentation (RSVP) of images at different rates (5Hz and 10Hz) on 8 users with different degrees of familiarization with BCI and the dataset. We then use the feedback from the BCI and mouse-based interfaces to retrieve localized objects in a subset of TRECVid images. We show that it is indeed possible to detect such objects in complex images and, also, that users with previous knowledge on the dataset or experience with the RSVP outperform others. When the users have limited time to annotate the images (100 seconds in our experiments) both interfaces are comparable in performance. Comparing our best users in a retrieval task, we found that EEG-based relevance feedback outperforms mouse-based feedback. The realistic and complex image dataset differentiates our work from previous studies on EEG for image retrieval.',\n",
       " \"Gesture interaction is a natural way of communicating with a robot as an alternative to speech. Gesture recognition methods leverage optical flow in order to understand human motion. However, while accurate optical flow estimation (i.e., traditional) methods are costly in terms of runtime, fast estimation (i.e., deep learning) methods' accuracy can be improved. In this paper, we present a pipeline for gesture-based human-robot interaction that uses a novel optical flow estimation method in order to achieve an improved speed-accuracy trade-off. Our optical flow estimation method introduces four improvements to previous deep learning-based methods: strong feature extractors, attention to contours, midway features, and a combination of these three. This results in a better understanding of motion, and a finer representation of silhouettes. In order to evaluate our pipeline, we generated our own dataset, MIBURI, which contains gestures to command a house service robot. In our experiments, we show how our method improves not only optical flow estimation, but also gesture recognition, offering a speed-accuracy trade-off more realistic for practical robot applications.\",\n",
       " \"Recent advancements in mobile devices encourage researchers to utilize them in collaborative environments as a medium to interact with large shared wall-displays. In this paper, we focus on a semi-controlled user study that we conducted to measure the collaborative coupling ratio between partners working in pairs in a collaborative setup equipped with a shared tiled-wall display and multiple mobile devices. We invited 36 participants in 18 pairs to take part in our experiment in order to analyze how they communicate and collaborate with each other during the experiment. We observed their collaborative coupling by measuring how often they verbally and visually communicated. Further, we found frequently used collaborative physical position patterns by observing the pairs' physical arrangements and standing positions. Moreover, we combined these factors to gain a clearer understanding of coupling in our setup, taking into account the mobility factor offered by the mobile devices. Results of the study show interesting findings about the coupling factors between the partners mainly due to the flexibility offered by including mobile devices in our collaborative setup.\",\n",
       " 'The development of real-time affect detection models often depends upon obtaining annotated data for supervised learning by employing human experts to label the student data. One open question in annotating affective data for affect detection is whether the labelers (i.e., human experts) need to be socio-culturally similar to the students being labeled, as this impacts the cost feasibility of obtaining the labels. In this study, we investigate the following research questions: For affective state annotation, how does the socio-cultural background of human expert labelers, compared to the subjects, impact the degree of consensus and distribution of affective states obtained? Secondly, how do differences in labeler background impact the performance of affect detection models that are trained using these labels?',\n",
       " \"The goal of our research is to contribute information about how useful the crowd is at anticipating stereotypes that may be biasing a data set without a researcher's knowledge. The results of the crowd's prediction can potentially be used during data collection to help prevent the suspected stereotypes from introducing bias to the dataset. We conduct our research by asking the crowd on Amazon's Mechanical Turk (AMT) to complete two similar Human Intelligence Tasks (HITs) by suggesting stereotypes relating to their personal experience. Our analysis of these responses focuses on determining the level of diversity in the workers' suggestions and their demographics. Through this process we begin a discussion on how useful the crowd can be in tackling this difficult problem within machine learning data collection.\",\n",
       " \"Microtask crowdsourcing is increasingly critical to the creation of extremely large datasets. As a result, crowd workers spend weeks or months repeating the exact same tasks, making it necessary to understand their behavior over these long periods of time. We utilize three large, longitudinal datasets of nine million annotations collected from Amazon Mechanical Turk to examine claims that workers fatigue or satisfice over these long periods, producing lower quality work. We find that, contrary to these claims, workers are extremely stable in their quality over the entire period. To understand whether workers set their quality based on the task's requirements for acceptance, we then perform an experiment where we vary the required quality for a large crowdsourcing task. Workers did not adjust their quality based on the acceptance threshold: workers who were above the threshold continued working at their usual quality level, and workers below the threshold self-selected themselves out of the task. Capitalizing on this consistency, we demonstrate that it is possible to predict workers' long-term quality using just a glimpse of their quality on the first five tasks.\",\n",
       " 'Showing flows of people and resources between multiple geographic locations is a challenging visualisation problem. We conducted two quantitative user studies to evaluate different visual representations for such dense many-to-many flows. In our first study we compared a bundled node-link flow map representation and OD Maps [37] with a new visualisation we call MapTrix. Like OD Maps, MapTrix overcomes the clutter associated with a traditional flow map while providing geographic embedding that is missing in standard OD matrix representations. We found that OD Maps and MapTrix had similar performance while bundled node-link flow map representations did not scale at all well. Our second study compared participant performance with OD Maps and MapTrix on larger data sets. Again performance was remarkably similar.',\n",
       " \"Crowd predictions have demonstrated powerful performance in predicting future events. We aim to understand crowd prediction efficacy in ascertaining the veracity of human emotional expressions. We discover that collective discernment can increase the accuracy of detecting emotion veracity from 63%, which is the average individual performance, to 80%. Constraining data to best performers can further increase the result up to 92%. Neural networks can achieve an accuracy to 99.69% by aggregating participants' answers. That is, assigning positive and negative weights to high and low human predictors, respectively. Furthermore, neural networks that are trained with one emotion data can also produce high accuracies on discerning the veracity of other emotion types: our crowdsourced transfer of emotion learning is novel. We find that our neural networks do not require a large number of participants, particularly, 30 randomly selected, to achieve high accuracy predictions, better than any individual participant. Our proposed method of assembling peoples' predictions with neural networks can provide insights for applications such as fake news prevention and lie detection.\",\n",
       " 'A device which contains number of symbol input keys, where the number of available keys is less than the number of symbols of an alphabet of any given language, screen, and dynamic reordering table of the symbols which are mapped onto those keys, according to a disambiguation method based on the previously entered symbols. The device incorporates a previously entered keystrokes tracking mechanism, and the key selected by the user detector, as well as a mechanism to select the dynamic symbol reordering mapped onto this key according to the information contained to the reordering table. The reordering table occurs from a disambiguation method which reorders the symbol appearance. The reordering information occurs from Bayesian Belief network construction and training from text corpora of the specific language.',\n",
       " 'Not all smartphone owners use their device in the same way. In this work, we uncover broad, latent patterns of mobile phone use behavior. We conducted a study where, via a dedicated logging app, we collected daily mobile phone activity data from a sample of 340 participants for a period of four weeks. Through an unsupervised learning approach and a methodologically rigorous analysis, we reveal five generic phone use profiles which describe at least 10% of the participants each: limited use, business use, power use, and personality- & externally induced problematic use. We provide evidence that intense mobile phone use alone does not predict negative well-being. Instead, our approach automatically revealed two groups with tendencies for lower well-being, which are characterized by nightly phone use sessions.',\n",
       " 'The ability of human beings to precisely recog- nize others intents is a significant mental activity in reasoning about actions, such as, what other people are doing and what they will do next. Recent research has revealed that human intents could be inferred by measuring human cognitive activities through heterogeneous body and brain sensors (e.g., sensors for detecting physiological signals like ECG, brain signals like EEG and IMU sensors like accelerometers and gyros etc.). In this proposal, we aim at developing a computa- tional framework for enabling reliable and precise real-time human intent recognition by measuring human cognitive and physiological activities through the heterogeneous body and brain sensors for improving human machine interactions, and serving intent-based human activity prediction.',\n",
       " 'Within the last decade, running has become one of the most popular physical activities in the world. Although the benefits of running are numerous, there is a risk of Running Related Injuries (RRI) of the lower extremities. Electromyography (EMG) techniques have previously been used to study causes of RRIs, but the complexity of this technology limits its use to a laboratory setting. As running is primarily an outdoors activity, this lack of technology acts as a barrier to the study of RRIs in natural environments. This study presents a minimally invasive wearable muscle sensing device consisting of jogging leggings with embroidered surface EMG (sEMG) electrodes capable of recording muscle activity data of the quadriceps group. To test the use of the device, a proof of concept study consisting of $N=2$ runners performing a set of $5km$ running trials is presented in which the effect of running surfaces on muscle fatigue, a potential cause of RRIs, is evaluated. Results show that muscle fatigue can be analysed from the sEMG data obtained through the wearable device, and that running on soft surfaces (such as sand) may increase the likelihood of suffering from RRIs.',\n",
       " \"This paper provides interested beginners with an updated and detailed introduction to the field of Intelligent Tutoring Systems (ITS). ITSs are computer programs that use artificial intelligence techniques to enhance and personalize automation in teaching. This paper is a literature review that provides the following: First, a review of the history of ITS along with a discussion on the interface between human learning and computer tutors and how effective ITSs are in contemporary education. Second, the traditional architectural components of an ITS and their functions are discussed along with approaches taken by various ITSs. Finally, recent innovative ideas in ITS systems are presented. This paper concludes with some of the author's views regarding future work in the field of intelligent tutoring systems.\",\n",
       " 'Due to the increasing complexity of modern automatic machines typically used in several industrial applications, the need for assistive technologies is becoming very relevant. Typical approaches consist in designing advanced and adaptive human-machine interfaces (HMIs) that can be effectively used by any operator and that provide guided procedures for the most common situations. However, when dealing with complex systems, infrequent and unforeseen situations may happen, whose solution require the experience owned by a limited number of skilled operators. To this end, in this paper we propose an industrial social network concept to allow an effective exchange of information among the operators and to facilitate the solution of unforeseen events, such as unscheduled maintenance activities or troubleshooting.',\n",
       " 'Human computation systems (HCSs) have been widely adopted in various domains. Their goal is to harness human intelligence to solve computational problems that are beyond the capability of modern computers. One of the most challenging problems in HCSs is how to incentivize a broad range of users to participate in the system and make high efforts. This article surveys the field of HCSs from the perspective of incentives and mechanism design. We first review state-of-the-art HCSs, focusing on how incentives are provided to users. We then use mechanism design to theoretically analyze different incentives. We survey the mechanisms derived from state-of-the-art HCSs as well as classic mechanisms that have been used in HCSs. Finally, we discuss eight promising research directions for designing incentives in HCSs.',\n",
       " \"Ensuring fairness of machine learning systems is a human-in-the-loop process. It relies on developers, users, and the general public to identify fairness problems and make improvements. To facilitate the process we need effective, unbiased, and user-friendly explanations that people can confidently rely on. Towards that end, we conducted an empirical study with four types of programmatically generated explanations to understand how they impact people's fairness judgments of ML systems. With an experiment involving more than 160 Mechanical Turk workers, we show that: 1) Certain explanations are considered inherently less fair, while others can enhance people's confidence in the fairness of the algorithm; 2) Different fairness problems--such as model-wide fairness issues versus case-specific fairness discrepancies--may be more effectively exposed through different styles of explanation; 3) Individual differences, including prior positions and judgment criteria of algorithmic fairness, impact how people react to different styles of explanation. We conclude with a discussion on providing personalized and adaptive explanations to support fairness judgments of ML systems.\",\n",
       " \"For decades, researchers in information visualisation and graph drawing have focused on developing techniques for the layout and display of very large and complex networks. Experiments involving human participants have also explored the readability of different styles of layout and representations for such networks. In both bodies of literature, networks are frequently referred to as being 'large' or 'complex', yet these terms are relative. From a human-centred, experiment point-of-view, what constitutes 'large' (for example) depends on several factors, such as data complexity, visual complexity, and the technology used. In this paper, we survey the literature on human-centred experiments to understand how, in practice, different features and characteristics of node-link diagrams affect visual complexity.\",\n",
       " 'Ensembles of classifier models typically deliver superior performance and can outperform single classifier models given a dataset and classification task at hand. However, the gain in performance comes together with the lack in comprehensibility, posing a challenge to understand how each model affects the classification outputs and where the errors come from. We propose a tight visual integration of the data and the model space for exploring and combining classifier models. We introduce a workflow that builds upon the visual integration and enables the effective exploration of classification outputs and models. We then present a use case in which we start with an ensemble automatically selected by a standard ensemble selection algorithm, and show how we can manipulate models and alternative combinations.',\n",
       " 'Elderly chronic diseases are the main cause of death in the world, accounting 60% of all death. Because elderly with chronic diseases at the early stages has no observed symptoms, and then symptoms starts to appear, it is critical to observe the symptoms as early as possible to avoid any complication. This paper presents an expert system for an Elderly Health Care (EHC) at elderly home tailored for the specific needs of Elderly. The proposed EHC aims to develop an integrated and multidisciplinary method to employ communication technologies and information for covering real health needs of elderly people, mainly of people at high risk due to social and geographic isolation in addition to specific chronic diseases. The proposed EHC provides personalized intervention plans covering chronic diseases such as (body temperature (BT), blood pressure (BP), and Heart beat rate (HR)). The processes and architecture of the proposed EHC are based on the server side and three main clients, one for the elderly and another two for the nurse and the physicians whom take care of them. The proposed EHC model is discussed for proving the usefulness and effectiveness of the expert system.',\n",
       " 'Human computer interaction facilitates intelligent communication between humans and computers, in which gesture recognition plays a prominent role. This paper proposes a machine learning system to identify dynamic gestures using tri-axial acceleration data acquired from two public datasets. These datasets, uWave and Sony, were acquired using accelerometers embedded in Wii remotes and smartwatches, respectively. A dynamic gesture signed by the user is characterized by a generic set of features extracted across time and frequency domains. The system was analyzed from an end-user perspective and was modelled to operate in three modes. The modes of operation determine the subsets of data to be used for training and testing the system. From an initial set of seven classifiers, three were chosen to evaluate each dataset across all modes rendering the system towards mode-neutrality and dataset-independence. The proposed system is able to classify gestures performed at varying speeds with minimum preprocessing, making it computationally efficient. Moreover, this system was found to run on a low-cost embedded platform - Raspberry Pi Zero (USD 5), making it economically viable.',\n",
       " 'Stereoscopic 3D (S3D) displays provide an additional sense of depth compared to non-stereoscopic displays by sending slightly different images to the two eyes. But conventional S3D displays do not reproduce all natural depth cues. In particular, focus cues are incorrect causing mismatches between accommodation and vergence: The eyes must accommodate to the display screen to create sharp retinal images even when binocular disparity drives the eyes to converge to other distances. This mismatch causes visual discomfort and reduces visual performance. We propose and assess two new techniques that are designed to reduce the vergence-accommodation conflict and thereby decrease discomfort and increase visual performance. These techniques are much simpler to implement than previous conflict-reducing techniques.',\n",
       " 'Near Field Communication (NFC) standards cover communications protocols and data exchange formats. They are based on existing radio-frequency identification (RFID) standards. In Japan, Felica card is a popular way to identify the unique ID. Recently, the attendance management system (AMS) with RFID technology has been developed as a part of Smart University, which is the educational infrastructure using high technologies, such as ICT. However, the reader/writer for Felica is too expensive to build the AMS. NFC technology includes not only Felica but other type of IC chips. The Android OS 2.3 and the later can provide access to NFC functionality. Therefore, we developed AMS for university with NFC on Nexus 7. Because Nexus 7 is a low cost smart tablet, a teacher can determine to use familiarly. Especially, this paper describes the method of early discovery for chronic non-attenders by using the AMS system on 2 or more Nexus 7 which is connected each other via peer-to-peer communication. The attendance situation collected from different Nexus 7 is merged into a SQLite file and then, the document is reported to operate with the trunk system in educational affairs section.',\n",
       " 'Conventional HVAC control systems are usually incognizant of the physical structures and materials of buildings. These systems merely follow pre-set HVAC control logic based on abstract building thermal response models, which are rough approximations to true physical models, ignoring dynamic spatial variations in built environments. To enable more accurate and responsive HVAC control, this paper introduces the notion of \"self-aware\" smart buildings, such that buildings are able to explicitly construct physical models of themselves (e.g., incorporating building structures and materials, and thermal flow dynamics). The question is how to enable self-aware buildings that automatically acquire dynamic knowledge of themselves. This paper presents a novel approach using \"augmented reality\". The extensive user-environment interactions in augmented reality not only can provide intuitive user interfaces for building systems, but also can capture the physical structures and possibly materials of buildings accurately to enable real-time building simulation and control. This paper presents a building system prototype incorporating augmented reality, and discusses its applications.',\n",
       " 'Natural Language Processing (NLP) systems often make use of machine learning techniques that are unfamiliar to end-users who are interested in analyzing clinical records. Although NLP has been widely used in extracting information from clinical text, current systems generally do not support model revision based on feedback from domain experts.\\n  We present a prototype tool that allows end users to visualize and review the outputs of an NLP system that extracts binary variables from clinical text. Our tool combines multiple visualizations to help the users understand these results and make any necessary corrections, thus forming a feedback loop and helping improve the accuracy of the NLP models. We have tested our prototype in a formative think-aloud user study with clinicians and researchers involved in colonoscopy research. Results from semi-structured interviews and a System Usability Scale (SUS) analysis show that the users are able to quickly start refining NLP models, despite having very little or no experience with machine learning. Observations from these sessions suggest revisions to the interface to better support review workflow and interpretation of results.',\n",
       " 'Modern industrial automatic machines and robotic cells are equipped with highly complex human-machine interfaces (HMIs) that often prevent human operators from an effective use of the automatic systems. In particular, this applies to vulnerable users, such as those with low experience or education level, the elderly and the disabled. To tackle this issue, it becomes necessary to design user-oriented HMIs, which adapt to the capabilities and skills of users, thus compensating their limitations and taking full advantage of their knowledge. In this paper, we propose a methodological approach to the design of complex adaptive human-machine systems that might be inclusive of all users, in particular the vulnerable ones. The proposed approach takes into account both the technical requirements and the requirements for ethical, legal and social implications (ELSI) for the design of automatic systems. The technical requirements derive from a thorough analysis of three use cases taken from the European project INCLUSIVE. To achieve the ELSI requirements, the MEESTAR approach is combined with the specific legal issues for occupational systems and requirements of the target users.',\n",
       " 'Providing opinions through labeling of images, tweets, etc. have drawn immense interest in crowdsourcing markets. This invokes a major challenge of aggregating multiple opinions received from different crowd workers for deriving the final judgment. Generally, opinion aggregation models deal with independent opinions, which are given unanimously and are not visible to all. However, in many real-life cases, it is required to make the opinions public as soon as they are received. This makes the opinions dependent and might incorporate some bias. In this paper, we address a novel problem, hereafter denoted as dependent judgment analysis, and discuss the requirements for developing an appropriate model to deal with this problem. The challenge remains to be improving the consensus by revealing true opinions.',\n",
       " 'Wikipedia articles about places, OpenStreetMap features, and other forms of peer-produced content have become critical sources of geographic knowledge for humans and intelligent technologies. In this paper, we explore the effectiveness of the peer production model across the rural/urban divide, a divide that has been shown to be an important factor in many online social systems. We find that in both Wikipedia and OpenStreetMap, peer-produced content about rural areas is of systematically lower quality, is less likely to have been produced by contributors who focus on the local area, and is more likely to have been generated by automated software agents (i.e. bots). We then codify the systemic challenges inherent to characterizing rural phenomena through peer production and discuss potential solutions.',\n",
       " 'Despite significant improvements in automatic speech recognition and spoken language understanding - human interaction with Virtual Personal Assistants (VPAs) through speech remains irregular and sporadic. According to recent studies, currently the usage of VPAs is constrained to basic tasks such as checking facts, playing music, and obtaining weather updates.In this paper, we present results of a survey (N = 118) that analyses usage of VPAs by frequent and infrequent users. We investigate how usage experience, performance expectations, and privacy concerns differ between these two groups. The results indicate that, compared with infrequent users, frequent users of VPAs are more satisfied with their assistants, more eager to use them in a variety of settings, yet equally concerned about their privacy.',\n",
       " 'A natural conversational interface that allows longitudinal symptom tracking would be extremely valuable in health/wellness applications. However, the task of designing emotionally-aware agents for behavior change is still poorly understood. In this paper, we present the design and evaluation of an emotion-aware chatbot that conducts experience sampling in an empathetic manner. We evaluate it through a human-subject experiment with N=39 participants over the course of a week. Our results show that extraverts preferred the emotion-aware chatbot significantly more than introverts. Also, participants reported a higher percentage of positive mood reports when interacting with the empathetic bot. Finally, we provide guidelines for the design of emotion-aware chatbots for potential use in mHealth contexts.',\n",
       " 'This paper describes the development of the Microsoft XiaoIce system, the most popular social chatbot in the world. XiaoIce is uniquely designed as an AI companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient (IQ) and emotional quotient (EQ) in system design, cast human-machine social chat as decision-making over Markov Decision Processes (MDPs), and optimize XiaoIce for long-term user engagement, measured in expected Conversation-turns Per Session (CPS). We detail the system architecture and key components including dialogue manager, core chat, skills, and an empathetic computing module. We show how XiaoIce dynamically recognizes human feelings and states, understands user intents, and responds to user needs throughout long conversations. Since the release in 2014, XiaoIce has communicated with over 660 million users and succeeded in establishing long-term relationships with many of them. Analysis of large-scale online logs shows that XiaoIce has achieved an average CPS of 23, which is significantly higher than that of other chatbots and even human conversations.',\n",
       " 'Designing conversational user interface experience is complicated because conversation comes with many expectations. When these expectations are met, we feel the interface is natural, but once violated, we feel something is amiss. The last decade witnessed human language technologies and behaviours to enable humans converse with software using spoken dialogue to access, create and process information. Less is known about the practicalities of designing chatbot interactions. In this paper, we introduce the nature of conversational user interfaces (CUIs) and describe the underlying technologies they are based on. Moreover, we define guidelines for designing conversational interfaces in various domains. This paper particularly focuses on classifying the elements and techniques used in CUI design patterns. After concluding certain challenges with CUI, we discuss important features and chatbot states to be considered in CUI design for specific domain. We envisage this study to support CUI researchers to design tailored chatbots applicable into certain domain and improve the current state of research challenges in the field of Artificial Intelligence and conversational agents.',\n",
       " \"Data collection and analysis in the field is critical for operations in domains such as environmental science and public safety. However, field workers currently face data- and platform-oriented issues in efficient data collection and analysis in the field, such as limited connectivity, screen space, and attentional resources. In this paper, we explore how visual analytics tools might transform field practices by more deeply integrating data into these operations. We use a design probe coupling mobile, cloud and immersive analytics components to guide interviews with ten experts from five domains to explore how visual analytics could support data collection and analysis needs in the field. The results identify shortcomings of current approaches and target scenarios and design considerations for future field analysis systems. We embody these findings in FieldView, an extensible, open-source prototype designed to support critical use cases for situated field analysis. Our findings suggest the potential for integrating mobile and immersive technologies to enhance data's utility for various field operations and new directions for visual analytics tools to transform fieldwork.\",\n",
       " 'Automatic detection of emergent leaders in small groups from nonverbal behaviour is a growing research topic in social signal processing but existing methods were evaluated on single datasets -- an unrealistic assumption for real-world applications in which systems are required to also work in settings unseen at training time. It therefore remains unclear whether current methods for emergent leadership detection generalise to similar but new settings and to which extent. To overcome this limitation, we are the first to study a cross-dataset evaluation setting for the emergent leadership detection task. We provide evaluations for within- and cross-dataset prediction using two current datasets (PAVIS and MPIIGroupInteraction), as well as an investigation on the robustness of commonly used feature channels (visual focus of attention, body pose, facial action units, speaking activity) and online prediction in the cross-dataset setting. Our evaluations show that using pose and eye contact based features, cross-dataset prediction is possible with an accuracy of 0.68, as such providing another important piece of the puzzle towards emergent leadership detection in the real world.',\n",
       " 'With the rapid development of mobile computing, wearable wrist-worn is becoming more and more popular. But the current vibrotactile feedback patterns of most wrist-worn devices are too simple to enable effective interaction in nonvisual scenarios. In this paper, we propose the wristband system with four vibrating motors placed in different positions in the wristband, providing multiple vibration patterns to transmit multi-semantic information for users in eyes-free scenarios. However, we just applied five vibrotactile patterns in experiments (positional up and down, horizontal diagonal, clockwise circular, and total vibration) after contrastive analyzing nine patterns in a pilot experiment. The two experiments with the same 12 participants perform the same experimental process in lab and outdoors. According to the experimental results, users can effectively distinguish the five patterns both in lab and outside, with approximately 90% accuracy (except clockwise circular vibration of outside experiment), proving these five vibration patterns can be used to output multi-semantic information. The system can be applied to eyes-free interaction scenarios for wrist-worn devices.',\n",
       " \"We developed a novel virtual reality [VR] platform with 3-dimensional sounds to help improve sensory integration and visuomotor processing for postural control and fall prevention in individuals with balance problems related to sensory deficits, such as vestibular dysfunction (disease of the inner ear). The system has scenes that simulate scenario-based environments. We can adjust the intensity of the visual and audio stimuli in the virtual scenes by controlling the user interface (UI) settings. A VR headset (HTC Vive or Oculus Rift) delivers stereo display while providing real-time position and orientation of the participants' head. The 3D game-like scenes make participants feel immersed and gradually exposes them to situations that may induce dizziness, anxiety or imbalance in their daily-living.\",\n",
       " \"We analyze the claims that video recreations of shoulder surfing attacks offer a suitable alternative and a baseline, as compared to evaluation in a live setting. We recreated a subset of the factors of a prior video-simulation experiment conducted by Aviv et al. (ACSAC 2017), and model the same scenario using live participants ($n=36$) instead (i.e., the victim and attacker were both present). The live experiment confirmed that for Android's graphical patterns video simulation is consistent with the live setting for attacker success rates. However, both 4- and 6-digit PINs demonstrate statistically significant differences in attacker performance, with live attackers performing as much 1.9x better than in the video simulation. The security benefits gained from removing feedback lines in Android's graphical patterns are also greatly diminished in the live setting, particularly under multiple attacker observations, but overall, the data suggests that video recreations can provide a suitable baseline measure for attacker success rate. However, we caution that researchers should consider that these baselines may greatly underestimate the threat of an attacker in live settings.\",\n",
       " \"Cross cultural research projects are becoming a norm in our global world. More and more projects are being executed using teams from eastern and western cultures. Cultural competence might help project managers to achieve project goals and avoid potential risks in cross cultural project environments and would also support them to promote creativity and motivation through flexible leadership. In our paper we introduce an idea for constructing an information system, a cross cultural knowledge space, which could support cross cultural communication, collaborative learning experiences and time based project management functions. The case cultures in our project are Finnish and Japanese. The system can be used both in virtual and in physical spaces for example to clarify cultural business etiquette. The core of our system design will be based on cross cultural ontology, and the system implementation on XML technologies. Our approach is a practical, step by step example of constructive research. In our paper we shortly describe Hofstede's dimensions for assessing cultures as one example of a larger framework for our study. We also discuss the concept of time in cultural context.\",\n",
       " 'Smartphone users benefit from content with dark color schemes: increasingly common OLED displays are more power efficient the darker the display, and many users prefer a dark display for night time use. Despite these benefits, many applications and the majority of web content are drawn with white backgrounds. There are many partial solutions to darken the displayed content, but none work in all situations. Enter SmartNight, a content-aware solution to dynamically darken content on Android. By trading off content fidelity, Android with SmartNight displays content with nearly 90% lower average picture level. It is implemented in the Android framework, and requires no external support. It seamlessly incorporates existing solutions, making it a bridge between the state-of-the-art and future solutions.',\n",
       " \"In the everyday context, e.g., a household, HMD users remain a part of the social life for Non-HMD users being co-located with them. Due to the social context situations arise that demand interaction between the HMD and the Non-HMD user. We focus on the challenge that the Non-HMD user is not able to interpret the HMD user's state -- e.g., attentiveness; the need for assistance --, as the HMD covers the wearer's face. We propose a front facing display attached to the HMD that supports collaboration by showing the state. We explore the impact of abstract and realistic visualizations for such displays on collaborative performance and social presence in a within-subject user study (N=25). We present to the Non-HMD user (1) a blank screen (baseline), (2) textual representation of the user's state and (3) a representation that looks like the HMD is see-through. The results show positive effects for textual representation on collaborative performance and a positive effect of realistic representation on social presence. We conclude that when developing HMDs we need to take into account the social needs of everyday life to reduce the risk of social separation in a household context.\",\n",
       " 'In this paper we present results from recent experiments that suggest that chess players associate emotions to game situations and reactively use these associations to guide search for planning and problem solving. We describe the design of an instrument for capturing and interpreting multimodal signals of humans engaged in solving challenging problems. We review results from a pilot experiment with human experts engaged in solving challenging problems in Chess that revealed an unexpected observation of rapid changes in emotion as players attempt to solve challenging problems. We propose a cognitive model that describes the process by which subjects select chess chunks for use in interpretation of the game situation and describe initial results from a second experiment designed to test this model.',\n",
       " \"Through a combination of experimental and simulation results, we illustrate that passive recommendations encoded in typical computer user-interfaces (UIs) can subdue users' natural proclivity to access diverse information sources. Inspired by traditional demonstrations of a part-set cueing effect in the cognitive science literature, we performed an online experiment manipulating the operation of the 'New Tab' page for consenting volunteers over a two month period. Examination of their browsing behavior reveals that typical frequency and recency-based methods for displaying websites in these displays subdues users' propensity to access infrequently visited pages compared to a situation wherein no web page icons are displayed on the new tab page. Using a carefully designed simulation study, representing user behavior as a random walk on a graph, we inferred quantitative predictions about the extent to which discovery of new sources of information may be hampered by personalized 'New Tab' recommendations in typical computer UIs. We show that our results are significant at the individual level and explain the potential consequences of the observed suppression in web-exploration.\",\n",
       " 'Affect (emotion) recognition has gained significant attention from researchers in the past decade. Emotion-aware computer systems and devices have many applications ranging from interactive robots, intelligent online tutor to emotion based navigation assistant. In this research data from multiple modalities such as face, head, hand, body and speech was utilized for affect recognition. The research used color and depth sensing device such as Kinect for facial feature extraction and tracking human body joints. Temporal features across multiple frames were used for affect recognition. Event driven decision level fusion was used to combine the results from each individual modality using majority voting to recognize the emotions. The study also implemented affect recognition by matching the features to the rule based emotion templates per modality. Experiments showed that multimodal affect recognition rates using combination of emotion templates and supervised learning were better compared to recognition rates based on supervised learning alone. Recognition rates obtained using temporal feature were higher compared to recognition rates obtained using position based features only.',\n",
       " \"Machine playtesting tools and game moment search engines require exposure to the diversity of a game's state space if they are to report on or index the most interesting moments of possible play. Meanwhile, mobile app distribution services would like to quickly determine if a freshly-uploaded game is fit to be published. Having access to a semantic map of reachable states in the game would enable efficient inference in these applications. However, human gameplay data is expensive to acquire relative to the coverage of a game that it provides. We show that off-the-shelf automatic exploration strategies can explore with an effectiveness comparable to human gameplay on the same timescale. We contribute generic methods for quantifying exploration quality as a function of time and demonstrate our metric on several elementary techniques and human players on a collection of commercial games sampled from multiple game platforms (from Atari 2600 to Nintendo 64). Emphasizing the diversity of states reached and the semantic map extracted, this work makes productive contrast with the focus on finding a behavior policy or optimizing game score used in most automatic game playing research.\",\n",
       " \"To overcome the travelling difficulty for the visually impaired group, this paper presents a novel ETA (Electronic Travel Aids)-smart guiding device in the shape of a pair of eyeglasses for giving these people guidance efficiently and safely. Different from existing works, a novel multi sensor fusion based obstacle avoiding algorithm is proposed, which utilizes both the depth sensor and ultrasonic sensor to solve the problems of detecting small obstacles, and transparent obstacles, e.g. the French door. For totally blind people, three kinds of auditory cues were developed to inform the direction where they can go ahead. Whereas for weak sighted people, visual enhancement which leverages the AR (Augment Reality) technique and integrates the traversable direction is adopted. The prototype consisting of a pair of display glasses and several low cost sensors is developed, and its efficiency and accuracy were tested by a number of users. The experimental results show that the smart guiding glasses can effectively improve the user's travelling experience in complicated indoor environment. Thus it serves as a consumer device for helping the visually impaired people to travel safely.\",\n",
       " 'From the dawn of civilization, people have used folktales and stories to share information and knowledge. After the invention of printing in the 15th century, technology provided helpful yet complicated utilities to exchange ideas. In the present computerized world, the art of storytelling is becoming more influential through the unprecedented multimedia capabilities of computers. In this article, we introduce a state-of-the-art presentation software by which academicians can present nonlinear topics efficiently and sharpen their storytelling skills. We show how the proposed software can improve the scientific presentation style. We conducted a survey to measure the attractiveness of proposed utility among other alternatives. Results show that academicians prefer the proposed platform to others.',\n",
       " 'One of the challenges in affect recognition is accurate estimation of the emotion intensity level. This research proposes development of an affect intensity estimation model based on a weighted sum of classification confidence levels, displacement of feature points and speed of feature point motion. The parameters of the model were calculated from data captured using multiple modalities such as face, body posture, hand movement and speech. A preliminary study was conducted to compare the accuracy of the model with the annotated intensity levels. An emotion intensity scale ranging from 0 to 1 along the arousal dimension in the emotion space was used. Results indicated speech and hand modality significantly contributed in improving accuracy in emotion intensity estimation using the proposed model.',\n",
       " \"Simulations are a pedagogical means of enabling a risk-free way for healthcare practitioners to learn, maintain, or enhance their knowledge and skills. Such simulations should provide an optimum amount of cognitive load to the learner and be tailored to their levels of expertise. However, most current simulations are a one-type-fits-all tool used to train different learners regardless of their existing skills, expertise, and ability to handle cognitive load. To address this problem, we propose an end-to-end framework for a trauma simulation that actively classifies a participant's level of cognitive load and expertise for the development of a dynamically adaptive simulation. To facilitate this solution, trauma simulations were developed for the collection of electrocardiogram (ECG) signals of both novice and expert practitioners. A multitask deep neural network was developed to utilize this data and classify high and low cognitive load, as well as expert and novice participants. A leave-one-subject-out (LOSO) validation was used to evaluate the effectiveness of our model, achieving an accuracy of 89.4% and 96.6% for classification of cognitive load and expertise, respectively.\",\n",
       " \"Automatically monitoring and quantifying stress-induced thermal dynamic information in real-world settings is an extremely important but challenging problem. In this paper, we explore whether we can use mobile thermal imaging to measure the rich physiological cues of mental stress that can be deduced from a person's nose temperature. To answer this question we build i) a framework for monitoring nasal thermal variable patterns continuously and ii) a novel set of thermal variability metrics to capture a richness of the dynamic information. We evaluated our approach in a series of studies including laboratory-based psychosocial stress-induction tasks and real-world factory settings. We demonstrate our approach has the potential for assessing stress responses beyond controlled laboratory settings.\",\n",
       " 'The teaching of abstract physics concepts can be enhanced by incorporating visual and haptic sensory modalities in the classroom, using the correct perspectives. We have developed virtual reality simulations to assist students in learning the Coriolis effect, an apparent deflection on an object in motion when observed from within a rotating frame of reference. Twenty four undergraduate physics students participated in this study. Students were able to feel the forces through feedback on a Novint Falcon device. The assessment results show an improvement in the learning experience and better content retention as compared with traditional instruction methods. We prove that large scale deployment of visuo-haptic reconfigurable applications is now possible and feasible in a science laboratory setup.',\n",
       " 'We propose to fuse two currently separate research lines on novel therapies for stroke rehabilitation: brain-computer interface (BCI) training and transcranial electrical stimulation (TES). Specifically, we show that BCI technology can be used to learn personalized decoding models that relate the global configuration of brain rhythms in individual subjects (as measured by EEG) to their motor performance during 3D reaching movements. We demonstrate that our models capture substantial across-subject heterogeneity, and argue that this heterogeneity is a likely cause of limited effect sizes observed in TES for enhancing motor performance. We conclude by discussing how our personalized models can be used to derive optimal TES parameters, e.g., stimulation site and frequency, for individual patients.',\n",
       " 'New machine learning algorithms are being developed to solve problems in different areas, including music. Intuitive, accessible, and understandable demonstrations of the newly built models could help attract the attention of people from different disciplines and evoke discussions. However, we notice that it has not been a common practice for researchers working on musical machine learning to demonstrate their models in an interactive way. To address this issue, we present in this paper an template that is specifically designed to demonstrate symbolic musical machine learning models on the web. The template comes with a small codebase, is open source, and is meant to be easy to use by any practitioners to implement their own demonstrations. Moreover, its modular design facilitates the reuse of the musical components and accelerates the implementation. We use the template to build interactive demonstrations of four exemplary music generation models. We show that the built-in interactivity and real-time audio rendering of the browser make the demonstration easier to understand and to play with. It also helps researchers to gain insights into different models and to A/B test them.',\n",
       " 'Articulated hand pose estimation is a challenging task for human-computer interaction. The state-of-the-art hand pose estimation algorithms work only with one or a few subjects for which they have been calibrated or trained. Particularly, the hybrid methods based on learning followed by model fitting or model based deep learning do not explicitly consider varying hand shapes and sizes. In this work, we introduce a novel hybrid algorithm for estimating the 3D hand pose as well as bone-lengths of the hand skeleton at the same time, from a single depth image. The proposed CNN architecture learns hand pose parameters and scale parameters associated with the bone-lengths simultaneously. Subsequently, a new hybrid forward kinematics layer employs both parameters to estimate 3D joint positions of the hand. For end-to-end training, we combine three public datasets NYU, ICVL and MSRA-2015 in one unified format to achieve large variation in hand shapes and sizes. Among hybrid methods, our method shows improved accuracy over the state-of-the-art on the combined dataset and the ICVL dataset that contain multiple subjects. Also, our algorithm is demonstrated to work well with unseen images.',\n",
       " \"Theories of knowledge reuse posit two distinct processes: reuse for replication and reuse for innovation. We identify another distinct process, reuse for customization. Reuse for customization is a process in which designers manipulate the parameters of metamodels to produce models that fulfill their personal needs. We test hypotheses about reuse for customization in Thingiverse, a community of designers that shares files for three-dimensional printing. 3D metamodels are reused more often than the 3D models they generate. The reuse of metamodels is amplified when the metamodels are created by designers with greater community experience. Metamodels make the community's design knowledge available for reuse for customization-or further extension of the metamodels, a kind of reuse for innovation.\",\n",
       " 'In this work we present a mobile application we designed and engineered to enable people to log their travels near and far, leave notes behind, and build a community around spaces in between destinations. Our design explores new ground for location-based social computing systems, identifying opportunities where these systems can foster the growth of on-line communities rooted at non-places. In our work we develop, explore, and evaluate several innovative features designed around four usage scenarios: daily commuting, long-distance traveling, quantified traveling, and journaling. We present the results of two small-scale user studies, and one large-scale, world-wide deployment, synthesizing the results as potential opportunities and lessons learned in designing social computing for non-places.',\n",
       " 'In contrast to typical laboratory experiments, the everyday use of online educational resources by large populations and the prevalence of software infrastructure for A/B testing leads us to consider how platforms can embed in vivo experiments that do not merely support research, but ensure practical improvements to their educational components. Examples are presented of randomized experimental comparisons conducted by subsets of the authors in three widely used online educational platforms Khan Academy, edX, and ASSISTments. We suggest design principles for platform technology to support randomized experiments that lead to practical improvements enabling Iterative Improvement and Collaborative Work and explain the benefit of their implementation by WPI co-authors in the ASSISTments platform.',\n",
       " 'Social media has become a major communication channel for communities centered around video games. Consequently, social media offers a rich data source to study online communities and the discussions evolving around games. Towards this end, we explore a large-scale dataset consisting of over 1 million tweets related to the online multiplayer shooter Destiny and spanning a time period of about 14 months using unsupervised clustering and topic modelling. Furthermore, we correlate Twitter activity of over 3,000 players with their playtime. Our results contribute to the understanding of online player communities by identifying distinct player groups with respect to their Twitter characteristics, describing subgroups within the Destiny community, and uncovering broad topics of community interest.',\n",
       " 'Inferring emotions from physiological signals has gained much traction in the last years. Physiological responses to emotions, however, are commonly interfered and overlapped by physical activities, posing a challenge towards emotion recognition in the wild. In this paper, we address this challenge by investigating new features and machine-learning models for emotion recognition, non-sensitive to physical-based interferences. We recorded physiological signals from 18 participants that were exposed to emotions before and while performing physical activities to assess the performance of non-sensitive emotion recognition models. We trained models with the least exhaustive physical activity (sitting) and tested with the remaining, more exhausting activities. For three different emotion categories, we achieve classification accuracies ranging from 47.88% - 73.35% for selected feature sets and per participant. Furthermore, we investigate the performance across all participants and of each activity individually. In this regard, we achieve similar results, between 55.17% and 67.41%, indicating the viability of emotion recognition models not being influenced by single physical activities.',\n",
       " 'Eye movement patterns reflect human latent internal cognitive activities. We aim to discover eye movement patterns during face recognition under different cognitions of information concealing. These cognitions include the degrees of face familiarity and deception or not, namely telling the truth when observing familiar and unfamiliar faces, and deceiving in front of familiar faces. We apply Hidden Markov models with Gaussian emission to generalize regions and trajectories of eye fixation points under the above three conditions. Our results show that both eye movement patterns and eye gaze regions become significantly different during deception compared with truth-telling. We show the feasibility of detecting deception and further cognitive activity classification using eye movement patterns.',\n",
       " \"We propose an active learning architecture for robots, capable of organizing its learning process to achieve a field of complex tasks by learning sequences of motor policies, called Intrinsically Motivated Procedure Babbling (IM-PB). The learner can generalize over its experience to continuously learn new tasks. It chooses actively what and how to learn based by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated tasks outcomes hierarchically organized. We introduce a framework called 'procedures', which are sequences of policies defined by the combination of previously learned skills. Our algorithmic architecture uses the procedures to autonomously discover how to combine simple skills to achieve complex goals. It actively chooses between 2 strategies of goal-directed exploration : exploration of the policy space or the procedural space. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our 'procedures' framework helps the learner to tackle difficult hierarchical tasks.\",\n",
       " \"Collaborative creativity is the approach of employing crowd to accomplish creative tasks. In this paper, we present a collaborative crowdsourcing platform for writing stories by means of connecting a series of `images'. These connected images are termed as Image Chains, reflecting successive scenarios. Users can either start or extend an Image Chain by uploading their own image or choosing from the available ones. These users are allowed to pen their stories from the Image Chains. Finally, stories get published based on the number of votes obtained. This provides an organized framework of story writing unlike most of the state-of-the-art collaborative editing platforms. Our experiments on 25 contributors highlight their interest in growing shorter Image Chains but voting longer Image Chains.\",\n",
       " 'Maximalism in art refers to drawing on and combining multiple different sources for art creation, embracing the resulting collisions and heterogeneity. This paper discusses the use of maximalism in game design and particularly in data games, which are games that are generated partly based on open data. Using Data Adventures, a series of generators that create adventure games from data sources such as Wikipedia and OpenStreetMap, as a lens we explore several tradeoffs and issues in maximalist game design. This includes the tension between transformation and fidelity, between decorative and functional content, and legal and ethical issues resulting from this type of generativity. This paper sketches out the design space of maximalist data-driven games, a design space that is mostly unexplored.',\n",
       " 'This paper explores the identification of smartphone users when certain samples collected while the subject felt happy, upset or stressed were absent or present. We employ data from 19 subjects using the StudentLife dataset, a dataset collected by researchers at Dartmouth College that was originally collected to correlate behaviors characterized by smartphone usage patterns with changes in stress and academic performance. Although many previous works on behavioral biometrics have implied that mood is a source of intra-person variation which may impact biometric performance, our results contradict this assumption. Our findings show that performance worsens when removing samples that were generated when subjects may be happy, upset, or stressed. Thus, there is no indication that mood negatively impacts performance. However, we do find that changes existing in smartphone usage patterns may correlate with mood, including changes in locking, audio, location, calling, homescreen, and e-mail habits. Thus, we show that while mood is a source of intra-person variation, it may be an inaccurate assumption that biometric systems (particularly, mobile biometrics) are likely influenced by mood.',\n",
       " 'Compared to other behavioural biometrics, mouse dynamics is a less explored area. General purpose data sets containing unrestricted mouse usage data are usually not available. The Balabit data set was released in 2016 for a data science competition, which against the few subjects, can be considered the first adequate publicly available one. This paper presents a performance evaluation study on this data set for impostor detection. The existence of very short test sessions makes this data set challenging. Raw data were segmented into mouse move, point and click and drag and drop types of mouse actions, then several features were extracted. In contrast to keystroke dynamics, mouse data is not sensitive, therefore it is possible to collect negative mouse dynamics data and to use two-class classifiers for impostor detection. Both action- and set of actions-based evaluations were performed. Set of actions-based evaluation achieves 0.92 AUC on the test part of the data set. However, the same type of evaluation conducted on the training part of the data set resulted in maximal AUC (1) using only 13 actions. Drag and drop mouse actions proved to be the best actions for impostor detection.',\n",
       " 'The present study has made a review of scientific publications on applications focused on autism, most of them developed for communication, social behavior and learning, which coincides with what is observed in a digital market that practically lacks scientific validation. The study has also found only 135 of these type of applications with a Spanish version available (in a practical sense), developed mostly for daily life of an autistic person and/or people from their immediate environment. By using these applications, there are positive results in terms of learning and permanent adoption of behaviors and skills, but it is necessary to deepen research and further development of applications focused on leisure, resources for parents and professionals, and supporting of autistic adult needs.',\n",
       " 'Self-tracking physiological and psychological data poses the challenge of presentation and interpretation. Insightful narratives for self-tracking data can motivate the user towards constructive self-reflection. One powerful form of narrative that engages audience across various culture and age groups is animated movies. We collected a week of self-reported mood and behavior data from each user and created in Unity a personalized animation based on their data. We evaluated the impact of their video in a randomized control trial with a non-personalized animated video as control. We found that personalized videos tend to be more emotionally engaging, encouraging greater and lengthier writing that indicated self-reflection about moods and behaviors, compared to non-personalized control videos.',\n",
       " 'Reusing passwords across multiple websites is a common practice that compromises security. Recently, Blum and Vempala have proposed password strategies to help people calculate, in their heads, passwords for different sites without dependence on third-party tools or external devices. Thus far, the security and efficiency of these \"mental algorithms\" has been analyzed only theoretically. But are such methods usable? We present the first usability study of humanly computable password strategies, involving a learning phase (to learn a password strategy), then a rehearsal phase (to login to a few websites), and multiple follow-up tests. In our user study, with training, participants were able to calculate a deterministic eight-character password for an arbitrary new website in under 20 seconds.',\n",
       " 'Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process.',\n",
       " \"One of the main methods for interacting with mobile devices today is the error-prone and inflexible touch-screen keyboard. This paper proposes MagBoard: a homomorphic ubiquitous keyboard for mobile devices. MagBoard allows application developers and users to design and print different custom keyboards for the same applications to fit different user's needs. The core idea is to leverage the triaxial magnetometer embedded in standard mobile phones to accurately localize the location of a magnet on a virtual grid superimposed on the printed keyboard. This is achieved through a once in a lifetime fingerprint. MagBoard also provides a number of modules that allow it to cope with background magnetic noise, heterogeneous devices, different magnet shapes, sizes, and strengths, as well as changes in magnet polarity. Our implementation of MagBoard on Android phones with extensive evaluation in different scenarios demonstrates that it can achieve a key detection accuracy of more than 91% for keys as small as 2cm*2cm, reaching 100% for 4cm*4cm keys. This accuracy is robust with different phones and magnets, highlighting MagBoard promise as a homomorphic ubiquitous keyboard for mobile devices.\",\n",
       " 'In addition to user-generated content, Open Educational Resources are increasingly made available on the Web by several institutions and organizations with the aim of being re-used. Nevertheless, it is still difficult for users to find appropriate resources for specific learning scenarios among the vast amount offered on the Web. Our goal is to give users the opportunity to search for authentic resources from the Web and reuse them in a learning context. The LearnWeb-OER platform enhances collaborative searching and sharing of educational resources providing specific means and facilities for education. In the following, we provide a description of the functionalities that support users in collaboratively collecting, selecting, annotating and discussing search results and learning resources.',\n",
       " 'We present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language. Using DSTC2 as seed data, we trained natural language understanding (NLU) and generation (NLG) networks for each agent and let the agents interact online. We model the interaction as a stochastic collaborative game where each agent (player) has a role (\"assistant\", \"tourist\", \"eater\", etc.) and their own objectives, and can only interact via natural language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own NLU and NLG, the other agent\\'s NLU, Policy, and NLG). In our evaluation, we show that the stochastic-game agents outperform deep learning based supervised baselines.',\n",
       " 'Swarm systems consist of large numbers of robots that collaborate autonomously. With an appropriate level of human control, swarm systems could be applied in a variety of contexts ranging from search-and-rescue situations to Cyber defence. The two decision making cycles of swarms and humans operate on two different time-scales, where the former is normally orders of magnitude faster than the latter. Closing the loop at the intersection of these two cycles will create fast and adaptive human-swarm teaming networks. This paper brings desperate pieces of the ground work in this research area together to review this multidisciplinary literature. We conclude with a framework to synthesize the findings and summarize the multi-modal indicators needed for closed-loop human-swarm adaptive systems.',\n",
       " 'Scholarly articles publishing and getting cited has become a way of life for academicians. These scholarly publications shape up the career growth of not only the authors but also of the country, continent and the technological domains. Author affiliations, country and other information of an author coupled with data analytics can provide useful and insightful results. However, massive and complete data is required to perform this research. Google scholar which is a comprehensive and free repository of scholarly articles has been used as a data source for this purpose. Data scraped from Google scholar when stored as a graph and visualized in the form of nodes and relationships, can offer discerning and concealed information. Such as, evident domain shift of an author, various research domains spread for an author, prediction of emerging domain and sub domains, detection of journal and author level citation cartel behaviors etc. The data from graph database is also used in computation of scholastic indicators for the journals. Eventually, econometric model, named Cobb Douglas model is used to compute the journals Modeling \"Internationality\" Index based on these scholastic indicators.',\n",
       " 'Multivariate spatial data plays an important role in computational science and engineering simulations. The potential features and hidden relationships in multivariate data can assist scientists to gain an in-depth understanding of a scientific process, verify a hypothesis and further discover a new physical or chemical law. In this paper, we present a comprehensive survey of the state-of-the-art techniques for multivariate spatial data visualization. We first introduce the basic concept and characteristics of multivariate spatial data, and describe three main tasks in multivariate data visualization: feature classification, fusion visualization, and correlation analysis. Finally, we prospect potential research topics for multivariate data visualization according to the current research.',\n",
       " 'We use an immersive virtual reality environment to explore the intricate social cues that underlie non-verbal communication involved in a pedestrian\\'s crossing decision. We \"hack\" non-verbal communication between pedestrian and vehicle by engineering a set of 15 vehicle trajectories, some of which follow social conventions and some that break them. By subverting social expectations of vehicle behavior we show that pedestrians may use vehicle kinematics to infer social intentions and not merely as the state of a moving object. We investigate human behavior in this virtual world by conducting a study of 22 subjects, with each subject experiencing and responding to each of the trajectories by moving their body, legs, arms, and head in both the physical and the virtual world. Both quantitative and qualitative responses are collected and analyzed, showing that, in fact, social cues can be engineered through vehicle trajectory manipulation. In addition, we demonstrate that immersive virtual worlds which allow the pedestrian to move around freely, provide a powerful way to understand both the mechanisms of human perception and the social signaling involved in pedestrian-vehicle interaction.',\n",
       " \"Notifications provide a unique mechanism for increasing the effectiveness of real-time information delivery systems. However, notifications that demand users' attention at inopportune moments are more likely to have adverse effects and might become a cause of potential disruption rather than proving beneficial to users. In order to address these challenges a variety of intelligent notification mechanisms based on monitoring and learning users' behavior have been proposed. The goal of such mechanisms is maximizing users' receptivity to the delivered information by automatically inferring the right time and the right context for sending a certain type of information.\\n  This article provides an overview of the current state of the art in the area of intelligent notification mechanisms that relies on the awareness of users' context and preferences. More specifically, we first present a survey of studies focusing on understanding and modeling users' interruptibility and receptivity to notifications from desktops and mobile devices. Then, we discuss the existing challenges and opportunities in developing mechanisms for intelligent notification systems in a variety of application scenarios.\",\n",
       " 'We present a novel platform for the interactive visualization of very large graphs. The platform enables the user to interact with the visualized graph in a way that is very similar to the exploration of maps at multiple levels. Our approach involves an offline preprocessing phase that builds the layout of the graph by assigning coordinates to its nodes with respect to a Euclidean plane. The respective points are indexed with a spatial data structure, i.e., an R-tree, and stored in a database. Multiple abstraction layers of the graph based on various criteria are also created offline, and they are indexed similarly so that the user can explore the dataset at different levels of granularity, depending on her particular needs. Then, our system translates user operations into simple and very efficient spatial operations (i.e., window queries) in the backend. This technique allows for a fine-grained access to very large graphs with extremely low latency and memory requirements and without compromising the functionality of the tool. Our web-based prototype supports three main operations: (1) interactive navigation, (2) multi-level exploration, and (3) keyword search on the graph metadata.',\n",
       " 'Sharing live telepresence experiences for teleconferencing or remote collaboration receives increasing interest with the recent progress in capturing and AR/VR technology. Whereas impressive telepresence systems have been proposed on top of on-the-fly scene capture, data transmission and visualization, these systems are restricted to the immersion of single or up to a low number of users into the respective scenarios. In this paper, we direct our attention on immersing significantly larger groups of people into live-captured scenes as required in education, entertainment or collaboration scenarios. For this purpose, rather than abandoning previous approaches, we present a range of optimizations of the involved reconstruction and streaming components that allow the immersion of a group of more than 24 users within the same scene - which is about a factor of 6 higher than in previous work - without introducing further latency or changing the involved consumer hardware setup. We demonstrate that our optimized system is capable of generating high-quality scene reconstructions as well as providing an immersive viewing experience to a large group of people within these live-captured scenes.',\n",
       " \"Data analytics software applications have become an integral part of the decision-making process of analysts. Users of such a software face challenges due to insufficient product and domain knowledge, and find themselves in need of help. To alleviate this, we propose a task-aware command recommendation system, to guide the user on what commands could be executed next. We rely on topic modeling techniques to incorporate information about user's task into our models. We also present a help prediction model to detect if a user is in need of help, in which case the system proactively provides the aforementioned command recommendations. We leverage the log data of a web-based analytics software to quantify the superior performance of our neural models, in comparison to competitive baselines.\",\n",
       " 'Laparoscopic Surgery (LS) is a modern surgical technique whereby the surgery is performed through an incision with tools and camera as opposed to conventional open surgery. This promises minimal recovery times and less hemorrhaging. Multi view LS is the latest development in the field, where the system uses multiple cameras to give the surgeon more information about the surgical site, potentially making the surgery easier. In this publication, we study the gaze patterns of a high performing subject in a multi-view LS environment and compare it with that of a novice to detect the differences between the gaze behavior. This was done by conducting a user study with 20 university students with varying levels of expertise in Multi-view LS. The subjects performed an laparoscopic task in simulation with three cameras (front/top/side). The subjects were then separated as high and low performers depending on the performance times and their data was analyzed. Our results show statistically significant differences between the two behaviors. This opens up new areas from of training novices to Multi-view LS to making smart displays that guide your shows the optimum view depending on the situation.',\n",
       " 'In this paper, we explore the role that attribution plays in shaping user reactions to content reuse, or remixing, in a large user-generated content community. We present two studies using data from the Scratch online community -- a social media platform where hundreds of thousands of young people share and remix animations and video games. First, we present a quantitative analysis that examines the effects of a technological design intervention introducing automated attribution of remixes on users\\' reactions to being remixed. We compare this analysis to a parallel examination of \"manual\" credit-giving. Second, we present a qualitative analysis of twelve in-depth, semi-structured, interviews with Scratch participants on the subject of remixing and attribution. Results from both studies suggest that automatic attribution done by technological systems (i.e., the listing of names of contributors) plays a role that is distinct from, and less valuable than, credit which may superficially involve identical information but takes on new meaning when it is given by a human remixer. We discuss the implications of these findings for the designers of online communities and social media platforms.',\n",
       " 'Gaze tracking is an important technology as the system can give information about a person from what and where the person is seeing. There have been many attempts to make robust and accurate gaze trackers using either monitor or wearable devices. However, those contraptions often require fine individual calibration per session and/or require a person wearing a device, which may not be suitable for certain situations. In this paper, we propose a robust and a completely noninvasive gaze tracking system that involves neither complex calibrations nor the use of wearable devices. We achieve this via direct eye reflection analysis by building a real-time system that effectively enables it. We also show several interesting applications for our system including experiments with young children.',\n",
       " 'When we interact with small screen devices, sometimes we make errors, due to our abilities/disabilities, contextual factors that distract our attention or problems related to the interface. Recovering from these errors may be time consuming or cause frustration. Predicting and learning these errors based on the previous user interaction and contextual factors, and adapting user interface to prevent from these errors can improve user performance and satisfaction. In this paper, we propose a system that aims to monitor user performance and contextual changes and do adaptations based on the user performance by using machine learning techniques. Here, we briefly present our systematic literature review findings and discuss our research questions towards developing such an adaptive system.',\n",
       " 'Loosely based on principles of similarity-attraction, robots intended for social contexts are being designed with increasing human similarity to facilitate their reception by and communication with human interactants. However, the observation of an uncanny valley - the phenomenon in which certain humanlike entities provoke dislike instead of liking - has lead some to caution against this practice. Substantial evidence supports both of these contrasting perspectives on the design of social technologies. Yet, owing to both empirical and theoretical inconsistencies, the relationship between anthropomorphic design and people\\'s liking of the technology remains poorly understood.\\n  Here we present three studies which investigate people\\'s explicit ratings of and behavior towards a large sample of real-world robots. The results show a profound \"valley effect\" on people\\'s \\\\emph{willingness} to interact with humanlike robots, thus highlighting the formidable design challenge the uncanny valley poses for social robotics. In addition to advancing uncanny valley theory, Studies 2 and 3 contribute and validate a novel laboratory task for objectively measuring people\\'s perceptions of humanlike robots.',\n",
       " 'BCI algorithm development has long been hampered by two major issues: small sample sets and a lack of reproducibility. We offer a solution to both of these problems via a software suite that streamlines both the issues of finding and preprocessing data in a reliable manner, as well as that of using a consistent interface for machine learning methods. By building on recent advances in software for signal analysis implemented in the MNE toolkit, and the unified framework for machine learning offered by the scikit-learn project, we offer a system that can improve BCI algorithm development. This system is fully open-source under the BSD licence and available at https://github.com/NeuroTechX/moabb. To validate our efforts, we analyze a set of state-of-the-art decoding algorithms across 12 open access datasets, with over 250 subjects. Our analysis confirms that different datasets can result in very different results for identical processing pipelines, highlighting the need for trustworthy algorithm benchmarking in the field of BCIs, and further that many previously validated methods do not hold up when applied across different datasets, which has wide-reaching implications for practical BCIs.',\n",
       " 'Detection of engagement during a conversation is an important function of human-robot interaction. The level of user engagement can influence the dialogue strategy of the robot. Our motivation in this work is to detect several behaviors which will be used as social signal inputs for a real-time engagement recognition model. These behaviors are nodding, laughter, verbal backchannels and eye gaze. We describe models of these behaviors which have been learned from a large corpus of human-robot interactions with the android robot ERICA. Input data to the models comes from a Kinect sensor and a microphone array. Using our engagement recognition model, we can achieve reasonable performance using the inputs from automatic social signal detection, compared to using manual annotation as input.',\n",
       " 'Despite the fact that advertisements (ads) often include strongly emotional content, very little work has been devoted to affect recognition (AR) from ads. This work explicitly compares content-centric and user-centric ad AR methodologies, and evaluates the impact of enhanced AR on computational advertising via a user study. Specifically, we (1) compile an affective ad dataset capable of evoking coherent emotions across users; (2) explore the efficacy of content-centric convolutional neural network (CNN) features for encoding emotions, and show that CNN features outperform low-level emotion descriptors; (3) examine user-centered ad AR by analyzing Electroencephalogram (EEG) responses acquired from eleven viewers, and find that EEG signals encode emotional information better than content descriptors; (4) investigate the relationship between objective AR and subjective viewer experience while watching an ad-embedded online video stream based on a study involving 12 users. To our knowledge, this is the first work to (a) expressly compare user vs content-centered AR for ads, and (b) study the relationship between modeling of ad emotions and its impact on a real-life advertising application.',\n",
       " 'Brain computer interfaces (BCIs) offer individuals suffering from major disabilities an alternative method to interact with their environment. Sensorimotor rhythm (SMRs) based BCIs can successfully perform control tasks; however, the traditional SMR paradigms intuitively disconnect the control and real task, making them non-ideal for complex control scenarios. In this study, we design a new, intuitively connected motor imagery (MI) paradigm using hierarchical common spatial patterns (HCSP) and context information to effectively predict intended hand grasps from electroencephalogram (EEG) data. Experiments with 5 participants yielded an aggregate classification accuracy--intended grasp prediction probability--of 64.5\\\\% for 8 different hand gestures, more than 5 times the chance level.',\n",
       " 'In this paper we outline an initial typology and framework for the purpose of profiling human-machine networks, that is, collective structures where humans and machines interact to produce synergistic effects. Profiling a human-machine network along the dimensions of the typology is intended to facilitate access to relevant design knowledge and experience. In this way the profiling of an envisioned or existing human-machine network will both facilitate relevant design discussions and, more importantly, serve to identify the network type. We present experiences and results from two case trials: a crisis management system and a peer-to-peer reselling network. Based on the lessons learnt from the case trials we suggest potential benefits and challenges, and point out needed future work.',\n",
       " 'Moderate to vigorous intensity physical activity has an established preventative role in obesity, cardiovascular disease, and diabetes. However recent evidence suggests that sitting time affects health negatively independent of whether adults meet prescribed physical activity guidelines. Since many of us spend long hours daily sitting in front of a host of electronic screens, this is cause for concern. In this paper, we describe a set of three prototype digital games created for encouraging light-intensity physical activity during short breaks at work. The design of these kinds of games is a complex process that must consider motivation strategies, interaction methodology, usability and ludic aspects. We present design guidelines for technologies that encourage physical activity in the workplace that we derived from a user evaluation using the prototypes. Although the design guidelines can be seen as general principles, we conclude that they have to be considered differently for different workplace cultures and workspaces. Our study was conducted with users who have some experience playing casual games on their mobile devices and were able and willing to increase their physical activity.',\n",
       " \"When people use electronic media for their communication, Computer-Mediated Communication (CMC) theories describe the social and communicative aspects of people's interpersonal transactions. When people interact via a remote-controlled robot, many of the CMC theses hold. Yet, what if people communicate with a conversation robot that is (partly) autonomous? Do the same theories apply? This paper discusses CMC theories in confrontation with observations and research data gained from human-robot communication. As a result, I argue for an addition to CMC theorizing when the robot as a medium itself becomes the communication partner. In view of the rise of social robots in coming years, I define the theoretical precepts of a possible next step in CMC, which I elaborate in a second paper.\",\n",
       " 'We conducted an eye-tracking study where 30 participants performed searches on the web. We measured their topical knowledge before and after each task. Their eye-fixations were labelled as \"reading\" or \"scanning\". The series of reading fixations in a line, called \"reading-sequences\" were characterized by their length in pixels, fixation duration, and the number of fixations making up the sequence. We hypothesize that differences in knowledge-change of participants are reflected in their eye-tracking measures related to reading. Our results show that the participants with higher change in knowledge differ significantly in terms of their total reading-sequence-length, reading-sequence-duration, and number of reading fixations, when compared to participants with lower knowledge-change.',\n",
       " \"Data-driven decision-making consequential to individuals raises important questions of accountability and justice. Indeed, European law provides individuals limited rights to 'meaningful information about the logic' behind significant, autonomous decisions such as loan approvals, insurance quotes, and CV filtering. We undertake three experimental studies examining people's perceptions of justice in algorithmic decision-making under different scenarios and explanation styles. Dimensions of justice previously observed in response to human decision-making appear similarly engaged in response to algorithmic decisions. Qualitative analysis identified several concerns and heuristics involved in justice perceptions including arbitrariness, generalisation, and (in)dignity. Quantitative analysis indicates that explanation styles primarily matter to justice perceptions only when subjects are exposed to multiple different styles---under repeated exposure of one style, scenario effects obscure any explanation effects. Our results suggests there may be no 'best' approach to explaining algorithmic decisions, and that reflection on their automated nature both implicates and mitigates justice dimensions.\",\n",
       " \"So-called 'social bots' have garnered a lot of attention lately. Previous research showed that they attempted to influence political events such as the Brexit referendum and the US presidential elections. It remains, however, somewhat unclear what exactly can be understood by the term 'social bot'. This paper addresses the need to better understand the intentions of bots on social media and to develop a shared understanding of how 'social' bots differ from other types of bots. We thus describe a systematic review of publications that researched bot accounts on social media. Based on the results of this literature review, we propose a scheme for categorising bot accounts on social media sites. Our scheme groups bot accounts by two dimensions - Imitation of human behaviour and Intent.\",\n",
       " 'Safety and reliability are the main issues for designing assistance wearable virtual environment of technical gesture in aerospace, or health application domains. That needs the integration in the same isomorphic engineering framework of human requirements, systems requirements and the rationale of their relation to the natural and artifactual environment.To explore coupling integration and design functional organization of support technical gesture systems, firstly ecological psychologyprovides usa heuristicconcept: the affordance. On the other hand mathematical theory of integrative physiology provides us scientific concepts: the stabilizing auto-association principle and functional interaction.After demonstrating the epistemological consistence of these concepts, we define an isomorphic framework to describe and model human systems integration dedicated to human in-the-loop system engineering.We present an experimental approach of safe design of assistance wearable virtual environment of gesture based in laboratory and parabolic flights. On the results, we discuss the relevance of our conceptual approach and the applications to future assistance of gesture wearable systems engineering.',\n",
       " \"The UITP workshop series brings together researchers interested in designing, developing and evaluating user interfaces for automated reasoning tools, such as interactive proof assistants, automated theorem provers, model finders, tools for formal methods, and tools for visualising and manipulating logical formulas and proofs. The twelth edition of UITP took place in Coimbra, Portugal, and was part of the International Joint Conference on Automated Reasoning (IJCAR'16). The workshop consisted of an invited talk, six presentations of submitted papers and lively hands-on session for reasoning tools and their user-interface. These post-proceedings contain four contributed papers accepted for publication after a second round of reviewing after the workshop as well as the invited paper.\",\n",
       " \"There has been increasing interest in deploying IoT devices to study human behaviour in locations such as homes and offices. Such devices can be deployed in a laboratory or `in the wild' in natural environments. The latter allows one to collect behavioural data that is not contaminated by the artificiality of a laboratory experiment. Using IoT devices in ordinary environments also brings the benefits of reduced cost, as compared with lab experiments, and less disturbance to the participants' daily routines which in turn helps with recruiting them into the research. However, in this case, it is essential to have an IoT infrastructure that can be easily and swiftly installed and from which real-time data can be securely and straightforwardly collected. In this paper, we present MakeSense, an IoT testbed that enables real-world experimentation for large scale social research on indoor activities through real-time monitoring and/or situation-aware applications. The testbed features quick setup, flexibility in deployment, the integration of a range of IoT devices, resilience, and scalability. We also present two case studies to demonstrate the use of the testbed, one in homes and one in offices.\",\n",
       " 'This paper presents a novel approach to understand specific student behavior in MOOCs. Instructors currently perceive participants only as one homogeneous group. In order to improve learning outcomes, they encourage students to get active in the discussion forum and remind them of approaching deadlines. While these actions are most likely helpful, their actual impact is often not measured. Additionally, it is uncertain whether such generic approaches sometimes cause the opposite effect, as some participants are bothered with irrelevant information. On the basis of fine granular events emitted by our learning platform, we derive metrics and enable teachers to employ clustering, in order to divide the vast field of participants into meaningful subgroups to be addressed individually.',\n",
       " 'Each time a learner in a self-paced online course seeks to answer an assessment question, it takes some time for the student to read the question and arrive at an answer to submit. If multiple attempts are allowed and the first answer is incorrect, it takes some time to provide a second answer. Here we study the distribution of such \"response times.\" We find that the log-normal statistical model for such times, previously suggested in the literature, holds for online courses. Users who, according to this model, tend to take longer on submits are more likely to complete the course, have a higher level of engagement, and achieve a higher grade. This finding can be the basis for designing interventions in online courses, such as MOOCs, which would encourage \"fast\" users to slow down.',\n",
       " 'Skyline queries have wide-ranging applications in fields that involve multi-criteria decision making, including tourism, retail industry, and human resources. By automatically removing incompetent candidates, skyline queries allow users to focus on a subset of superior data items (i.e., the skyline), thus reducing the decision-making overhead. However, users are still required to interpret and compare these superior items manually before making a successful choice. This task is challenging because of two issues. First, people usually have fuzzy, unstable, and inconsistent preferences when presented with multiple candidates. Second, skyline queries do not reveal the reasons for the superiority of certain skyline points in a multi-dimensional space. To address these issues, we propose SkyLens, a visual analytic system aiming at revealing the superiority of skyline points from different perspectives and at different scales to aid users in their decision making. Two scenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of attributes. A qualitative study is also conducted to show that users can efficiently accomplish skyline understanding and comparison tasks with SkyLens.',\n",
       " 'Security and usability issues with passwords suggest a need for a new authentication scheme. Several alternatives involve a physical device or token. We investigate one such alternative, Pico: an authentication scheme that utilizes multiple wearable devices. We present the grounded theory results of a series of semi-structured interviews for exploring perceptions of this scheme. We found that the idea of carrying physical devices increases perceived personal responsibility for secure authentication, making the risks and inconvenience associated with loss and theft salient for participants. Although our work is focused on Pico, the results of the study contribute to a broader understanding of user perception and concerns of responsibility for any token-based authentication schemes.',\n",
       " 'We present the Pascal animal classes Eye Tracking database. Our database comprises eye movement recordings compiled from forty users for the bird, cat, cow, dog, horse and sheep {trainval} sets from the VOC 2012 image set. Different from recent eye-tracking databases such as \\\\cite{kiwon_cvpr13_gaze,PapadopoulosCKF14}, a salient aspect of PET is that it contains eye movements recorded for both the free-viewing and visual search task conditions. While some differences in terms of overall gaze behavior and scanning patterns are observed between the two conditions, a very similar number of fixations are observed on target objects for both conditions. As a utility application, we show how feature pooling around fixated locations enables enhanced (animal) object classification accuracy.',\n",
       " 'In recent years, Augmented Reality (AR) and Virtual Reality (VR) have gained considerable commercial traction, with Facebook acquiring Oculus VR for \\\\$2 billion, Magic Leap attracting more than \\\\$500 million of funding, and Microsoft announcing their HoloLens head-worn computer. Where is humanity headed: a brave new dystopia-or a paradise come true?\\n  In this article, we present discussions, which started at the symposium \"Making Augmented Reality Real\", held at Nara Institute of Science and Technology in August 2014. Ten scientists were invited to this three-day event, which started with a full day of public presentations and panel discussions (video recordings are available at the event web page), followed by two days of roundtable discussions addressing the future of AR and VR.',\n",
       " 'Visualization tools usually leverage a single interaction paradigm (e.g., manual view specification, visualization by demonstration, etc.), which fosters the process of visualization construction. A large body of work has investigated the effectiveness of individual interaction paradigms, building an understanding of advantages and disadvantages of each in isolation. However, how can we leverage the benefits of multiple interaction paradigms by combining them into a single tool? We currently lack a holistic view of how interaction paradigms that use the same input modality (e.g., mouse) can be combined into a single tool and how people use such tools. To investigate opportunities and challenges in combining paradigms, we first created a multi-paradigm prototype (Liger) that combines two mouse-based interaction paradigms (manual view specification and visualization by demonstration) in a unified tool. We then conducted an exploratory study with Liger, providing initial evidence that people 1) use both paradigms interchangeably, 2) seamlessly switch between paradigms based on the operation at hand, and 3) choose to successfully complete a single operation using a combination of both paradigms.',\n",
       " 'Health trackers are widely adopted to support individuals with daily health and wellness activity tracking. They can help increase steps taken, enhance sleeping pattern, improve healthy diet, and promote the overall health. Despite the growth in wearable adoption, their real-life use is still questionable. While some users derive long-term values from their trackers, others face barriers to integrate it into their daily routine. Studies have analysed technical aspects of these barriers. In this paper, we analyse the behavioural factors of discouragement and wearable abandonment strictly tied to user habits and lifestyle circumstances. A data analysis was conducted on 8 of the highly rated wearables for 2017. The analysis collected sale posts on Kijiji and Gumtree, the second sales online retailers for both the Italian and UK market, respectively. We extracted insights from the posts about user motives, highlighted technology condition and limitations, and timeframe before the abandonment. The findings revealed certain user behavioural patterns when abandoning their wearables. In addition, analysing the posts showed other motives for the posts and not strictly related to wearable abandonment.',\n",
       " 'Movement disorders are becoming one of the leading causes of functional disability due to aging populations and extended life expectancy. Wearable health monitoring is emerging as an effective way to augment clinical care for movement disorders. However, wearable devices face a number of adaptation and technical challenges that hinder their widespread adoption. To address these challenges, we introduce OpenHealth, an open source platform for wearable health monitoring. OpenHealth aims to design a standard set of hardware/software and wearable devices that can enable autonomous collection of clinically relevant data. The OpenHealth platform includes a wearable device, standard software interfaces and reference implementations of human activity and gesture recognition applications.',\n",
       " 'This paper presents a new design suggestion for cascading pull-down menus to make user interaction with it faster and therefore easier: The Wing Expansion Menu (WEM). The proposal is based on the Steering Law, which implies a wider steering path for menu items. Our Approach combines this enlargement with a heuristic function that provides a probability with which the user will select an menu item. The menu can also be adapted to a wide variety of situations using certain variables. A user study of a WEM against a standard pull-down menu showed an average improvement of 18.63% in user interaction speed. A second user study, which evaluated one of the significant innovations of the WEM compared to a similar approach, showed an average improvement of 7.01% in user interaction speed.',\n",
       " 'It is important for organisations to ensure that their privacy policies are General Data Protection Regulation (GDPR) compliant, and this has to be done by the May 2018 deadline. However, it is also important for these policies to be designed with the needs of the human recipient in mind. We carried out an investigation to find out how best to achieve this.\\n  We commenced by synthesising the GDPR requirements into a checklist-type format. We then derived a list of usability design guidelines for privacy notifications from the research literature. We augmented the recommendations with other findings reported in the research literature, in order to confirm the guidelines. We conclude by providing a usable and GDPR-compliant privacy policy template for the benefit of policy writers.',\n",
       " \"With the increasing complexity of modern industrial automatic and robotic systems, an increasing burden is put on the operators, who are requested to supervise and interact with very complex systems, typically under challenging and stressful conditions. To overcome this issue, it is necessary to adopt a responsible approach based on the anthropocentric design methodology, such that machines adapt to the humans capabilities, and not vice versa. Moving along these lines, in this paper we consider an integrated methodological design approach, which we call MATE, consisting in devising complex automatic or robotic solutions that measure current operator's status, adapting the interaction accordingly, and providing her/him with proper training to improve the interaction and learn lacking skills and expertise. Accordingly, a MATE system is intended to be easily usable for all users, thus meeting the principles of inclusive design. Using such a MATE system gives rise to several ethical and social implications, which are discussed in this paper. Additionally, a discussion about which factors in the organization of companies are critical with respect to the introduction of a MATE system is presented.\",\n",
       " \"Crowdsourcing information constitutes an important aspect of human-in-the-loop learning for researchers across multiple disciplines such as AI, HCI, and social science. While using crowdsourced data for subjective tasks is not new, eliciting useful insights from such data remains challenging due to a variety of factors such as difficulty of the task, personal prejudices of the human evaluators, lack of question clarity, etc. In this paper, we consider one such subjective evaluation task, namely that of estimating experienced emotions of distressed individuals who are conversing with a human listener in an online coaching platform. We explore strategies to aggregate the evaluators choices, and show that a simple voting consensus is as effective as an optimum aggregation method for the task considered. Intrigued by how an objective assessment would compare to the subjective evaluation of evaluators, we also designed a machine learning algorithm to perform the same task. Interestingly, we observed a machine learning algorithm that is not explicitly modeled to characterize evaluators' subjectivity is as reliable as the human evaluation in terms of assessing the most dominant experienced emotions.\",\n",
       " 'Humanoid robots have apparently similar body structure like human beings. Due to their technical design, they are sharing the same workspace with humans. They are placed to clean things, to assist old age people, to entertain us and most importantly to serve us. To be acceptable in the household, they must have higher level of intelligence than industrial robots and they must be social and capable of interacting people around it, who are not supposed to be robot specialist. All these come under the field of human robot interaction (HRI). There are various modes like speech, gesture, behavior etc. through which human can interact with robots. To solve all these challenges, a multimodel technique has been introduced where gesture as well as speech is used as a mode of interaction.',\n",
       " 'The ability of sensing breathing is becoming an increasingly important function for technology that aims at supporting both psychological and physical wellbeing. We demonstrate ThermSense, a new breathing sensing platform based on smartphone technology and low-cost thermal camera, which allows a user to measure his/her breathing pattern in a contact-free manner. With the designed key functions of Thermal Voxel Integration-based breathing estimation and respiration variability spectrogram (RVS, bi-dimensional representation of breathing dynamics), the developed platform provides scalability and flexibility for gathering respiratory physiological measurements ubiquitously. The functionality could be used for a variety of applications from stress monitoring to respiration training.',\n",
       " 'Multidimensional scaling allows visualizing high-dimensional data as 2D maps with the premise that insights in 2D reveal valid information in high-dimensions. However, the resulting projections suffer from artifacts such as bad local neighborhood preservation and clusters tearing. Interactively coloring the projection according to the discrepancy between original proximities relative to a reference item reveals these artifacts, but it is not clear if conveying these proximities using color and displaying only local information really helps the visual analysis of projections. We conducted a controlled experiment to investigate the relevance of this interactive technique to help the visual analysis of any projection regardless its quality. We compared the bare projection to the interactive coloring of the original proximities on different visual analysis tasks involving outliers and clusters. Results indicate that the interactive coloring is worthwhile for local tasks as it is significantly robust to projection artifacts whereas the projection is not. However this interactive technique does not help significantly for visual clustering tasks for that projections already give a suitable overview.',\n",
       " \"Ontologies are formal representations of concepts and complex relationships among them. They have been widely used to capture comprehensive domain knowledge in areas such as biology and medicine, where large and complex ontologies can contain hundreds of thousands of concepts. Especially due to the large size of ontologies, visualisation is useful for authoring, exploring and understanding their underlying data. Existing ontology visualisation tools generally focus on the hierarchical structure, giving much less emphasis to non-hierarchical associations. In this paper we present OntoPlot, a novel visualisation specifically designed to facilitate the exploration of all concept associations whilst still showing an ontology's large hierarchical structure. This hybrid visualisation combines icicle plots, visual compression techniques and interactivity, improving space-efficiency and reducing visual structural complexity. We conducted a user study with domain experts to evaluate the usability of OntoPlot, comparing it with the de facto ontology editor Prot{\\\\'e}g{\\\\'e}. The results confirm that OntoPlot attains our design goals for association-related tasks and is strongly favoured by domain experts.\",\n",
       " 'The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.',\n",
       " \"To make evidence-based recommendations to decision-makers, researchers conducting systematic reviews and meta-analyses must navigate a garden of forking paths: a series of analytical decision-points, each of which has the potential to influence findings. To identify challenges and opportunities related to designing systems to help researchers manage uncertainty around which of multiple analyses is best, we interviewed 11 professional researchers who conduct research synthesis to inform decision-making within three organizations. We conducted a qualitative analysis identifying 480 analytical decisions made by researchers throughout the scientific process. We present descriptions of current practices in applied research synthesis and corresponding design challenges: making it more feasible for researchers to try and compare analyses, shifting researchers' attention from rationales for decisions to impacts on results, and supporting communication techniques that acknowledge decision-makers' aversions to uncertainty. We identify opportunities to design systems which help researchers explore, reason about, and communicate uncertainty in decision-making about possible analyses in research synthesis.\",\n",
       " 'Over the last years, most websites on which users can register (e.g., email providers and social networks) adopted CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) as a countermeasure against automated attacks. The battle of wits between designers and attackers of CAPTCHAs led to current ones being annoying and hard to solve for users, while still being vulnerable to automated attacks.\\n  In this paper, we propose CAPTCHaStar, a new image-based CAPTCHA that relies on user interaction. This novel CAPTCHA leverages the innate human ability to recognize shapes in a confused environment. We assess the effectiveness of our proposal for the two key aspects for CAPTCHAs, i.e., usability, and resiliency to automated attacks. In particular, we evaluated the usability, carrying out a thorough user study, and we tested the resiliency of our proposal against several types of automated attacks: traditional ones; designed ad-hoc for our proposal; and based on machine learning. Compared to the state of the art, our proposal is more user friendly (e.g., only some 35% of the users prefer current solutions, such as text-based CAPTCHAs) and more resilient to automated attacks.',\n",
       " 'eSports is a developing multidisciplinary research area. At present, there is a lack of relevant data collected from real eSports athletes and lack of platforms which could be used for the data collection and further analysis. In this paper, we present a sensing system for enabling the data collection from professional athletes. Also, we report on the case study about collecting and analyzing the gaze data from Monolith professional eSports team specializing in Counter-Strike: Global Offensive (CS:GO) discipline. We perform a comparative study on assessing the gaze of amateur players and professional athletes. The results of our work are vital for ensuring eSports data collection and the following analysis in the scope of scouting or assessing the eSports players and athletes.',\n",
       " \"Nowadays many software development frameworks implement Behavior-Driven Development (BDD) as a mean of automating the test of interactive systems under construction. Automated testing helps to simulate user's action on the User Interface and therefore check if the system behaves properly and in accordance to Scenarios that describe functional requirements. However, most of tools supporting BDD requires that tests should be written using low-level events and components that only exist when the system is already implemented. As a consequence of such low-level of abstraction, BDD tests can hardly be reused with diverse artifacts and with versions of the system. To address this problem, this paper proposes to raise the abstraction level by the means of a behavior-based ontology that is aimed at supporting test automation. The paper presents an ontology and an on-tology-based approach for automating the test of functional requirements of interactive systems. With the help of a case study for the flight tickets e-commerce domain, we demonstrate how tests written using our ontology can be used to assess functional requirements using different artifacts, from low-fidelity to full-fledged UI Prototypes.\",\n",
       " \"Vygotsky's notions of Zone of Proximal Development and Dynamic Assessment emphasize the importance of personalized learning that adapts to the needs and abilities of the learners and enables more efficient learning. In this work we introduce a novel adaptive learning engine called E-gostky that builds on these concepts to personalize the learning path within an e-learning system. E-gostky uses machine learning techniques to select the next content item that will challenge the student but will not be overwhelming, keeping students in their Zone of Proximal Development.\\n  To evaluate the system, we conducted an experiment where hundreds of students from several different elementary schools used our engine to learn fractions for five months. Our results show that using E-gostky can significantly reduce the time required to reach similar mastery. Specifically, in our experiment, it took students who were using the adaptive learning engine $17\\\\%$ less time to reach a similar level of mastery as of those who didn't. Moreover, students made greater efforts to find the correct answer rather than guessing and class teachers reported that even students with learning disabilities showed higher engagement.\",\n",
       " \"The importance of community resilience has become increasingly recognized in emergency management and post-disaster community well-being. To this end, three seismic resilience planning initiatives have been conducted in the U.S. in the last decade to envision the current state of community resilience. Experts who participated in these initiatives confronted challenges that must be addressed for future planning initiatives. We interviewed eighteen participants to learn about the community resilience planning process, its characteristics, and challenges. Conducting qualitative content analysis, we identify six main challenges to community resilience planning: complex network systems; interdependencies among built environment systems; inter-organizational collaboration; connections between the built environment and social systems; communications between built environment and social institutions' experts; and communication among decision-makers, social stakeholders, and community members. To overcome the identified challenges, we discuss the capability of human-centered simulation modeling as a combination of simulation modeling and human-centered design to facilitate community resilience planning.\",\n",
       " 'Crowd creativity is typically associated with peer-production communities focusing on artistic products like animations, video games, and music, but less frequently to Open Source Software (OSS), despite the fact that also developers must be creative to come up with new solutions to their technical challenges. In this paper, we conduct a study to further the understanding of which factors from prior work in both OSS and art communities are predictive of successful collaboration - defined as reuse of previous songs - in three different songwriting communities, namely Songtree, Splice, and ccMixter. The main findings from this study confirm that the success of collaborations is associated with high community status of recognizable authors and low degree of derivativity of songs.',\n",
       " 'eSports industry has greatly progressed within the last decade in terms of audience and fund rising, broadcasting, networking and hardware. Since the number and quality of professional team has evolved too, there is a reasonable need in improving skills and training process of professional eSports athletes. In this work, we demonstrate a system able to collect heterogeneous data (physiological, environmental, video, telemetry) and guarantying synchronization with 10 ms accuracy. In particular, we demonstrate how to synchronize various sensors and ensure post synchronization, i.e. logged video, a so-called demo file, with the sensors data. Our experimental results achieved on the CS:GO game discipline show up to 3 ms accuracy of the time synchronization of the gaming computer.',\n",
       " 'In this work, we address the problem of measuring and predicting temporal video saliency -- a measure which defines the importance of a video frame for human attention. Unlike the conventional spatial saliency which defines the location of the salient regions within a frame (as it is done for still images), temporal saliency considers importance of a frame as a whole and may not exist apart from context. The proposed interface is an interactive cursor-based algorithm for collecting experimental data about temporal saliency. We collect the first human responses and perform their analysis. As a result, we show that qualitatively, the produced scores have very explicit meaning of the semantic changes in a frame, while quantitatively being highly correlated between all the observers. Apart from that, we show that the proposed tool can simultaneously collect fixations similar to the ones produced by eye-tracker in a more affordable way. Further, this approach may be used for creation of first temporal saliency datasets which will allow training computational predictive algorithms. The proposed interface does not rely on any special equipment, which allows to run it remotely and cover a wide audience.',\n",
       " 'This article presents our steps to integrate complex and partly unstructured medical data into a clinical research database with subsequent decision support. Our main application is an integrated faceted search tool, accompanied by the visualisation of results of automatic information extraction from textual documents. We describe the details of our technical architecture (open-source tools), to be replicated at other universities, research institutes, or hospitals. Our exemplary use cases are nephrology and mammography. The software was first developed in the nephrology domain and then adapted to the mammography use case. We report on these case studies, illustrating how the application can be used by a clinician and which questions can be answered. We show that our architecture and the employed software modules are suitable for both areas of application with a limited amount of adaptations. For example, in nephrology we try to answer questions about the temporal characteristics of event sequences to gain significant insight from the data for cohort selection. We present a versatile time-line tool that enables the user to explore relations between a multitude of diagnosis and laboratory values.',\n",
       " \"Popular crowdsourcing techniques mostly focus on evaluating workers' labeling quality before adjusting their weights during label aggregation. Recently, another cohort of models regard crowdsourced annotations as incomplete tensors and recover unfilled labels by tensor completion. However, mixed strategies of the two methodologies have never been comprehensively investigated, leaving them as rather independent approaches. In this work, we propose $\\\\textit{MiSC}$ ($\\\\textbf{Mi}$xed $\\\\textbf{S}$trategies $\\\\textbf{C}$rowdsourcing), a versatile framework integrating arbitrary conventional crowdsourcing and tensor completion techniques. In particular, we propose a novel iterative Tucker label aggregation algorithm that outperforms state-of-the-art methods in extensive experiments.\",\n",
       " \"The accuracy of Automated Speech Recognition (ASR) technology has improved, but it is still imperfect in many settings. Researchers who evaluate ASR performance often focus on improving the Word Error Rate (WER) metric, but WER has been found to have little correlation with human-subject performance on many applications. We propose a new captioning-focused evaluation metric that better predicts the impact of ASR recognition errors on the usability of automatically generated captions for people who are Deaf or Hard of Hearing (DHH). Through a user study with 30 DHH users, we compared our new metric with the traditional WER metric on a caption usability evaluation task. In a side-by-side comparison of pairs of ASR text output (with identical WER), the texts preferred by our new metric were preferred by DHH participants. Further, our metric had significantly higher correlation with DHH participants' subjective scores on the usability of a caption, as compared to the correlation between WER metric and participant subjective scores. This new metric could be used to select ASR systems for captioning applications, and it may be a better metric for ASR researchers to consider when optimizing ASR systems.\",\n",
       " \"This article introduces an open-source web component, Instant Expert, which allows robust and efficient integration of a natural language question answering system to web-based platforms in any domain. Web Components are a set of web technologies to allow the creation of reusable, customizable, and encapsulated HTML elements. The Instant Expert web component consists of the user input (i.e. text, voice, multi-selection), question processing, and user interface modules. Two use cases are developed to demonstrate the component's features, benefits, and usage. The goal of this project is to pave the way for next-generation information systems by mitigating the challenges of developing voice-enabled and domain-informed smart assistants for communicating knowledge in any domain.\",\n",
       " 'Inertial measurement units have the ability to accurately record the acceleration and angular velocity of human limb segments during discrete joint movements. These movements are commonly used in exercise rehabilitation programmes following orthopaedic surgery such as total knee replacement. This provides the potential for a biofeedback system with data mining technique for patients undertaking exercises at home without physician supervision. We propose to use machine learning techniques to automatically analyse inertial measurement unit data collected during these exercises, and then assess whether each repetition of the exercise was executed correctly or not. Our approach consists of two main phases: signal segmentation, and segment classification. Accurate pre-processing and feature extraction are paramount topics in order for the technique to work. In this paper, we present a classification method for unsupervised rehabilitation exercises, based on a segmentation process that extracts repetitions from a longer signal activity. The results obtained from experimental datasets of both clinical and healthy subjects, for a set of 4 knee exercises commonly used in rehabilitation, are very promising.',\n",
       " \"Internet of Things (IoT) systems have aroused enthusiasm and concerns. Enthusiasm comes from their utilities in people daily life, and concerns may be associated with privacy issues. By using two IoT systems as case-studies, we examine users' privacy beliefs, concerns and attitudes. We focus on four major dimensions: the collection of personal data, the inference of new information, the exchange of information to third parties, and the risk-utility trade-off posed by the features of the system. Altogether, 113 Brazilian individuals answered a survey about such dimensions. Although their perceptions seem to be dependent on the context, there are recurrent patterns. Our results suggest that IoT users can be classified into unconcerned, fundamentalists and pragmatists. Most of them exhibit a pragmatist profile and believe in privacy as a right guaranteed by law. One of the most privacy concerning aspect is the exchange of personal information to third parties. Individuals' perceived risk is negatively correlated with their perceived utility in the features of the system. We discuss practical implications of these results and suggest heuristics to cope with privacy concerns when designing IoT systems.\",\n",
       " 'The health and various ways to improve healthcare systems are one of the most concerns of human in history. By the growth of mobile technology, different mobile applications in the field of the healthcare system are developed. These mobile applications instantly gather and analyze the data of their users to help them in the health area. This volume of data will be a critical problem. Big data in healthcare mobile applications have its challenges and opportunities for the users and developers. Does this amount of gathered data which is increasing day by day can help the human to design new tools in healthcare systems and improve health condition? In this chapter, we will discuss meticulously the challenges and opportunities of big data in the healthcare mobile applications.',\n",
       " \"Mixed reality (MR) ethics occupies a space that intersects with web ethics, emerging tech ethics, healthcare ethics and product ethics (among others). This paper focuses on how we can build an immersive web that encourages ethical development and usage. The technology is beyond emerging (footnote: generally, the ethics of emerging technologies are focused on ethical assessments of research and innovation), but not quite entrenched. We're still in a position to intervene in the development process, instead of attempting to retrofit ethical decisions into an established design. While we have a wider range of data to analyze than most emerging technologies, we're still in a much more speculative state than entrenched technologies. This space is a challenge and an opportunity.\",\n",
       " 'A person\\'s weight status can have profound implications on their life, ranging from mental health, to longevity, to financial income. At the societal level, \"fat shaming\" and other forms of \"sizeism\" are a growing concern, while increasing obesity rates are linked to ever raising healthcare costs. For these reasons, researchers from a variety of backgrounds are interested in studying obesity from all angles. To obtain data, traditionally, a person would have to accurately self-report their body-mass index (BMI) or would have to see a doctor to have it measured. In this paper, we show how computer vision can be used to infer a person\\'s BMI from social media images. We hope that our tool, which we release, helps to advance the study of social aspects related to body weight.',\n",
       " 'A plethora of biometric measures have been proposed in the past. In this paper we introduce a new potential biometric measure: the human tremor. We present a new method for identifying the user of a handheld device using characteristics of the hand tremor measured with a smartphone built-in inertial sensors (accelerometers and gyroscopes). The main challenge of the proposed method is related to the fact that human normal tremor is very subtle while we aim to address real-life scenarios. To properly address the issue, we have relied on weighted Fourier linear combiner for retrieving only the tremor data from the hand movement and random forest for actual recognition. We have evaluated our method on a database with 10 000 samples from 17 persons reaching an accuracy of 76%.',\n",
       " 'We consider the $M$-ary classification problem via crowdsourcing, where crowd workers respond to simple binary questions and the answers are aggregated via decision fusion. The workers have a reject option to skip answering a question when they do not have the expertise, or when the confidence of answering that question correctly is low. We further consider that there are spammers in the crowd who respond to the questions with random guesses. Under the payment mechanism that encourages the reject option, we study the behavior of honest workers and spammers, whose objectives are to maximize their monetary rewards. To accurately characterize human behavioral aspects, we employ prospect theory to model the rationality of the crowd workers, whose perception of costs and probabilities are distorted based on some value and weight functions, respectively. Moreover, we estimate the number of spammers and employ a weighted majority voting decision rule, where we assign an optimal weight for every worker to maximize the system performance. The probability of correct classification and asymptotic system performance are derived. We also provide simulation results to demonstrate the effectiveness of our approach.',\n",
       " \"Quantification of human attention is key to several tasks in mobile human-computer interaction (HCI), such as predicting user interruptibility, estimating noticeability of user interface content, or measuring user engagement. Previous works to study mobile attentive behaviour required special-purpose eye tracking equipment or constrained users' mobility. We propose a novel method to sense and analyse visual attention on mobile devices during everyday interactions. We demonstrate the capabilities of our method on the sample task of eye contact detection that has recently attracted increasing research interest in mobile HCI. Our method builds on a state-of-the-art method for unsupervised eye contact detection and extends it to address challenges specific to mobile interactive scenarios. Through evaluation on two current datasets, we demonstrate significant performance improvements for eye contact detection across mobile devices, users, or environmental conditions. Moreover, we discuss how our method enables the calculation of additional attention metrics that, for the first time, enable researchers from different domains to study and quantify attention allocation during mobile interactions in the wild.\",\n",
       " 'Around-device interaction techniques aim at extending the input space using various sensing modalities on mobile and wearable devices. In this paper, we present our work towards extending the input area of mobile devices using front-facing device-centered cameras that capture reflections in the human eye. As current generation mobile devices lack high resolution front-facing cameras we study the feasibility of around-device interaction using corneal reflective imaging based on a high resolution camera. We present a workflow, a technical prototype and an evaluation, including a migration path from high resolution to low resolution imagers. Our study indicates, that under optimal conditions a spatial sensing resolution of 5 cm in the vicinity of a mobile phone is possible.',\n",
       " 'Virtual reality offers the unique possibility to experience a virtual representation as our own body. In contrast to previous research that predominantly studied this phenomenon for humanoid avatars, our work focuses on virtual animals. In this paper, we discuss different body tracking approaches to control creatures such as spiders or bats and the respective virtual body ownership effects. Our empirical results demonstrate that virtual body ownership is also applicable for nonhumanoids and can even outperform human-like avatars in certain cases. An additional survey confirms the general interest of people in creating such experiences and allows us to initiate a broad discussion regarding the applicability of animal embodiment for educational and entertainment purposes.',\n",
       " 'Computer-based tests (CBTs) play an important role in the professional career of any person. Universities use CBTs for admissions. Further, many large courses use CBTs for evaluation and grading. Almost all software companies use CBTs to offer jobs. However, many of these CBTs do not take attention to the accessibility barriers for persons with disabilities, specifically visually impaired persons. In this paper, we present a study of accessibility barriers in various CBTs as faced by visually impaired persons in India. These barriers have been identified by a questionnaire survey approach. Our analysis of the responses shows that most CBTs do not meet the expectations of visually impaired persons. We conclude the paper with some recommendations to improve accessibility.',\n",
       " 'Traditional data mining algorithms are exceptional at seeing patterns in data that humans cannot, but are often confused by details that are obvious to the organic eye. Algorithms that include humans \"in-the-loop\" have proved beneficial for accuracy by allowing a user to provide direction in these situations, but the slowness of human interactions causes execution times to increase exponentially. Thus, we seek to formalize frameworks that include humans \"over-the-loop\", giving the user an option to intervene when they deem it necessary while not having user feedback be an execution requirement. With this strategy, we hope to increase the accuracy of solutions with minimal losses in execution time. This paper describes our vision of this strategy and associated problems.',\n",
       " \"Consumers often react expressively to products such as food samples, perfume, jewelry, sunglasses, and clothing accessories. This research discusses a multimodal affect recognition system developed to classify whether a consumer likes or dislikes a product tested at a counter or kiosk, by analyzing the consumer's facial expression, body posture, hand gestures, and voice after testing the product. A depth-capable camera and microphone system - Kinect for Windows - is utilized. An emotion identification engine has been developed to analyze the images and voice to determine affective state of the customer. The image is segmented using skin color and adaptive threshold. Face, body and hands are detected using the Haar cascade classifier. Canny edges are identified and the lip, body and hand contours are extracted using spatial filtering. Edge count and orientation around the mouth, cheeks, eyes, shoulders, fingers and the location of the edges are used as features. Classification is done by an emotion template mapping algorithm and training a classifier using support vector machines. The real-time performance, accuracy and feasibility for multimodal affect recognition in feedback assessment are evaluated.\",\n",
       " \"Tracking the movement of human eyes is expected to yield natural and convenient applications based on human-computer interaction (HCI). To implement an effective eye-tracking system, eye movements must be recorded without placing any restriction on the user's behavior or user discomfort. This paper describes an eye movement recording system that offers free-head, simple configuration. It does not require the user to wear anything on her head, and she can move her head freely. Instead of using a computer, the system uses a visual digital signal processor (DSP) camera to detect the position of eye corner, the center of pupil and then calculate the eye movement. Evaluation tests show that the sampling rate of the system can be 300 Hz and the accuracy is about 1.8 degree/s.\",\n",
       " \"Inspired by a European project, PHEME, that requires the close analysis of Twitter-based conversations in order to look at the spread of rumors via social media, this paper has two objectives. The first of these is to take the analysis of microblogs back to first principles and lay out what microblog analysis should look like as a foundational programme of work. The other is to describe how this is of fundamental relevance to Human-Computer Interaction's interest in grasping the constitution of people's interactions with technology within the social order. Our critical finding is that, despite some surface similarities, Twitter-based conversations are a wholly distinct social phenomenon requiring an independent analysis that treats them as unique phenomena in their own right, rather than as another species of conversation that can be handled within the framework of existing Conversation Analysis. This motivates the argument that Microblog Analysis be established as a foundationally independent programme, examining the organizational characteristics of microblogging from the ground up. We articulate how aspects of this approach have already begun to shape our design activities within the PHEME project.\",\n",
       " 'As the Internet of Things continues to take hold in the commercial world, the teams designing these new technologies are constantly evolving and turning their hand to uncharted territory. This is especially key within the field of secondary service design as businesses attempt to utilize and find value in the sensor data being produced by connected products. This paper discusses the ways in which a commercial design team use smart thermostat data to prototype an advice-giving chatbot. The team collaborate to produce a chat sequence through careful ordering of data & reasoning about customer reactions. The paper contributes important insights into design methods being used in practice within the under researched areas of chatbot prototyping and secondary service design.',\n",
       " \"We introduce Jo, a mobile application that attempts to improve user's well-being. Jo is a journaling application--users log their important moments via short texts and optionally an attached photo. Unlike a static journal, Jo analyzes these moments and helps users take action towards increased well-being. For example, Jo annotates each moment with a set of values (e.g., family, socialization, mindfulness), thereby giving the user insights about the balance in their lives. In addition, Jo helps the user create reminders that enable them to create additional happy moments. We describe the results of fielding Jo in a study of 39 participants. The results illustrate the promise of a journaling application that provides personalized feedback, and points at further research.\",\n",
       " \"People with visual impairment (PVI) must interact with a world they cannot see. Remote sighted assistance has emerged as a conversational/social support system. We interviewed participants who either provide or receive assistance via a conversational/social prosthetic called Aira (https://aira.io/). We identified four types of support provided: scene description, performance, social interaction, and navigation. We found that conversational style is context-dependent. Sighted assistants make intentional efforts to elicit PVI's personal knowledge and leverage it in the guidance they provide. PVI used non-verbal behaviors (e.g. hand gestures) as a parallel communication channel to provide feedback or guidance to sighted assistants. We also discuss implications for design.\",\n",
       " 'Voice controlled virtual assistants (VAs) are now available in smartphones, cars, and standalone devices in homes. In most cases, the user needs to first \"wake-up\" the VA by saying a particular word/phrase every time he or she wants the VA to do something. Eliminating the need for saying the wake-up word for every interaction could improve the user experience. This would require the VA to have the capability to detect the speech that is being directed at it and respond accordingly. In other words, the challenge is to distinguish between system-directed and non-system-directed speech utterances. In this paper, we present a number of neural network architectures for tackling this classification problem based on using only acoustic features. These architectures are based on using convolutional, recurrent and feed-forward layers. In addition, we investigate the use of an attention mechanism applied to the output of the convolutional and the recurrent layers. It is shown that incorporating the proposed attention mechanism into the models always leads to significant improvement in classification accuracy. The best model achieved equal error rates of 16.25 and 15.62 percents on two distinct realistic datasets.',\n",
       " 'The use of maker community tools and IoT technologies inside classrooms is spreading in an increasing number of education and science fields. GAIA is a European research project focused on achieving behavior change for sustainability and energy awareness in schools. In this work, we report on how a large IoT deployment in a number of educational buildings and real-world data from this infrastructure, are utilized to support a \"maker\" lab kit activity inside the classroom, together with a serious game. We also provide some insights to the integration of these activities in the school curriculum, along with a discussion on our feedback so far from a series of workshop activities in a number of schools. Our initial results show strong acceptance by the school community.',\n",
       " 'Humans, as both pedestrians and drivers, generally skillfully navigate traffic intersections. Despite the uncertainty, danger, and the non-verbal nature of communication commonly found in these interactions, there are surprisingly few collisions considering the total number of interactions. As the role of automation technology in vehicles grows, it becomes increasingly critical to understand the relationship between pedestrian and driver behavior: how pedestrians perceive the actions of a vehicle/driver and how pedestrians make crossing decisions. The relationship between time-to-arrival (TTA) and pedestrian gap acceptance (i.e., whether a pedestrian chooses to cross under a given window of time to cross) has been extensively investigated. However, the dynamic nature of vehicle trajectories in the context of non-verbal communication has not been systematically explored. Our work provides evidence that trajectory dynamics, such as changes in TTA, can be powerful signals in the non-verbal communication between drivers and pedestrians. Moreover, we investigate these effects in both simulated and real-world datasets, both larger than have previously been considered in literature to the best of our knowledge.',\n",
       " 'This paper presents a cooperative location-based game for the elderly with the use of tablets equipped with mobile application. The game was designed to tackle at once several crucial topics related to the issue of aging, namely the social inclusion, education in the field of modern technology, motivation for learning as well as physical activity. Mixed-aged teams consisting of two players: a junior and a senior took part in the game. The preliminary results suggest that the game can successfully address a number of issues including improving the elderly technical skills, increasing the elderly physical activity as well as positive intergenerational interaction. The paper describes the game setup in details and presents some initial data gathered during the gameplay.',\n",
       " 'Crowd-labeling emerged from the need to label large-scale and complex data, a tedious, expensive, and time-consuming task. One of the main challenges in the crowd-labeling task is to control for or determine in advance the proportion of low-quality/malicious labelers. If that proportion grows too high, there is often a phase transition leading to a steep, non-linear drop in labeling accuracy as noted by Karger et al. [2014]. To address these challenges, we propose a new framework called Expert Label Injected Crowd Estimation (ELICE) and extend it to different versions and variants that delay phase transition leading to a better labeling accuracy. ELICE automatically combines and boosts bulk crowd labels supported by labels from experts for limited number of instances from the dataset. The expert-labels help to estimate the individual ability of crowd labelers and difficulty of each instance, both of which are used to aggregate the labels. Empirical evaluation shows the superiority of ELICE as compared to other state-of-the-art methods. We also derive a lower bound on the number of expert-labeled instances needed to estimate the crowd ability and dataset difficulty as well as to get better quality labels.',\n",
       " 'We describe the experimental procedures for a dataset that we have made publicly available at https://doi.org/10.5281/zenodo.2617084 in mat (Mathworks, Natick, USA) and csv formats. This dataset contains electroencephalographic recordings of 12 subjects listening to music with and without a passive head-mounted display, that is, a head-mounted display which does not include any electronics at the exception of a smartphone. The electroencephalographic headset consisted of 16 electrodes. Data were recorded during a pilot experiment taking place in the GIPSA-lab, Grenoble, France, in 2017 (Cattan and al, 2018). Python code for manipulating the data is available at https://github.com/plcrodrigues/py.PHMDML.EEG.2017-GIPSA. The ID of this dataset is PHMDML.EEG.2017-GIPSA.',\n",
       " 'Accurately and efficiently crowdsourcing complex, open-ended tasks can be difficult, as crowd participants tend to favor short, repetitive \"microtasks\". We study the crowdsourcing of large networks where the crowd provides the network topology via microtasks. Crowds can explore many types of social and information networks, but we focus on the network of causal attributions, an important network that signifies cause-and-effect relationships. We conduct experiments on Amazon Mechanical Turk (AMT) testing how workers propose and validate individual causal relationships and introduce a method for independent crowd workers to explore large networks. The core of the method, Iterative Pathway Refinement, is a theoretically-principled mechanism for efficient exploration via microtasks. We evaluate the method using synthetic networks and apply it on AMT to extract a large-scale causal attribution network, then investigate the structure of this network as well as the activity patterns and efficiency of the workers who constructed this network. Worker interactions reveal important characteristics of causal perception and the network data they generate can improve our understanding of causality and causal inference.',\n",
       " 'We discuss issues of Artificial Intelligence (AI) fairness for people with disabilities, with examples drawn from our research on human-computer interaction (HCI) for AI-based systems for people who are Deaf or Hard of Hearing (DHH). In particular, we discuss the need for inclusion of data from people with disabilities in training sets, the lack of interpretability of AI systems, ethical responsibilities of access technology researchers and companies, the need for appropriate evaluation metrics for AI-based access technologies (to determine if they are ready to be deployed and if they can be trusted by users), and the ways in which AI systems influence human behavior and influence the set of abilities needed by users to successfully interact with computing systems.',\n",
       " \"Modern manufacturing systems typically require high degrees of flexibility, in terms of ability to customize the production lines to the constantly changing market requests. For this purpose, manufacturing systems are required to be able to cope with changes in the types of products, and in the size of the production batches. As a consequence, the human-machine interfaces (HMIs) are typically very complex, and include a wide range of possible operational modes and commands. This generally implies an unsustainable cognitive workload for the human operators, in addition to a non-negligible training effort. To overcome this issue, in this paper we present a methodology for the design of adaptive human-centred HMIs for industrial machines and robots. The proposed approach relies on three pillars: measurement of user's capabilities, adaptation of the information presented in the HMI, and training of the user. The results expected from the application of the proposed methodology are investigated in terms of increased customization and productivity of manufacturing processes, and wider acceptance of automation technologies. The proposed approach has been devised in the framework of the European project INCLUSIVE.\",\n",
       " 'Anxiety is a common health issue that can occur throughout one\\'s existence. In this pilot study we explore an alternative technique to regulate it: biofeedback. The long-term objective is to offer an ecological device that could help people cope with anxiety, by exposing their inner state in a comprehensive manner. We propose a first iteration of this device, \"Inner Flower\", that uses heart rate to adapt a breathing guide to the user, and we investigate its efficiency and usability. Traditionally, such device requires user\\'s full attention. We propose an ambient modality during which the device operates in the peripheral vision. Beside comparing \"Ambient\" and \"Focus\" conditions, we also compare the biofeedback with a sham feedback (fixed breathing guide). We found that the Focus group demonstrated higher relaxation and performance on a cognitive task (N-back). However, there was no noticeable effect of the Ambient feedback, and the biofeedback condition did not yield any significant difference when compared to the sham feedback. These results, while promising, highlight the pitfalls of any research related to biofeedback, where it is difficult to fully comprehend the underlying mechanisms of such technique.',\n",
       " 'Within this chapter we consider the emergence of ambient domestic computing systems, both conceptually and empirically. We critically assess visions of post-desktop computing, paying particular attention to one contemporary trend: the internet of things (IoT). We examine the contested nature of this term, looking at the historical trajectory of similar technologies, and the regulatory issues they can pose, particularly in the home. We also look to the emerging regulatory solution of privacy by design, unpacking practical challenges it faces. The novelty of our contribution stems from a turn to practice through a set of empirical perspectives. We present findings that document the practical experiences and viewpoints of leading experts in technology law and design.',\n",
       " 'In this work, we present Web-STAR, an online platform for story understanding built on top of the STAR (STory comprehension through ARgumentation) reasoning engine. This platform includes a web-based IDE, integration with the STAR system and a web service infrastructure to support integration with other systems that rely on story understanding functionality to complete their tasks. The platform also delivers a number of \"social\" features like public story sharing with a built-in commenting system, a public repository for sharing stories with the community and collaboration tools that can be used from both project team members for development and educators for teaching. Moreover, we discuss the ongoing work on adding new features and functionality to this platform.',\n",
       " \"Human computation games (HCGs) can provide novel solutions to intractable computational problems, help enable scientific breakthroughs, and provide datasets for artificial intelligence. However, our knowledge about how to design and deploy HCGs that appeal to players and solve problems effectively is incomplete. We present an investigatory HCG based on Super Mario Bros. We used this game in a human subjects study to investigate how different social conditions---singleplayer and multiplayer---and scoring mechanics---collaborative and competitive---affect players' subjective experiences, accuracy at the task, and the completion rate. In doing so, we demonstrate a novel design approach for HCGs, and discuss the benefits and tradeoffs of these mechanics in HCG design.\",\n",
       " 'This is the preprint version of our paper on 2015 9th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth2015). An assistive training tool software for rehabilitation of dysphonic patients is evaluated according to the practical clinical feedback from the treatments. One stroke sufferer and one parkinson sufferer have provided earnest suggestions for the improvement of our tool software. The assistive tool employs a serious game as the attractive logic part, and running on the tablet with normal microphone as input device. Seven pitch estimation algorithms have been evaluated and compared with selected patients voice database. A series of benchmarks have been generated during the evaluation process for technology selection.',\n",
       " 'The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. In the game, the answerer first thinks of an object such as a famous person or a kind of animal. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. However, the optimal policy of question selection is hard to be derived due to the complexity and volatility of the game environment. In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users. To facilitate training, we also propose to use a reward network to estimate the more informative reward. Compared to previous methods, our RL method is robust to noisy answers and does not rely on the Knowledge Base of objects. Experimental results show that our RL method clearly outperforms an entropy-based engineering system and has competitive performance in a noisy-free simulation environment.',\n",
       " 'Developments in information and communication technologies have been greatly influential on the practices in all fields, and education is not an exception to this. To illustrate with, computers were first used in computer assisted education in order to increase the efficiency of teaching process. Recently, computer has contributed more to the field through interactive and smart class applications that are specially designed for classroom use. The aim of this study is to develop a low cost, portable and projection supported touchscreen to be used in educational environments by using FPGA technology and to test its usability. For the purposes of the study, the above mentioned system was developed by using the necessary hardware and software, and later it was tested in terms of usability. This usability test was administered to teachers, who were the target end users of this touchscreen writing / drawing system. The aim of this test was to determine user friendliness, subservientness and usability of the system. Several tools were used to obtain data from the users that participated in the study. The analysis and evaluation of the data collected revealed that the system has achieved its objectives successfully.',\n",
       " 'Graphical interfaces and interactive visualisations are typical mediators between human users and data analytics systems. HCI researchers and developers have to be able to understand both human needs and back-end data analytics. Participants of our tutorial will learn how visualisation and interface design can be combined with data analytics to provide better visualisations. In the first of three parts, the participants will learn about visualisations and how to appropriately select them. In the second part, restrictions and opportunities associated with different data analytics systems will be discussed. In the final part, the participants will have the opportunity to develop visualisations and interface designs under given scenarios of data and system settings.',\n",
       " 'Humans are able to understand and perform complex tasks by strategically structuring the tasks into incremental steps or subgoals. For a robot attempting to learn to perform a sequential task with critical subgoal states, such states can provide a natural opportunity for interaction with a human expert. This paper analyzes the benefit of incorporating a notion of subgoals into Inverse Reinforcement Learning (IRL) with a Human-In-The-Loop (HITL) framework. The learning process is interactive, with a human expert first providing input in the form of full demonstrations along with some subgoal states. These subgoal states define a set of subtasks for the learning agent to complete in order to achieve the final goal. The learning agent queries for partial demonstrations corresponding to each subtask as needed when the agent struggles with the subtask. The proposed Human Interactive IRL (HI-IRL) framework is evaluated on several discrete path-planning tasks. We demonstrate that subgoal-based interactive structuring of the learning task results in significantly more efficient learning, requiring only a fraction of the demonstration data needed for learning the underlying reward function with the baseline IRL model.',\n",
       " 'Supporting programming on touchscreen devices requires effective text input and editing methods. Unfortunately, the virtual keyboard can be inefficient and uses valuable screen space on already small devices. Recent advances in stylus input make handwriting a potentially viable text input solution for programming on touchscreen devices. The primary barrier, however, is that handwriting recognition systems are built to take advantage of the rules of natural language, not those of a programming language. In this paper, we explore this particular problem of handwriting recognition for source code. We collect and make publicly available a dataset of handwritten Python code samples from 15 participants and we characterize the typical recognition errors for this handwritten Python source code when using a state-of-the-art handwriting recognition tool. We present an approach to improve the recognition accuracy by augmenting a handwriting recognizer with the programming language grammar rules. Our experiment on the collected dataset shows an 8.6% word error rate and a 3.6% character error rate which outperforms standard handwriting recognition systems and compares favorably to typing source code on virtual keyboards.',\n",
       " 'Culture is core to human civilization, and is essential for human intellectual achievements in social context. Culture also influences how humans work together, perform particular task and overall lifestyle and dealing with other groups of civilization. Thus, culture is concerned with establishing shared ideas, particularly those playing a key role in success. Does it impact on how two individuals can work together in achieving certain goals? In this paper, we establish a means to derive cultural association and map it to culturally mediated success. Human interactions with the environment are typically in the form of expressions. Association between culture and behavior produce similar beliefs which lead to common principles and actions, while cultural similarity as a set of common expressions and responses. To measure cultural association among different candidates, we propose the use of a Graphical Association Method (GAM). The behaviors of candidates are captured through series of expressions and represented in the graphical form. The association among corresponding node and core nodes is used for the same. Our approach provides a number of interesting results and promising avenues for future applications.',\n",
       " 'Artificial intelligence is revolutionizing formal education, fueled by innovations in learning assessment, content generation, and instructional delivery. Informal, lifelong learning settings have been the subject of less attention. We provide a proof-of-concept for an embodied book discussion companion, designed to stimulate conversations with readers about particularly creative metaphors in fiction literature. We collect ratings from 26 participants, each of whom discuss Jane Austen\\'s \"Pride and Prejudice\" with the robot across one or more sessions, and find that participants rate their interactions highly. This suggests that companion robots could be an interesting entryway for the promotion of lifelong learning and cognitive exercise in future applications.',\n",
       " 'Machine learning has been applied to a number of creative, design-oriented tasks. However, it remains unclear how to best empower human users with these machine learning approaches, particularly those users without technical expertise. In this paper we propose a general framework for turn-based interaction between human users and AI agents designed to support human creativity, called {co-creative systems}. The framework can be used to better understand the space of possible designs of co-creative systems and reveal future research directions. We demonstrate how to apply this framework in conjunction with a pair of recent human subject studies, comparing between the four human-AI systems employed in these studies and generating hypotheses towards future studies.',\n",
       " 'Deep learning, including convolutional neural networks (CNNs), has started finding applications in brain-computer interfaces (BCIs). However, so far most such approaches focused on BCI classification problems. This paper extends EEGNet, a 3-layer CNN model for BCI classification, to BCI regression, and also utilizes a novel spectral meta-learner for regression (SMLR) approach to aggregate multiple EEGNets for improved performance. Our model uses the power spectral density (PSD) of EEG signals as the input. Compared with raw EEG inputs, the PSD inputs can reduce the computational cost significantly, yet achieve much better regression performance. Experiments on driver drowsiness estimation from EEG signals demonstrate the outstanding performance of our approach.',\n",
       " 'Mobile device users avoiding observational attacks and coping with situational impairments may employ techniques for eyes-free mobile unlock authentication, where a user enters his/her passcode without looking at the device. This study supplies an initial description of user accu- racy in performing this authentication behavior with PIN and pattern passcodes, with varying lengths and visual characteristics. Additionally, we inquire if tactile-only feedback can provide assistive spatialization, finding that orientation cues prior to unlocking do not help. Measure- ments of edit distance and dynamic time warping accuracy were collected, using a within-group, randomized study of 26 participants. 1,021 passcode entry gestures were collected and classified, identifying six user strategies for using the pre-entry tactile feedback, and ten codes for types of events and errors that occurred during entry. We found that users who focused on orienting themselves to position the first digit of the passcode using the tactile feedback performed better in the task. These results could be applied to better define eyes-free behavior in further research, and to design better and more secure methods for eyes-free authentication.',\n",
       " \"In this paper we present the first results of a pilot experiment in the capture and interpretation of multimodal signals of human experts engaged in solving challenging chess problems.  Our goal is to investigate the extent to which observations of eye-gaze, posture, emotion and other physiological signals can be used to model the cognitive state of subjects, and to explore the integration of multiple sensor modalities to improve the reliability of detection of human displays of awareness and emotion. We observed chess players engaged in problems of increasing difficulty while recording their behavior. Such recordings can be used to estimate a participant's awareness of the current situation and to predict ability to respond effectively to challenging situations.  Results show that a multimodal approach is more accurate than a unimodal one. By combining body posture, visual attention and emotion, the multimodal approach can reach up to 93% of accuracy when determining player's chess expertise while unimodal approach reaches 86%.  Finally this experiment validates the use of our equipment as a general and reproducible tool for the study of participants engaged in screen-based interaction and/or problem solving.\",\n",
       " 'From Harry Potter to American Horror Story, fanfiction is extremely popular among young people. Sites such as Fanfiction.net host millions of stories, with thousands more posted each day. Enthusiasts are sharing their writing and reading stories written by others. Exactly how does a generation known more for videogame expertise than long-form writing become so engaged in reading and writing in these communities? Via a nine-month ethnographic investigation of fanfiction communities that included participant observation, interviews, a thematic analysis of 4,500 reader reviews and an in-depth case study of a discussion group, we found that members of fanfiction communities spontaneously mentor each other in open forums, and that this mentoring builds upon previous interactions in a way that is distinct from traditional forms of mentoring and made possible by the affordances of networked publics. This work extends and develops the theory of distributed mentoring. Our findings illustrate how distributed mentoring supports fanfiction authors as they work to develop their writing skills. We believe distributed mentoring holds potential for supporting learning in a variety of formal and informal learning environments.',\n",
       " 'As the use of crowdsourcing increases, it is important to think about performance optimization. For this purpose, it is possible to think about each worker as a HPU(Human Processing Unit), and to draw inspiration from performance optimization on traditional computers or cloud nodes with CPUs. However, as we characterize HPUs in detail for this purpose, we find that there are important differences between CPUs and HPUs, leading to the need for completely new optimization algorithms.\\n  In this paper, we study the specific optimization problem of obtaining results fastest for a crowd sourced job with a fixed total budget. In crowdsourcing, jobs are usually broken down into sets of small tasks, which are assigned to workers one at a time. We consider three scenarios of increasing complexity: Identical Round Homogeneous tasks, Multiplex Round Homogeneous tasks, and Multiple Round Heterogeneous tasks. For each scenario, we analyze the stochastic behavior of the HPU clock-rate as a function of the remuneration offered. After that, we develop an optimum Budget Allocation strategy to minimize the latency for job completion. We validate our results through extensive simulations and experiments on Amazon Mechanical Turk.',\n",
       " 'Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well-known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.',\n",
       " 'Applied visualization researchers often work closely with domain collaborators to explore new and useful applications of visualization. The early stages of collaborations are typically time consuming for all stakeholders as researchers piece together an understanding of domain challenges from disparate discussions and meetings. A number of recent projects, however, report on the use of creative visualization-opportunities (CVO) workshops to accelerate the early stages of applied work, eliciting a wealth of requirements in a few days of focused work. Yet, there is no established guidance for how to use such workshops effectively. In this paper, we present the results of a 2-year collaboration in which we analyzed the use of 17 workshops in 10 visualization contexts. Its primary contribution is a framework for CVO workshops that: 1) identifies a process model for using workshops; 2) describes a structure of what happens within effective workshops; 3) recommends 25 actionable guidelines for future workshops; and 4) presents an example workshop and workshop methods. The creation of this framework exemplifies the use of critical reflection to learn about visualization in practice from diverse studies and experience.',\n",
       " 'In this paper a review is presented of the research on eye gaze estimation techniques and applications, that has progressed in diverse ways over the past two decades. Several generic eye gaze use-cases are identified: desktop, TV, head-mounted, automotive and handheld devices. Analysis of the literature leads to the identification of several platform specific factors that influence gaze tracking accuracy. A key outcome from this review is the realization of a need to develop standardized methodologies for performance evaluation of gaze tracking systems and achieve consistency in their specification and comparative evaluation. To address this need, the concept of a methodological framework for practical evaluation of different gaze tracking systems is proposed.',\n",
       " 'Emotions that are perceived as \"negative\" are inherent in the human experience. Yet not much work in the field of HCI has looked into the role of these emotions in interaction with technology. As technology is becoming more social, personal and emotional by mediating our relationships and generating new social entities (such as conversational agents and robots), it is valuable to consider how it can support people\\'s negative emotions and behaviors. Research in Psychology shows that interacting with negative emotions correctly can benefit well-being, yet the boundary between helpful and harmful is delicate. This workshop paper looks at the opportunities of designing for negative affect, and the challenge of \"causing no harm\" that arises in an attempt to do so.',\n",
       " 'The development of computer technology has been rapid. Not so long ago, the first computer was developed which was large and bulky. Now, the latest generation of smartphones has a calculation power, which would have been considered those of supercomputers in 1990. For a smart environment, the person recognition and re-recognition is an important topic. The distribution of new technologies like wearable computing is a new approach to the field of person recognition and re-recognition. This article lays out the idea of identifying and re-identifying wearable computing devices by listening to their wireless communication connectivity like Wi-Fi and Bluetooth and building a classification of interaction scenarios for the combination of human-wearable-environment.',\n",
       " 'Emotional and physical well-being at workplace is important for a positive work environment and higher productivity. Jobs such as software programming lead to a sedentary lifestyle and require high interaction with computers. Working at the same job for years can cause a feeling of intellectual stagnation and lack of drive. Many employees experience lack of motivation, mild to extreme depression due to reasons such as aversion towards job responsibilities and incompatibility with coworkers or boss. This research proposed an affect monitoring system EmoFit that would play the role of psychological and physical health trainer. The day to day computer activity and body language was analyzed to detect the physical and emotional well-being of the user. Keystrokes, activity interruptions, eye tracking, facial expressions, body posture and speech were monitored to gauge the users health. The system also provided activities such as at-desk exercise and stress relief game and motivational quotes in an attempt to promote users well-being. The experimental results and positive feedback from test subjects showed that EmoFit would help improve emotional and physical well-being at jobs that involve significant computer usage.',\n",
       " 'Users giving relevance feedback in exploratory search are often uncertain about the correctness of their feedback, which may result in noisy or even erroneous feedback. Additionally, the search intent of the user may be volatile as the user is constantly learning and reformulating her search hypotheses during the search. This may lead to a noticeable concept drift in the feedback. We formulate a Bayesian regression model for predicting the accuracy of each individual user feedback and thus find outliers in the feedback data set. Additionally, we introduce a timeline interface that visualizes the feedback history to the user and gives her suggestions on which past feedback is likely in need of adjustment. This interface also allows the user to adjust the feedback accuracy inferences made by the model. Simulation experiments demonstrate that the performance of the new user model outperforms a simpler baseline and that the performance approaches that of an oracle, given a small amount of additional user interaction. A user study shows that the proposed modelling technique, combined with the timeline interface, makes it easier for the users to notice and correct mistakes in their feedback, and to discover new items.',\n",
       " 'Call and messaging logs from mobile devices have been used to predict human personality traits successfully in recent years. However, the widely available accelerometer data is not yet utilized for this purpose. In this research, we explored some important features describing human physical activity intensity, used for the very first time to predict human personality traits through raw accelerometer data. Using a set of newly introduced metrics, we combined physical activity intensity features with traditional phone activity features for personality prediction. The experiment results show that the predicted personality scores are closer to the ground truth, with observable reduction of errors in predicting the Big-5 personality traits across male and female.',\n",
       " 'Scalable interactive visual data exploration is crucial in many domains due to increasingly large datasets generated at rapid rates. Details-on-demand provides a useful interaction paradigm for exploring large datasets, where users start at an overview, find regions of interest, zoom in to see detailed views, zoom out and then repeat. This paradigm is the primary user interaction mode of widely-used systems such as Google Maps, Aperture Tiles and ForeCache. These earlier systems, however, are highly customized with hardcoded visual representations and optimizations. A more general framework is needed to facilitate the development of visual data exploration systems at scale. In this paper, we present Kyrix, an end-to-end system for developing scalable details-on-demand data exploration applications. Kyrix provides developers with a declarative model for easy specification of general visualizations. Behind the scenes, Kyrix utilizes a suite of performance optimization techniques to achieve a response time within 500ms for various user interactions. We also report results from a performance study which shows that a novel dynamic fetching scheme adopted by Kyrix outperforms tile-based fetching used in earlier systems.',\n",
       " 'Most software systems today do not support cognitive diversity. Further, because of differences in problem-solving styles that cluster by gender, software that poorly supports cognitive diversity can also embed gender biases. To help software professionals fix gender bias \"bugs\" related to people\\'s problem-solving styles for information processing and learning of new software we collected inclusivity fixes from three sources. The first two are empirical studies we conducted: a heuristics-driven user study and a field research industry study. The third is data that we obtained about a before/after user study of inclusivity bugs. The resulting seven potential inclusivity fixes show how to debug software to be more inclusive for diverse problem-solving styles.',\n",
       " 'Due to concerns about human error in crowdsourcing, it is standard practice to collect labels for the same data point from multiple internet workers. We here show that the resulting budget can be used more effectively with a flexible worker assignment strategy that asks fewer workers to analyze easy-to-label data and more workers to analyze data that requires extra scrutiny. Our main contribution is to show how the allocations of the number of workers to a task can be computed optimally based on task features alone, without using worker profiles. Our target tasks are delineating cells in microscopy images and analyzing the sentiment toward the 2016 U.S. presidential candidates in tweets. We first propose an algorithm that computes budget-optimized crowd worker allocation (BUOCA). We next train a machine learning system (BUOCA-ML) that predicts an optimal number of crowd workers needed to maximize the accuracy of the labeling. We show that the computed allocation can yield large savings in the crowdsourcing budget (up to 49 percent points) while maintaining labeling accuracy. Finally, we envisage a human-machine system for performing budget-optimized data analysis at a scale beyond the feasibility of crowdsourcing.',\n",
       " \"Computational Intelligence (CI) in computer games plays an important role that could simulate various aspects of real-life problems. CI in real-time decision-making games can provide a platform for the examination of tree search algorithms. In this paper, we present a rehabilitation serious game (ReHabgame) in which the Monte-Carlo Tree Search (MCTS) algorithm is utilized. The game is designed to combat the physical impairment of post-stroke/brain injury casualties in order to improve upper limb movement. Through the process of ReHabgame the player chooses paths via upper limb according to his/her movement ability to reach virtual goal objects. The system adjusts the difficulty level of the game based on the player's quality of activity through MCTS. It learns from the movements made by a player and generates further subsequent objects for collection. The system collects orientation, muscle and joint activity data and utilizes them to make decisions. Players data are collected through Kinect Xbox One and Myo Armband. The results show the effectiveness of the MCTS in the ReHabgame that progresses from highly achievable paths to the less achievable ones, thus configuring and personalizing the rehabilitation process.\",\n",
       " \"Intelligent conversational assistants, such as Apple's Siri, Microsoft's Cortana, and Amazon's Echo, have quickly become a part of our digital life. However, these assistants have major limitations, which prevents users from conversing with them as they would with human dialog partners. This limits our ability to observe how users really want to interact with the underlying system. To address this problem, we developed a crowd-powered conversational assistant, Chorus, and deployed it to see how users and workers would interact together when mediated by the system. Chorus sophisticatedly converses with end users over time by recruiting workers on demand, which in turn decide what might be the best response for each user sentence. Up to the first month of our deployment, 59 users have held conversations with Chorus during 320 conversational sessions. In this paper, we present an account of Chorus' deployment, with a focus on four challenges: (i) identifying when conversations are over, (ii) malicious users and workers, (iii) on-demand recruiting, and (iv) settings in which consensus is not enough. Our observations could assist the deployment of crowd-powered conversation systems and crowd-powered systems in general.\",\n",
       " \"The number and quality of user reviews greatly affects consumer purchasing decisions. While reviews in all languages are increasing, it is still often the case (especially for non-English speakers) that there are only a few reviews in a person's first language. Using an online experiment, we examine the value that potential purchasers receive from interfaces showing additional reviews in a second language. The results paint a complicated picture with both positive and negative reactions to the inclusion of foreign-language reviews. Roughly 26-28% of subjects clicked to see translations of the foreign-language content when given the opportunity, and those who did so were more likely to select the product with foreign-language reviews than those who did not.\",\n",
       " 'Information extraction is a critical step in the practice of conducting biomedical systematic literature reviews. Extracted structured data can be aggregated via methods such as statistical meta-analysis. Typically highly trained domain experts extract data for systematic reviews. The high expense of conducting biomedical systematic reviews has motivated researchers to explore lower cost methods that achieve similar rigor without compromising quality. Crowdsourcing represents one such promising approach. In this work-in-progress study, we designed a crowdsourcing task for biomedical information extraction. We briefly report the iterative design process and the results of two pilot testings. We found that giving more concrete examples in the task instruction can help workers better understand the task, especially for concepts that are abstract and confusing. We found a few workers completed most of the work, and our payment level appeared more attractive to workers from low-income countries. In the future, we will further evaluate our results with reference to gold standard extractions, thus assessing the feasibility of tasking crowd workers with extracting biomedical intervention information for systematic reviews.']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set(re.findall(\"[a-z0-9']+\",absdata['cs.HC'][0].lower()))\n",
    "absdata['cs.HC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again using the `Counter()` object's convenient `most_common` method, you can look at the numbers for the most frequent words (where 900 would mean they occurred in all 900 training documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 896.5),\n",
       " ('and', 894.5),\n",
       " ('of', 893.5),\n",
       " ('to', 878.5),\n",
       " ('a', 858.5),\n",
       " ('in', 845.5),\n",
       " ('for', 735.5),\n",
       " ('this', 725.5),\n",
       " ('we', 705.5),\n",
       " ('that', 673.5)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(HCvocab).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or to see the 285th through 296th, use (try it, should give words appearing in only forty eight of the training documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('health', 48.5),\n",
       " ('goal', 48.5),\n",
       " ('survey', 48.5),\n",
       " ('implications', 48.5),\n",
       " ('if', 48.5),\n",
       " ('technique', 48.5),\n",
       " ('presented', 48.5),\n",
       " ('less', 48.5),\n",
       " ('significantly', 48.5),\n",
       " ('language', 48.5),\n",
       " ('investigate', 48.5)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(HCvocab).most_common()[285:296]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "#2A"
   },
   "source": [
    "and similarly for `LGvocab` for the machine learning documents.<br>\n",
    "Finally you need to write the classifier code (will be roughly a grand total of less than 10 lines of code) to implement the Naive Bayes classifier (or repurpose the code from lec3.ipynb). The formula to implement (for a flat prior $p({\\rm HC})=p({\\rm LG})=.5$) is\n",
    "$$\n",
    "p({\\rm HC}\\ |\\ words) = \\frac{p(words\\ |\\ {\\rm HC})\\,p({\\rm HC})}\n",
    "{p(words\\ |\\ {\\rm HC})\\,p({\\rm HC})+p(words\\ |\\ {\\rm LG})\\,p({\\rm LG})}\n",
    "\\approx\\frac{\\prod_i p(w_i\\ |\\ {\\rm HC})}{\\prod_i p(w_i\\ |\\ {\\rm HC})+\\prod_i p(w_i\\ |\\ {\\rm LG})}\n",
    "$$\n",
    "<br>\n",
    "This formula might look unfamiliar so let's try to unwrap it. (This will be done in Lec4.) The conditional word probabilities, e.g., $p(w_i\\ |\\ {\\rm HC})$ (\"the probability that word $w_i$ occurs if the abstract is labelled HC), are estimated based on the number of HC documents in which they occur in the training set. So if the word 'health' appears in 48 such documents, then we would estimate that $p($'health' $|\\ {\\rm HC}) = 48/1000$. The product signs $\\Pi_i$ in the above just mean that we multiply those probabilities for each word that appears in the test document.<br>\n",
    "Intuitively, the above formula considers the words in a text document, and if more of those words tend to occur preferentially in the HC abstracts in the training set, then the test abstract is determined to be more likely an HC abstract (because the numerator is more than half the denominator), and vice versa if its words tend to occur more often in the LG abstracts in the training set.\n",
    "<br><br>\n",
    "\n",
    "As in [lec3.ipynb](https://nbviewer.jupyter.org/url/courses.cit.cornell.edu/info3950_2023sp/lec3.ipynb), you can try using the full vocabulary, and then experiment with using the most common words, or the most discriminating:\n",
    "\n",
    "**A.** i) Train on the first 900 documents in each of the cs.HC and cs.LG abstracts (a total of 1800 documents), and test on the last 100 in each of the classes (a total of 200 documents). What is the test score: i.e., on the 200 test documents, what is the percentage predicted correctly?<br>\n",
    "\n",
    "ii) In class (lec4), the importance of \"feature set selection\" will be mentioned. Instead of using the full vocabulary, try using just the 500 most common words (highest percentage of documents) from each of the two categories, for a total of somewhat under 1000 words (due to overlaps between the two lists). How does that affect the test score? (note that the full vocabulary for the two classes consisted of more than 15,000 words in each class)\n",
    "\n",
    "iii) The above feature set, of 1000 most frequent terms, might not be optimal for this classification task, since many of those terms (the, of, and, or, ...) might not discriminate systematically between the two classes. They could just add noise and have an adverse effect on classifier performance. Instead we can try to use the terms that are most discriminating, in the sense of having the largest disparities in numbers of occurrences between the two classes (as will be illustrated in lec5 for the biology/physics classifier).\n",
    "\n",
    "First list the top 20 terms most discriminating in the HC direction (highest ratio (.5 + #HCtexts with word)/(.5 + #LGtexts with word)),$^{*}$ and the top 20 most discriminating in the LG direction (highest reciprocal of above ratio).\n",
    "\n",
    "Then construct a new feature set consisting of just the 200 most discriminating terms in each direction *and* which occur in at least 10 of the 1800 texts$^{**}$ (a total of 400 since there won't be overlap). What are the 20 most discriminating terms in this set?\n",
    "\n",
    "How does the test score of the classifier on the 200 test documents compare with parts i, ii)?\n",
    "\n",
    "$^*$The .5 is again \"smoothing\", to avoid division by zero for words that occur in only one of the two classes.\n",
    "\n",
    "$^{**}$This is so that excessive bias isn't given to terms that happen to occur very few times in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 95.0 %\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "# a)\n",
    "\n",
    "# i)\n",
    "\n",
    "# training\n",
    "\n",
    "vocab_dict = dict()\n",
    "\n",
    "for subj in [i for i in sorted(absdata)]:\n",
    "        vocab_dict[subj] = defaultdict(lambda: .5) # creates \n",
    "        for txt in absdata[subj][:900]:\n",
    "            for w in set(words(txt)): vocab_dict[subj][w] += 1\n",
    "\n",
    "def classify(txt, cats, vocab_dict=vocab_dict):\n",
    "    txt_words = set(words(txt))\n",
    "    prob_dict = defaultdict(lambda: 0)\n",
    "    \n",
    "    for a in cats:\n",
    "        p_a = np.prod([vocab_dict[a][w] for w in txt_words]) #p(words | avocab)\n",
    "        \n",
    "        b_cats = [cat for cat in filter(lambda q: q!=a , cats)]\n",
    "        p_bs = 0\n",
    "        for b in b_cats:\n",
    "            p_bs += np.prod([vocab_dict[b][w] for w in txt_words]) #p(words | bvocab)\n",
    "\n",
    "        r = p_a/(p_bs + p_a) #p(a | words)\n",
    "        prob_dict[a] = r\n",
    "    \n",
    "    highest_p = max(prob_dict, key=prob_dict.get)\n",
    "    \n",
    "    return (prob_dict[highest_p],highest_p)\n",
    "\n",
    "# testing\n",
    "\n",
    "HCtest = [classify(txt,['cs.HC','cs.LG']) for txt in absdata['cs.HC'][900:]]\n",
    "LGtest = [classify(txt,['cs.HC','cs.LG']) for txt in absdata['cs.LG'][900:]]\n",
    "\n",
    "HCcorrect = sum([i[1]=='cs.HC' for i in HCtest])\n",
    "LGcorrect = sum([i[1]=='cs.LG' for i in LGtest])\n",
    "\n",
    "test_scorei = (HCcorrect+LGcorrect)/2\n",
    "\n",
    "print(\"Test score:\",test_scorei,\"%\") # test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New test score: 90.5 %\n",
      "Change in test score: -4.5 %\n"
     ]
    }
   ],
   "source": [
    "# ii)\n",
    "\n",
    "# training\n",
    "\n",
    "vocab500_dict = dict()\n",
    "\n",
    "for subj in [i for i in sorted(absdata)]:\n",
    "    vocab500_dict[subj] = defaultdict(lambda: .5)\n",
    "        \n",
    "    for w,n in Counter(vocab_dict[subj]).most_common(500):\n",
    "        vocab500_dict[subj][w] = n\n",
    "\n",
    "HCtest500 = [classify(txt,['cs.HC','cs.LG'],vocab500_dict) for txt in absdata['cs.HC'][900:]]\n",
    "LGtest500 = [classify(txt,['cs.HC','cs.LG'],vocab500_dict) for txt in absdata['cs.LG'][900:]]\n",
    "\n",
    "HCcorrect500 = sum([i[1]=='cs.HC' for i in HCtest500])\n",
    "LGcorrect500 = sum([i[1]=='cs.LG' for i in LGtest500])\n",
    "\n",
    "test_scoreii = (HCcorrect500+LGcorrect500)/2\n",
    "\n",
    "print(\"New test score:\",test_scoreii,\"%\") # test score\n",
    "print(\"Change in test score:\",test_scoreii-test_scorei,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'eye': 47.0, \"user's\": 47.0, 'participants': 39.0, 'touch': 37.0, 'designers': 36.0, 'technologies': 33.5, 'eeg': 33.0, 'usability': 31.0, 'hci': 30.0, 'perceived': 29.0, 'movements': 28.0, 'gestures': 28.0, 'gesture': 24.0, 'gaze': 23.0, 'gaming': 23.0, 'haptic': 22.0, \"users'\": 21.0, 'interfaces': 20.25, 'voice': 20.0, 'tracking': 19.666666666666668})\n",
      "Counter({'variational': 42.0, 'cifar': 38.0, 'convex': 29.0, 'imagenet': 28.0, 'loss': 26.0, 'regret': 26.0, 'bounds': 23.5, 'batch': 23.0, 'gradients': 23.0, 'perturbation': 22.0, 'mnist': 22.0, 'regularization': 22.0, 'encoder': 22.0, 'margin': 21.0, 'benchmarks': 18.0, 'minimization': 18.0, 'guarantees': 18.0, 'bound': 17.666666666666668, 'parameter': 16.666666666666668, 'gradient': 16.0})\n"
     ]
    }
   ],
   "source": [
    "# iii)\n",
    "\n",
    "n_most_common = 20\n",
    "\n",
    "HCratios = defaultdict(lambda: .5)\n",
    "for w in HCvocab:\n",
    "    HCratios[w] = (.5 + HCvocab[w])/(.5 + LGvocab[w]) \n",
    "\n",
    "LGratios = defaultdict(lambda: .5)\n",
    "for w in LGvocab:\n",
    "    LGratios[w] = (.5 + LGvocab[w])/(.5 + HCvocab[w]) \n",
    "    \n",
    "HCtop20discr = defaultdict(lambda: .5)\n",
    "for w in Counter(HCratios).most_common(n_most_common):\n",
    "    HCtop20discr[w[0]] = w[1]\n",
    "\n",
    "LGtop20discr = defaultdict(lambda: .5)\n",
    "for w in Counter(LGratios).most_common(n_most_common):\n",
    "    LGtop20discr[w[0]] = w[1]\n",
    "\n",
    "print(Counter(HCtop20discr))\n",
    "print(Counter(LGtop20discr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('eye', 47.0), (\"user's\", 47.0), ('variational', 42.0), ('participants', 39.0), ('cifar', 38.0), ('touch', 37.0), ('designers', 36.0), ('technologies', 33.5), ('eeg', 33.0), ('usability', 31.0), ('hci', 30.0), ('perceived', 29.0), ('convex', 29.0), ('movements', 28.0), ('gestures', 28.0), ('imagenet', 28.0), ('loss', 26.0), ('regret', 26.0), ('gesture', 24.0), ('bounds', 23.5)]\n",
      "\n",
      "New test score: 95.0 %\n",
      "Change from (i) test score: 0.0 %\n",
      "Change from (ii) test score: 4.5 %\n"
     ]
    }
   ],
   "source": [
    "def basic_classify(txt, vocab1, vocab2):\n",
    "    txt_words = set(words(txt))\n",
    "    hc = np.prod([vocab1[w] for w in txt_words]) #p(words | HCvocab)\n",
    "    lg = np.prod([vocab2[w] for w in txt_words]) #p(words | LGvocab)\n",
    "    r = hc/(hc + lg) #p(HC | words)\n",
    "    return (r, 'HC' if r>.5 else 'LG')\n",
    "\n",
    "\n",
    "\n",
    "n_most_common = 200\n",
    "\n",
    "HCratios = defaultdict(lambda: .5)\n",
    "for w in HCvocab:\n",
    "    if HCvocab[w]+LGvocab[w] >= 12:\n",
    "        HCratios[w] = (.5 + HCvocab[w])/(.5 + LGvocab[w])\n",
    "    else: next\n",
    "        \n",
    "LGratios = defaultdict(lambda: .5)\n",
    "for w in LGvocab:\n",
    "    if HCvocab[w]+LGvocab[w] >= 12:\n",
    "        LGratios[w] = (.5 + LGvocab[w])/(.5 + HCvocab[w]) \n",
    "    else: next\n",
    "\n",
    "HCLGtop200discr = defaultdict(lambda: .5)\n",
    "\n",
    "HCtop200discr = defaultdict(lambda: .5)\n",
    "for w in Counter(HCratios).most_common(n_most_common):\n",
    "    HCtop200discr[w[0]] = w[1]\n",
    "    HCLGtop200discr[w[0]] = w[1]\n",
    "    \n",
    "LGtop200discr = defaultdict(lambda: .5)\n",
    "for w in Counter(LGratios).most_common(n_most_common):\n",
    "    LGtop200discr[w[0]] = w[1]\n",
    "    HCLGtop200discr[w[0]] = w[1]\n",
    "\n",
    "\n",
    "print(Counter(HCLGtop200discr).most_common(20))\n",
    "\n",
    "HCtest200 = [basic_classify(txt,HCtop200discr,LGtop200discr) for txt in absdata['cs.HC'][900:]]\n",
    "LGtest200 = [basic_classify(txt,HCtop200discr,LGtop200discr) for txt in absdata['cs.LG'][900:]]\n",
    "\n",
    "HCcorrect200 = sum([i[1]=='HC' for i in HCtest200])\n",
    "LGcorrect200 = sum([i[1]=='LG' for i in LGtest200])\n",
    "\n",
    "test_scoreiii = (HCcorrect200+LGcorrect200)/2\n",
    "\n",
    "print(\"\\nNew test score:\",test_scoreiii,\"%\") # test score\n",
    "print(\"Change from (i) test score:\",test_scoreiii-test_scorei,\"%\")\n",
    "print(\"Change from (ii) test score:\",test_scoreiii-test_scoreii,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "problem": "#2B"
   },
   "source": [
    "---\n",
    "**B.** As mentioned above, there are 10000 more abstracts in the ps1data.py (for a total of 12 classes).\n",
    "\n",
    "Implement a four way classifier (cs.HC, cs.LG, cs.CV = computer vision, q-bio.NC = neurons and cognition), where for example the probability of cs.HC would now be:\n",
    "$$p({\\rm HC}\\ |\\ words) = \\frac{p(words\\ |\\ {\\rm HC})\\,p({\\rm HC})}{p(words\\ |\\ {\\rm HC})\\,p({\\rm HC}) + p(words\\ |\\ {\\rm LG})\\,p({\\rm LG}) + p(words\\ |\\ {\\rm CV})\\,p({\\rm CV}) + p(words\\ |\\ {\\rm NC})\\,p({\\rm NC})} \\approx \\frac{\\prod_i p(w_i\\ |\\ {\\rm HC})}{\\prod_i p(w_i\\ |\\ {\\rm HC}) + \\prod_i p(w_i\\ |\\ {\\rm LG}) + \\prod_i p(w_i\\ |\\ {\\rm CV}) + \\prod_i p(w_i\\ |\\ {\\rm NC})}$$\n",
    "<br>\n",
    "and similarly for cs.LG, cs.CV, and q-bio.NC. The predicted classification is the one with the highest probability.\n",
    "\n",
    "Train it on the first 900 abstracts in each of those four categories (a total of 3600 documents), and test on the last 100 from each of those four categories (a total of 400 documents).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 87.5 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cs.HC': 90.0, 'cs.LG': 77.0, 'cs.CV': 88.0, 'q-bio.NC': 95.0}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [\"cs.HC\", \"cs.LG\", \"cs.CV\", \"q-bio.NC\"]\n",
    "\n",
    "# testing\n",
    "\n",
    "test_dict = dict()\n",
    "total_corr = 0\n",
    "\n",
    "for cat in categories:\n",
    "    test = [classify(txt,categories) for txt in absdata[cat][900:]]\n",
    "    n_corr = sum([i[1]==cat for i in test])\n",
    "    test_dict[cat] = n_corr/len(test)*100\n",
    "    total_corr += n_corr\n",
    "    \n",
    "\n",
    "\n",
    "test_score = total_corr/len(categories)\n",
    "\n",
    "print(\"Test score:\",test_score,\"%\")\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**C. [bonus]** Implement a twelve way classifier (all twelve classes in cell [1] above), \n",
    "trained on the first 900 abstracts in each of the categories (a total of 10800 documents), and test on the last 100 from each of those twelve categories (a total of 1200 documents). In class we'll consider metrics to disentangle the performance on multiple classes (using [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html),\n",
    "[confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html), or\n",
    "[ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 87.08333333333333 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'astro-ph.GA': 100.0,\n",
       " 'cond-mat.mes-hall': 75.0,\n",
       " 'cs.CV': 88.0,\n",
       " 'cs.HC': 90.0,\n",
       " 'cs.LG': 77.0,\n",
       " 'hep-ph': 90.0,\n",
       " 'hep-th': 92.0,\n",
       " 'math.AP': 92.0,\n",
       " 'physics.app-ph': 89.0,\n",
       " 'physics.comp-ph': 76.0,\n",
       " 'q-bio.NC': 92.0,\n",
       " 'quant-ph': 84.0}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C)\n",
    "categories = sorted(absdata)\n",
    "\n",
    "# testing\n",
    "\n",
    "test_dict = dict()\n",
    "total_corr = 0\n",
    "\n",
    "for cat in categories:\n",
    "    test = [classify(txt,categories) for txt in absdata[cat][900:]]\n",
    "    n_corr = sum([i[1]==cat for i in test])\n",
    "    test_dict[cat] = n_corr/len(test)*100\n",
    "    total_corr += n_corr\n",
    "    \n",
    "\n",
    "\n",
    "test_score = total_corr/len(categories)\n",
    "\n",
    "print(\"Test score:\",test_score,\"%\")\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "**D. [really optional bonus]** If you're already familiar with scikit-learn data formats, then it's instructive (and very quick) to redo the above using either [Bernoulli Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) (i.e., True/False whether or not word is in document as above), or \n",
    "[Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "(using the number counts within documents). This will be the subject of next week's lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = Counter()\n",
    "for category in categories:\n",
    "    allwords += Counter(vocab_dict[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38249"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,n = zip(*allwords.most_common())\n",
    "vocab = [w for w,n in allwords.most_common()]\n",
    "kvocab = {w:k for k,w in enumerate(vocab)} #assigns each word an index by rank\n",
    "svocab = set(vocab)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(txt):\n",
    "    f = np.zeros(len(allwords))\n",
    "    for w in set(words(txt)) & svocab: f[kvocab[w]] = 1\n",
    "    return f  #array of zeros and ones for whether or not feature words occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data is a list of feature vectors\n",
    "X_train = np.array([features(txt) for txt in absdata[cs.HC[:900] + phys[:900]])\n",
    "#training labels a corresponding list\n",
    "y_train = np.array([0]*900 + [1]*900)  #0 = bio, 1=phys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(absdata['cs.HC'][:900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_docs = []\n",
    "for cat in ['cs.HC','cs.LG']:\n",
    "    x_train_docs.append(absdata[cat][:900]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-802189c6a40d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cs.HC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cs.LG'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "len(absdata['cs.HC'][:900].append(absdata['cs.LG'][:900]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data-driven decision making related to individuals has become increasingly pervasive, but the issue concerning the potential discrimination has been raised by recent studies. In response, researchers have made efforts to propose and implement fairness measures and algorithms, but those efforts have not been translated to the real-world practice of data-driven decision making. As such, there is still an urgent need to create a viable tool to facilitate fair decision making. We propose FairSight, a visual analytic system to address this need; it is designed to achieve different notions of fairness in ranking decisions through identifying the required actions -- understanding, measuring, diagnosing and mitigating biases -- that together lead to fairer decision making. Through a case study and user study, we demonstrate that the proposed visual analytic and diagnostic modules in the system are effective in understanding the fairness-aware decision pipeline and obtaining more fair outcomes.',\n",
       " 'Eyewear devices, such as augmented reality displays, increasingly integrate eye tracking but the first-person camera required to map a user\\'s gaze to the visual scene can pose a significant threat to user and bystander privacy. We present PrivacEye, a method to detect privacy-sensitive everyday situations and automatically enable and disable the eye tracker\\'s first-person camera using a mechanical shutter. To close the shutter in privacy-sensitive situations, the method uses a deep representation of the first-person video combined with rich features that encode users\\' eye movements. To open the shutter without visual input, PrivacEye detects changes in users\\' eye movements alone to gauge changes in the \"privacy level\" of the current situation. We evaluate our method on a first-person video dataset recorded in daily life situations of 17 participants, annotated by themselves for privacy sensitivity, and show that our method is effective in preserving privacy in this challenging setting.',\n",
       " 'What we eat is one of the most frequent and important health decisions we make in daily life, yet it remains notoriously difficult to capture and understand. Effective food journaling is thus a grand challenge in personal health informatics. In this paper we describe a system for food journaling called I Ate This, which is inspired by the Remote Food Photography Method (RFPM). I Ate This is simple: you use a smartphone app to take a photo and give a very basic description of any food or beverage you are about to consume. Later, a qualified dietitian will evaluate your photo, giving you feedback on how you did and where you can improve. The aim of I Ate This is to provide a convenient, visual and reliable way to help users learn from their eating habits and nudge them towards better choices each and every day. Ultimately, this incremental approach can lead to long-term behaviour change. Our goal is to bring RFPM to a wider audience, through APIs that can be incorporated into other apps.',\n",
       " 'Ubiquitous sensing is tightly coupled with activity recognition. This survey reviews recent advances in Ubiquitous sensing and looks ahead on promising future directions. In particular, Ubiquitous sensing crosses new barriers giving us new ways to interact with the environment or to inspect our psyche. Through sensing paradigms that parasitically utilise stimuli from the noise of environmental, third-party pre-installed systems, sensing leaves the boundaries of the personal domain. Compared to previous environmental sensing approaches, these new systems mitigate high installation and placement cost by providing a robustness towards process noise. On the other hand, sensing focuses inward and attempts to capture mental activities such as cognitive load, fatigue or emotion through advances in, for instance, eye-gaze sensing systems or interpretation of body gesture or pose. This survey summarises these developments and discusses current research questions and promising future directions.',\n",
       " 'Virtual reality setups are particularly suited to create a tight bond between players and their avatars up to a degree where we start perceiving the virtual representation as our own body. We hypothesize that such an illusion of virtual body ownership (IVBO) has a particularly high, yet overlooked potential for nonhumanoid avatars. To validate our claim, we use the example of three very different creatures---a scorpion, a rhino, and a bird---to explore possible avatar controls and game mechanics based on specific animal abilities. A quantitative evaluation underpins the high game enjoyment arising from embodying such nonhuman morphologies, including additional body parts and obtaining respective superhuman skills, which allows us to derive a set of novel design implications. Furthermore, the experiment reveals a correlation between IVBO and game enjoyment, which is a further indication that nonhumanoid creatures offer a meaningful design space for VR games worth further investigation.',\n",
       " 'This paper overviews the state of the art, research challenges, and future opportunities in an emerging research direction: Social Sensing based Edge Computing (SSEC). Social sensing has emerged as a new sensing application paradigm where measurements about the physical world are collected from humans or from devices on their behalf. The advent of edge computing pushes the frontier of computation, service, and data along the cloud-to-things continuum. The merging of these two technical trends generates a set of new research challenges that need to be addressed. In this paper, we first define the new SSEC paradigm that is motivated by a few underlying technology trends. We then present a few representative real-world case studies of SSEC applications and several key research challenges that exist in those applications. Finally, we envision a few exciting research directions in future SSEC. We hope this paper will stimulate discussions of this emerging research direction in the community.',\n",
       " 'Many professional services are provided through text and voice systems, from voice calls over the internet to messaging and emails. There is a growing need for both individuals and organizations to understand these online conversations better and find actionable insights. One method that allows the user to explore insights is to build intuitive and rich visualizations that illustrate the content of the conversation. In this paper, we present a systematic survey of the various methods of visualizing a conversation and research papers involving interactive visualizations and human participants. Findings from the survey show that there have been attempts to visualize most, if not all, of the types of conversation that are taking place digitally, from speech to messages and emails. Through this survey, we make two contributions. One, we summarize the current practices in the domain of visualizing dyadic conversations. Two, we provide suggestions for future dialogue visualization research.',\n",
       " 'Shopping is difficult for people with motor impairments. This includes online shopping. Proprietary software can emulate mouse and keyboard via head tracking. However, such a solution is not common for smartphones. Unlike desktop and laptop computers, they are also much easier to carry indoors and outdoors.To address this, we implement and open source button that is sensitive to head movements tracked from the front camera of iPhone X. This allows developers to integrate in eCommerce applications easily without requiring specialized knowledge. Other applications include gaming and use in hands-free situations such as during cooking, auto-repair. We built a sample online shopping application that allows users to easily browse between items from various categories and take relevant action just by head movements. We present results of user studies on this sample application and also include sensitivity studies based on two independent tests performed at 3 different distances to the screen.',\n",
       " 'Advances in user interfaces, pattern recognition, and ubiquitous computing continue to pave the way for better navigation towards our health goals. Quantitative methods which can guide us towards our personal health goals will help us optimize our daily life actions, and environmental exposures. Ubiquitous computing is essential for monitoring these factors and actuating timely interventions in all relevant circumstances. We need to combine the events recognized by different ubiquitous systems and derive actionable causal relationships from an event ledger. Understanding of user habits and health should be teleported between applications rather than these systems working in silos, allowing systems to find the optimal guidance medium for required interventions. We propose a method through which applications and devices can enhance the user experience by leveraging event relationships, leading the way to more relevant, useful, and, most importantly, pleasurable health guidance experience.',\n",
       " '- Like consumer electronic products, medical devices are becoming more complicated, with performance doubling every two years. With multiple commands and systems to negotiate, cognitive load can make it difficult for users to execute commands effectively. In the case of medical devices, which use advanced technology and require multidisciplinary inputs for design and development, cognitive workload is a significant factor. As these devices are very expensive and operators require specialized training, effective and economical methods are needed to evaluate the user experience. Heuristic evaluation is an effective method of identifying major usability problems and related issues. This study used heuristic evaluation to assess the usability of a CT scan and associated physical and mental loads for Saudi Arabian users. The findings indicate a gender difference in terms of consistency, flexibility, and document attributes, with a statistically significant gender difference in mental load.',\n",
       " 'Browsing and finding relevant information for Bangladeshi laws is a challenge faced by all law students and researchers in Bangladesh, and by citizens who want to learn about any legal procedure. Some law archives in Bangladesh are digitized, but lack proper tools to organize the data meaningfully. We present a text visualization tool that utilizes machine learning techniques to make the searching of laws quicker and easier. Using Doc2Vec to layout law article nodes, link mining techniques to visualize relevant citation networks, and named entity recognition to quickly find relevant sections in long law articles, our tool provides a faster and better search experience to the users. Qualitative feedback from law researchers, students, and government officials show promise for visually intuitive search tools in the context of governmental, legal, and constitutional data in developing countries, where digitized data does not necessarily pave the way towards an easy access to information.',\n",
       " \"The Google's frugal Cardboard solution for immersive Virtual Reality experiences has come a long way in the VR market. The Google Cardboard VR applications will support us in the fields such as education, virtual tourism, entertainment, gaming, design etc. Recently, Qualcomm's Vuforia SDK has introduced support for developing mixed reality applications for Google Cardboard which can combine Virtual and Augmented Reality to develop exciting and immersive experiences. In this work, we present a comprehensive review of Google Cardboard for AR and also highlight its technical and subjective limitations by conducting a feasibility study through the inspection of a Desktop computer use-case. Additionally, we recommend the future avenues for the Google Cardboard in AR. This work also serves as a guide for Android/iOS developers as there are no published scholarly articles or well documented studies exclusively on Google Cardboard with both user and developer's experience captured at one place.\",\n",
       " 'We propose technology to enable a new medium of expression, where video elements can be looped, merged, and triggered, interactively. Like audio, video is easy to sample from the real world but hard to segment into clean reusable elements. Reusing a video clip means non-linear editing and compositing with novel footage. The new context dictates how carefully a clip must be prepared, so our end-to-end approach enables previewing and easy iteration.\\n  We convert static-camera videos into loopable sequences, synthesizing them in response to simple end-user requests. This is hard because a) users want essentially semantic-level control over the synthesized video content, and b) automatic loop-finding is brittle and leaves users limited opportunity to work through problems. We propose a human-in-the-loop system where adding effort gives the user progressively more creative control. Artists help us evaluate how our trigger interfaces can be used for authoring of videos and video-performances.',\n",
       " 'Mobile devices such as smartphones, smartwatches or smart glasses have revolutionized how we interact. We are interested in smart glasses because they have the advantage of providing a simultaneous view of both physical and digital worlds. Despite this potential, pointing task on smart glasses is not really widespread. In this paper, we compared four interaction techniques for selecting targets : (a) the Absolute Head Movement and (b) the Relative Head Movement, where head movement controls the cursor on smart glasses in absolute or relative way, (c) the Absolute Free Hand interaction, where the forefinger control the cursor and (d) the Tactile Surface interaction, where the user controls the cursor via a small touchpad connected to smart glasses. We conducted an experiment with 18 participants. The Tactile Surface and Absolute Head Movement were the most efficient. The Relative Head Movement and Absolute Free Hand interactions were promising and require more exploration for other tasks.',\n",
       " \"The goal of visual analytics is to create a symbiosis between human and computer by leveraging their unique strengths. While this model has demonstrated immense success, we are yet to realize the full potential of such a human-computer partnership. In a perfect collaborative mixed-initiative system, the computer must possess skills for learning and anticipating the users' needs. Addressing this gap, we propose a framework for inferring focus areas from passive observations of the user's actions, thereby allowing accurate predictions of future events. We evaluate this technique with a crime map and demonstrate that users' clicks appear in our prediction set 95% - 97% of the time. Further analysis shows that we can achieve high prediction accuracy typically after three clicks. Altogether, we show that passive observations of interaction data can reveal valuable information that will allow the system to learn and anticipate future events, laying the foundation for next-generation tools.\",\n",
       " 'Recommender systems rely heavily on the predictive accuracy of the learning algorithm. Most work on improving accuracy has focused on the learning algorithm itself. We argue that this algorithmic focus is myopic. In particular, since learning algorithms generally improve with more and better data, we propose shaping the feedback generation process as an alternate and complementary route to improving accuracy. To this effect, we explore how changes to the user interface can impact the quality and quantity of feedback data -- and therefore the learning accuracy. Motivated by information foraging theory, we study how feedback quality and quantity are influenced by interface design choices along two axes: information scent and information access cost. We present a user study of these interface factors for the common task of picking a movie to watch, showing that these factors can effectively shape and improve the implicit feedback data that is generated while maintaining the user experience.',\n",
       " 'In recent years, bike-sharing systems have been deployed in many cities, which provide an economical lifestyle. With the prevalence of bike-sharing systems, a lot of companies join the market, leading to increasingly fierce competition. To be competitive, bike-sharing companies and app developers need to make strategic decisions for mobile apps development. Therefore, it is significant to predict and compare the popularity of different bike-sharing apps. However, existing works mostly focus on predicting the popularity of a single app, the popularity contest among different apps has not been explored yet. In this paper, we aim to forecast the popularity contest between Mobike and Ofo, two most popular bike-sharing apps in China. We develop CompetitiveBike, a system to predict the popularity contest among bike-sharing apps. Moreover, we conduct experiments on real-world datasets collected from 11 app stores and Sina Weibo, and the experiments demonstrate the effectiveness of our approach.',\n",
       " 'Digital ink promises to combine the flexibility and aesthetics of handwriting and the ability to process, search and edit digital text. Character recognition converts handwritten text into a digital representation, albeit at the cost of losing personalized appearance due to the technical difficulties of separating the interwoven components of content and style. In this paper, we propose a novel generative neural network architecture that is capable of disentangling style from content and thus making digital ink editable. Our model can synthesize arbitrary text, while giving users control over the visual appearance (style). For example, allowing for style transfer without changing the content, editing of digital ink at the word level and other application scenarios such as spell-checking and correction of handwritten text. We furthermore contribute a new dataset of handwritten text with fine-grained annotations at the character level and report results from an initial user evaluation.',\n",
       " 'Advances in machine learning have produced systems that attain human-level performance on certain visual tasks, e.g., object identification. Nonetheless, other tasks requiring visual expertise are unlikely to be entrusted to machines for some time, e.g., satellite and medical imagery analysis. We describe a human-machine cooperative approach to visual search, the aim of which is to outperform either human or machine acting alone. The traditional route to augmenting human performance with automatic classifiers is to draw boxes around regions of an image deemed likely to contain a target. Human experts typically reject this type of hard highlighting. We propose instead a soft highlighting technique in which the saliency of regions of the visual field is modulated in a graded fashion based on classifier confidence level. We report on experiments with both synthetic and natural images showing that soft highlighting achieves a performance synergy surpassing that attained by hard highlighting.',\n",
       " 'The article presents the dynamics of social networks users increase, depending on the total world population from 2010 to 2018. It also identifies the most popular social networks in Ukraine. The systematic risk indicator of using social networks relative to the total number of Internet resources users is determined. Types of social intercourse in the process of the higher education institution image creation are presented. The peculiarities of using social networks in the formation of a positive image of an educational institution are highlighted. The statistical indicators of user actions in the official group of the Faculty of Mathematics and Information Technologies of Vasyl Stus Donetsk National University in January, February and March 2019 are presented, as well as the average attraction coefficient of users depending on the subject of publications. The main technologies of astroturfing in the creation process of the higher education institution negative image are considered.',\n",
       " \"This work covers multiple aspects of overt visual attention on 3D renders: measurement, projection, visualization, and application to studying the influence of material appearance on looking behaviour. In the scope of this work, we ran an eye-tracking experiment in which the observers are presented with animations of rotating 3D objects. The objects were rendered to simulate different metallic appearance, particularly smooth (glossy), rough (matte), and coated gold. The eye-tracking results illustrate how material appearance itself influences the observer's attention, while all the other parameters remain unchanged. In order to make visualization of the attention maps more natural and also make the analysis more accurate, we develop a novel technique of projection of gaze fixations on the 3D surface of the figure itself, instead of the conventional 2D plane of the screen. The proposed methodology will be useful for further studies of attention and saliency in the computer graphics domain.\",\n",
       " \"Three-dimensional (3D) applications have come to every corner of life. We present 3DTouch, a novel 3D wearable input device worn on the fingertip for interacting with 3D applications. 3DTouch is self-contained, and designed to universally work on various 3D platforms. The device employs touch input for the benefits of passive haptic feedback, and movement stability. Moreover, with touch interaction, 3DTouch is conceptually less fatiguing to use over many hours than 3D spatial input devices such as Kinect. Our approach relies on relative positioning technique using an optical laser sensor and a 9-DOF inertial measurement unit. We implemented a set of 3D interaction techniques including selection, translation, and rotation using 3DTouch. An evaluation also demonstrates the device's tracking accuracy of 1.10 mm and 2.33 degrees for subtle touch interaction in 3D space. With 3DTouch project, we would like to provide an input device that reduces the gap between 3D applications and users.\",\n",
       " 'Lecture notes are important for students to review and understand the key points in the class. Unfortunately, the students often miss or lose part of the lecture notes. In this paper, we design and implement an infrared sensor based system, InfraNotes, to automatically record the notes on the board by sensing and analyzing hand gestures of the lecturer. Compared with existing techniques, our system does not require special accessories with lecturers such as sensor-facilitated pens, writing surfaces or the video-taping infrastructure. Instead, it only has an infrared-sensor module on the eraser holder of black/white board to capture handwritten trajectories. With a lightweight framework for handwritten trajectory processing, clear lecture notes can be generated automatically. We evaluate the quality of lecture notes by three standard character recognition techniques. The results indicate that InfraNotes is a promising solution to create clear and complete lectures to promote the education.',\n",
       " 'Cartograms combine statistical and geographical information in thematic maps, where areas of geographical regions (e.g., countries, states) are scaled in proportion to some statistic (e.g., population, income). Cartograms make it possible to gain insight into patterns and trends in the world around us and have been very popular visualizations for geo-referenced data for over a century. This work surveys cartogram research in visualization, cartography and geometry, covering a broad spectrum of different cartogram types: from the traditional rectangular and table cartograms, to Dorling and diffusion cartograms. A particular focus is the study of the major cartogram dimensions: statistical accuracy, geographical accuracy, and topological accuracy. We review the history of cartograms, describe the algorithms for generating them, and consider task taxonomies. We also review quantitative and qualitative evaluations, and we use these to arrive at design guidelines and research challenges.',\n",
       " 'This paper outlines the development of a wearable game controller incorporating vibrotacticle haptic feedback that provides a low cost, versatile and intuitive interface for controlling digital games. The device differs from many traditional haptic feedback implementation in that it combines vibrotactile based haptic feedback with gesture based input, thus becoming a two way conduit between the user and the virtual environment. The device is intended to challenge what is considered an \"interface\" and draws on work in the area of Actor-Network theory to purposefully blur the boundary between man and machine. This allows for a more immersive experience, so rather than making the user feel like they are controlling an aircraft the intuitive interface allows the user to become the aircraft that is controlled by the movements of the user\\'s hand. This device invites playful action and thrill. It bridges new territory on portable and low cost solutions for haptic controllers in a gaming context.',\n",
       " \"Expert crowdsourcing marketplaces have untapped potential to empower workers' career and skill development. Currently, many workers cannot afford to invest the time and sacrifice the earnings required to learn a new skill, and a lack of experience makes it difficult to get job offers even if they do. In this paper, we seek to lower the threshold to skill development by repurposing existing tasks on the marketplace as mentored, paid, real-world work experiences, which we refer to as micro-internships. We instantiate this idea in Atelier, a micro-internship platform that connects crowd interns with crowd mentors. Atelier guides mentor-intern pairs to break down expert crowdsourcing tasks into milestones, review intermediate output, and problem-solve together. We conducted a field experiment comparing Atelier's mentorship model to a non-mentored alternative on a real-world programming crowdsourcing task, finding that Atelier helped interns maintain forward progress and absorb best practices.\",\n",
       " 'Surveillance is essential for the safety of power substation. The detection of whether wearing safety helmets or not for perambulatory workers is the key component of overall intelligent surveillance system in power substation. In this paper, a novel and practical safety helmet detection framework based on computer vision, machine learning and image processing is proposed. In order to ascertain motion objects in power substation, the ViBe background modelling algorithm is employed. Moreover, based on the result of motion objects segmentation, real-time human classification framework C4 is applied to locate pedestrian in power substation accurately and quickly. Finally, according to the result of pedestrian detection, the safety helmet wearing detection is implemented using the head location, the color space transformation and the color feature discrimination. Extensive compelling experimental results in power substation illustrate the efficiency and effectiveness of the proposed framework.',\n",
       " 'Humans are the most effective integrators and producers of information, directly and through the use of information-processing inventions. As these inventions become increasingly sophisticated, the substantive role of humans in processing information will tend toward capabilities that derive from our most complex cognitive processes, e.g., abstraction, creativity, and applied world knowledge. Through the advancement of human computation - methods that leverage the respective strengths of humans and machines in distributed information-processing systems - formerly discrete processes will combine synergistically into increasingly integrated and complex information processing systems. These new, collective systems will exhibit an unprecedented degree of predictive accuracy in modeling physical and techno-social processes, and may ultimately coalesce into a single unified predictive organism, with the capacity to address societies most wicked problems and achieve planetary homeostasis.',\n",
       " 'In this research, a literature in human-computer interaction is reviewed and the technology aspect of human computer interaction related with digital academic supportive devices is also analyzed. According to all these concerns, recommendations to design good human-computer digital academic supportive devices are analyzed and proposed. Due to improvements in both hardware and software, digital devices have unveiled continuous advances in efficiency and processing capacity. However, many of these systems are also becoming larger and increasingly more complex. Although such complexity usually poses no difficulties for many users, it often creates barriers for academic users while using digital devices. Usually, in designing those digital devices, the human-computer interaction is left behind without consideration. To achieve dependable, usable, and well-engineered interactive digital academic supportive devices requires applied human computer interaction research and awareness of its issues.',\n",
       " 'Deep learning is one of the fastest growing technologies in computer science with a plethora of applications. But this unprecedented growth has so far been limited to the consumption of deep learning experts. The primary challenge being a steep learning curve for learning the programming libraries and the lack of intuitive systems enabling non-experts to consume deep learning. Towards this goal, we study the effectiveness of a no-code paradigm for designing deep learning models. Particularly, a visual drag-and-drop interface is found more efficient when compared with the traditional programming and alternative visual programming paradigms. We conduct user studies of different expertise levels to measure the entry level barrier and the developer load across different programming paradigms. We obtain a System Usability Scale (SUS) of 90 and a NASA Task Load index (TLX) score of 21 for the proposed visual programming compared to 68 and 52, respectively, for the traditional programming methods.',\n",
       " \"The widespread popularity of Pok\\\\'emon GO presents the first opportunity to observe the geographic effects of location-based gaming at scale. This paper reports the results of a mixed methods study of the geography of Pok\\\\'emon GO that includes a five-country field survey of 375 Pok\\\\'emon GO players and a large scale geostatistical analysis of game elements. Focusing on the key geographic themes of places and movement, we find that the design of Pok\\\\'emon GO reinforces existing geographically-linked biases (e.g. the game advantages urban areas and neighborhoods with smaller minority populations), that Pok\\\\'emon GO may have instigated a relatively rare large-scale shift in global human mobility patterns, and that Pok\\\\'emon GO has geographically-linked safety risks, but not those typically emphasized by the media. Our results point to geographic design implications for future systems in this space such as a means through which the geographic biases present in Pok\\\\'emon GO may be counteracted.\",\n",
       " 'With the large number of partially or completely visually impaired persons in society, their integration as productive, educated and capable members of society is hampered heavily by a pervasively high level of braille illiteracy. This problem is further compounded by the fact that braille printers are prohibitively expensive - generally starting from two thousand US dollars, beyond the reach of the common man. Over the period of a year, the authors have tried to develop a Braille printer which attempts to overcome the problems inherent in commercial printers. The purpose of this paper, therefore, is to introduce two prototypes - the first with an emphasis of cost-effectiveness, and the second prototype, which is more experimental and aims to eliminate several demerits of Braille printing. The first prototype has been constructed at a cost significantly less than the existing commercial braille printers. Both the prototypes of the device have been constructed, which will be shown.',\n",
       " \"Screencasts, where computer screen is broadcast to a large audience on the web, are becoming popular as an online educational tool. Among various types of screencast content, popular are the contents that involve text editing, including computer programming. There are emerging platforms that support such text-based screencasts by recording every character insertion/deletion from the creator and reconstructing its playback on the viewer's screen. However, these platforms lack rich support for creating and editing the screencast itself, mainly due to the difficulty of manipulating recorded text changes; the changes are tightly coupled in sequence, thus modifying arbitrary part of the sequence is not trivial. We present a non-linear editing tool for text-based screencasts. With the proposed selective history rewrite process, our editor allows users to substitute an arbitrary part of a text-based screencast while preserving overall consistency of the rest of the text-based screencast.\",\n",
       " 'Designing and building automated systems with which people can interact naturally is one of the emerging objective of Mechatronics. In this perspective multimodality and adaptivity represent focal issues, enabling users to communicate more freely and naturally with automated systems. One of the basic problem of multimodal interaction is the fusion process. Current approaches to fusion are mainly two: the former implements the multimodal fusion at dialogue management level, whereas the latter at grammar level. In this paper, we propose a multimodal attribute grammar, that provides constructions both for representing input symbols from different modalities and for modeling semantic and temporal features of multimodal input symbols, enabling the specification of multimodal languages. Moreover, an application of the proposed approach in the context of a multimodal language specification to control a driver assistance system, as robots using different integrated interaction modalities, is given.',\n",
       " \"The paper analyzes the interaction between humans and computers in terms of response time in solving the image-based CAPTCHA. In particular, the analysis focuses on the attitude of the different Internet users in easily solving four different types of image-based CAPTCHAs which include facial expressions like: animated character, old woman, surprised face, worried face. To pursue this goal, an experiment is realized involving 100 Internet users in solving the four types of CAPTCHAs, differentiated by age, Internet experience, and education level. The response times are collected for each user. Then, association rules are extracted from user data, for evaluating the dependence of the response time in solving the CAPTCHA from age, education level and experience in internet usage by statistical analysis. The results implicitly capture the users' psychological states showing in what states the users are more sensible. It reveals to be a novelty and a meaningful analysis in the state-of-the-art.\",\n",
       " 'We consider a crowdsourcing model in which $n$ workers are asked to rate the quality of $n$ items previously generated by other workers. An unknown set of $\\\\alpha n$ workers generate reliable ratings, while the remaining workers may behave arbitrarily and possibly adversarially. The manager of the experiment can also manually evaluate the quality of a small number of items, and wishes to curate together almost all of the high-quality items with at most an $\\\\epsilon$ fraction of low-quality items. Perhaps surprisingly, we show that this is possible with an amount of work required of the manager, and each worker, that does not scale with $n$: the dataset can be curated with $\\\\tilde{O}\\\\Big(\\\\frac{1}{\\\\beta\\\\alpha^3\\\\epsilon^4}\\\\Big)$ ratings per worker, and $\\\\tilde{O}\\\\Big(\\\\frac{1}{\\\\beta\\\\epsilon^2}\\\\Big)$ ratings by the manager, where $\\\\beta$ is the fraction of high-quality items. Our results extend to the more general setting of peer prediction, including peer grading in online classrooms.',\n",
       " 'The Audio/Visual Emotion Challenge and Workshop (AVEC 2019) \"State-of-Mind, Detecting Depression with AI, and Cross-cultural Affect Recognition\" is the ninth competition event aimed at the comparison of multimedia processing and machine learning methods for automatic audiovisual health and emotion analysis, with all participants competing strictly under the same conditions. The goal of the Challenge is to provide a common benchmark test set for multimodal information processing and to bring together the health and emotion recognition communities, as well as the audiovisual processing communities, to compare the relative merits of various approaches to health and emotion recognition from real-life data. This paper presents the major novelties introduced this year, the challenge guidelines, the data used, and the performance of the baseline systems on the three proposed tasks: state-of-mind recognition, depression assessment with AI, and cross-cultural affect sensing, respectively.',\n",
       " \"Throughout the course of product experience, a user employs multiple senses, including vision, hearing, and touch. Previous cross-modal studies have shown that multiple senses interact with each other and change perceptions. In this paper, we propose a methodology for designing multisensory product experiences by applying cross-modal effect to simultaneous stimuli. In this methodology, we first obtain a model of the comprehensive cognitive structure of user's multisensory experience by applying Kansei modeling methodology and extract opportunities of cross-modal effect from the structure. Second, we conduct experiments on these cross-modal effects and formulate them by obtaining a regression curve through analysis. Finally, we find solutions to improve the product sensory experience from the regression model of the target cross-modal effects. We demonstrated the validity of the methodology with SLR cameras as a case study, which is a typical product with multisensory perceptions.\",\n",
       " 'Recent advances in haptic hardware and software technology have generated interest in novel, multimodal interfaces based on the sense of touch. Such interfaces have the potential to revolutionize the way we think about human computer interaction and open new possibilities for simulation and training in a variety of fields. In this paper we review several frameworks, APIs and toolkits for haptic user interface development. We explore these software components focusing on minimally invasive surgical simulation systems. In the area of medical diagnosis, there is a strong need to determine mechanical properties of biological tissue for both histological and pathological considerations. Therefore we focus on the development of affordable visuo-haptic simulators to improve practice-based education in this area. We envision such systems, designed for the next generations of learners that enhance their knowledge in connection with real-life situations while they train in mandatory safety conditions.',\n",
       " 'The regular K-10 curriculums often do not get the necessary of affordable technology involving interactive ways of teaching the prescribed curriculum with effective analytical skill building. In this paper, we present \"PlutoAR\", a paper-based augmented reality interpreter which is scalable, affordable, portable and can be used as a platform for skill building for the kids. PlutoAR manages to overcome the conventional albeit non-interactive ways of teaching by incorporating augmented reality (AR) through an interactive toolkit to provide students with the best of both worlds. Students cut out paper \"tiles\" and place these tiles one by one on a larger paper surface called \"Launchpad\" and use the PlutoAR mobile application which runs on any Android device with a camera and uses augmented reality to output each step of the program like an interpreter. PlutoAR has inbuilt AR experiences like stories, maze solving using conditional loops, simple elementary mathematics and the intuition of gravity.',\n",
       " \"While a number of touch-based visualization systems have appeared in recent years, relatively little work has been done to evaluate these systems. The prevailing methods compare these systems to desktop-class applications or utilize traditional training-based usability studies. We argue that existing studies, while useful, fail to address a key aspect of mobile application usage - initial impression and discoverability-driven usability. Over the past few years, we have developed a tablet-based visualization system, Tangere, for analyzing tabular data in a multiple coordinated view configuration. This article describes a discoverability-based user study of Tangere in which the system is compared to a commercially available visualization system for tablets - Tableau's Vizable. The study highlights aspects of each system's design that resonate with study participants, and we reflect upon those findings to identify design principles for future tablet-based data visualization systems.\",\n",
       " \"This bachelor's thesis describes the conception and implementation of an augmented reality application for the Android platform. The intention is to demonstrate some possibilities of interaction within an augmented reality environment on mobile devices. For that purpose, a 3D-model is displayed on the devices' touchscreen using marker-based tracking. This enables the user to translate, rotate or scale the model as he wishes. He can additionally select and highlight preassigned parts of the model to display specific information for that element. To assist developers in modifying the application for changing requirements without re-writing large portions of the source code, the information for each part have been encapsulated into its own data type. After an introduction to augmented reality, its underlying technology and the Android platform, some possible usage scenarios and the resulting functionalities are outlined. Finally, the design as well as the developed implementation are described.\",\n",
       " 'Thanks to mobile apps such as Periscope and Facebook Live, live-streaming video is having a moment again. It has not been clear, however, to what extent the current ubiquity of smartphones is impacting this technology\\'s acceptance in everyday social situations and how mobile contexts or affordances will affect and be affected by shifts in social norms and policy debates regarding privacy, surveillance and intellectual property. This ethnographic-style research explores familiarity with and attitudes about mobile live-streaming video and related legal and ethical issues among a sample of \"Middle America\" participants at two typical outdoor social events: sports tailgating and a rooftop party. In situ observations of n=110 bystanders to the use of a smartphone, including interviews with n=20, revealed that many are not fully aware of when their image or speech is being live-streamed in a casual context and want stronger notifications of and ability to consent to such broadcasting.',\n",
       " 'It remains uncertain regarding the safety of driving in autonomous vehicles that, after a long, passive control and inattention to the driving situation, how the drivers will be effectively informed to take-over the control in emergency. In particular, the active role of vehicle force feedback on the driver\\'s risk perception on curves has not been fully explored. To investigate it, the current paper examined the driver\\'s cognitive and visual responses to the whole-body haptic feedback during curve negotiations. The effects of force feedback on drivers\\' responses on curves were investigated in a high-fidelity driving simulator while measuring EEG and visual gaze over ten participants. The preliminary analyses of the first two participants revealed that pupil diameter and fixation time on the curves were significantly longer when the driver received whole-body feedback, compared to none. The findings suggest that whole-body feedback can be used as an effective \"advance notification\" of hazards.',\n",
       " 'The aim of the study is the description of problem of developing web design for people with color blindness. The objectives of the study are familiarising with the exiting algorithms of simulation color blindness and searching the most appropriate color models to realize a filter of disputed colors. The object of the study is the convertation of color models and algorithms of filtration. The subject of the study are methods of recognition disputed colors. In the study were investigated the problems of color blind people, examined the basic concepts of trichromatic color vision theory, substantiated the necessity of changing different types of color models, given formulas convertation from RGB-color model to HSL-color model, systematized the algorithms of imitation and filtration of colors for different types of dichromacy: protanopia, deuteranopia and tritanopia. The results of the study are planned using in development of adapting website design for people with color blindness.',\n",
       " 'We are developing a system for human-robot communication that enables people to communicate with robots in a natural way and is focused on solving problems in a shared space. Our strategy for developing this system is fundamentally data-driven: we use data from multiple input sources and train key components with various machine learning techniques. We developed a web application that is collecting data on how two humans communicate to accomplish a task, as well as a mobile laboratory that is instrumented to collect data on how two humans communicate to accomplish a task in a physically shared space. The data from these systems will be used to train and fine-tune the second stage of our system, in which the robot will be simulated through software. A physical robot will be used in the final stage of our project. We describe these instruments, a test-suite and performance metrics designed to evaluate and automate the data gathering process as well as evaluate an initial data set.',\n",
       " 'In this paper, we examine the statistical soundness of comparative assessments within the field of recommender systems in terms of reliability and human uncertainty. From a controlled experiment, we get the insight that users provide different ratings on same items when repeatedly asked. This volatility of user ratings justifies the assumption of using probability densities instead of single rating scores. As a consequence, the well-known accuracy metrics (e.g. MAE, MSE, RMSE) yield a density themselves that emerges from convolution of all rating densities. When two different systems produce different RMSE distributions with significant intersection, then there exists a probability of error for each possible ranking. As an application, we examine possible ranking errors of the Netflix Prize. We are able to show that all top rankings are more or less subject to high probabilities of error and that some rankings may be deemed to be caused by mere chance rather than system quality.',\n",
       " 'Riemannian geometry has been successfully used in many brain-computer interface (BCI) classification problems and demonstrated superior performance. In this paper, for the first time, it is applied to BCI regression problems, an important category of BCI applications. More specifically, we propose a new feature extraction approach for Electroencephalogram (EEG) based BCI regression problems: a spatial filter is first used to increase the signal quality of the EEG trials and also to reduce the dimensionality of the covariance matrices, and then Riemannian tangent space features are extracted. We validate the performance of the proposed approach in reaction time estimation from EEG signals measured in a large-scale sustained-attention psychomotor vigilance task, and show that compared with the traditional powerband features, the tangent space features can reduce the root mean square estimation error by 4.30-8.30%, and increase the estimation correlation coefficient by 6.59-11.13%.',\n",
       " 'Prison facilities, mental correctional institutions, sports bars and places of public protest are prone to sudden violence and conflicts. Surveillance systems play an important role in mitigation of hostile behavior and improvement of security by detecting such provocative and aggressive activities. This research proposed using automatic aggressive behavior and anger detection to improve the effectiveness of the surveillance systems. An emotion and aggression aware component will make the surveillance system highly responsive and capable of alerting the security guards in real time. This research proposed facial expression, head, hand and body movement and speech tracking for detecting anger and aggressive actions. Recognition was achieved using support vector machines and rule based features. The multimodal affect recognition precision rate for anger improved by 15.2% and recall rate improved by 11.7% when behavioral rule based features were used in aggressive action detection.',\n",
       " 'Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.',\n",
       " 'Even in the digital age, designers largely rely on physical material samples to illustrate their products, as existing visual representations fail to sufficiently reproduce the look and feel of real world materials. Here, we investigate the use of interactive material sonification as an additional sensory modality for communicating well-established material qualities like softness, pleasantness or value. We developed a custom application for touchscreen devices that receives tactile input and translate it into material rubbing sound using granular synthesis. We used this system to perform a psychophysical study, in which the ability of the user to rate subjective material qualities is evaluated, with the actual material samples serving as reference stimulus. Our experimental results indicate that the considered audio cues do not significantly contribute to the perception of material qualities but are able to increase the level of immersion when interacting with digital samples.',\n",
       " \"Music history, referring to the records of users' listening or downloading history in online music services, is the primary source for music service providers to analyze users' preferences on music and thus to provide personalized recommendations to users. In order to engage users into the service and to improve user experience, it would be beneficial to provide visual analyses of one user's music history as well as visualized recommendations to that user. In this paper, we take a user-centric approach to the design of such visual analyses. We start by investigating user needs on such visual analyses and recommendations, then propose several different visualization schemes, and perform a pilot study to collect user feedback on the designed schemes. We further conduct user studies to verify the utility of the proposed schemes, and the results not only demonstrate the effectiveness of our proposed visualization, but also provide important insights to guide the visualization design in the future.\",\n",
       " 'Social media enjoys a growing popularity as a platform to seek and share personal health information. For sleep studies using data from social media, most researchers focused on inferring sleep-related artifacts from self-reported anecdotal pointers to sleep patterns or issues such as insomnia. The data shared by \"quantified-selfers\" on social media presents an opportunity to study more quantitative and objective measures of sleep. We propose and validate the approach of collecting and analyzing sleep logs that are generated and shared through a sleep-tracking mobile application. We highlight the value of this data by combining it with users\\' social media data. The results provide a validation of using social media for sleep studies as the collected sleep data is aligned with sleep data from other sources. The results of combining social media data with sleep data provide preliminary evidence that higher social media activity is associated with lower sleep duration and quality.',\n",
       " \"Recent methods to automatically calibrate stationary eye trackers were shown to effectively reduce inherent calibration distortion. However, these methods require additional information, such as mouse clicks or on-screen content. We propose the first method that only requires users' eye movements to reduce calibration distortion in the background while users naturally look at an interface. Our method exploits that calibration distortion makes straight saccade trajectories appear curved between the saccadic start and end points. We show that this curving effect is systematic and the result of distorted gaze projection plane. To mitigate calibration distortion, our method undistorts this plane by straightening saccade trajectories using image warping. We show that this approach improves over the common six-point calibration and is promising for reducing distortion. As such, it provides a non-intrusive solution to alleviating accuracy decrease of eye tracker during long-term use.\",\n",
       " 'There are numerous possibilities and motivations for an adaptive BCI, which may not be easy to clarify and organize for a newcomer to the field. To our knowledge, there has not been any work done in classifying the literature on adaptive BCI in a comprehensive and structured way. We propose a conceptual framework, a taxonomy of adaptive BCI methods which encompasses most important approaches to fit them in such a way that a reader can clearly visualize which elements are being adapted and for what reason. In the interest of having a clear review of existing adaptive BCIs, this framework considers adaptation approaches for both the user and the machine, i.e., using instructional design observations as well as the usual machine learning techniques. This framework not only provides a coherent review of such extensive literature but also enables the reader to perceive gaps and flaws in the current BCI systems, which would hopefully bring novel solutions for an overall improvement.',\n",
       " 'In this paper we measure the step-wise latency in the pipeline of three kinds of interactive mobile video applications that are rapidly gaining popularity, namely Remote Graphics Rendering (RGR) of which we focus on mobile cloud gaming, Mobile Augmented Reality (MAR), and Mobile Virtual Reality (MVR). The applications differ from each other by the way in which the user interacts with the application, i.e., video I/O and user controls, but they all share in common the fact that their user experience is highly sensitive to end-to-end latency. Long latency between a user control event and display update renders the application unusable. Hence, understanding the nature and origins of latency of these applications is of paramount importance. We show through extensive measurements that control input and display buffering have a substantial effect on the overall delay. Our results shed light on the latency bottlenecks and the maturity of technology for seamless user experience with these applications.',\n",
       " \"In this paper, we report a method of intuitively transmitting symbolic information to untrained users via only their hands without using any visual or auditory cues. Our simple concept is presenting three-dimensional letter trajectories to the user's hand via a stylus which is mechanically manipulated. By this simple method, in our experiments, participants were able to read 14 mm-high lower-case letters displayed at a rate of one letter per second with an accuracy rate of 71.9% in their first trials, which was improved to 91.3% after a five-minute training period. These results showed small individual differences among participants (standard deviation of 12.7% in the first trials and 6.7% after training). We also found that this accuracy was still retained to a high level (85.1% with SD of 8.2%) even when the letters were reduced to a height of 7 mm. Thus, we revealed that sighted adults potentially possess the ability to read small letters accurately at normal writing speed using their hands.\",\n",
       " 'Haptic interfaces have untapped the sense of touch to assist multimodal music learning. We have recently seen various improvements of interface design on tactile feedback and force guidance aiming to make instrument learning more effective. However, most interfaces are still quite static; they cannot yet sense the learning progress and adjust the tutoring strategy accordingly. To solve this problem, we contribute an adaptive haptic interface based on the latest design of haptic flute. We first adopted a clutch mechanism to enable the interface to turn on and off the haptic control flexibly in real time. The interactive tutor is then able to follow human performances and apply the \"teacher force\" only when the software instructs so. Finally, we incorporated the adaptive interface with a step-by-step dynamic learning strategy. Experimental results showed that dynamic learning dramatically outperforms static learning, which boosts the learning rate by 45.3% and shrinks the forgetting chance by 86%.',\n",
       " 'Signatures have been traditionally acquired in pen-based office-like scenarios using devices specifically designed for signing. However, the high deployment of devices such as smartphones and tablets has given rise to new and thriving scenarios where signatures can be performed using not only a pen stylus but also the finger. Some preliminary studies have highlighted the challenge of this new scenario and the necessity of further research on the topic. The main contribution of this work is to propose a new signature verification architecture adapted to the signature complexity in order to tackle this new and challenging scenario. Additionally, an exhaustive comparative analysis of both pen- and touch-based scenarios using our proposed methodology is carried out along with a review of the most relevant and recent studies in the field. Significant improvements of biometric verification performance and practical insights are extracted for the application of signature verification in real scenarios.',\n",
       " 'We consider the problem of designing an artificial agent capable of interacting with humans in collaborative dialogue to produce creative, engaging narratives. In this task, the goal is to establish universe details, and to collaborate on an interesting story in that universe, through a series of natural dialogue exchanges. Our model can augment any probabilistic conversational agent by allowing it to reason about universe information established and what potential next utterances might reveal. Ideally, with each utterance, agents would reveal just enough information to add specificity and reduce ambiguity without limiting the conversation. We empirically show that our model allows control over the rate at which the agent reveals information and that doing so significantly improves accuracy in predicting the next line of dialogues from movies. We close with a case-study with four professional theatre performers, who preferred interactions with our model-augmented agent over an unaugmented agent.',\n",
       " \"In this study, a novel control algorithm for a P-300 based brain-computer interface is fully developed to control a 2-DoF robotic arm. Eight subjects including 5 men and 3 women, perform a 2-dimensional target tracking task in a simulated environment. Their EEG signals from visual cortex are recorded and P-300 components are extracted and evaluated to perform a real-time BCI based controller. The volunteer's intention is recognized and will be decoded as an appropriate command to control the cursor. The final goal of the system is to control a simulated robotic arm in a 2-dimensional space for writing some English letters. The results show that the system allows the robot end-effector to move between arbitrary positions in a point-to-point session with the desired accuracy. This model is tested on and compared with Dataset II of the BCI Competition. The best result is obtained with a multi-class SVM solution as the classifier, with a recognition rate of 97 percent, without pre-channel selection.\",\n",
       " \"The vast majority of recommender systems model preferences as static or slowly changing due to observable user experience. However, spontaneous changes in user preferences are ubiquitous in many domains like media consumption and key factors that drive changes in preferences are not directly observable. These latent sources of preference change pose new challenges. When systems do not track and adapt to users' tastes, users lose confidence and trust, increasing the risk of user churn. We meet these challenges by developing a model of novelty preferences that learns and tracks latent user tastes. We combine three innovations: a new measure of item similarity based on patterns of consumption co-occurrence; model for {\\\\em spontaneous} changes in preferences; and a learning agent that tracks each user's dynamic preferences and learns individualized policies for variety. The resulting framework adaptively provides users with novelty tailored to their preferences for change per se.\",\n",
       " 'Across- and within-recording variabilities in electroencephalographic (EEG) activity is a major limitation in EEG-based brain-computer interfaces (BCIs). Specifically, gradual changes in fatigue and vigilance levels during long EEG recording durations and BCI system usage bring along significant fluctuations in BCI performances even when these systems are calibrated daily. We address this in an experimental offline study from EEG-based BCI speller usage data acquired for one hour duration. As the main part of our methodological approach, we propose the concept of adversarial invariant feature learning for BCIs as a regularization approach on recently expanding EEG deep learning architectures, to learn nuisance-invariant discriminative features. We empirically demonstrate the feasibility of adversarial feature learning on eliminating drowsiness effects from event related EEG activity features, by using temporal recording block ordering as the source of drowsiness variability.',\n",
       " 'In this paper, we propose a novel continuous authentication system for smartphone users. The proposed system entirely relies on unlabeled phone movement patterns collected through smartphone accelerometer. The data was collected in a completely unconstrained environment over five to twelve days. The contexts of phone usage were identified using k-means clustering. Multiple profiles, one for each context, were created for every user. Five machine learning algorithms were employed for classification of genuine and impostors. The performance of the system was evaluated over a diverse population of 57 users. The mean equal error rates achieved by Logistic Regression, Neural Network, kNN, SVM, and Random Forest were 13.7%, 13.5%, 12.1%, 10.7%, and 5.6% respectively. A series of statistical tests were conducted to compare the performance of the classifiers. The suitability of the proposed system for different types of users was also investigated using the failure to enroll policy.',\n",
       " \"Online labor platforms, such as the Amazon Mechanical Turk, provide an effective framework for eliciting responses to judgment tasks. Previous work has shown that workers respond best to financial incentives, especially to extra bonuses. However, most of the tested incentives involve describing the bonus conditions in formulas instead of plain English. We believe that different incentives given in English (or in qualitative framing) will result in differences in workers' performance, especially when task difficulties vary. In this paper, we report the preliminary results of a crowdsourcing experiment comparing workers' performance using only qualitative framings of financial incentives. Our results demonstrate a significant increase in workers' performance using a specific well-formulated qualitative framing inspired by the Peer Truth Serum. This positive effect is observed only when the difficulty of the task is high, while when the task is easy there is no difference of which incentives to use.\",\n",
       " 'Situationally-induced impairments and disabilities (SIIDs) make it difficult for users of interactive computing systems to perform tasks due to context (e.g., listening to a phone call when in a noisy crowd) rather than a result of a congenital or acquired impairment (e.g., hearing damage). SIIDs are a great concern when considering the ubiquitousness of technology in a wide range of contexts. Considering our daily reliance on technology, and mobile technology in particular, it is increasingly important that we fully understand and model how SIIDs occur. Similarly, we must identify appropriate methods for sensing and adapting technology to reduce the effects of SIIDs. In this workshop, we will bring together researchers working on understanding, sensing, modelling, and adapting technologies to ameliorate the effects of SIIDs. This workshop will provide a venue to identify existing research gaps, new directions for future research, and opportunities for future collaboration.',\n",
       " \"Scatter plots carry an implicit if subtle message about causality. Whether we look at functions of one variable in pure mathematics, plots of experimental measurements as a function of the experimental conditions, or scatter plots of predictor and response variables, the value plotted on the vertical axis is by convention assumed to be determined or influenced by the value on the horizontal axis. This is a problem for the public understanding of scientific results and perhaps also for professional scientists' interpretations of scatter plots. To avoid suggesting a causal relationship between the x and y values in a scatter plot, we propose a new type of data visualization, the diamond plot. Diamond plots are essentially 45 degree rotations of ordinary scatter plots; by visually jarring the viewer they clearly indicate that she should not draw the usual distinction between independent/predictor variable and dependent/response variable. Instead, she should see the relationship as purely correlative.\",\n",
       " \"The emergence of smartwatches poses new challenges to information security. Although there are mature touch-based authentication methods for smartphones, the effectiveness of using these methods on smartwatches is still unclear. We conducted a user study (n=16) to evaluate how authentication methods (PIN and Pattern), UIs (Square and Circular), and display sizes (38mm and 42mm) affect authentication accuracy, speed, and security. Circular UIs are tailored to smartwatches with fewer UI elements. Results show that 1) PIN is more accurate and secure than Pattern; 2) Pattern is much faster than PIN; 3) Square UIs are more secure but less accurate than Circular UIs; 4) display size does not affect accuracy or speed, but security; 5) Square PIN is the most secure method of all. The study also reveals a security concern that participants' favorite method is not the best in any of the measures. We finally discuss implications for future touch-based smartwatch authentication design.\",\n",
       " \"Collective urban mobility embodies the residents' local insights on the city. Mobility practices of the residents are produced from their spatial choices, which involve various considerations such as the atmosphere of destinations, distance, past experiences, and preferences. The advances in mobile computing and the rise of geo-social platforms have provided the means for capturing the mobility practices; however, interpreting the residents' insights is challenging due to the scale and complexity of an urban environment, and its unique context. In this paper, we present MobInsight, a framework for making localized interpretations of urban mobility that reflect various aspects of the urbanism. MobInsight extracts a rich set of neighborhood features through holistic semantic aggregation, and models the mobility between all-pairs of neighborhoods. We evaluate MobInsight with the mobility data of Barcelona and demonstrate diverse localized and semantically-rich interpretations.\",\n",
       " 'Sensory evaluation is used to assess the consumer acceptance of foods or other consumer products, so as to improve industrial processes and marketing strategies. The procedures currently involved are time-consuming because they require a statistical approach from measurements and feedback reports from a wide set of evaluators under a well-established measurement setup. In this paper, we propose to collect directly the signal of the perceived quality of the food from Event-related potentials (ERPs) that are the outcome of the processing of visual stimuli. This permits to narrow the number of evaluators since errors related to psychological factors are by-passed. We present the design of a wearable system for ERP measurement and we present preliminary results on the use of ERP to give a quantitative measure to the appearance of a food product. The system is developed to be wearable and our experiments demonstrate that is possible to use it to identify and classify the grade of acceptance of the food.',\n",
       " 'Drones are being used in many industries for a variety of applications, including inspecting bridges, surveying farm land, and delivering cargo. Automating these kinds of scenarios requires more than following a sequence of GPS waypoints; they require integrating on-device hardware with real-time analysis to provide feedback and control to the drone. Currently, implementing these kinds of advanced scenarios is a complex task, requiring skilled software engineers programming with drone APIs. We envision an alternate model to enable drone operators to orchestrate advanced behaviors using a card-based approach. We describe the design of our card-based programming model, position it relative to other visual programming metaphors, share results from our paper prototype user study, and discuss our learnings from its implementation. Results suggest that a wide range of scenarios can be implemented with moderate mental effort and learning, balanced by intuitiveness and engagement.',\n",
       " 'In online communities, recent studies have strongly improved our knowledge about the different types or profiles of contributors, from casual to very involved ones, through focused people. However they do so by using very complex methodologies (qualitative-quantitative mix, with a high workload to manually codify/characterize the edits), making their replication for the practitioners limited. These studies are on the English Wikipedia only. The objective of this paper is to highlight different profiles of contributors with clustering techniques. The originality is to show how using only the edits, and their distribution over time, allows to build these contributors profiles with a good accuracy and stability amongst languages. The methodology is validated with both Romanian and Danish wikis. The highlighted profiles are identifiable early in the history of involvement, suggesting that light monitoring of newcomers may be sufficient to adapt the interaction with them and increase the retention rate.',\n",
       " 'Despite gaining traction in North America, live streaming has not reached the popularity it has in China, where livestreaming has a tremendous impact on the social behaviors of users. To better understand this socio-technological phenomenon, we conducted a mixed methods study of live streaming practices in China. We present the results of an online survey of 527 live streaming users, focusing on their broadcasting or viewing practices and the experiences they find most engaging. We also interviewed 14 active users to explore their motivations and experiences. Our data revealed the different categories of content that was broadcasted and how varying aspects of this content engaged viewers. We also gained insight into the role reward systems and fan group-chat play in engaging users, while also finding evidence that both viewers and streamers desire deeper channels and mechanisms for interaction in addition to the commenting, gifting, and fan groups that are available today.',\n",
       " \"We want to understand the human capabilities to perceive amplitude similarities between a haptic and an audio signal. So, four psychophysical experiments were performed. Three of them measured the asynchrony JND (Just Noticeable Difference) at the signals' attack, release and decay, while the forth experiment measured the amplitude decrease on the middle of the signal. All the experiments used a combination of the constant stimulus and staircase methods to present two stimuli, while the participants' (N=12) task was to identify which of the two stimuli was synchronized. The audiotactile stimulus was defined using an stereo audio signal with an ADSR (Attack Decay Sustain Release) envelope. The partial results reveal JNDs for temporal asynchrony of: 54ms for attack, 265ms for decay and 57ms for release. Also the results reveal an amplitude decrease JND of 25\\\\%. Although for decay the results were to disperse, therefore we suspect that the participants were not able to the changes on the haptic signal.\",\n",
       " 'The emergence of social virtual reality (VR) experiences, such as Facebook Spaces, Oculus Rooms, and Oculus Venues, will generate increased interest from users who want to share real places (both personal and public) with their fellow users in VR. At the same time, advances in scanning and reconstruction technology are making the realistic capture of real places more and more feasible. These complementary pressures mean that the representation of real places in virtual reality will be an increasingly common use case for VR. Despite this, there has been very little research into how users perceive such replicated spaces. This paper reports the results from a series of three user studies investigating this topic. Taken together, these results show that getting the scale of the space correct is the most important factor for generating a \"feeling of reality\", that it is important to avoid incoherent behaviors (such as floating objects), and that lighting makes little difference to perceptual similarity.',\n",
       " 'The landscape of analytics is changing rapidly. Much of online user analytics, however, is based on collection of various user analytics numbers. Understanding these numbers, and then relating them to higher numerical analysis for the evaluation of key performance indicators (KPIs) can be quite challenging, especially with large volumes of data. There is a plethora of tools and software packages that one can employ. However, these tools and packages require a quantitative competence and analytical sophistication that average end users often do not possess. Additionally, they often do little to reduce the complexity of numerical data in a manner that allows ease of use in decision making and communication. Dealing with numbers poses cognitive challenges for individuals who often do cannot recall many numbers at a time. Here, we explore the concept of automatic analytics by demonstrating use case examples and discussion on the current state and future of automated insights.',\n",
       " 'Multimodal features play a key role in wearable sensor-based human activity recognition (HAR). Selecting the most salient features adaptively is a promising way to maximize the effectiveness of multimodal sensor data. In this regard, we propose a \"collect fully and select wisely\" principle as well as an interpretable parallel recurrent model with convolutional attentions to improve the recognition performance. We first collect modality features and the relations between each pair of features to generate activity frames, and then introduce an attention mechanism to select the most prominent regions from activity frames precisely. The selected frames not only maximize the utilization of valid features but also reduce the number of features to be computed effectively. We further analyze the accuracy and interpretability of the proposed model based on extensive experiments. The results show that our model achieves competitive performance on two benchmarked datasets and works well in real life scenarios.',\n",
       " 'A well-designed control-to-display (CD) gain function can improve pointing performance with an indirect pointing device such as a trackpad. However, the design of gain functions has been challenging and mostly based on trial and error. AutoGain is an unobtrusive method to obtain a gain function for an indirect pointing device in contexts where cursor trajectories can be tracked. It gradually improves pointing efficiency by using a novel submovement-level tracking+optimization technique. In a study, we show that AutoGain can produce gain functions with performance comparable to commercial designs in less than a half hour of active use. This is attributable to reductions in aiming error (undershooting/overshooting) for each submovement. Our second study shows that AutoGain can be used to obtain gain functions for emerging input devices (here, a Leap Motion controller) for which no good gain function may exist yet. Finally, we discuss deployment in a real interactive system.',\n",
       " 'Crowdsourcing systems accomplish large tasks with scale and speed by breaking work down into independent parts. However, many types of complex creative work, such as fiction writing, have remained out of reach for crowds because work is tightly interdependent: changing one part of a story may trigger changes to the overall plot and vice versa. Taking inspiration from how expert authors write, we propose a technique for achieving interdependent complex goals with crowds. With this technique, the crowd loops between reflection, to select a high-level goal, and revision, to decompose that goal into low-level, actionable tasks. We embody this approach in Mechanical Novel, a system that crowdsources short fiction stories on Amazon Mechanical Turk. In a field experiment, Mechanical Novel resulted in higher-quality stories than an iterative crowdsourcing workflow. Our findings suggest that orienting crowd work around high-level goals may enable workers to coordinate their effort to accomplish complex work.',\n",
       " \"Crowd workers are distributed and decentralized. While decentralization is designed to utilize independent judgment to promote high-quality results, it paradoxically undercuts behaviors and institutions that are critical to high-quality work. Reputation is one central example: crowdsourcing systems depend on reputation scores from decentralized workers and requesters, but these scores are notoriously inflated and uninformative. In this paper, we draw inspiration from historical worker guilds (e.g., in the silk trade) to design and implement crowd guilds: centralized groups of crowd workers who collectively certify each other's quality through double-blind peer assessment. A two-week field experiment compared crowd guilds to a traditional decentralized crowd work model. Crowd guilds produced reputation signals more strongly correlated with ground-truth worker quality than signals available on current crowd working platforms, and more accurate than in the traditional model.\",\n",
       " 'Leveraging human grasping skills to teach a robot to perform a manipulation task is appealing, but there are several limitations to this approach: time-inefficient data capture procedures, limited generalization of the data to other grasps and objects, and inability to use that data to learn more about how humans perform and evaluate grasps. This paper presents a data capture protocol that partially addresses these deficiencies by asking participants to specify ranges over which a grasp is valid. The protocol is verified both qualitatively through online survey questions (where 95.38% of within-range grasps are identified correctly with the nearest extreme grasp) and quantitatively by showing that there is small variation in grasps ranges from different participants as measured by joint angles, contact points, and position. We demonstrate that these grasp ranges are valid through testing on a physical robot (93.75% of grasps interpolated from grasp ranges are successful).',\n",
       " \"The Experience Sampling Method (ESM) introduces in-situ sampling of human behaviour, and provides researchers and behavioural therapists with ecologically valid and timely assessments of a person's psychological state. This, in turn, opens up new opportunities for understanding behaviour at a scale and granularity that was not possible just a few years ago. The practical applications are many, such as the delivery of personalised and agile behaviour interventions. Mobile computing devices represent a revolutionary platform for improving ESM. They are an inseparable part of our daily lives, context-aware, and can interact with people at suitable moments. Furthermore, these devices are equipped with sensors, and can thus take part of the reporting burden off the participant, and collect data automatically. The goal of this survey is to discuss recent advancements in using mobile technologies for ESM (mESM), and present our vision of the future of mobile experience sampling.\",\n",
       " 'In this article, we study the Cyber-Human Interaction (CHI) based approach that the \"Human\" part sets a list of location-based objectives and makes the pathway decision whereas the \"Cyber\" part provides the pathway suggestion, infer heuristics from the environment along the pathway and incrementally resolve the location-based objectives with new heuristics for indoor localization. For this study, we implement a CHI-based system on mobile phone. The CHI-based system offers the pathway suggestion and the solution of the location-based objectives based on its trajectory management. Without any priori knowledge on the area of interest and any aid from other equipments, a laborer can achieve his location-based objectives by walking through the area of interest and simultaneously online interacting with the CHI-based system installed in his phone. In evaluation, we conduct the experiments and show the advantage CHI in reducing the time cost and the expense cost for the laborer.',\n",
       " 'In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory. These techniques often require researchers to engineer abstract \"features\" that encode chemical concepts into a mathematical form compatible with the input to machine-learning models. However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features. We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem. The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots. The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection. It is built to be scalable, and integrate with existing infrastructure.',\n",
       " \"Process Mining is a famous technique which is frequently applied to Software Development Processes, while being neglected in Human-Computer Interaction (HCI) recommendation applications. Organizations usually train employees to interact with required IT systems. Often, employees, or users in general, develop their own strategies for solving repetitive tasks and processes. However, organizations find it hard to detect whether employees interact efficiently with IT systems or not. Hence, we have developed a method which detects inefficient behavior assuming that at least one optimal HCI strategy is known. This method provides recommendations to gradually adapt users' behavior towards the optimal way of interaction considering satisfaction of users. Based on users' behavior logs tracked by a Java application suitable for multi-application and multi-instance environments, we demonstrate the applicability for a specific task in a common Windows environment utilizing realistic simulated behaviors of users.\",\n",
       " 'With the popularization of cell phones, laptops, and tablets, Liquid Crystal Displays (LCDs) have become one of the main types of User Interface (UI) in the modern world. While LCDs are widely used for retrieving text information, the impact of text formatting on the legibility is often overlooked. With the goal of improving recognition efficiency (RE) on LCDs, this paper studies the impact of font-background colors on RE of texts being presented on LCD. For this purpose, difference between font/background color combinations, Primary Color Difference (PCD), is introduced that brings efficient RE assessment under wider spectrum. Accordingly, a testing platform is designed in C#. NET that captures participants response time to different font-background color combination stimuli. Based on the results, black background and green font color outperform other tested colors especially when the PCD is maximized. In correspond to results, Implications for using research outcome in prototype LCDs are suggested.',\n",
       " \"Scientists increasingly rely on simulation runs of complex models in lieu of cost-prohibitive or infeasible experimentation. The data output of many controlled simulation runs, the ensemble, is used to verify correctness and quantify uncertainty. However, due to their size and complexity, ensembles are difficult to visually analyze because the working set often exceeds strict memory limitations. We present a navigable ensemble analysis tool, NEA, for interactive exploration of ensembles. NEA's pre-processing component takes advantage of the data similarity characteristics of ensembles to represent the data in a new, spatially-efficient data structure which does not require fully reconstructing the original data at visualization time. This data structure allows a fine degree of control in working set management, which enables interactive ensemble exploration while fitting within memory limitations. Scientists can also gain new insights from the data-similarity analysis in the pre-processing component.\",\n",
       " 'Camera arrays (CamArrays) are widely used in commercial filming projects for achieving special visual effects such as bullet time effect, but are very expensive to set up. We propose CamSwarm, a low-cost and lightweight alternative to professional CamArrays for consumer applications. It allows the construction of a collaborative photography platform from multiple mobile devices anywhere and anytime, enabling new capturing and editing experiences that a single camera cannot provide. Our system allows easy team formation; uses real-time visualization and feedback to guide camera positioning; provides a mechanism for synchronized capturing; and finally allows the user to efficiently browse and edit the captured imagery. Our user study suggests that CamSwarm is easy to use; the provided real-time guidance is helpful; and the full system achieves high quality results promising for non-professional use.\\n  A demo video is provided at https://www.youtube.com/watch?v=LgkHcvcyTTM.',\n",
       " 'One of the fundamental aspects of ubiquitous computing is the instrumentation of the real world by smart devices. This instrumentation constitutes an opportunity to rethink the interactions between human beings and their environment on the one hand, and between the components of this environment on the other. In this paper we discuss what this understanding of ubiquitous computing can bring to geographic science and particularly to GIS technology. Our main idea is the instrumentation of the geographic environment through the instrumentation of geographic objects composing it. And then investigate how this instrumentation can meet the current limitations of GIS technology, and offers a new stage of rapprochement between the earth and its abstraction. As result, the current research work proposes a new concept we named Smart Geographic Object SGO. The latter is a convergence point between the smart objects and geographic objects, two concepts appertaining respectively to t',\n",
       " 'Research on science fiction (sci-fi) in scientific publications has indicated the usage of sci-fi stories, movies or shows to inspire novel Human-Computer Interaction (HCI) research. Yet no studies have analysed sci-fi in a top-ranked computer science conference at present. For that reason, we examine the CHI main track for the presence and nature of sci-fi referrals in relationship to HCI research. We search for six sci-fi terms in a dataset of 5812 CHI main proceedings and code the context of 175 sci-fi referrals in 83 papers indexed in the CHI main track. In our results, we categorize these papers into five contemporary HCI research themes wherein sci-fi and HCI interconnect: 1) Theoretical Design Research; 2) New Interactions; 3) Human-Body Modification or Extension; 4) Human-Robot Interaction and Artificial Intelligence; and 5) Visions of Computing and HCI. In conclusion, we discuss results and implications located in the promising arena of sci-fi and HCI research.',\n",
       " 'It is often desirable to analyse trajectory data in local coordinates relative to a reference location. Similarly, temporal data also needs to be transformed to be relative to an event. Together, temporal and spatial contextualisation permits comparative analysis of similar trajectories taken across multiple reference locations. To the GIS professional, the procedures to establish a reference frame at a location and reproject the data into local coordinates are well known, albeit tedious. However, GIS tools are now often used by subject matter experts who may not have the deep knowledge of coordinate frames and projections required to use these techniques effectively.\\n  We introduce a novel method for representing spatio-temporal reference frames using ordinary geographic objects available in GIS tools. We argue that our method both reduces the number of manual steps required to reproject data to a local reference frame, in addition to reducing the number of concepts a novice user would need to learn.',\n",
       " 'Parallel coordinates plotting is one of the most popular methods for multivariate visualization. However, when applied to larger data sets, there tends to be a \"black screen problem,\" with the screen becoming so cluttered and full that patterns are difficult or impossible to discern. Xie and Matloff (2014) proposed remedying this problem by plotting only the most frequently-appearing patterns, with frequency defined in terms of nonparametrically estimated multivariate density. This approach displays \"typical\" patterns, which may reveal important insights for the data. However, this remedy does not cover variables that are discrete or categorical. An alternate method, still frequency-based, is presented here for such cases. We discretize all continuous variables, retaining the discrete/categorical ones, and plot the patterns having the highest counts in the dataset. In addition, we propose some novel approaches to handling missing values in parallel coordinates settings.',\n",
       " 'We introduce Code Park, a novel tool for visualizing codebases in a 3D game-like environment. Code Park aims to improve a programmer\\'s understanding of an existing codebase in a manner that is both engaging and intuitive, appealing to novice users such as students. It achieves these goals by laying out the codebase in a 3D park-like environment. Each class in the codebase is represented as a 3D room-like structure. Constituent parts of the class (variable, member functions, etc.) are laid out on the walls, resembling a syntax-aware \"wallpaper\". The users can interact with the codebase using an overview, and a first-person viewer mode. We conducted two user studies to evaluate Code Park\\'s usability and suitability for organizing an existing project. Our results indicate that Code Park is easy to get familiar with and significantly helps in code understanding compared to a traditional IDE. Further, the users unanimously believed that Code Park was a fun tool to work with.',\n",
       " \"Both dialogue systems and chatbots aim at putting into action communication between humans and computers. However, instead of focusing on sophisticated techniques to perform natural language understanding, as the former usually do, chatbots seek to mimic conversation. Since Eliza, the first chatbot ever, developed in 1966, there were many interesting ideas explored by the chatbots' community. Actually, more than just ideas, some chatbots' developers also provide free resources, including tools and large-scale corpora. It is our opinion that this know-how and materials should not be neglected, as they might be put to use in the human-computer communication field (and some authors already do it). Thus, in this paper we present a historical overview of the chatbots' developments, we review what we consider to be the main contributions of this community, and we point to some possible ways of coupling these with current work in the human-computer communication research line.\",\n",
       " 'Recent years have seen an explosion in the availability of Voice User Interfaces. However, user surveys suggest that there are issues with respect to usability, and it has been hypothesised that contemporary voice-enabled systems are missing crucial behaviours relating to user engagement and vocal interactivity. However, it is well established that such ostensive behaviours are ubiquitous in the animal kingdom, and that vocalisation provides a means through which interaction may be coordinated and managed between individuals and within groups. Hence, this paper reports results from a study aimed at identifying generic mechanisms that might underpin coordinated collective vocal behaviour with a particular focus on closed-loop negative-feedback control as a powerful regulatory process. A computer-based real-time simulation of vocal interactivity is described which has provided a number of insights, including the enumeration of a number of key control variables that may be worthy of further investigation.',\n",
       " \"The delivery of mental health interventions via ubiquitous devices has shown much promise. A conversational chatbot is a promising oracle for delivering appropriate just-in-time interventions. However, designing emotionally-aware agents, specially in this context, is under-explored. Furthermore, the feasibility of automating the delivery of just-in-time mHealth interventions via such an agent has not been fully studied. In this paper, we present the design and evaluation of EMMA (EMotion-Aware mHealth Agent) through a two-week long human-subject experiment with N=39 participants. EMMA provides emotionally appropriate micro-activities in an empathetic manner. We show that the system can be extended to detect a user's mood purely from smartphone sensor data. Our results show that our personalized machine learning model was perceived as likable via self-reports of emotion from users. Finally, we provide a set of guidelines for the design of emotion-aware bots for mHealth.\",\n",
       " 'The multimodal web elements such as text and images are associated with inherent memory costs to store and transfer over the Internet. With the limited network connectivity in developing countries, webpage rendering gets delayed in the presence of high-memory demanding elements such as images (relative to text). To overcome this limitation, we propose a Canonical Correlation Analysis (CCA) based computational approach to replace high-cost modality with an equivalent low-cost modality. Our model learns a common subspace for low-cost and high-cost modalities that maximizes the correlation between their visual features. The obtained common subspace is used for determining the low-cost (text) element of a given high-cost (image) element for the replacement. We analyze the cost-saving performance of the proposed approach through an eye-tracking experiment conducted on real-world webpages. Our approach reduces the memory-cost by at least 83.35% by replacing images with text.',\n",
       " \"This paper presents a novel approach to simulate human wayfinding behaviour incorporating visual cognition into a software agent for a computer aided evaluation of wayfinding systems in large infrastructures. The proposed approach follows the Sense-Plan-Act paradigm comprised of a model for visual attention, navigation behaviour and pedestrian movement. Stochastic features of perception are incorporated to enhance generality and diversity of the developed wayfinding simulation to reflect a variety of behaviours. The validity of the proposed approach was evaluated based on empirical data collected through wayfinding experiments with 20 participants in an immersive virtual reality environment using a life-sized 3D replica of Vienna's new central railway station. The results show that the developed cognitive agent-based simulation provides a further contribution to the simulation of human wayfinding and subsequently a further step to an effective evaluation tool for the planning of wayfinding and signage.\",\n",
       " 'Given the choice, users produce passwords reflecting common strategies and patterns that ease recall but offer uncertain and often weak security. System-assigned passwords provide measurable security but suffer from poor memorability. To address this usability-security tension, we argue that systems should assign random passwords but also help with memorization and recall. We investigate the feasibility of this approach with CuedR, a novel cued-recognition authentication scheme that provides users with multiple cues (visual, verbal, and spatial) and lets them choose the cues that best fit their learning process for later recognition of system-assigned keywords. In our lab study, all 37 of our participants could log in within three attempts one week after registration (mean login time: 38.0 seconds). A pilot study on using multiple CuedR passwords also showed 100% recall within three attempts. Based on our results, we suggest appropriate applications for CuedR, such as financial and e-commerce accounts.',\n",
       " 'Promoting exercise and promoting fun in sport and activity is a common goal of schools. However, children and adolescents do not exercise enough, which can favor a number of chronic illnesses. Exercise and sports often require coordination of visual perception and reaction, which is an additional barrier for visually impaired (blind and partially sighted) people. Due to their highly motivating appeal, games promoting physical activity (exertion games) have become increasingly popular. Although accessible exertion games have been developed, they do not consider the different abilities of players. Especially in team sports player roles that consider individual abilities can foster inclusion. To personalize roles and assign certain abilities to players, wearable technology can play an important role. In this position paper we present ideas how digital objects can be used to design exertion games for visually impaired students and we reflect how wearable technology can be used for personalized player roles.',\n",
       " \"Is it possible to predict the affect of a user just by observing her behavioral interaction through a video? How can we, for instance, predict a user's arousal in games by merely looking at the screen during play? In this paper we address these questions by employing three dissimilar deep convolutional neural network architectures in our attempt to learn the underlying mapping between video streams of gameplay and the player's arousal. We test the algorithms in an annotated dataset of 50 gameplay videos of a survival shooter game and evaluate the deep learned models' capacity to classify high vs low arousal levels. Our key findings with the demanding leave-one-video-out validation method reveal accuracies of over 78% on average and 98% at best. While this study focuses on games and player experience as a test domain, the findings and methodology are directly relevant to any affective computing area, introducing a general and user-agnostic approach for modeling affect.\",\n",
       " 'In human computer interaction (HCI), it is common to evaluate the value of HCI designs, techniques, devices, and systems in terms of their benefit to users. It is less common to discuss the benefit of HCI to computers. Every HCI task allows a computer to receive some data from the user. In many situations, the data received by the computer embodies human knowledge and intelligence in handling complex problems, and/or some critical information without which the computer cannot proceed. In this paper, we present an information-theoretic framework for quantifying the knowledge received by the computer from its users via HCI. We apply information-theoretic measures to some common HCI tasks as well as HCI tasks in complex data intelligence processes. We formalize the methods for estimating such quantities analytically and measuring them empirically. Using theoretical reasoning, we can confirm the significant but often undervalued role of HCI in data intelligence workflows.',\n",
       " \"Tools for quadrotor trajectory design have enabled single videographers to create complex aerial video shots that previously required dedicated hardware and several operators. We build on this prior work by studying film-maker's working practices which informed a system design that brings expert workflows closer to end-users. For this purpose, we propose WYFIWYG, a new quadrotor camera tool which (i) allows to design a video solely via specifying its frames, (ii) encourages the exploration of the scene prior to filming and (iii) allows to continuously frame a camera target according to compositional intentions. Furthermore, we propose extensions to an existing algorithm, generating more intuitive angular camera motions and producing spatially and temporally smooth trajectories. Finally, we conduct a user study where we evaluate how end-users work with current videography tools. We conclude by summarizing the findings of work as implications for the design of UIs and algorithms of quadrotor camera tools.\",\n",
       " 'Exploring data requires a fast feedback loop from the analyst to the system, with a latency below about 10 seconds because of human cognitive limitations. When data becomes large or analysis becomes complex, sequential computations can no longer be completed in a few seconds and data exploration is severely hampered. This article describes a novel computation paradigm called Progressive Computation for Data Analysis or more concisely Progressive Analytics, that brings at the programming language level a low-latency guarantee by performing computations in a progressive fashion. Moving this progressive computation at the language level relieves the programmer of exploratory data analysis systems from implementing the whole analytics pipeline in a progressive way from scratch, streamlining the implementation of scalable exploratory data analysis systems. This article describes the new paradigm through a prototype implementation called ProgressiVis, and explains the requirements it implies through examples.',\n",
       " 'In the complex manufacturing sector a considerable amount of resources are focused on developing new skills and training workers. In that context, increasing the effectiveness of those processes and reducing the investment required is an outstanding issue. In this paper we present an experiment that shows how modern Human Computer Interaction (HCI) metaphors such as collaborative mixed-reality can be used to transmit procedural knowledge and could eventually replace other forms of face-to-face training. We implement a real-time Immersive Augmented Reality (IAR) setup with see-through cameras that allows for collaborative interactions that can simulate conventional forms of training. The obtained results indicate that people who took the IAR training achieved the same performance than people in the conventional face-to-face training condition. These results, their implications for future training and the use of HCI paradigms in this context are discussed in this paper.',\n",
       " \"Thispaper presents the design and realization of a context-aware wireless health monitoring system for recording the heartbeat (HR) and respiration (RR) rate based on an indirect measurement approach. The system consists of a contact-less medical sensor as well as a communication infrastructure for handling the transmission and reception of the measured results. The contact-less sensor includes a highly sensitive tri-axial accelerometer, an accurate temperature and air pressure sensor that enable one to inspect patients' health condition by continuously monitoring of two critical signs related to the cardiorespiratory system. The developed system can also be utilized in performing a number of long-term inspection on the heart and lungs while measuring the HR and RR values in addition to calculating the HR and RR ratio, which is denoted by HRR. The obtained results show the potential of the developed system for versatile monitoring applications applied to telemedicine\",\n",
       " \"The head-up display (HUD) is an emerging device which can project information on a transparent screen. The HUD has been used in airplanes and vehicles, and it is usually placed in front of the operator's view. In the case of the vehicle, the driver can see not only various information on the HUD but also the backgrounds (driving environment) through the HUD. However, the projected information on the HUD may interfere with the colors in the background because the HUD is transparent. For example, a red message on the HUD will be less noticeable when there is an overlap between it and the red brake light from the front vehicle. As the first step to solve this issue, how to evaluate the mutual interference between the information on the HUD and backgrounds is important. Therefore, this paper proposes a method to evaluate the mutual interference based on saliency. It can be evaluated by comparing the HUD part cut from a saliency map of a measured image with the HUD image.\",\n",
       " 'A lack of awareness surrounding secure online behaviour can lead to end-users, and their personal details becoming vulnerable to compromise. This paper describes an ongoing research project in the field of usable security, examining the relationship between end-user-security behaviour, and the use of affective feedback to educate end-users. Part of the aforementioned research project considers the link between categorical information users reveal about themselves online, and the information users believe, or report that they have revealed online. The experimental results confirm a disparity between information revealed, and what users think they have revealed, highlighting a deficit in security awareness. Results gained in relation to the affective feedback delivered are mixed, indicating limited short-term impact. Future work seeks to perform a long-term study, with the view that positive behavioural changes may be reflected in the results as end-users become more knowledgeable about security awareness.',\n",
       " \"Online communities have been able to develop large, open-source software (OSS) projects like Linux and Firefox throughout the successful collaborations carried out by their members over the Internet. However, online communities also involve creative arts domains such as animation, video games, and music. Despite their growing popularity, the factors that lead to successful collaborations in these communities are not entirely understood. In this paper, we present a study on creative collaboration in a music community where authors write songs together by 'overdubbing,' that is, by mixing a new track with an existing audio recording. We analyzed the relationship between song- and author-related measures and the likelihood of a song being overdubbed. We found that recent songs, as well as songs with many reactions, are more likely to be overdubbed; authors with a high status in the community and a recognizable identity write songs that the community tends to build upon.\",\n",
       " \"Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers' hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.\",\n",
       " 'Cognitive processes involved in both allocation of attention during decision making as well as surprise when making mistakes trigger release of the neurotransmitter norepinephrine, which has been shown to be correlated with an increase in pupil dilation, in turn reflecting raised levels of arousal. Extending earlier experiments based on the Attention Network Test (ANT), separating the neural components of alertness and spatial re-orientation from the attention involved in more demanding conflict resolution tasks, we demonstrate that these signatures of attention are so robust that they may be retrieved even when applying low cost eye tracking in an everyday mobile computing context. Furthermore we find that the reaction of surprise elicited when committing mistakes in a decision task, which in the neuroimaging EEG literature have been referred to as a negativity feedback error correction signal, may likewise be retrieved solely based on an increase in pupil dilation.',\n",
       " 'Recent years have seen significant market penetration for voice-based personal assistants such as Apple\\'s Siri. However, despite this success, user take-up is frustratingly low. This position paper argues that there is a habitability gap caused by the inevitable mismatch between the capabilities and expectations of human users and the features and benefits provided by contemporary technology. Suggestions are made as to how such problems might be mitigated, but a more worrisome question emerges: \"is spoken language all-or-nothing\"? The answer, based on contemporary views on the special nature of (spoken) language, is that there may indeed be a fundamental limit to the interaction that can take place between mismatched interlocutors (such as humans and machines). However, it is concluded that interactions between native and non-native speakers, or between adults and children, or even between humans and dogs, might provide critical inspiration for the design of future speech-based human-machine interaction.',\n",
       " 'Modern interactive visualizations are akin to distributed systems, where user interactions, background data processing, remote requests, and streaming data read and modify the interface at the same time. This concurrency is crucial to provide an interactive user experience---forbidding it can cripple responsiveness. However, it is notoriously challenging to program distributed systems, and concurrency can easily lead to ambiguous or confusing interface behaviors. In this paper, we present DIEL, a declarative programming model to help developers reason about and reconcile concurrency-related issues. Using DIEL, developers no longer need to procedurally describe how the interface should update based on different input events, but rather declaratively specify what the state of the interface should be as queries over event history. We show that resolving conflicts from concurrent processes in real-world interactive visualizations can be done in a few lines of DIEL code.',\n",
       " \"The growing popularity of chatbots has brought new needs for HCI since it has changed the patterns of human interactions with computers. The conversational aspect of the interaction increases the necessity for chatbots to present social behaviors that are habitual in human-human conversations. In this survey, we argue that chatbots should be enriched with social characteristics that are coherent with users' expectations, ultimately avoiding frustration and dissatisfaction. We bring together the literature on disembodied, text-based chatbots to derive a conceptual model of social characteristics for chatbots. We analyzed 58 papers from various domains to understand how social characteristics can benefit the interactions and identify the challenges and strategies to designing them. Additionally, we discussed how characteristics may influence one another. Our results provide relevant opportunities to both researchers and designers to advance human-chatbot interactions.\",\n",
       " \"Lifestyle and environment interacting with our biological machine are primarily responsible for shaping our health and wellbeing. Continuous, multi-modal, and quantitative approaches to understanding and controlling these factors will allow each person to better reach their desired quality of life. A navigational paradigm can help users towards a specified health goal by using constantly captured measurements to feed estimations of how a user's health is continuously changing in order to provide actionable guidance. As various actions are taken by the user, measurements of the resulting effects loop back into the estimation and the next step of guidance. This perpetual cycle of measuring, estimating, guiding, and acting articulates a Personal Health Navigation information and actuation framework. Personal Health Navigation focuses on fulfilling a user's health goals by ensuring minimal deviation from healthy states, rather than treating disease or symptoms after derailment from proper biological function.\",\n",
       " 'The P300 speller is a well known Brain-Computer Interface paradigm that has been used for over two decades. A new P300 speller paradigm (XP300) is proposed. It includes several characteristics: (i) the items are not intensified by using rows and columns, (ii) the order of the visual stimuli is pseudo-random, (iii) a visual feedback is added on each item to increase the stimulus meaning, which is the main novelty. XP300 has been tested on ten healthy subjects on copy spelling mode, with only eight sensors. It has been compared with the classical P300 paradigm (CP300). With five repetitions, the average recognition rate across subjects is 85.25% for XP300 and 77.25% for CP300. Single-trial detection is significantly higher with XP300 by comparing the AUC (Area Under Curve) of the ROC (Receiver Operating Characteristic) curve. The mean AUC is 0.86 for XP300, 0.80 for CP300. More importantly, XP300 has also been judged as more convenient and user-friendly than CP300, hence being able to allow longer sessions.',\n",
       " 'The quality of high-end videoconferencing systems has improved significantly over the last few years enabling a class of applications known as \"telepresence\" wherein the users engaged in a communication session experience a feeling of mutual presence in a shared virtual space. Telepresence systems have reached a maturity level that seriously challenges the old familiar truism that a face-to-face meeting is always better than a technology-mediated alternative. To explore the state of the art in telepresence technology and outline future opportunities, this paper proposes an optimality condition, expressed as a \"Turing Test,\" whereby the subjective experience of using a telepresence system is compared to the corresponding face-to-face situation. The requirements and challenges of designing a system passing such a Turing Test for telepresence are analyzed with respect to the limits of human perception, and the feasibility of achieving this goal with currently available or near future technology is discussed.',\n",
       " 'Joint narratives are often used in the context of reconciliation interventions for people in social conflict situations, which arise, for example, due to ethnic or religious differences. The interventions aim to encourage a change in attitudes of the participants towards each other. Typically, a human mediator is fundamental for achieving a successful intervention. In this work, we present an automated approach to support remote interactions between pairs of participants as they contribute to a shared story in their own language. A key component is an automated cognitive tutor that guides the participants through a controlled escalation/de-escalation process during the development of a joint narrative. We performed a controlled study comparing a trained human mediator to the automated mediator. The results demonstrate that an automated mediator, although simple at this stage, effectively supports interactions and helps to achieve positive outcomes comparable to those attained by the trained human mediator.',\n",
       " 'The need for interpretable and accountable intelligent system gets sensible as artificial intelligence plays more role in human life. Explainable artificial intelligence systems can be a solution by self-explaining the reasoning behind the decisions and predictions of the intelligent system. Researchers from different disciplines work together to define, design and evaluate interpretable intelligent systems for the user. Our work supports the different evaluation goals in interpretable machine learning research by a thorough review of evaluation methodologies used in machine-explanation research across the fields of human-computer interaction, visual analytics, and machine learning. We present a 2D categorization of interpretable machine learning evaluation methods and show a mapping between user groups and evaluation measures. Further, we address the essential factors and steps for a right evaluation plan by proposing a nested model for design and evaluation of explainable artificial intelligence systems.',\n",
       " 'Brain-Computer Interface (BCI) uses brain signals in order to provide a new method for communication between human and outside world. Feature extraction, selection and classification are among the main matters of concerns in signal processing stage of BCI. In this article, we present our findings about the most effective features and classifiers in some brain tasks. Six different groups of classical features and twelve classifiers have been examined in nine datasets of brain signal. The results indicate that energy of brain signals in {\\\\alpha} and \\\\b{eta} frequency bands, together with some statistical parameters are more effective, comparing to the other types of extracted features. In addition, Bayesian classifier with Gaussian distribution assumption and also Support Vector Machine (SVM) show to classify different BCI datasets more accurately than the other classifiers. We believe that the results can give an insight about a strategy for blind classification of brain signals in brain-computer interface.',\n",
       " 'Using Low Cost Portable Eye Tracking for Biometric Identification Or Verification: Eye tracking technologies have in recent years become available outside of specialised labs, and are starting to become integrated in tablets and virtual reality headsets. This offers new opportunities for use in common office- and home environments, such as for biometric recognition (identification or verification), alone or in combination with other technologies. This paper exposes two fundamentally different approaches that have been suggested, based on spatial and temporal signatures respectively. While deploying different stimulation paradigms for recording, it also proposes an alternative way to analyze spatial domain signatures using Fourier transformation. Empirical data recorded from two subjects over two weeks, three months apart, are found to support previous results. Further, variations and stability of some of the proposed signatures are analyzed over the extended timeframe and under slightly varying conditions.',\n",
       " 'Gait adaptation is an important part of gait analysis and its neuronal origin and dynamics has been studied extensively. In neurorehabilitation, it is important as it perturbs neuronal dynamics and allows patients to restore some of their motor function. Exoskeletons and robotics of the lower limbs are increasingly used to facilitate rehabilitation as well as supporting daily function. Their efficiency and safety depends on how well can sense the human intention to move and adapt the gait accordingly. This paper presents a gait adaptation scheme in natural settings. It allows monitoring of subjects in more realistic environment without the requirement of specialized equipment such as treadmill and foot pressure sensors. We extract gait characteristics based on a single RBG camera whereas wireless EEG signals are monitored simultaneously. We demonstrate that the method can not only successfully detect adaptation steps but also detect efficiently whether the subject adjust their pace to higher or lower speed.',\n",
       " 'Smartphone usage while driving is unanimously considered to be a really dangerous habit due to strong correlation with road accidents. In this paper, the problem of detecting whether the driver is using the phone during a trip is addressed. To do this, high-frequency data from the triaxial inertial measurement unit (IMU) integrated in almost all modern phone is processed without relying on external inputs so as to provide a self-contained approach. By resorting to a frequency-domain analysis, it is possible to extract from the raw signals the useful information needed to detect when the driver is using the phone, without being affected by the effects that vehicle motion has on the same signals. The selected features are used to train a Support Vector Machine (SVM) algorithm. The performance of the proposed approach are analyzed and tested on experimental data collected during mixed naturalistic driving scenarios, proving the effectiveness of the proposed approach.',\n",
       " 'Human-human joint-action in short-cycle repetitive handover tasks was investigated for a bottle handover task using a three-fold approach: work-methods field studies in multiple supermarkets, simulation analysis using an ergonomics software package and by conducting an in-house lab experiment on human-human collaboration by re-creating the environment and conditions of a supermarket. Evaluation included both objective and subjective measures. Subjective evaluation was done taking a psychological perspective and showcases among other things, the differences in the way a common joint-action is being perceived by individual team partners depending upon their role (giver or receiver). The proposed approach can provide a systematic method to analyze similar tasks. Combining the results of all the three analyses, this research gives insight into the science of joint-action for short-cycle repetitive tasks and its implications for human-robot collaborative system design.',\n",
       " 'Consider unsupervised clustering of objects drawn from a discrete set, through the use of human intelligence available in crowdsourcing platforms. This paper defines and studies the problem of universal clustering using responses of crowd workers, without knowledge of worker reliability or task difficulty. We model stochastic worker response distributions by incorporating traits of memory for similar objects and traits of distance among differing objects. We are particularly interested in two limiting worker types---temporary workers who retain no memory of responses and long-term workers with memory. We first define clustering algorithms for these limiting cases and then integrate them into an algorithm for the unified worker model. We prove asymptotic consistency of the algorithms and establish sufficient conditions on the sample complexity of the algorithm. Converse arguments establish necessary conditions on sample complexity, proving that the defined algorithms are asymptotically order-optimal in cost.',\n",
       " 'Diabetes is an epidemic disease of the 21st century and is growing globally. Although, final diabetes treatments and cure are still on research phase, related complications of diabetes endanger life of diabetic patients. Diabetic coma which happens with extreme high or low blood glucose is one of the risk factor for diabetic patients and if it remains unattended will lead to patient death or permanent brain damage. To reduce the risk of such deaths or damages, a novel algorithm for wearable devices application, especially for smart watches are proposed. Such application can inform the patients relatives or emergency centers, if the person falls in coma or irresponsive condition based on readouts from smart watches sensors including mobility, heart rate and skin moisture. However; such an application is not a final solution to detect all types of coma, but it potentially could save lives of many patients, if widely used among the diabetic patients around the world.',\n",
       " \"In this paper we describe the ways participants of the Scratch online community, primarily young people, engage in remixing of each others' shared animations, games, and interactive projects. In particular, we try to answer the following questions: How do users respond to remixing in a social media environment where remixing is explicitly permitted? What qualities of originators and their projects correspond to a higher likelihood of plagiarism accusations? Is there a connection between plagiarism complaints and similarities between a remix and the work it is based on? Our findings indicate that users have a very wide range of reactions to remixing and that as many users react positively as accuse remixers of plagiarism. We test several hypotheses that might explain the high number of plagiarism accusations related to original project complexity, cumulative remixing, originators' integration into remixing practice, and remixee-remixer project similarity, and find support for the first and last explanations.\",\n",
       " 'Hiring robots for the workplaces is a challenging task as robots have to cater to customer demands, follow organizational protocols and behave with social etiquette. In this study, we propose to have a humanoid social robot, Nadine, as a customer service agent in an open social work environment. The objective of this study is to analyze the effects of humanoid robots on customers at work environment, and see if it can handle social scenarios. We propose to evaluate these objectives through two modes, namely, survey questionnaire and customer feedback. We also propose a novel approach to analyze customer feedback data (text) using sentic computing methods. Specifically, we employ aspect extraction and sentiment analysis to analyze the data. From our framework, we detect sentiment associated to the aspects that mainly concerned the customers during their interaction. This allows us to understand customers expectations and current limitations of robots as employees.',\n",
       " 'Smartphones are equipped with sensors such as accelerometers, gyroscope, and GPS in one cost-effective device with an acceptable level of accuracy. There have been some research studies carried out in terms of using smartphones to measure the pavement roughness. However, a little attention has been paid to investigate the validity of the measured pavement roughness by smartphones via other subjective methods such as the user opinion. This paper aims at calculating the pavement roughness data with a smartphone using its embedded sensors and investigating its correlation with a user opinion about the ride quality. In addition, the applicability of using smartphones to assess the pavement surface distresses is examined. Furthermore, to validate the smartphone sensor outputs objectively, the Road Surface Profiler is applied. Finally, a good roughness model is developed which demonstrates an acceptable level of correlation between the pavement roughness measured by smartphones and the ride quality rated by users.',\n",
       " 'Websites and applications that match and connect individuals for romantic purposes are commonly used in the Western world. However, there have not been many previous investigations focusing on cultural factors that affect the adoption of similar technologies in religiously conservative non-Western cultures. In this study, we examine the socio-technical and cultural factors that influence the perceptions and use of matchmaking technologies in Saudi Arabia. We report the methods and findings of interviews with 18 Saudi nationals (nine males and nine females) with diverse demographics and backgrounds. We provide qualitatively generated insights into the major themes reported by our participants related to the common approaches to matchmaking, the current role of technology, and concerns regarding matchmaking technologies in this cultural con-text. We relate these themes to specific implications for designing marital matchmaking technologies in Saudi Arabia and we outline opportunities for future investigations.',\n",
       " 'Latest research revealed a considerable lack of reliability within user feedback and discussed striking impacts for the assessment of adaptive web systems and content personalisation approaches, e.g. ranking errors, systematic biases to accuracy metrics as well as its natural offset (the magic barrier). In order to perform holistic assessments and to improve web systems, a variety of strategies have been proposed to deal with this so-called human uncertainty. In this contribution we discuss the most relevant strategies to handle uncertain feedback and demonstrate that these approaches are more or less ineffective to fulfil their objectives. In doing so, we consider human uncertainty within a purely probabilistic framework and utilise hypothesis testing as well as a generalisation of the magic barrier to compare the effects of recently proposed algorithms. On this basis we recommend a novel strategy of acceptance which turns away from mere filtering and discuss potential benefits for the community of the WWW.',\n",
       " 'In this paper we argue, drawing from the perspectives of cybersecurity and social psychology, that Internet-based manipulation of an individual or group reality using ambient tactical deception is possible using only software and changing words in a web browser. We call this attack Ambient Tactical Deception (ATD). Ambient, in artificial intelligence, describes software that is \"unobtrusive,\" and completely integrated into a user\\'s life. Tactical deception is an information warfare term for the use of deception on an opposing force. We suggest that an ATD attack could change the sentiment of text in a web browser. This could alter the victim\\'s perception of reality by providing disinformation. Within the limit of online communication, even a pause in replying to a text can affect how people perceive each other. The outcomes of an ATD attack could include alienation, upsetting a victim, and influencing their feelings about an election, a spouse, or a corporation.',\n",
       " 'In recent years, deep learning poses a deep technical revolution in almost every field and attracts great attentions from industry and academia. Especially, the convolutional neural network (CNN), one representative model of deep learning, achieves great successes in computer vision and natural language processing. However, simply or blindly applying CNN to the other fields results in lower training effects or makes it quite difficult to adjust the model parameters. In this poster, we propose a general methodology named V-CNN by introducing data visualizing for CNN. V-CNN introduces a data visualization model prior to CNN modeling to make sure the data after processing is fit for the features of images as well as CNN modeling. We apply V-CNN to the network intrusion detection problem based on a famous practical dataset: AWID. Simulation results confirm V-CNN significantly outperforms other studies and the recall rate of each invasion category is more than 99.8%.',\n",
       " 'Head gesture is a natural means of face-to-face communication between people but the recognition of head gestures in the context of virtual reality and use of head gesture as an interface for interacting with virtual avatars and virtual environments have been rarely investigated. In the current study, we present an approach for real-time head gesture recognition on head-mounted displays using Cascaded Hidden Markov Models. We conducted two experiments to evaluate our proposed approach. In experiment 1, we trained the Cascaded Hidden Markov Models and assessed the offline classification performance using collected head motion data. In experiment 2, we characterized the real-time performance of the approach by estimating the latency to recognize a head gesture with recorded real-time classification data. Our results show that the proposed approach is effective in recognizing head gestures. The method can be integrated into a virtual reality system as a head gesture interface for interacting with virtual worlds.',\n",
       " 'We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this method we have assembled SciQ, a dataset of 13.7K multiple choice science exam questions (Dataset available at http://allenai.org/data.html). We demonstrate that the method produces in-domain questions by providing an analysis of this new dataset and by showing that humans cannot distinguish the crowdsourced questions from original questions. When using SciQ as additional training data to existing questions, we observe accuracy improvements on real science exams.',\n",
       " \"Modern software developers rely on an extensive set of social media tools and communication channels. The adoption of team communication platforms has led to the emergence of conversation-based tools and integrations, many of which are chatbots. Understanding how software developers manage their complex constellation of collaborators in conjunction with the practices and tools they use can bring valuable insights into socio-technical collaborative work in software development and other knowledge work domains.\\n  In this paper, we explore how chatbots can help reduce the friction points software developers face when working collaboratively. Using a socio-technical model for collaborative work, we identify three main areas for conflict: friction stemming from team interactions with each other, an individual's interactions with technology, and team interactions with technology. Finally, we provide a set of open questions for discussion within the research community.\",\n",
       " 'In real settings, natural body movements can be erroneously recognized by whole-body input systems as explicit input actions. We call body activity not intended as input actions \"background activity.\" We argue that understanding background activity is crucial to the success of always-available whole-body input in the real world. To operationalize this argument, we contribute a reusable study methodology and software tools to generate standardized background activity datasets composed of data from multiple Kinect cameras, a Vicon tracker, and two high-definition video cameras. Using our methodology, we create an example background activity dataset for a television-oriented living room setting. We use this dataset to demonstrate how it can be used to redesign a gestural interaction vocabulary to minimize conflicts with the real world. The software tools and initial living room dataset are publicly available (http://www.dgp.toronto.edu/~dustin/backgroundactivity/).',\n",
       " \"Speech emotion recognition is an important task in human-machine interaction. However, it faces many challenges such as the ambiguity of emotion expression and the lack of training samples. To solve these problems, we propose a novel 'Pairwise discriminative task', which attempts to learn the similarity and distinction between two audios rather than specific labels. In the task, pairwise audios are fed into audio encode networks to extract audio vectors, followed with discrimination networks behind to judge whether audios belong to the same emotion category or not. The system is optimized in an end-to-end manner to minimize the loss function, which cooperates cosine similarity loss and cross entropy loss together. To verify the performance of audio representation vectors extracted from the system, we test them on IEMOCAP database-a common evaluation corpus. We gain 56.33% unweighted accuracy on the test database, which surpasses above 5% compared with traditional end-to-end speech emotion recognition networks.\",\n",
       " 'This paper examines some of the potential challenges associated with enabling a seamless web experience on underpowered mobile devices such as Google Glass from the perspective of web content providers, device, and the network. We conducted experiments to study the impact of webpage complexity, individual web components and different application layer protocols while accessing webpages on the performance of Glass browser, by measuring webpage load time, temperature variation and power consumption and compare it to a smartphone. Our findings suggest that (a) performance of Glass compared to a smartphone in terms of power consumption and webpage load time deteriorates with increasing webpage complexity (b) execution time for popular JavaScript benchmarks is about 3-8 times higher on Glass compared to a smartphone, (c) WebP is more energy efficient image format than JPEG and PNG, and (d) seven out of 50 websites studied are optimized for content delivery to Glass.',\n",
       " \"We introduce Blocks, a mobile application that enables people to co-create AR structures that persist in the physical environment. Using Blocks, end users can collaborate synchronously or asynchronously, whether they are colocated or remote. Additionally, the AR structures can be tied to a physical location or can be accessed from anywhere. We evaluated how people used Blocks through a series of lab and field deployment studies with over 160 participants, and explored the interplay between two collaborative dimensions: space and time. We found that participants preferred creating structures synchronously with colocated collaborators. Additionally, they were most active when they created structures that were not restricted by time or place. Unlike most of today's AR experiences, which focus on content consumption, this work outlines new design opportunities for persistent and collaborative AR experiences that empower anyone to collaborate and create AR content.\",\n",
       " 'Static and dynamic hand movements are basic way for human-machine interactions. To recognize and classify these movements, first these movements are captured by the cameras mounted on the augmented reality (AR) or virtual reality (VR) wearable devices. The hand is segmented using segmentation method and its gestures are passed to hand gesture recognition algorithm, which depends on depth-wise separable convolutional neural network for training, testing and finally running smoothly on mobile AR/VR devices, while maintaining the accuracy and balancing the load. A number of gestures are processed for identification of right gesture and to classify the gesture and ignore the all intermittent gestures. With proposed method, a user can write letters and numbers in air by just moving his/her hand in air. Gesture based operations are performed, and trajectory of hand is recorded as handwritten text. Finally, that handwritten text is processed for the text recognition.',\n",
       " 'Virtual globes have progressed from little-known technology to broadly popular software in a mere few years. We investigated this phenomenon through a survey and discovered that, while virtual globes are en vogue, their use is restricted to a small set of tasks so simple that they do not involve any spatial thinking. Spatial thinking requires that users ask \"what is where\" and \"why\"; the most common virtual globe tasks only include the \"what\". Based on the results of this survey, we have developed a multi-touch virtual globe derived from an adapted virtual globe paradigm designed to widen the potential uses of the technology by helping its users to inquire about both the \"what is where\" and \"why\" of spatial distribution. We do not seek to provide users with full GIS (geographic information system) functionality, but rather we aim to facilitate the asking and answering of simple \"why\" questions about general topics that appeal to a wide virtual globe user base.',\n",
       " 'Providing vibrotactile feedback that corresponds to the state of the virtual texture surfaces allows users to sense haptic properties of them. However, hand-tuning such vibrotactile stimuli for every state of the texture takes much time. Therefore, we propose a new approach to create models that realize the automatic vibrotactile generation from texture images or attributes. In this paper, we make the first attempt to generate the vibrotactile stimuli leveraging the power of deep generative adversarial training. Specifically, we use conditional generative adversarial networks (GANs) to achieve generation of vibration during moving a pen on the surface. The preliminary user study showed that users could not discriminate generated signals and genuine ones and users felt realism for generated signals. Thus our model could provide the appropriate vibration according to the texture images or the attributes of them. Our approach is applicable to any case where the users touch the various surfaces in a predefined way.',\n",
       " \"Working memory accounts for the ability of humans to perform cognitive processing, by handling both the representation of information (the mental picture forming the situation awareness) and the space required for processing these information (skill processing). The more complex the skills are, the more processing space they require, the less space becomes available for storage of information. This interplay between situation awareness and skills is critical in many applications. Theoretically, it is less understood in cognition and neuroscience. In the meantime, and practically, it is vital when analysing the mental processes involved in safety-critical domains.\\n  In this paper, we use the Sudoku game as a vehicle to study this trade-off. This game combines two features that are present during a user interaction with a software in many safety critical domains: scanning for information and processing of information. We use a society of agents for investigating how this trade-off influences player's proficiency.\",\n",
       " \"This paper outlines the development of a sensory feedback device providing a tangible interface for controlling digital environments, in this example a flight simulator, where the intention for the device is that it is relatively low cost, versatile and intuitive. Gesture based input allows for a more immersive experience, so rather than making the user feel like they are controlling an aircraft the intuitive interface allows the user to become the aircraft that is controlled by the movements of the user's hand. The movements are designed to allow a sense of immersion that would be difficult to achieve with an alternative interface. A vibrotactile based haptic feedback is incorporated in the device to further enhance the connection between the user and the game environment by providing immediate confirmation of game events. When used for navigating an aircraft simulator, this device invites playful action and thrill. It bridges new territory on portable, low cost solutions for haptic devices in gaming contexts.\",\n",
       " 'A carefully constructed scatterplot can reveal plenty about an underlying data set. However, in most cases visually mining and understanding a large multivariate data set requires more finesse, and greater level of interactivity to really grasp the full spectrum of the information being presented. We present a paradigm for glyph design and use in the creation of single plots presenting multiple variables of information. We center our design on two key concepts. The first concept is that visually it is easier to discriminate between completely distinct shapes rather than subtly different ones, specially when partially occluded. The second one is that users ingest information in layers, i.e. in an order of visual relevance. Using this paradigm, we present complex data as binned into desired and relevant discrete categories. We show results in the areas of high energy physics and security, displaying over 6 distinct data variables in each single plot, yielding a clear, highly readable, and effective visualization.',\n",
       " 'Many real-world networks are globally sparse but locally dense. Typical examples are social networks, biological networks, and information networks. This double structural nature makes it difficult to adopt a homogeneous visualization model that clearly conveys an overview of the network and the internal structure of its communities at the same time. As a consequence, the use of hybrid visualizations has been proposed. For instance, NodeTrix combines node-link and matrix-based representations (Henry et al., 2007). In this paper we describe ChordLink, a hybrid visualization model that embeds chord diagrams, used to represent dense subgraphs, into a node-link diagram, which shows the global network structure. The visualization is intuitive and makes it possible to interactively highlight the structure of a community while keeping the rest of the layout stable. We discuss the intriguing algorithmic challenges behind the ChordLink model, present a prototype system, and illustrate case studies on real-world networks.',\n",
       " 'Virtual reality (VR) games are gradually becoming more elaborated and feature-rich, but fail to reach the complexity of traditional digital games. One common feature that is used to extend and organize complex gameplay is the in-game inventory, which allows players to obtain and carry new tools and items throughout their journey. However, VR imposes additional requirements and challenges that impede the implementation of this important feature and hinder games to unleash their full potential. Our current work focuses on the design space of inventories in VR games. We introduce this sparsely researched topic by constructing a first taxonomy of the underlying design considerations and building blocks. Furthermore, we present three different inventories that were designed using our taxonomy and evaluate them in an early qualitative study. The results underline the importance of our research and reveal promising insights that show the huge potential for VR games.',\n",
       " 'We describe the experimental procedures for a dataset that we have made publicly available at https://doi.org/10.5281/zenodo.1494163 in mat and csv formats. This dataset contains electroencephalographic (EEG) recordings of 24 subjects doing a visual P300 Brain-Computer Interface experiment on PC.  The visual P300 is an event-related potential elicited by visual stimulation, peaking 240-600 ms after stimulus onset. The experiment was designed in order to compare the use of a P300-based brain-computer interface on a PC with and without adaptive calibration using Riemannian geometry. The brain-computer interface is based on electroencephalography (EEG). EEG data were recorded thanks to 16 electrodes. Data were recorded during an experiment taking place in the GIPSA-lab, Grenoble, France, in 2013 (Congedo, 2013). Python code for manipulating the data is available at https://github.com/plcrodrigues/py.BI.EEG.2013-GIPSA. The ID of this dataset is BI.EEG.2013-GIPSA.',\n",
       " 'Unhealthy lifestyles could cause many chronic diseases, which bring patients and their families much burden. Research has shown the potential of digital technologies for supporting health behavior change to help us prevent these chronic diseases. The HCI community has contributed to the research on health behavior change for more than a decade. In this paper, we aim to explore the research trends and patterns of health behavior change in HCI. Our systematic review showed that physical activity drew much more attention than other behaviors. Most of the participants in the reviewed studies were adults, while children and the elderly were much less addressed. Also, we found there is a lack of standardized approaches to evaluating the user experience of interventions for health behavior change in HCI. Based on the reviewed studies, we provide suggestions and research opportunities on six topics, e.g., game integration, social support, and relevant AI application.',\n",
       " 'This paper reports on a simple visual technique that boils extracting a subgraph down to two operations---pivots and filters---that is agnostic to both the data abstraction, and its visual complexity scales independent of the size of the graph. The system\\'s design, as well as its qualitative evaluation with users, clarifies exactly when and how the user\\'s intent in a series of pivots is ambiguous---and, more usefully, when it is not. Reflections on our results show how, in the event of an ambiguous case, this innately practical operation could be further extended into \"smart pivots\" that anticipate the user\\'s intent beyond the current step. They also reveal ways that a series of graph pivots can expose the semantics of the data from the user\\'s perspective, and how this information could be leveraged to create adaptive data abstractions that do not rely as heavily on a system designer to create a comprehensive abstraction that anticipates all the user\\'s tasks.',\n",
       " 'Tactile displays have a wide potential field of applications, ranging from enhancing Virtual-Reality scenarios up to aiding telesurgery as well as in fundamental psychological and neurophysiological research. In this paper, we describe an open source hardware and software architecture that is designed to drive a variety of different tactile displays. For demonstration purposes, a tactile computer mouse featuring a simple tactile display, based on lateral piezoelectric (PZT) actuators, is presented. Even though we will focus on driving mechanical actuators in this paper, the system can be extended to different working principles. The suggested architecture is supplied with a custom, easy to use, software stack allowing a simple definition of tactile scenarios as well as user studies while being especially tailored to non-computer scientists. By releasing the OpenTactile system under MIT license we hope to ease the burden of controlling tactile displays as well as designing and reproducing the related experiments.',\n",
       " 'In order for people to be able to trust and take advantage of the results of advanced machine learning and artificial intelligence solutions for real decision making, people need to be able to understand the machine rationale for given output. Research in explain artificial intelligence (XAI) addresses the aim, but there is a need for evaluation of human relevance and understandability of explanations. Our work contributes a novel methodology for evaluating the quality or human interpretability of explanations for machine learning models. We present an evaluation benchmark for instance explanations from text and image classifiers. The explanation meta-data in this benchmark is generated from user annotations of image and text samples. We describe the benchmark and demonstrate its utility by a quantitative evaluation on explanations generated from a recent machine learning algorithm. This research demonstrates how human-grounded evaluation could be used as a measure to qualify local machine-learning explanations.',\n",
       " \"We use the term borg to refer to the complex organizations composed of people, machines, and processes with which users frequently interact using computer interfaces and websites. Unlike interfaces to pure machines, we contend that borg-human interaction (BHI) happens in a context combining the anthropomorphization of the interface, conflict with users, and dramatization of the interaction process. We believe this context requires designers to construct the human facet of the borg, a structure encompassing the borg's personality, social behavior, and embodied actions; and the strategies to co-create dramatic narratives with the user. To design the human facet of a borg, different concepts and models are explored and discussed, borrowing ideas from psychology, sociology, and arts. Based on those foundations, we propose six design methodologies to complement traditional computer-human interface design techniques, including play-and-freeze enactment of conflicts and the use of giant puppets as interface prototypes.\",\n",
       " 'The objective of this research was to find out how the two search engines Google and Bing perform when users work freely on pre-defined tasks, and judge the relevance of the results immediately after finishing their search session. In a user study, 64 participants conducted two search tasks each, and then judged the results on the following: (1) The quality of the results they selected in their search sessions, (2) The quality of the results they were presented with in their search sessions (but which they did not click on), (3) The quality of the results from the competing search engine for their queries (which they did not see in their search session). We found that users heavily relied on Google, that Google produced more relevant results than Bing, that users were well able to select relevant results from the results lists, and that users judged the relevance of results lower when they regarded a task as difficult and did not find the correct information.',\n",
       " \"As the quantity of human knowledge increasing rapidly, it is harder and harder to evaluate a knowledge worker's knowledge quantitatively. There are lots of demands for evaluating a knowledge worker's knowledge. For example, accurately finding out a researcher's research concentrations for the last three years; searching for common topics for two scientists with different academic backgrounds; helping a researcher discover his deficiencies on a research field etc. This paper proposes a method named knowledge model to evaluate a knowledge worker's knowledge quantitatively without taking an examination. It records and analyzes an individual's each learning experience, discovering all the involved knowledge points and calculating their shares by analyzing the text learning contents with topic model. It calculates a score for a knowledge point by accumulating the effects of one's all learning experiences about it. A preliminary knowledge evaluating system is developed to testify the practicability of knowledge model.\",\n",
       " 'As people store more personal data in their smartphones, the consequences of having it stolen or lost become an increasing concern. A typical counter-measure to avoid this risk is to set up a secret code that has to be entered to unlock the device after a period of inactivity. However, for blind users, PINs and passwords are inadequate, since entry 1) consumes a non-trivial amount of time, e.g. using screen readers, 2) is susceptible to observation, where nearby people can see or hear the secret code, and 3) might collide with social norms, e.g. disrupting personal interactions. Tap-based authentication methods have been presented and allow unlocking to be performed in a short time and support naturally occurring inconspicuous behavior (e.g. concealing the device inside a jacket) by being usable with a single hand. This paper presents a study with blind users (N = 16) where an authentication method based on tap phrases is evaluated. Results showed the method to be usable and to support the desired inconspicuity.',\n",
       " 'Mutually beneficial behavior in repeated games can be enforced via the threat of punishment, as enshrined in game theory\\'s well-known \"folk theorem.\" There is a cost, however, to a player for generating these disincentives. In this work, we seek to minimize this cost by computing a \"Stackelberg punishment,\" in which the player selects a behavior that sufficiently punishes the other player while maximizing its own score under the assumption that the other player will adopt a best response. This idea generalizes the concept of a Stackelberg equilibrium. Known efficient algorithms for computing a Stackelberg equilibrium can be adapted to efficiently produce a Stackelberg punishment. We demonstrate an application of this idea in an experiment involving a virtual autonomous vehicle and human participants. We find that a self-driving car with a Stackelberg punishment policy discourages human drivers from bullying in a driving scenario requiring social negotiation.',\n",
       " 'Crowdsourcing is a popular means to obtain labeled data at moderate costs, for example for tweets, which can then be used in text mining tasks. To alleviate the problem of low-quality labels in this context, multiple human factors have been analyzed to identify and deal with workers who provide such labels. However, one aspect that has been rarely considered is the inherent difficulty of tweets to be labeled and how this affects the reliability of the labels that annotators assign to such tweets. Therefore, we investigate in this preliminary study this connection using a hierarchical sentiment labeling task on Twitter. We find that there is indeed a relationship between both factors, assuming that annotators have labeled some tweets before: labels assigned to easy tweets are more reliable than those assigned to difficult tweets. Therefore, training predictors on easy tweets enhances the performance by up to 6% in our experiment. This implies potential improvements for active learning techniques and crowdsourcing.',\n",
       " 'Web portals are being considered as excellent means for conducting teaching and learning activities electronically. The number of online services such as course enrollment, tutoring through online course materials, evaluation and even certification through web portals is increasing day by day. However, the effectiveness of an educational web portal depends on its accessibility to a wide range of students irrespective of their age, and physical abilities. Accessibility of web portals largely depends on their userfriendliness in terms of design, contents, assistive features, and online support. In this paper, we have critically analyzed the web portals of thirty Indian Universities of different categories based on the WCAG 2.0 guidelines. The purpose of this study is to point out the deficiencies that are commonly observed in web portals and help web designers to remove such deficiencies from the academic web portals with a view to enhance their accessibility.',\n",
       " 'Social media are a rich source of insight for data mining and user-centred research, but the question of consent arises when studying such data without the express knowledge of the creator. Case studies that mine social data from users of online services such as Facebook and Twitter are becoming increasingly common. This has led to calls for an open discussion into how researchers can best use these vast resources to make innovative findings while still respecting fundamental ethical principles. In this position paper we highlight some key considerations for this topic and argue that the conditions of informed consent are often not being met, and that using social media data that some deem free to access and analyse may result in undesirable consequences, particularly within the domain of health research and other sensitive topics. We posit that successful exploitation of online personal data, particularly for health and other sensitive research, requires new and usable methods of obtaining consent from the user.',\n",
       " \"As online health communities (OHCs) grow, users find it challenging to properly search, read, and contribute to the community because of its overwhelming content. Our goal is to understand OHC users' needs and requirements for better delivering large-scale OHC content. We interviewed 14 OHC users with interests in diabetes to investigate their attitudes and needs towards using OHCs and 2 OHC administrators to assess our findings. Four personas -Coddlers, Scientists, Adventurers, and Opportunists- emerged, which inform users' interaction behavior and attitudes with OHCs. An individual can possess the characteristics of multiple personas, which can also change over time. Our personas uniquely describe users' OHC participation intertwined with illness contexts compared to existing social types in general online communities. We discuss broader implications back to the literature and how our findings apply to other illness contexts in OHCs. We end with requirements for personalized delivery of large-scale OHC content.\",\n",
       " 'The process of learning involves interaction with the learning environment through our five senses (sight, hearing, touch, smell, and taste). Until recently, distance education focused only on the first two of those senses, sight and sound. Internet-based learning environments are predominantly visual with auditory components. With the advent of haptic technology we can now simulate/generate forces and, as a result, the sense of touch. The gaming industry has promoted the \"touch\" on the \"wire\", allowing complex multi-modal interactions online. In this article we provide a brief overview of the evolution of haptic technology, its potential for education, and existing challenges. We review recent data on 21st century students\\' behaviors, and share our experiences in designing interactive haptic environments for education. From the \"Community of Inquiry\" framework perspective, we discuss the potential impact of haptic feedback on cognitive and social presence.',\n",
       " 'Traditional literature on camera network design focuses on constructing automated algorithms. These require problem specific input from experts in order to produce their output. The nature of the required input is highly unintuitive leading to an unpractical workflow for human operators. In this work we focus on developing a virtual reality user interface allowing human operators to manually design camera networks in an intuitive manner. From real world practical examples we conclude that the camera networks designed using this interface are highly competitive with, or superior to those generated by automated algorithms, but the associated workflow is much more intuitive and simple. The competitiveness of the human-generated camera networks is remarkable because the structure of the optimization problem is a well known combinatorial NP-hard problem. These results indicate that human operators can be used in challenging geometrical combinatorial optimization problems given an intuitive visualization of the problem.',\n",
       " 'We are experiencing an upcoming trend of using head mounted display systems in games and serious games, which is likely to become an established practice in the near future. While these systems provide highly immersive experiences, many users have been reporting discomfort symptoms, such as nausea, sickness, and headaches, among others. When using VR for health applications, this is more critical, since the discomfort may interfere a lot in treatments. In this work we discuss possible causes of these issues, and present possible solutions as design guidelines that may mitigate them. In this context, we go deeper within a dynamic focus solution to reduce discomfort in immersive virtual environments, when using first-person navigation. This solution applies an heuristic model of visual attention that works in real time. This work also discusses a case study (as a first-person spatial shooter demo) that applies this solution and the proposed design guidelines.',\n",
       " 'Get-Up-and-Go Test is commonly used for assessing the physical mobility of the elderly by physicians. This paper presents a method for automatic analysis and classification of human gait in the Get-Up-and-Go Test using a Microsoft Kinect sensor. Two types of features are automatically extracted from the human skeleton data provided by the Kinect sensor. The first type of feature is related to the human gait (e.g., number of steps, step duration, and turning duration); whereas the other one describes the anatomical configuration (e.g., knee angles, leg angle, and distance between elbows). These features characterize the degree of human physical mobility. State-of-the-art machine learning algorithms (i.e. Bag of Words and Support Vector Machines) are used to classify the severity of gaits in 12 subjects with ages ranging between 65 and 90 enrolled in a pilot study. Our experimental results show that these features can discriminate between patients who have a high risk for falling and patients with a lower fall risk.',\n",
       " 'We design a system, SolarGest, which can recognize hand gestures near a solar-powered device by analyzing the patterns of the photocurrent. SolarGest is based on the observation that each gesture interferes with incident light rays on the solar panel in a unique way, leaving its distinguishable signature in harvested photocurrent. Using solar energy harvesting laws, we develop a model to optimize design and usage of SolarGest. To further improve the robustness of SolarGest under non-deterministic operating conditions, we combine dynamic time warping with Z-score transformation in a signal processing pipeline to pre-process each gesture waveform before it is analyzed for classification. We evaluate SolarGest with both conventional opaque solar cells as well as emerging see-through transparent cells. Our experiments with 6,960 gesture samples for 6 different gestures reveal that even with transparent cells, SolarGest can detect 96% of the gestures while consuming 44% less power compared to light sensor based systems.',\n",
       " 'This work describes a new human-in-the-loop (HitL) assistive grasping system for individuals with varying levels of physical capabilities. We investigated the feasibility of using four potential input devices with our assistive grasping system interface, using able-bodied individuals to define a set of quantitative metrics that could be used to assess an assistive grasping system. We then took these measurements and created a generalized benchmark for evaluating the effectiveness of any arbitrary input device into a HitL grasping system. The four input devices were a mouse, a speech recognition device, an assistive switch, and a novel sEMG device developed by our group that was connected either to the forearm or behind the ear of the subject. These preliminary results provide insight into how different interface devices perform for generalized assistive grasping tasks and also highlight the potential of sEMG based control for severely disabled individuals.',\n",
       " 'Brain Computer Interface (BCI) can help patients of neuromuscular diseases restore parts of the movement and communication abilities that they have lost. Most of BCIs rely on mapping brain activities to device instructions, but limited number of brain activities decides the limited abilities of BCIs. To deal with the problem of limited ablility of BCI, this paper verified the feasibility of constructing BCI based on decoding imagined speech electroencephalography (EEG). As sentences decoded from EEG can have rich meanings, BCIs based on EEG decoding can achieve numerous control instructions. By combining a modified EEG feature extraction mehtod with connectionist temporal classification (CTC), this paper simulated decoding imagined speech EEG using synthetic EEG data without help of speech signal. The performance of decoding model over synthetic data to a certain extent demonstrated the feasibility of constructing BCI based on imagined speech brain signal.',\n",
       " 'Online creative communities allow creators to share their work with a large audience, maximizing opportunities to showcase their work and connect with fans and peers. However, sharing in-progress work can be technically and socially challenging in environments designed for sharing completed pieces. We propose an online creative community where sharing process, rather than showcasing outcomes, is the main method of sharing creative work. Based on this, we present Mosaic---an online community where illustrators share work-in-progress snapshots showing how an artwork was completed from start to finish. In an online deployment and observational study, artists used Mosaic as a vehicle for reflecting on how they can improve their own creative process, developed a social norm of detailed feedback, and became less apprehensive of sharing early versions of artwork. Through Mosaic, we argue that communities oriented around sharing creative process can create a collaborative environment that is beneficial for creative growth.',\n",
       " 'Online deliberation offers a way for citizens to collectively discuss an issue and provide input for policy makers. The overall experience of online deliberation can be affected by multiple factors. We decided to investigate the effects of moderation and opinion heterogeneity on the perceived deliberation experience, by running the first online deliberation experiment in Singapore. Our study took place in three months with three phases. In phase 1, our 2,006 participants answered a survey, that we used to create groups of different opinion heterogeneity. During the second phase, 510 participants discussed about the population issue on the online platform we developed. We gathered data on their online deliberation experience during phase 3. We found out that higher levels of moderation negatively impact the experience of deliberation on perceived procedural fairness, validity claim and policy legitimacy; and that high opinion heterogeneity is important in order to get a fair assessment of the deliberation experience.',\n",
       " \"Web-based systems for assessment or homework are commonly used in many different domains. Several studies show that these systems can have positive effects on learning outcomes. Many research efforts also have made these systems quite flexible with respect to different item formats and exercise styles. However, there is still a lack of support for complex exercises in several domains at university level. Although there are systems that allow for quite sophisticated operations for generating exercise contents, there is less support for using similar operations for evaluating students' input and for feedback generation. This paper elaborates on filling this gap in the specific case of statistics. We present both the conceptional requirements for this specific domain as well as a fully implemented solution. Furthermore, we report on using this solution for formative and summative assessments in lectures with large numbers of participants at a big university.\",\n",
       " 'Digital health interventions have been emerging in the last decade. Due to their interdisciplinary nature, digital health interventions are guided and influenced by theories (e.g., behavioral theories, behavior change technologies, persuasive technology) from different research communities. However, digital health interventions are always coded using various taxonomies and reported in insufficient perspectives. The inconsistency and incomprehensiveness will bring difficulty for conducting systematic reviews and sharing contributions among communities. Based on existing related work, therefore, we propose a holistic framework that embeds behavioral theories, behavior change technique (BCT) taxonomy, and persuasive system design (PSD) principles. Including four development steps, two toolboxes, and one workflow, our framework aims to guide digital health intervention developers to design, evaluate, and report their work in a formative and comprehensive way.',\n",
       " \"Breast cancer is the most common cancer in women both in developed and developing countries. More than half of all cancer mobile application concern breast cancer. Gamification is widely used in mobile software applications created for health-related services. Current prevalence of gamification in breast cancer apps is unknown and detection must be manually performed. The purpose of this study is to describe and produce a tool allowing automatic detection of apps which contain gamification elements and thus empowering researchers to study gamification using large data samples. Predictive logistic regression model was designed on data extracted from breast cancer apps' title and description text available in app stores. Model was validated comparing estimated and benchmark values, observed by gamification specialists. Study's outcome can be applied as a screening tool to efficiently identify gamification presence in breast cancer apps for further research.\",\n",
       " 'Conversational agents are systems with a conversational interface that afford interaction in spoken language. These systems are becoming prevalent and are preferred in various contexts and for many users. Despite their increasing success, the automated testing infrastructure to support the effective and efficient development of such systems compared to traditional software systems is still limited. Automated testing framework for conversational systems can improve the quality of these systems by assisting developers to write, execute, and maintain test cases. In this paper, we introduce our work-in-progress automated testing framework, and its realization in the Python programming language. We discuss some research problems in the development of such an automated testing framework for conversational agents. In particular, we point out the problems of the specification of the expected behavior, known as test oracles, and semantic comparison of utterances.',\n",
       " \"We present RealPen, an augmented stylus for capacitive tablet screens that recreates the physical sensation of writing on paper with a pencil, ball-point pen or marker pen. The aim is to create a more engaging experience when writing on touch surfaces, such as screens of tablet computers. This is achieved by re-generating the friction-induced oscillation and sound of a real writing tool in contact with paper. To generate realistic tactile feedback, our algorithm analyses the frequency spectrum of the friction oscillation generated when writing with traditional tools, extracts principal frequencies, and uses the actuator's frequency response profile for an adjustment weighting function. We enhance the realism by providing the sound feedback aligned with the writing pressure and speed. Furthermore, we investigated the effects of superposition and fluctuation of several frequencies on human tactile perception, evaluated the performance of RealPen, and characterized users' perception and preference of each feedback type.\",\n",
       " 'This article proposes the analysis on novel human computer interaction (HCI) platform based college mathematical education methodology. Above for the application of virtual reality technology in teaching the problems in the study, only through the organization focus on the professional and technical personnel, and constantly improve researchers in development process of professional knowledge, close to the actual needs of the teaching can we achieve the satisfactory result. To obtain better education output, we combine the Kinect to form the HCI based teaching environment. We firstly review the latest HCI technique and principles of college math courses, then we introduce basic components of the Kinect including the gesture segmentation, systematic implementation and the primary characteristics of the platform. As the further step, we implement the system with the re-write of script code to build up the personalized HCI assisted education scenario. The verification and simulation proves the feasibility of our method.',\n",
       " 'With the arrival of digital maps, the ubiquity of maps has increased sharply and new map functionalities have become available such as changing the scale on the fly or displaying/hiding layers. Users can now interact with maps on multiple devices (e.g. smartphones, desktop computers, large-scale displays, head-mounted displays) using different means of interaction such as touch, voice or gestures. However, ensuring map functionalities and good user experience across these devices and modalities frequently entails dedicated development efforts for each combination. In this paper, we argue that introducing an abstract representation of what a map contains and affords can unlock new opportunities. For this purpose, we propose the concept of map plasticity, the capability of a map-based system to support different contexts of use while preserving usability and functionality. Based on this definition, we discuss core components and an example. We also propose a research agenda for realising map plasticity and its benefits.',\n",
       " \"The success of smart environments largely depends on their smartness of understanding the environments' ongoing situations. Accordingly, this task is an essence to smart environment central processors. Obtaining knowledge from the environment is often through sensors, and the response to a particular circumstance is offered by actuators. This can be improved by getting user feedback, and capturing environmental changes. Machine learning techniques and semantic reasoning tools are widely used in this area to accomplish the goal of interpretation. In this paper, we have proposed a hybrid approach utilizing both machine learning and semantic reasoning tools to derive a better understanding from sensors. This method uses situation templates jointly with a decision tree to adapt the system knowledge to the environment. To test this approach we have used a simulation process which has resulted in a better precision for detecting situations in an ongoing environment involving living agents while capturing its dynamic nature.\",\n",
       " \"Public sharing is integral to online platforms. This includes the popular multimedia messaging application Snapchat, on which public sharing is relatively new and unexplored in prior research. In mobile-first applications, sharing contexts are dynamic. However, it is unclear how context impacts users' sharing decisions. As platforms increasingly rely on user-generated content, it is important to also broadly understand user motivations and considerations in public sharing. We explored these aspects of content sharing through a survey of 1,515 Snapchat users. Our results indicate that users primarily have intrinsic motivations for publicly sharing Snaps, such as to share an experience with the world, but also have considerations related to audience and sensitivity of content. Additionally, we found that Snaps shared publicly were contextually different from those privately shared. Our findings suggest that content sharing systems can be designed to support sharing motivations, yet also be sensitive to private contexts.\",\n",
       " 'Recent research in the enteric nervous system, sometimes called the second brain, has revealed potential of the digestive system in predicting emotion. Even though people regularly experience changes in their gastrointestinal (GI) tract which influence their mood and behavior multiple times per day, robust measurements and wearable devices are not quite developed for such phenomena. However, other manifestations of the autonomic nervous system such as electrodermal activity, heart rate, and facial muscle movement have been extensively used as measures of emotions or in biofeedback applications, while neglecting the gut.  We expose electrogastrography (EGG), i.e., recordings of the myoelectric activity of the GI tract, as a possible measure for inferring human emotions. In this paper, we also wish to bring into light some fundamental questions about emotions, which are often taken for granted in the field of Human Computer Interaction, but are still a great debate in the fields of cognitive neuroscience and psychology.',\n",
       " 'Through Augmented Reality (AR), virtual graphics can transform the physical world. This offers benefits to mobile tourism, where points of interest (POIs) can be annotated on a smartphone screen. Although several of these applications exist, usability issues can discourage adoption. User-centred design (UCD) solicits frequent feedback, often contributing to usable products. While AR mock-ups have been constructed through UCD, we develop a novel and functional tourism app. We solicit requirements through a synthesis of domain analysis, tourist observation and semi-structured interviews. Through four rounds of iterative development, users test and refine the app. The final product, dubbed ToARist, is evaluated by 20 participants, who engage in a tourism task around a UK city. Users regard the system as usable, but find technical issues can disrupt AR. We finish by reflecting on our design and critiquing the challenges of a strict user-centred methodology.',\n",
       " 'A building design aiding tool for space allocation and thermal performance optimization is being developed to help practitioners during the building space planning phase, predicting how it will behave regarding energy consumption and thermal comfort. The tool evaluates, ranks, and optimizes generated floor plans according to thermal performance criteria, using the dynamic simulation program EnergyPlus. The tool is currently able to use a wide variety of EnergyPlus objects, allowing for various template and detailed HVAC, DHW, and thermal and electrical energy production systems and components, as well as numerous internal gains types, construction elements and energy saving controls, to be accounted for and simulated in the generated buildings. This paper presents the tool overall concept as well as the main features regarding dynamic simulation. Some performance results are presented for distinct systems to illustrate the use and potential of the tool.',\n",
       " 'This paper is the basis paper for the accepted IJCNN challenge One-Minute Gradual-Emotion Recognition (OMG-Emotion) by which we hope to foster long-emotion classification using neural models for the benefit of the IJCNN community. The proposed corpus has as the novelty the data collection and annotation strategy based on emotion expressions which evolve over time into a specific context. Different from other corpora, we propose a novel multimodal corpus for emotion expression recognition, which uses gradual annotations with a focus on contextual emotion expressions. Our dataset was collected from Youtube videos using a specific search strategy based on restricted keywords and filtering which guaranteed that the data follow a gradual emotion expression transition, i.e. emotion expressions evolve over time in a natural and continuous fashion. We also provide an experimental protocol and a series of unimodal baseline experiments which can be used to evaluate deep and recurrent neural models in a fair and standard manner.',\n",
       " 'Dynamic hand tracking and gesture recognition is a hard task since there are many joints on the fingers and each joint owns many degrees of freedom. Besides, object occlusion is also a thorny issue in finger tracking and posture recognition. Therefore, we propose a robust and customized system for realtime hand tracking and gesture recognition under occlusion environment. First, we model the angles between hand keypoints and encode their relative coordinate vectors, then we introduce GAN to generate raw discrete sequence dataset. Secondly we propose a time series forecasting method in the prediction of defined hand keypoint location. Finally, we define a sliding window matching method to complete gesture recognition. We analyze 11 kinds of typical gestures and show how to perform gesture recognition with the proposed method. Our work can reach state of the art results and contribute to build a framework to implement customized gesture recognition task.',\n",
       " '\"How common is interactive visualization on the web?\" \"What is the most popular visualization design?\" \"How prevalent are pie charts really?\" These questions intimate the role of interactive visualization in the real (online) world. In this paper, we present our approach (and findings) to answering these questions. First, we introduce Beagle, which mines the web for SVG-based visualizations and automatically classifies them by type (i.e., bar, pie, etc.). With Beagle, we extract over 41,000 visualizations across five different tools and repositories, and classify them with 86% accuracy, across 24 visualization types. Given this visualization collection, we study usage across tools. We find that most visualizations fall under four types: bar charts, line charts, scatter charts, and geographic maps. Though controversial, pie charts are relatively rare in practice. Our findings also indicate that users may prefer tools that emphasize a succinct set of visualization types, and provide diverse expert visualization examples.',\n",
       " 'This paper suggests that recent developments in video game technology have occurred in parallel to play being moved from public into private spaces, which has had impact on the way people interact with games. The paper also argues and that there is potentially value in the creation of public play spaces to create opportunities to utilise both technology and body for the benefit of community culture and experiences through gaming. Co-located social gaming coupled with tangible interfaces offer alternative possibilities for the local video game scene. This paper includes a descriptive account of Rabble Room Arcade, an experimental social event combining custom-built tangible interface devices and multiplayer video games. The event was designed around games that promoted a return to simplicity through the use of unique tangible controllers to allow casual gamers to connect to the game and to each other, whilst also transforming the event into a spectacle.',\n",
       " 'Thermal imaging-based physiological and affective computing is an emerging research area enabling technologies to monitor our bodily functions and understand psychological and affective needs in a contactless manner. However, up to recently, research has been mainly carried out in very controlled lab settings. As small size and even low-cost versions of thermal video cameras have started to appear on the market, mobile thermal imaging is opening its door to ubiquitous and real-world applications. Here we review the literature on the use of thermal imaging to track changes in physiological cues relevant to affective computing and the technological requirements set so far. In doing so, we aim to establish computational and methodological pipelines from thermal images of the human skin to affective states and outline the research opportunities and challenges to be tackled to make ubiquitous real-life thermal imaging-based affect monitoring a possibility.',\n",
       " 'Live animation of 2D characters has recently become a popular way for storytelling, and has potential application scenarios like tele-present agents or robots. As an extension of human-human communication, there is a need for augmenting the emotional communication experience of live animation. In this paper, we explore the emotional expressiveness issue of 2D live animation. In particular, we propose a descriptive emotion command model to bind a triggering action, the semantic meaning, psychology measurements, and behaviors of an emotional expression. Based on the model, we designed and implemented a proof-of-concept 2D live animation system, where a novel visual programming tool for editing the behaviors of 2D digital characters, and an emotion command recommendation algorithm are proposed. Through a user evaluation, we showcase the usability of our system and its potential for boosting creativity and enhancing the emotional communication experience.',\n",
       " 'Point sets in 2D with multiple classes are a common type of data. A canonical visualization design for them are scatterplots, which do not scale to large collections of points. For these larger data sets, binned aggregation (or binning) is often used to summarize the data, with many possible design alternatives for creating effective visual representations of these summaries. There are a wide range of designs to show summaries of 2D multi-class point data, each capable of supporting different analysis tasks. In this paper, we explore the space of visual designs for such data, and provide design guidelines for different analysis scenarios. To support these guidelines, we compile a set of abstract tasks and ground them in concrete examples using multiple sample datasets. We then assess designs, and survey a range of design decisions, considering their appropriateness to the tasks. In addition, we provide a web-based implementation to experiment with design choices, supporting the validation of designs based on task needs.',\n",
       " 'Developing cross-device multi-user interfaces (UIs) is a challenging problem. There are numerous ways in which content and interactivity can be distributed. However, good solutions must consider multiple users, their roles, their preferences and access rights, as well as device capabilities. Manual and rule-based solutions are tedious to create and do not scale to larger problems nor do they adapt to dynamic changes, such as users leaving or joining an activity. In this paper, we cast the problem of UI distribution as an assignment problem and propose to solve it using combinatorial optimization. We present a mixed integer programming formulation which allows real-time applications in dynamically changing collaborative settings. It optimizes the allocation of UI elements based on device capabilities, user roles, preferences, and access rights. We present a proof-of-concept designer-in-the-loop tool, allowing for quick solution exploration. Finally, we compare our approach to traditional paper prototyping in a lab study.',\n",
       " 'Most tabular data visualization techniques focus on overviews, yet many practical analysis tasks are concerned with investigating individual items of interest. At the same time, relating an item to the rest of a potentially large table is important. In this work we present Taggle, a tabular visualization technique for exploring and presenting large and complex tables. Taggle takes an item-centric,spreadsheet-like approach, visualizing each row in the source data individually using visual encodings for the cells. At the same time, Taggle introduces data-driven aggregation of data subsets. The aggregation strategy is complemented by interaction methods tailored to answer specific analysis questions, such as sorting based on multiple columns and rich data selection and filtering capabilities. We evaluate Taggle using a qualitative user study and a case study conducted by a domain expert on complex genomics data analysis for the purpose of drug discovery.',\n",
       " 'At present many blind assistive systems have been implemented but there is no such kind of good system to navigate a blind person and also to track the movement of a blind person and rescue him/her if he/she is lost. In this paper, we have presented a blind assistive and tracking embedded system. In this system the blind person is navigated through a spectacle interfaced with an android application. The blind person is guided through Bengali/English voice commands generated by the application according to the obstacle position. Using voice command a blind person can establish voice call to a predefined number without touching the phone just by pressing the headset button. The blind assistive application gets the latitude and longitude using GPS and then sends them to a server. The movement of the blind person is tracked through another android application that points out the current position in Google map. We took distances from several surfaces like concrete and tiles floor in our experiment where the error rate is 5%.',\n",
       " 'Today, in the digital age, the mobile devices are more and more used to aid people in the struggle to improve or maintain their health. In this paper, the mobile eHealth solution for remote patient monitoring during clinical trials is presented, together with the outcomes of quantitative and qualitative performance evaluation. The evaluation is a third step to improve the quality of the application after earlier Good Clinical Practice certification and validation with the participation of 10 patients and three general practitioners. This time, the focus was on the usability which was evaluated by the seventeen participants divided into three age groups (18-28, 29-50, and 50+). The results, from recorded sessions and the eye tracking, show that there is no difference in performance between the first group and the second group, while for the third group the performance was worse, however, it was still good enough to complete task within reasonable time.',\n",
       " 'To enhance the performance of affective models and reduce the cost of acquiring physiological signals for real-world applications, we adopt multimodal deep learning approach to construct affective models from multiple physiological signals. For unimodal enhancement task, we indicate that the best recognition accuracy of 82.11% on SEED dataset is achieved with shared representations generated by Deep AutoEncoder (DAE) model. For multimodal facilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE) achieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets, respectively, which are much superior to the state-of-the-art approaches. For cross-modal learning task, our experimental results demonstrate that the mean accuracy of 66.34% is achieved on SEED dataset through shared representations generated by EEG-based DAE as training samples and shared representations generated by eye-based DAE as testing sample, and vice versa.',\n",
       " 'Young people worldwide are participating in ever-increasing numbers in online fan communities. Far from mere shallow repositories of pop culture, these sites are accumulating significant evidence that sophisticated informal learning is taking place online in novel and unexpected ways. In order to understand and analyze in more detail how learning might be occurring, we conducted an in-depth nine-month ethnographic investigation of online fanfiction communities, including participant observation and fanfiction author interviews. Our observations led to the development of a theory we term distributed mentoring, which we present in detail in this paper. Distributed mentoring exemplifies one instance of how networked technology affords new extensions of behaviors that were previously bounded by time and space. Distributed mentoring holds potential for application beyond the spontaneous mentoring observed in this investigation and may help students receive diverse, thoughtful feedback in formal learning environments as well.',\n",
       " \"This is the preprint version of our paper on Personal and Ubiquitous Computing. There is an increasing interest in creating pervasive games based on emerging interaction technologies. In order to develop touch-less, interactive and augmented reality games on vision-based wearable device, a touch-less motion interaction technology is designed and evaluated in this work. Users interact with the augmented reality games with dynamic hands/feet gestures in front of the camera, which triggers the interaction event to interact with the virtual object in the scene. Three primitive augmented reality games with eleven dynamic gestures are developed based on the proposed touch-less interaction technology as proof. At last, a comparing evaluation is proposed to demonstrate the social acceptability and usability of the touch-less approach, running on a hybrid wearable framework or with Google Glass, as well as workload assessment, user's emotions and satisfaction.\",\n",
       " 'Virtual learning environments are a useful modality for engaging students in the classroom by affording them a sense of presence and immersion. The motivation of this project was to create an open-source augmented reality electrical circuit application for use in lower division engineering courses to teach students about electricity fundamentals. Softwares that are readily available for use on virtual and augmented reality devices do not typically apply to all disciplines and do not necessarily have a pedagogical or accessibility focus. Considering this lack of appropriate educational applications for the current virtual and augmented reality devices, a team of interdisciplinary students was assembled to create such software. With extensive usability studies, the application was designed for quick adoption and improve accessibility by providing multimodal access such as voice assistant, gray scaling for depth perception and daltonize the app. The software is available as part of VITaL Laboratory, Sonoma State University.',\n",
       " 'An attempt is made to develop a smart toy to help the children suffering with communication disorders. The children suffering with such disorders need additional attention and guidance to understand different types of social events and life activities. Various issues and features of the children with speech disorders are identified in this study and based on the inputs from the study, a working architecture is proposed with suitable policies. A prediction module with a checker component is designed in this work to produce alerts in at the time of abnormal behaviour of the child with communication disorder. The model is designed very sensitively to the behaviour of the child for a particular voice tone, based on which the smart toy will change to tones automatically. Such an arrangement proved to be helpful for the children to improve the communication with other due to the inclusion of continuous training for the smart toy from the prediction module.',\n",
       " 'Distributed systems technologies supporting 3D visualization and social collaboration will be increasing in frequency and type over time. An emerging type of head-mounted display referred to as the head-mounted projection display (HMPD) was recently developed that only requires ultralight optics (i.e., less than 8 g per eye) that enables immersive multiuser, mobile augmented reality 3D visualization, as well as remote 3D collaborations. In this paper a review of the development of lightweight HMPD technology is provided, together with insight into what makes this technology timely and so unique. Two novel emerging HMPD-based technologies are then described: a teleportal HMPD(T-HMPD) enabling face-to-face communication and visualization of shared 3D virtual objects, and a mobile HMPD (M-HMPD) designed for outdoor wearable visualization and communication. Finally, the use of HMPD in medical visualization and training, as well as in infospaces, two applications developed in the ODA and MIND labs respectively, are discussed.',\n",
       " \"Virtual Reality (VR) provides immersive experiences in the virtual world, but it may reduce users' awareness of physical surroundings and cause safety concerns and psychological discomfort. Hence, there is a need of an ambient information design to increase users' situational awareness (SA) of physical elements when they are immersed in VR environment. This is challenging, since there is a tradeoff between the awareness in reality and the interference with users' experience in virtuality. In this paper, we design five representations (indexical, symbolic, and iconic with three emotions) based on two dimensions (vividness and emotion) to address the problem. We conduct an empirical study to evaluate participants' SA, perceived breaks in presence (BIPs), and perceived engagement through VR tasks that require movement in space. Results show that designs with higher vividness evoke more SA, designs that are more consistent with the virtual environment can mitigate the BIP issue, and emotion-evoking designs are more engaging.\",\n",
       " 'How could we gather affect annotations in a rapid, unobtrusive, and accessible fashion? How could we still make sure that these annotations are reliable enough for data-hungry affect modelling methods? This paper addresses these questions by introducing PAGAN, an accessible, general-purpose, online platform for crowdsourcing affect labels in videos. The design of PAGAN overcomes the accessibility limitations of existing annotation tools, which often require advanced technical skills or even the on-site involvement of the researcher. Such limitations often yield affective corpora that are restricted in size, scope and use, as the applicability of modern data-demanding machine learning methods is rather limited. The description of PAGAN is accompanied by an exploratory study which compares the reliability of three continuous annotation tools currently supported by the platform. Our key results reveal higher inter-rater agreement when annotation traces are processed in a relative manner and collected via unbounded labelling.',\n",
       " 'Animated GIFs are increasingly popular in text-based communication. Finding the perfect GIF can make conversations funny, interesting, and engaging, but GIFs also introduce potentials for miscommunication. Through 24 in-depth qualitative interviews, this empirical, exploratory study examines the nuances of communication practices with animated GIFs to better understand why and how GIFs can send unintentional messages. We find participants leverage contexts like source material and interpersonal relationship to find the perfect GIFs for different communication scenarios, while these contexts are also the primary reason for miscommunication and some technical usability issues in GIFs. This paper concludes with a discussion of the important role that different types of context play in the use and interpretations of GIFs, and argues that nonverbal communication tools should account for complex contexts and common ground that communication media rely on.',\n",
       " 'Socially Assistive Robots (SARs) offer great promise for improving outcomes in paediatric rehabilitation. However, the design of software and interactive capabilities for SARs must be carefully considered in the context of their intended clinical use. While previous work has explored specific roles and functionalities to support paediatric rehabilitation, few have considered the design of such capabilities in the context of ongoing clinical deployment. In this paper we present a two-phase In-situ design process for SARs in health care, emphasising stakeholder engagement and on-site development. We explore this in the context of developing the humanoid social robot NAO as a socially assistive rehabilitation aid for children with cerebral palsy. We present and evaluate our design process, outcomes achieved, and preliminary results from ongoing clinical testing with 9 patients and 5 therapists over 14 sessions. We argue that our in-situ Design methodology has been central to the rapid and successful deployment of our system.',\n",
       " 'The aim of this research is development of rule based decision model for emotion recognition. This research also proposes using the rules for augmenting inter-corporal recognition accuracy in multimodal systems that use supervised learning techniques. The classifiers for such learning based recognition systems are susceptible to over fitting and only perform well on intra-corporal data. To overcome the limitation this research proposes using rule based model as an additional modality. The rules were developed using raw feature data from visual channel, based on human annotator agreement and existing studies that have attributed movement and postures to emotions. The outcome of the rule evaluations was combined during the decision phase of emotion recognition system. The results indicate rule based emotion recognition augment recognition accuracy of learning based systems and also provide better recognition rate across inter corpus emotion test data.',\n",
       " 'Crowdsourcing has revolutionized the process of knowledge building on the web. Wikipedia and StackOverflow are witness to this uprising development. However, the dynamics behind the process of crowdsourcing in the domain of knowledge building is an area relatively unexplored. It has been observed that an ecosystem exists in the collaborative knowledge building environments (KBE), which puts users of a KBE into various categories based on their expertise. Classical cognitive theories indicate triggering among the knowledge units to be one of the most important reasons behind accelerated knowledge building in collaborative KBEs. We use the concept of ecosystem and the triggering phenomenon to highlight the necessity for the right mix of users in a KBE. We provide a hill climbing based algorithm which gives the ideal mixture of users in a KBE, given the amount of triggering that takes place among the users of various categories. The study will help the portal designers to accordingly build suitable crowdsourced environments.',\n",
       " \"Location sensing is a key enabling technology for Ubicomp to support contextual interaction. However, the laboratories where calibrated testing of location technologies is done are very different to the domestic situations where `context' is a problematic social construct. This study reports measurements of Bluetooth beacons, informed by laboratory studies, but done in diverse domestic settings. The design of these surveys has been motivated by the natural environment implied in the Bluetooth beacon standards - relating the technical environment of the beacon to the function of spaces within the home. This research method can be considered as a situated, `ethnographic' technical response to the study of physical infrastructure that arises through social processes. The results offer insights for the future design of `seamful' approaches to indoor location sensing, and to the ways that context might be constructed and interpreted in a seamful manner.\",\n",
       " 'Information visualization limits itself, per definition, to the domain of symbolic information. This paper discusses arguments why the field should also consider forms of data that are not symbolically encoded, including physical traces and material indicators. Continuing a provocation presented by Pat Hanrahan in his 2004 IEEE Vis capstone address, this paper compares physical traces to visualizations and describes the techniques and visual practices for producing, revealing, and interpreting them. By contrasting information visualization with a speculative counter model of autographic visualization, this paper examines the design principles for material data. Autographic visualization addresses limitations of information visualization, such as the inability to directly reflect the material circumstances of data generation. The comparison between the two models allows probing the epistemic assumptions behind information visualization and uncovers linkages with the rich history of scientific visualization and trace reading.',\n",
       " 'Landmark-based navigation systems have proven benefits relative to traditional turn-by-turn systems that use street names and distances. However, one obstacle to the implementation of landmark-based navigation systems is the complex challenge of selecting salient local landmarks at each decision point for each user. In this paper, we present Pharos, a novel system that extends turn-by-turn navigation instructions using a single global landmark (e.g. the Eiffel Tower, the Burj Khalifa, municipal TV towers) rather than multiple, hard-to-select local landmarks. We first show that our approach is feasible in a large number of cities around the world through the use of computer vision to select global landmarks. We then present the results of a study demonstrating that by including global landmarks in navigation instructions, users navigate more confidently and build a more accurate mental map of the navigated area than using turn-by-turn instructions.',\n",
       " 'We study the performance of brain computer interface (BCI) system in a virtual reality (VR) environment and compare it to 2D regular displays. First, we design a headset that consists of three components: a wearable electroencephalography (EEG) device, a VR headset and an interface. Recordings of brain and behavior from human subjects, performing a wide variety of tasks using our device are collected. The tasks consist of object rotation or scaling in VR using either mental commands or facial expression (smile and eyebrow movement). Subjects are asked to repeat similar tasks on regular 2D monitor screens. The performance in 3-D virtual reality environment is considerably higher compared to the to the 2D screen. Particularly, the median number of success rate across trials for VR setting is double of that for the 2D setting (8 successful command in VR setting compared to 4 successful command in 2D screen in 1 minute trials). Our results suggest that the design of future BCI systems can remarkably benefit from the VR setting.',\n",
       " \"Mental State Transition Network (MSTN) is a basic concept of approximating to human psychological and mental responses. A stimulus calculated by Emotion Generating Calculations (EGC) method can cause the transition of mood from an emotional state to others. In this paper, the agent can interact with human to realize smooth communication by an adaptive learning method of the user's personality trait based mood. The learning method consists of the profit sharing (PS) method and the recurrent neural network (RNN). An emotion for sensor inputs to MSTN is calculated by EGC and the variance of emotion leads to the change of mental state, and then the sequence of states forms an episode. In order to learn the tendency of personality trait effectively, the ineffective rules should be removed from the episode. PS method finds out a detour in episode and should be deleted. Furthermore, RNN works to realize the variance of user's mood. Some experimental results were shown the success of representing a various human's delicate emotion.\",\n",
       " 'Users\\' persistent social media contents like posts on Facebook Timeline are presented as an \"exhibition\" about the person to others, and managing these exhibitional contents for impression management needs intentional and manual efforts. To raise awareness of and facilitate impression management around past contents, we developed a prototype called PersonalityInsight. The system employs computational psycho-linguistic analysis to help users visualize the way their past text posts might convey impressions of their personality and allowed users to modify their posts based on these visualizations. We conducted a user study to evaluate the design; users overall found that such a tool raised awareness of the fact and the ways personality might be conveyed through their past content as one aspect of impression management, but that it needs design improvement to offer action-able suggestions for content modification, as well as careful thinking about impression management as one of many values people have about their digital past.',\n",
       " 'Usability is a key quality attribute of successful software systems. Unfortunately, there is no common understanding of the factors influencing usability and their interrelations. Hence, the lack of a comprehensive basis for designing, analyzing, and improving user interfaces. This paper proposes a 2-dimensional model of usability that associates system properties with the activities carried out by the user. By separating activities and properties, sound quality criteria can be identified, thus facilitating statements concerning their interdependencies. This model is based on a tested quality meta-model that fosters preciseness and completeness. A case study demonstrates the manner by which such a model aids in revealing contradictions and omissions in existing usability standards. Furthermore, the model serves as a central and structured knowledge base for the entire quality assurance process, e.g. the automatic generation of guideline documents.',\n",
       " \"This study describes a detailed analysis of museum visitors' decoding process as they used a visualization designed to support exploration of a large, complex dataset. Quantitative and qualitative analyses revealed that it took, on average, 43 seconds for visitors to decode enough of the visualization to see patterns and relationships in the underlying data represented, and 54 seconds to arrive at their first correct data interpretation. Furthermore, visitors decoded throughout and not only upon initial use of the visualization. The study analyzed think-aloud data to identify issues visitors had mapping the visual representations to their intended referents, examine why they occurred, and consider if and how these decoding issues were resolved. The paper also describes how multiple visual encodings both helped and hindered decoding and concludes with implications on the design and adaptation of visualizations for informal science learning venues.\",\n",
       " 'Quality of Experience (QoE) typically involves conducting experiments in which stimuli are presented to participants and their judgments as well as behavioral data are collected. Nowadays, many experiments require software for the presentation of stimuli and the data collection from participants. While different software solutions exist, these are not tailored to conduct experiments on QoE. Moreover, replicating experiments or repeating the same experiment in different settings (e. g., laboratory vs. crowdsourcing) can further increase the software complexity. TheFragebogen is an open-source, versatile, extendable software framework for the implementation of questionnaires - especially for research on QoE. Implemented questionnaires can be presented with a state-of-the-art web browser to support a broad range of devices while the use of a web server being optional. Out-of-the-box, TheFragebogen provides graphical exact scales as well as free-hand input, the ability to collect behavioral data, and playback multimedia content.',\n",
       " \"With eye tracking being increasingly integrated into virtual and augmented reality (VR/AR) head-mounted displays, preserving users' privacy is an ever more important, yet under-explored, topic in the eye tracking community. We report a large-scale online survey (N=124) on privacy aspects of eye tracking that provides the first comprehensive account of with whom, for which services, and to what extent users are willing to share their gaze data. Using these insights, we design a privacy-aware VR interface that uses differential privacy, which we evaluate on a new 20-participant dataset for two privacy sensitive tasks: We show that our method can prevent user re-identification and protect gender information while maintaining high performance for gaze-based document type classification. Our results highlight the privacy challenges particular to gaze data and demonstrate that differential privacy is a potential means to address them. Thus, this paper lays important foundations for future research on privacy-aware gaze interfaces.\",\n",
       " 'Social media platforms such as Twitter are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve and evade traditional detection techniques. In this work, we propose VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling techniques, which offer new insights that enable the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We conducted a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool.',\n",
       " 'Large-scale labeled datasets are the indispensable fuel that ignites the AI revolution as we see today. Most such datasets are constructed using crowdsourcing services such as Amazon Mechanical Turk which provides noisy labels from non-experts at a fair price. The sheer size of such datasets mandates that it is only feasible to collect a few labels per data point. We formulate the problem of test-time label aggregation as a statistical estimation problem of inferring the expected voting score in an ideal world where all workers label all items. By imitating workers with supervised learners and using them in a doubly robust estimation framework, we prove that the variance of estimation can be substantially reduced, even if the learner is a poor approximation. Synthetic and real-world experiments show that by combining the doubly robust approach with adaptive worker/item selection, we often need as low as 0.1 labels per data point to achieve nearly the same accuracy as in the ideal world where all workers label all data points.',\n",
       " \"Near distances are overestimated in virtual reality, and far distances are underestimated, but an explanation for these distortions remains elusive. One potential concern is that whilst the eye rotates to look at the virtual scene, the virtual cameras remain static. Could using eye-tracking to change the perspective of the virtual cameras as the eye rotates improve depth perception in virtual reality? This paper identifies 14 distinct perspective distortions that could in theory occur from keeping the virtual cameras fixed whilst the eye rotates in the context of near-eye displays. However, the impact of eye movements on the displayed image depends on the optical, rather than physical, distance of the display. Since the optical distance of most head-mounted displays is over 1m, most of these distortions will have only a negligible effect. The exception are 'gaze-contingent disparities', which will leave near virtual objects looking displaced from physical objects that are meant to be at the same distance in augmented reality.\",\n",
       " \"Physical agents that can autonomously generate engaging, life-like behaviour will lead to more responsive and interesting robots and other autonomous systems. Although many advances have been made for one-to-one interactions in well controlled settings, future physical agents should be capable of interacting with humans in natural settings, including group interaction. In order to generate engaging behaviours, the autonomous system must first be able to estimate its human partners' engagement level. In this paper, we propose an approach for estimating engagement from behaviour and use the measure within a reinforcement learning framework to learn engaging interactive behaviours. The proposed approach is implemented in an interactive sculptural system in a museum setting. We compare the learning system to a baseline using pre-scripted interactive behaviours. Analysis based on sensory data and survey data shows that adaptable behaviours within a perceivable and understandable range can achieve higher engagement and likeability.\",\n",
       " 'Due to the increasing number of mobile robots including domestic robots for cleaning and maintenance in developed countries, human activity recognition is inevitable for congruent human-robot interaction. Needless to say that this is indeed a challenging task for robots, it is expedient to learn human activities for autonomous mobile robots (AMR) for navigating in an uncontrolled environment without any guidance. Building a correct classifier for complex human action is non-trivial since simple actions can be combined to recognize a complex human activity. In this paper, we trained a model for human activity recognition using convolutional neural network. We trained and validated the model using the Vicon physical action dataset and also tested the model on our generated dataset (VMCUHK). Our experiment shows that our method performs with high accuracy, human activity recognition task both on the Vicon physical action dataset and VMCUHK dataset.',\n",
       " 'The Unified Modeling Language (UML) is a widely used general purpose modeling language. Together with the Object Constraint Language (OCL), formal models can be described by defining the structure and behavior with UML and additional OCL constraints. In the development process for formal models, it is important to make sure that these models are (a) correct, i.e. consistent and complete, and (b) testable in the sense that the developer is able to interactively check model properties. The USE tool (UML-based Specification Environment) allows both characteristics to be studied. We demonstrate how the tool supports modelers to analyze, validate and verify UML and OCL models via the use of several graphical means that assist the modeler in interpreting and visualizing formal model descriptions. In particular, we discuss how the so-called USE model validator plugin is integrated into the USE environment in order to allow non domain experts to use it and construct object models that help to verify properties like model consistency.',\n",
       " 'Drawing principles, or aesthetics, are important in graph drawing. They are used as criteria for algorithm design and for quality evaluation. Current aesthetics are described as visual properties that a drawing is required to have to be visually pleasing. However, most of these aesthetics are originally proposed without consideration of graph structure information. Therefore their ability in visually revealing graph structural features are not guaranteed and indeed mixed results have been reported in the literature regarding their impact on user graph comprehension. In this paper, we propose to derive aesthetics based on graph internal structural features. Further, graphs are often evaluated based on controlled experiments with simple perception tasks to avoid possible confounding factors caused by complex tasks. This leaves their value in supporting complex tasks unevaluated. To fill this gap, we also discuss the possibility of applying evaluation methodologies used in the Cognitive Load Theory research for graph evaluation.',\n",
       " 'We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.',\n",
       " 'We present some results concerning the dialogue behavior and inferred sentiment of a group of older adults interacting with a computer-based avatar. Our avatar is unique in its ability to hold natural dialogues on a wide range of everyday topics---27 topics in three groups, developed with the help of gerontologists. The three groups vary in ``degrees of intimacy\", and as such in degrees of difficulty for the user. Each participant interacted with the avatar for 7-9 sessions over a period of 3-4 weeks; analysis of the dialogues reveals correlations such as greater verbosity for more difficult topics, increasing verbosity with successive sessions, especially for more difficult topics, stronger sentiment on topics concerned with life goals rather than routine activities, and stronger self-disclosure for more intimate topics. In addition to their intrinsic interest, these results also reflect positively on the sophistication of our dialogue system.',\n",
       " 'Regional Intergovernmental Organizations (RIGOs) are constituted by the local governments within their respective regions and are supported by the active engagement of the regions community and citizens. Metropolitan Statistical Areas (MSAs), on the other hand, are classified by the federal government based on commuting and commerce patterns. They do not adhere to any local government. The Graduate School of Policy and International Affairs Center for Metropolitan Studies (GSPIA) at the University of Pittsburgh have been researching the boundaries of RIGOs and the characteristics defining them. In this paper, we propose, design, and implement an approach to enhance the current visualization by visualizing two categorical data: RIGOs and MSAs and the overlapping between them. We attempted to use a combination of visual attributes that leverage human perception system and do not impose cognitive and mental effort. The overall result of the evaluation shows that our work proved to be more effective than the current visualization.',\n",
       " \"By collecting the data of eyeball movement of pilots, it is possible to monitor pilot's operation in the future flight in order to detect potential accidents. In this paper, we designed a novel SVS system that is integrated with an eye tracking device, and is able to achieve the following functions:1) A novel method that is able to learn from the eyeball movements of pilots and preload or render the terrain data in various resolutions, in order to improve the quality of terrain display by comprehending the interested regions of the pilot. 2) A warning mechanism that may detect the risky operation via analyzing the aviation information from the SVS and the eyeball movement from the eye tracking device, in order to prevent the maloperations or human factor accidents. The user study and experiments show that the proposed SVS-Eyetracking system works efficiently and is capable of avoiding potential risked caused by fatigue in the flight simulation.\",\n",
       " 'In this position paper we describe select aspects of our experience with health-related self-tracking, the data generated, and processes surrounding those. In particular we focus on how bilateral patient-clinician engagement may be fostered by the combination of technology and method. We exemplify with a case study where a PTSD-suffering veteran has been self-tracking a specific symptom precursor. The availability of high-resolution self-tracking data on the occurrences of even a single symptom created new opportunities in the therapeutic process for identifying underlying triggers of symptoms. The patient was highly engaged in self-tracking and sharing the collected data. We suggest a key reason was the collaborative effort in defining the data collection protocol and discussion of the data. The therapist also engaged highly in the self-tracking data, as it supported the existing therapeutic process in reaching insights otherwise unobtainable.',\n",
       " \"This paper studies the use of eye tracking in a First-Person Shooter (FPS) game as a~mechanism to: (1) control the attention of the player's avatar according to the attention deployed by the player, and (2) guide the gameplay and game's procedural content generation, accordingly. This results in a more natural use of eye tracking in comparison to a use in which the eye tracker directly substitutes control input devices, such as gamepads. The study was conducted on a custom endless runner FPS, Zombie Runner, using an affordable eye tracker. Evaluation sessions showed that the proposed use of eye tracking provides a more challenging and immersive experience to the player, when compared to its absence. However, a strong correlation between eye tracker calibration problems and player's overall experience was found. This means that eye tracking technology still needs to evolve but also means that once technology gets mature enough players are expected to benefit greatly from the inclusion of eye tracking in their gaming experience.\",\n",
       " \"Microtask crowdsourcing has enabled dataset advances in social science and machine learning, but existing crowdsourcing schemes are too expensive to scale up with the expanding volume of data. To scale and widen the applicability of crowdsourcing, we present a technique that produces extremely rapid judgments for binary and categorical labels. Rather than punishing all errors, which causes workers to proceed slowly and deliberately, our technique speeds up workers' judgments to the point where errors are acceptable and even expected. We demonstrate that it is possible to rectify these errors by randomizing task order and modeling response latency. We evaluate our technique on a breadth of common labeling tasks such as image verification, word similarity, sentiment analysis and topic classification. Where prior work typically achieves a 0.25x to 1x speedup over fixed majority vote, our approach often achieves an order of magnitude (10x) speedup.\",\n",
       " 'User Experience (UX) has been a buzzword in agile literature in recent years. However, often UX remains as a vague concept and it may be hard to understand the very nature of it in the context of agile software development. This paper explores the multifaceted UX literature, emphasizes the multi-dimensional nature of the concept and organizes the current state-of-the-art knowledge. As a starting point to better understand the contemporary meaning of UX assigned by practitioners, we selected four UX blogs and performed an analysis using a framework derived from the literature review. The preliminary results show that the practitioners more often focus on interaction between product and user and view UX from design perspective predominantly. While the economical perspective receives little attention in literature, it is evident in practitioners writings. Our study opens up a promising line of request of the contemporary meaning of UX in practice.',\n",
       " \"In this paper, we explore the potential impact of Internet of Things (IoT) technology may have on the cosplay community. We developed a costume (an IoT Skullfort) and embedded IoT technology to enhance its capabilities and user interactions. Sensing technologies are widely used in many different wearable domains including cosplay scenarios. However, in most of these scenarios, typical interaction pattern is that the costume responds to its environment or the player's behaviour (e.g., colour of lights may get changed when player moves hands). In contrast, our research focuses on exploring scenarios where the audience (third party) get to manipulate the costume behaviour (e.g., the audience get to change the colour of the Skullfort using a mobile application). We believe such an audience (third party) influenced cosplay brings new opportunities for enhanced entertainment. However, it also creates significant challenges. We report the results gathered through a focus group conducted in collaboration with cosplay community experts.\",\n",
       " \"As the capability and complexity of UAVs continue to increase, the human-robot interface community has a responsibility to design better ways of specifying the complex 3D flight paths necessary for instructing them. Immersive interfaces, such as those afforded by virtual reality (VR), have several unique traits which may improve the user's ability to perceive and specify 3D information. These traits include stereoscopic depth cues which induce a sense of physical space as well as six degrees of freedom (DoF) natural head-pose and gesture interactions. This work introduces an open-source platform for 3D aerial path planning in VR and compares it to existing UAV piloting interfaces. Our study has found statistically significant improvements in safety and subjective usability over a manual control interface, while achieving a statistically significant efficiency improvement over a 2D touchscreen interface. The results illustrate that immersive interfaces provide a viable alternative to touchscreen interfaces for UAV path planning.\",\n",
       " \"Mobile Learning Games (MLGs) show great potential for increasing engagement, creativity and authentic learning. Yet, despite their great potential for education, the use of MLGs by teachers, remains limited. This is partly due to the fact that MLGs are often designed to match a specific learning context, and thus cannot be reusable for other contexts. Therefore, researchers have recently designed various types of MLG authoring tools. However, these authoring tools are not always adapted to non-computer-scientists or non-game-designers. Hence, we propose in this paper to focus on five existing MLG authoring tools, in order to assess their features and usability with the help of five teachers, who are used to organizing educational field trips. In the second part of this paper, we present an approach for designing a MLG authoring tool, based on the lacks identified through the analysis, and tailored to the teachers' different profiles and needs.\",\n",
       " 'Individual thermal comfort perception gives important feedback signals for energy efficient control of smart buildings. However, there is no effective method to measure real-time thermal comfort status of individual occupant until now. For overcoming this challenge, a novel macro posed based non-invasive perception method for thermal comfort (NIMAP) was presented. The occupant pose images were captured by normal phone camera (computer or cell phone) and the corresponding 2D coordinates can be obtained. Based on this, a novel pose recognition algorithm for thermal comfort, including 12 sub-algorithms, was presented. The 12 thermal comfort related macro poses can be recognized. Further, based on Fanger theory, 369 subjects were invited for subjective questionnaire survey. 3 human occupants participated in the validation of the proposed method and massive data were collected. All the 12 thermal comfort related poses can be recognized effectively.',\n",
       " 'Brain Computer Interfaces (BCIs) based on visual evoked potentials (VEP) allow for spelling from a keyboard of flashing characters. Among VEP BCIs, code-modulated visual evoked potentials (c-VEPs) are designed for high-speed communication . In c-VEPs, all characters flash simultaneously. In particular, each character flashes according to a predefined 63-bit binary sequence (m-sequence), circular-shifted by a different time lag. For a given character, the m-sequence evokes a VEP in the electroencephalogram (EEG) of the subject, which can be used as a template. This template is obtained during a calibration phase at the beginning of each session. Then, the system outputs the desired character after a predefined number of repetitions by estimating its time lag with respect to the template. Our work avoids the calibration phase, by extracting from the VEP relative lags between successive characters, and predicting the full word using a dictionary.',\n",
       " 'One of the big restrictions in brain computer interface field is the very limited training samples, it is difficult to build a reliable and usable system with such limited data. Inspired by generative adversarial networks, we propose a conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks method to generate more artificial EEG signal automatically for data augmentation to improve the performance of convolutional neural networks in brain computer interface field and overcome the small training dataset problems. We evaluate the proposed cDCGAN method on BCI competition dataset of motor imagery. The results show that the generated artificial EEG data from Gaussian noise can learn the features from raw EEG data and has no less than the classification accuracy of raw EEG data in the testing dataset. Also by using generated artificial data can effectively improve classification accuracy at the same model with limited training data.',\n",
       " 'Developing information technology to democratize scientific knowledge and support citizen empowerment is a challenging task. In our case, a local community suffered from air pollution caused by industrial activity. The residents lacked the technological fluency to gather and curate diverse scientific data to advocate for regulatory change. We collaborated with the community in developing an air quality monitoring system which integrated heterogeneous data over a large spatial and temporal scale. The system afforded strong scientific evidence by using animated smoke images, air quality data, crowdsourced smell reports, and wind data. In our evaluation, we report patterns of sharing smoke images among stakeholders. Our survey study shows that the scientific knowledge provided by the system encourages agonistic discussions with regulators, empowers the community to support policy making, and rebalances the power relationship between stakeholders.',\n",
       " \"Assessing the influence of a scholar's work is an important task for funding organizations, academic departments, and researchers. Common methods, such as measures of citation counts, can ignore much of the nuance and multidimensionality of scholarly influence. We present an approach for generating dynamic visualizations of scholars' careers. This approach uses an animated node-link diagram showing the citation network accumulated around the researcher over the course of the career in concert with key indicators, highlighting influence both within and across fields. We developed our design in collaboration with one funding organization---the Pew Biomedical Scholars program---but the methods are generalizable to visualizations of scholarly influence. We applied the design method to the Microsoft Academic Graph, which includes more than 120 million publications. We validate our abstractions throughout the process through collaboration with the Pew Biomedical Scholars program officers and summative evaluations with their scholars.\",\n",
       " 'BRICKxAR is an Augmented Reality-based construction method applied to LEGO as a case study. With BRICKxAR, real LEGO brick construction is guided by virtual bricks in the right place at the right time, step by step. Virtual and real object occlusions are implemented to enable a natural appearance of virtual bricks inside real bricks. LEGO players\\' hand detection and occlusion are realized to allow a realistic immersive AR experience, in which virtual bricks can be \"grasped\" by the real hand, facilitating hand-eye coordination in AR. High accuracy and high precision of registration, i.e. the virtual - physical model alignment, are achieved. In the best case, the average error of registration is less than 1mm throughout the model. BRICKxAR is expected to enhance Learning Through Play, as well as assembly and construction by human workers. In the experiment, LEGO Arc de Triomphe is built completely with BRICKxAR, without the instruction booklet.',\n",
       " 'As various driving automation system (DAS) are commonly used in the vehicle, the over-trust in the DAS may put the driver in the risk. In order to prevent the over-trust while driving, the trust state of the driver should be recognized. However, description variables of the trust state are not distinct. This paper assumed that the outward expressions of a driver can represent the trust state of him/her-self. The explicit behaviors when driving with DAS is seen as those outward expressions. In the experiment, a driving simulator with a driver monitoring system was used for simulating a vehicle with the adaptive cruise control (ACC) and observing the motion information of the driver. Results show that if the driver completely trusted in the ACC, then 1) the participants were likely to put their feet far away from the pedals; 2) the operational intervention of the driver will delay in dangerous situations. In the future, a machine learning model will be tried to predict the trust state by using the motion information of the driver.',\n",
       " \"The recent growth of sophisticated digital gaming technologies has spawned an \\\\$8.1B industry around using these games for pedagogical purposes. Though Digital Game-Based Learning Systems have been adopted by industries ranging from military to medical applications, these systems continue to rely on traditional measures of explicit interactions to gauge player performance which can be subject to guessing and other factors unrelated to actual performance. This study presents a novel implicit eye-tracking based metric for digital game-based learning environments. The proposed metric introduces a weighted eye-tracking measure of traditional in-game scoring to consider the mental schema of a player's decision making. In order to validate the efficacy of this metric, we conducted an experiment with 25 participants playing a game designed to evaluate Chinese cultural competency and communication. This experiment showed strong correlation between the novel eye-tracking performance metric and traditional measures of in-game performance.\",\n",
       " 'Audio description, a form of trans-modal media translation, allows people who are blind or visually impaired access to visually-oriented, socio-cultural, or historical public discourse alike. Although audio description has gained more prominence in media policy and research lately, it rarely has been studied empirically. Yet this paper presents quantitative and qualitative survey data on its challenges and opportunities, through the analysis of responses from 483 participants in a national sample, with 334 of these respondents being blind. Our results give insight into audio description use in broadcast TV, streaming services, for physical media, such as DVDs, and in movie theaters. We further discover a multiplicity of barriers and hindrances which prevent a better adoption and larger proliferation of audio description. In our discussion, we present a possible answer to these problems - the UniDescription Project - a media ecosystem for the creation, curation, and dissemination of audio description for multiple media platforms.',\n",
       " 'Static visualizations have analytic and expressive value. However, many interactive tasks cannot be completed using static visualizations. As datasets grow in size and complexity, static visualizations start losing their analytic and expressive power for interactive data exploration. Despite this limitation of static visualizations, there are still many cases where visualizations are limited to being static (e.g., visualizations on presentation slides or posters). We believe in many of these cases, static visualizations will benefit from allowing users to perform interactive tasks on them. Inspired by the introduction of numerous commercial personal augmented reality (AR) devices, we propose an AR solution that allows interactive data exploration of datasets on static visualizations. In particular, we present a prototype system named VisAR that uses the Microsoft Hololens to enable users to complete interactive tasks on static visualizations.',\n",
       " \"Current technologies have enabled us to track and quantify our physical state and behavior. Self-tracking aims to achieve increased awareness to decrease undesired behaviors and lead to a healthier lifestyle. However, inappropriately communicated self-tracking results might cause the opposite effect. In this work, we propose a subtle self-tracking feedback by mirroring the self's state into an artificial agent. By eliciting empathy towards the artificial agent and fostering helping behaviors, users would help themselves as well. Finally, we reflected on the implications of this design framework, and the methodology to design and implement it. A series of interviews to expert designers pointed out to the importance of having multidisciplinary teams working in parallel. Moreover, an agile methodology with a sprint zero for the initial design, and shifted user research, design, and implementation sprints were proposed. Similar systems with data flow and hardware dependencies would also benefit from the proposed agile design process.\",\n",
       " \"Online writers and journalism media are increasingly combining visualization (and other multimedia content) with narrative text to create narrative visualizations. Often, however, the two elements are presented independently of one another. We propose an approach to automatically integrate text and visualization elements. We begin with a writer's narrative that presumably can be supported with visual data evidence. We leverage natural language processing, quantitative narrative analysis, and information visualization to (1) automatically extract narrative components (who, what, when, where) from data-rich stories, and (2) integrate the supporting data evidence with the text to develop a narrative visualization. We also employ bidirectional interaction from text to visualization and visualization to text to support reader exploration in both directions. We demonstrate the approach with a case study in the data-rich field of sports journalism.\",\n",
       " \"Engagement in educational games, a recently popular academic topic, has been shown to increase learning performance, as well as a number of attitudinal factors, such as intrinsic interest and motivation. However, there is a lack of research on how games can be designed to promote engagement. This mixed methods case study aimed to discover effective game elements for promoting 17-18 year old high school students' engagement with an educational game. Using within-case and cross-case analyses and triangulated data, 10 elements emerged and were categorized into the constructs of story, gameplay, and atmosphere. Examples and connections to the literature for each element are reported. Findings implicate that educational game design for both learning and engagement is composed of educational-game specific elements, game design for solely engagement is similar for both educational and entertainment games, and a gap on educational game design technique instead of theory should be addressed to further benefit educational game development.\",\n",
       " 'We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.',\n",
       " 'Intelligent personal assistants (IPAs) are supposed to help us multitask. Yet the impact of IPA use on multitasking is not clearly quantified, particularly in situations where primary tasks are also language based. Using a dual task paradigm, our study observes how IPA interactions impact two different types of writing primary tasks; copying and generating content. We found writing tasks that involve content generation, which are more cognitively demanding and share more of the resources needed for IPA use, are significantly more disrupted by IPA interaction than less demanding tasks such as copying content. We discuss how theories of cognitive resources, including multiple resource theory and working memory, explain these results. We also outline the need for future work how interruption length and relevance may impact primary task performance as well as the need to identify effects of interruption timing in user and IPA led interruptions.',\n",
       " 'Designing for situational awareness could lead to better solutions for disabled people, likewise, exploring the needs of disabled people could lead to innovations that can address situational impairments. This in turn can create non-stigmatising assistive technology for disabled people from which eventually everyone could benefit. In this paper, we investigate the potential for advanced haptics to compliment the graphical user interface of mobile devices, thereby enhancing user experiences of all people in some situations (e.g. sunlight interfering with interaction) and visually impaired people. We explore technical solutions to this problem space and demonstrate our justification for a focus on the creation of kinaesthetic force feedback. We propose initial design concepts and studies, with a view to co-create delightful and expressive haptic interactions with potential users motivated by scenarios of situational and permanent impairments.',\n",
       " \"Engaging residential communities with each other and with management remains a challenge. Housing providers deploy a variety of engagement strategies, some of which are supported by digital technologies. Their individual success is varied and integrated, multipronged approaches are seen to be more successful. As part of those, it is important to address people's perceptions of community and places, as well as any practical issues that they face. We present the design and evaluation of Photo Screen, a situated, public photo taking and viewing screen which was deployed in the context of a new flagship housing estate as part of a range of community engagement measures. In a new context, we confirm the high levels of engagement that can be achieved with this simple mechanism. We propose that photo 'tagging' might offer a second-stage engagement mechanism and enable meaningful dialogue between residents and management. Finally, we discuss how this playful activity allowed residents to positively shape the perception of their community.\",\n",
       " \"Determining and identifying opportune moments for interruptions is a challenging task in Ubiquitous Computing and Human-Computer-Interaction. The current state-of-the-art approaches do this by identifying breakpoints either in user tasks, activities or by processing social relationships and contents of interruptions. However, from a psychological perspective, not all of these breakpoints represent opportune moments for interruptions. In this paper, we propose a new concept in the field of interruptibility. The concept is based on role theory and psychological interruption research. In particular, we argue that social roles which define sets of norms, expectations, rules and behaviours can provide useful information about the user's current context that can be used to enhance interruption management systems. Based on this concept, we propose a prototype system architecture that uses social roles to detect opportune moments for interruptions.\",\n",
       " \"Situation awareness (SA) is an important constituent in human information processing and essential in pilots' decision-making processes. Acquiring and maintaining appropriate levels of SA is critical in aviation environments as it affects all decisions and actions taking place in flights and air traffic control. This paper provides an overview of recent measurement models and approaches to establishing and enhancing SA in aviation environments. Many aspects of SA are examined including the classification of SA techniques into six categories, and different theoretical SA models from individual, to shared or team, and to distributed or system levels. Quantitative and qualitative perspectives pertaining to SA methods and issues of SA for unmanned vehicles are also addressed. Furthermore, future research directions regarding SA assessment approaches are raised to deal with shortcomings of the existing state-of-the-art methods in the literature.\",\n",
       " 'Recently, increasing attention has been directed to the study of the speech emotion recognition, in which global acoustic features of an utterance are mostly used to eliminate the content differences. However, the expression of speech emotion is a dynamic process, which is reflected through dynamic durations, energies, and some other prosodic information when one speaks. In this paper, a novel local dynamic pitch probability distribution feature, which is obtained by drawing the histogram, is proposed to improve the accuracy of speech emotion recognition. Compared with most of the previous works using global features, the proposed method takes advantage of the local dynamic information conveyed by the emotional speech. Several experiments on Berlin Database of Emotional Speech are conducted to verify the effectiveness of the proposed method. The experimental results demonstrate that the local dynamic information obtained with the proposed method is more effective for speech emotion recognition than the traditional global features.',\n",
       " 'Understanding human mobility is important for the development of intelligent mobile service robots as it can provide prior knowledge and predictions of human distribution for robot-assisted activities. In this paper, we propose a probabilistic method to model human motion behaviors which is determined by both internal and external factors in an indoor environment. While the internal factors are represented by the individual preferences, aims and interests, the external factors are indicated by the stimulation of the environment. We model the randomness of human macro-level movement, e.g., the probability of visiting a specific place and staying time, under the Bayesian framework, considering the influence of both internal and external variables. We use two case studies in a shopping mall and in a college student dorm building to show the effectiveness of our proposed probabilistic human mobility model. Real surveillance camera data are used to validate the proposed model together with survey data in the case study of student dorm.',\n",
       " 'We live in an emerging hyper-connected era in which people are in contact and interacting with an increasing number of other people and devices. Increasingly, modern IT systems form networks of humans and machines that interact with one another. As machines take a more active role in such networks, they exert an in-creasing level of influence on other participants. We review the existing literature on agency and propose a definition of agency that is practical for describing the capabilities and impact human and machine actors may have in a human-machine network. On this basis, we discuss and demonstrate the impact and trust implica-tions for machine actors in human-machine networks for emergency decision support, healthcare and future smart homes. We maintain that machine agency not only facilitates human to machine trust, but also interpersonal trust; and that trust must develop to be able to seize the full potential of future technology.',\n",
       " 'The use of computer-aided and web-based educational technologies such as Virtual Learning Environments (VLE) has increased significantly in the recent past. One example of such a VLE is Virtual Interactive Engineering on the Web (VIEW). VIEW is a 3D virtual, interactive, student centered, framework of web-based modules based on the Extensible 3D standard. These modules are dedicated to the improvement of student success and learning. In this paper, an overview of the recent developments in VIEW along with associated assessment results is presented. An experimental study was also performed to compare the learning experience and performance of students in a physical dissection activity vs. that in a virtual dissection activity using a VIEW module. The results of this study show that students can meet given learning objectives and that there is limited difference in their learning and performance irrespective of a physical or virtual setting.',\n",
       " 'A goal of Interactive Machine Learning (IML) is to enable people without specialized training to teach agents how to perform tasks. Many of the existing machine learning algorithms that learn from human instructions are evaluated using simulated feedback and focus on how quickly the agent learns. While this is valuable information, it ignores important aspects of the human-agent interaction such as frustration. In this paper, we present the Newtonian Action Advice agent, a new method of incorporating human verbal action advice with Reinforcement Learning (RL) in a way that improves the human-agent interaction. In addition to simulations, we validated the Newtonian Action Advice algorithm by conducting a human-subject experiment. The results show that Newtonian Action Advice can perform better than Policy Shaping, a state-of-the-art IML algorithm, both in terms of RL metrics like cumulative reward and human factors metrics like frustration.',\n",
       " 'We propose a toolkit for creating Tangible Out-of-Body Experiences: exposing the inner states of users using physiological signals such as heart rate or brain activity. Tobe can take the form of a tangible avatar displaying live physiological readings to reflect on ourselves and others. Such a toolkit could be used by researchers and designers to create a multitude of potential tangible applications, including (but not limited to) educational tools about Science Technologies Engineering and Mathematics (STEM) and cognitive science, medical applications or entertainment and social experiences with one or several users or Tobes involved. Through a co-design approach, we investigated how everyday people picture their physiology and we validated the acceptability of Tobe in a scientific museum. We also give a practical example where two users relax together, with insights on how Tobe helped them to synchronize their signals and share a moment.',\n",
       " 'We explain and provide examples of a formalism that supports the methodology of discovering how to adapt and personalize technology by combining randomized experiments with variables associated with user models. We characterize a formal relationship between the use of technology to conduct A/B experiments and use of technology for adaptive personalization. The MOOClet Formalism [11] captures the equivalence between experimentation and personalization in its conceptualization of modular components of a technology. This motivates a unified software design pattern that enables technology components that can be compared in an experiment to also be adapted based on contextual data, or personalized based on user characteristics. With the aid of a concrete use case, we illustrate the potential of the MOOClet formalism for a methodology that uses randomized experiments of alternative micro-designs to discover how to adapt technology based on user characteristics, and then dynamically implements these personalized improvements in real time.',\n",
       " 'Today, many of the aid systems deployed for visually impaired people are mostly made for a single purpose. Be it navigation, object detection, or distance perceiving. Also, most of the deployed aid systems use indoor navigation which requires a pre-knowledge of the environment. These aid systems often fail to help visually impaired people in the unfamiliar scenario. In this paper, we propose an aid system developed using object detection and depth perceivement to navigate a person without dashing into an object. The prototype developed detects 90 different types of objects and compute their distances from the user. We also, implemented a navigation feature to get input from the user about the target destination and hence, navigate the impaired person to his/her destination using Google Directions API. With this system, we built a multi-feature, high accuracy navigational aid system which can be deployed in the wild and help the visually impaired people in their daily life by navigating them effortlessly to their desired destination.',\n",
       " 'Web search is among the most ubiquitous online activities, commonly used to acquire new knowledge and to satisfy learning-related objectives through informational search sessions. The importance of learning as an outcome of web search has been recognized widely, leading to a variety of research at the intersection of information retrieval, human computer interaction and learning-oriented sciences. Given the lack of explicit information, understanding of users and their learning needs has to be derived from their search behavior and resource interactions. In this paper, we introduce the involved research challenges and survey related work on the detection of learning needs, understanding of users, e.g. with respect to their knowledge state, learning tasks and learning progress throughout a search session as well as the actual consideration of learning needs throughout the retrieval and ranking process. In addition, we summarise our own research contributing to the aforementioned tasks and describe our research agenda in this context.',\n",
       " 'Throughout the past decade, many studies have classified human emotions using only a single sensing modality such as face video, electroencephalogram (EEG), electrocardiogram (ECG), galvanic skin response (GSR), etc. The results of these studies are constrained by the limitations of these modalities such as the absence of physiological biomarkers in the face-video analysis, poor spatial resolution in EEG, poor temporal resolution of the GSR etc. Scant research has been conducted to compare the merits of these modalities and understand how to best use them individually and jointly. Using multi-modal AMIGOS dataset, this study compares the performance of human emotion classification using multiple computational approaches applied to face videos and various bio-sensing modalities. Using a novel method for compensating physiological baseline we show an increase in the classification accuracy of various approaches that we use. Finally, we present a multi-modal emotion-classification approach in the domain of affective computing research.',\n",
       " 'Analytical tools in business management are understood as a combination of information technologies and quantitative methods used to assist stakeholders to make better decisions. The contemporary business environment is dramatically changing by the massive accumulation of data. Now, as never before, the use of analytical tools must be expanded to take advantage of this growing digital universe. This article will apply the laddering technique to see how personal values (or managerial functions) influence a companys adoption of analytical tools. A set of ten in-depth interviews are conducted with CEOs, analytics consultants, academics and businessmen in order to establish quantitative relations among attributes, consequences and personal values. Two easy-to-read outputs are provided to interpret our results. The most important links are quantitatively associated through an implication matrix, and then visually represented on a hierarchical value map. Guidelines for improving the use of analytical tools are provided in the last section',\n",
       " \"Social Networking Site (SNS) is a great innovation of modern times. Facebook, Twitter etc. have become an everyday part of peoples' life. Among all SNSs, Facebook is the most popular social network all over the world. Bangladesh is no exception. People of Bangladesh use Facebook for social communication, online shopping, business, knowledge and experience sharing etc. As well as the various uses of SNSs, people sometimes find themselves involved in real life violence, provoked by some social media posts or activities. In this paper, we discussed some case studies in which real life violence is originated based on Facebook activities in Bangladesh. Facebook was used in these incidents intentionally or unintentionally mostly as a tool to trigger hatred and violence. We analyzed and discussed the real-world consequences of these virtual activities in social media. Lastly, we recommended possible future measurements to prevent such violence.\",\n",
       " 'As we increasingly delegate important decisions to intelligent systems, it is essential that users understand how algorithmic decisions are made. Prior work has often taken a technocentric approach to transparency. In contrast, we explore empirical user-centric methods to better understand user reactions to transparent systems. We assess user reactions to global and incremental feedback in two studies. In Study 1, users anticipated that the more transparent incremental system would perform better, but retracted this evaluation after experience with the system. Qualitative data suggest this may arise because incremental feedback is distracting and undermines simple heuristics users form about system operation. Study 2 explored these effects in depth, suggesting that users may benefit from initially simplified feedback that hides potential system errors and assists users in building working heuristics about system operation. We use these findings to motivate new progressive disclosure principles for transparency in intelligent systems.',\n",
       " \"Working in complex industrial facilities requires spatial navigation skills that people build up with time and field experience. Training sessions consisting in guided tours help discover places but they are insufficient to become intimately familiar with their layout. They imply passive learning postures, are time-limited and can be experienced only once because of organization constraints and potential interferences with ongoing activities in the buildings. To overcome these limitations and improve the acquisition of navigation skills, we developed Indy, a virtual reality system consisting in a collaborative game of treasure hunting. It has several key advantages: it focuses learners' attention on navigation tasks, implies their active engagement and provides them with feedbacks on their achievements. Virtual reality makes it possible to multiply the number and duration of situations that learners can experience to better consolidate their skills. This paper discusses the main design principles and a typical usage scenario of Indy.\",\n",
       " \"This paper proposes an image-processing-based method for personalization of calorie consumption assessment during exercising. An experiment is carried out where several actions are required in an exercise called broadcast gymnastics, especially popular in Japan and China. We use Kinect, which captures body actions by separating the body into joints and segments that contain them, to monitor body movements to test the velocity of each body joint and capture the subject's image for calculating the mass of each body joint that differs for each subject. By a kinetic energy formula, we obtain the kinetic energy of each body joint, and calories consumed during exercise are calculated in this process. We evaluate the performance of our method by benchmarking it to Fitbit, a smart watch well-known for health monitoring during exercise. The experimental results in this paper show that our method outperforms a state-of-the-art calorie assessment method, which we base on and improve, in terms of the error rate from Fitbit's ground-truth values.\",\n",
       " 'While clustering is one of the most popular methods for data mining, analysts lack adequate tools for quick, iterative clustering analysis, which is essential for hypothesis generation and data reasoning. We introduce Clustrophile, an interactive tool for iteratively computing discrete and continuous data clusters, rapidly exploring different choices of clustering parameters, and reasoning about clustering instances in relation to data dimensions. Clustrophile combines three basic visualizations -- a table of raw datasets, a scatter plot of planar projections, and a matrix diagram (heatmap) of discrete clusterings -- through interaction and intermediate visual encoding. Clustrophile also contributes two spatial interaction techniques, $\\\\textit{forward projection}$ and $\\\\textit{backward projection}$, and a visualization method, $\\\\textit{prolines}$, for reasoning about two-dimensional projections obtained through dimensionality reductions.',\n",
       " 'Comparing many long time series is challenging to do by hand. Clustering time series enables data analysts to discover relevance between and anomalies among multiple time series. However, even after reasonable clustering, analysts have to scrutinize correlations between clusters or similarities within a cluster. We developed SAX Navigator, an interactive visualization tool, that allows users to hierarchically explore global patterns as well as individual observations across large collections of time series data. Our visualization provides a unique way to navigate time series that involves a \"vocabulary of patterns\" developed by using a dimensionality reduction technique,Symbolic Aggregate approXimation(SAX). With SAX, the time series data clusters efficiently and is quicker to query at scale. We demonstrate the ability of SAX Navigator to analyze patterns in large time series data based on three case studies for an astronomy data set. We verify the usability of our system through a think-aloud study with an astronomy domain scientist.',\n",
       " 'With an ever-increasing number of mobile devices competing for our attention, quantifying when, how often, or for how long users visually attend to their devices has emerged as a core challenge in mobile human-computer interaction. Encouraged by recent advances in automatic eye contact detection using machine learning and device-integrated cameras, we provide a fundamental investigation into the feasibility of quantifying visual attention during everyday mobile interactions. We identify core challenges and sources of errors associated with sensing attention on mobile devices in the wild, including the impact of face and eye visibility, the importance of robust head pose estimation, and the need for accurate gaze estimation. Based on this analysis, we propose future research directions and discuss how eye contact detection represents the foundation for exciting new applications towards next-generation pervasive attentive user interfaces.',\n",
       " 'Clinical decision support tools (DST) promise improved healthcare outcomes by offering data-driven insights. While effective in lab settings, almost all DSTs have failed in practice. Empirical research diagnosed poor contextual fit as the cause. This paper describes the design and field evaluation of a radically new form of DST. It automatically generates slides for clinicians\\' decision meetings with subtly embedded machine prognostics. This design took inspiration from the notion of \"Unremarkable Computing\", that by augmenting the users\\' routines technology/AI can have significant importance for the users yet remain unobtrusive. Our field evaluation suggests clinicians are more likely to encounter and embrace such a DST. Drawing on their responses, we discuss the importance and intricacies of finding the right level of unremarkableness in DST design, and share lessons learned in prototyping critical AI systems as a situated experience.',\n",
       " 'Environmental concerns have driven an interest in sustainable smart cities, through the monitoring and optimisation of networked infrastructure processes. At the same time, there are concerns about who these interventions and services are for, and who benefits. HCI researchers and designers interested in civic life have started to call for the democratisation of urban space through resistance and political action to challenge state and corporate claims. This paper aims to add to the growing body of critical and civic led smart city literature in HCI by leveraging concepts from the environmental humanities about more than human worlds, as a way to shift understandings within HCI of smart cities away from the exceptional and human centered, towards a more inclusive understanding that incorporates and designs for other others and other species. We illustrate through a case study that involved codesigning Internet of Things with urban agricultural communities, possibilities for creating more environmentally and socially just smart cities.',\n",
       " 'EMG-based gesture recognition shows promise for human-machine interaction. Systems are often afflicted by signal and electrode variability which degrades performance over time. We present an end-to-end system combating this variability using a large-area, high-density sensor array and a robust classification algorithm. EMG electrodes are fabricated on a flexible substrate and interfaced to a custom wireless device for 64-channel signal acquisition and streaming. We use brain-inspired high-dimensional (HD) computing for processing EMG features in one-shot learning. The HD algorithm is tolerant to noise and electrode misplacement and can quickly learn from few gestures without gradient descent or back-propagation. We achieve an average classification accuracy of 96.64% for five gestures, with only 7% degradation when training and testing across different days. Our system maintains this accuracy when trained with only three trials of gestures; it also demonstrates comparable accuracy with the state-of-the-art when trained with one trial.',\n",
       " 'In gaming, customizing individual characters, can create personal bonds between players and their characters. Hence, character customization is a standard component in many games. While mobile Augmented Reality (AR) games become popular, to date, no 3D character editor for AR games exists. We investigate the feasibility of 3D character customization for smartphone-based AR in an iterative design process.\\n  Specifically, we present findings from creating AR prototypes in a handheld AR setting. In a first user study, we found that a tangible AR prototype resulted in higher hedonistic measures than a camera-based approach. In a follow up study, we compared the tangible AR prototype with a non-AR touchscreen version for selection, scaling, translation and rotation tasks in a 3D character customization setting. The tangible AR version resulted in significantly better results for stimulation and novelty measures than the non-AR version. At the same time, it maintained a proficient level in pragmatic measures such as accuracy and efficiency.',\n",
       " 'According to Brooke* \"Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts.\" That is, one cannot speak of usability without specifying what that particular usability is characterized by. Driven by the feedback of a reviewer at an international conference, I explore in which way one can precisely specify the kind of usability they are investigating in a given setting. Finally, I come up with a formalism that defines usability as a quintuple comprising the elements level of usability metrics, product, users, goals and context of use. Providing concrete values for these elements then constitutes the investigated type of usability. The use of this formalism is demonstrated in two case studies.\\n  * J. Brooke. SUS: A \"quick and dirty\" usability scale. In P. W. Jordan, B. Thomas, B. A. Weerdmeester, and A. L. McClelland, editors, Usability Evaluation in Industry. Taylor and Francis, 1996.',\n",
       " \"Mood disorders are common and associated with significant morbidity and mortality. Early diagnosis has the potential to greatly alleviate the burden of mental illness and the ever increasing costs to families and society. Mobile devices provide us a promising opportunity to detect the users' mood in an unobtrusive manner. In this study, we use a custom keyboard which collects keystrokes' meta-data and accelerometer values. Based on the collected time series data in multiple modalities, we propose a deep personalized mood prediction approach, called {\\\\pro}, by integrating convolutional and recurrent deep architectures as well as exploring each individual's circadian rhythm. Experimental results not only demonstrate the feasibility and effectiveness of using smart-phone meta-data to predict the presence and severity of mood disturbances in bipolar subjects, but also show the potential of personalized medical treatment for mood disorders.\",\n",
       " 'In human-in-the-loop machine learning, the user provides information beyond that in the training data. Many algorithms and user interfaces have been designed to optimize and facilitate this human--machine interaction; however, fewer studies have addressed the potential defects the designs can cause. Effective interaction often requires exposing the user to the training data or its statistics. The design of the system is then critical, as this can lead to double use of data and overfitting, if the user reinforces noisy patterns in the data. We propose a user modelling methodology, by assuming simple rational behaviour, to correct the problem. We show, in a user study with 48 participants, that the method improves predictive performance in a sparse linear regression sentiment analysis task, where graded user knowledge on feature relevance is elicited. We believe that the key idea of inferring user knowledge with probabilistic user models has general applicability in guarding against overfitting and improving interactive machine learning.',\n",
       " 'Thermostats are primary interfaces for occupants of office buildings to express their comfort preferences. However, standard thermostats are often ineffective due to inaccessibility, lack of information, or limited responsiveness, leading to occupant discomfort. Software thermostats based on web or smartphone applications provide alternative interfaces to occupants with minimal deployment cost. However, their usage and effectiveness have not been studied extensively in real settings. In this paper we present Genie, a novel software-augmented thermostat that we deployed and studied at our university over a period of 21 months. Our data shows that providing wider thermal control to users does not lead to system abuse and that the effect on energy consumption is minimal while improving comfort and energy awareness. We believe that increased introduction of software thermostats in office buildings will have important effects on comfort and energy consumption and we provide key design recommendations for their implementation and deployment.',\n",
       " 'This paper presents a telepresence interaction framework based on touchscreen and telepresence-robot technologies. The core of the framework is a new user interface, Touchable live video Image based User Interface, called TIUI. The TIUI allows a remote operator to not just drive the telepresence robot but operate and interact with real objects by touching their live video images on a pad with finger touch gestures. We implemented a telepresence interaction system which is composed of a telepresence robot and tele-interactive objects located in a local space, the TIUI of a pad located in a remote space, and the wireless networks connecting the two spaces. Our system can be a perfect embodiment of a remote operator to do most of daily living tasks, such as opening a door, drawing a curtain, pushing a wheelchair, and other like tasks. The evaluation and demonstration results show the effectiveness and promising applications of our system.',\n",
       " 'The amount of visual communication we are facing is rapidly increasing, and skills to process, understand, and generate visual representations are in high demand. Especially students focusing on computer graphics and visualization can benefit from a more diverse education on visual literacy, as they often have to work on graphical representations for broad masses after their graduation. Our proposed teaching approach incorporates basic design thinking principles into traditional visualization and graphics education. Our course was inspired by the book Dear Data that was the subject of a lively discussion at the closing capstone of IEEE VIS 2017. The paper outlines our 12-week teaching experiment and summarizes the results extracted from accompanying questionnaires and interviews. In particular, we provide insights into the creation process and pain points of visualization novices, discuss the observed interplay between visualization tasks and design thinking, and finally draw design implications for visual literacy education in general.',\n",
       " 'In this paper, we consider the problem of tracking the eye-gaze of individuals while they engage in reading. Particularly, we develop ways to accurately track the line being read by an individual using commercially available eye tracking devices. Such an approach will enable futuristic functionalities such as comprehension evaluation, interest level detection, and user-assisting applications like hands-free navigation and automatic scrolling. Existing commercial eye trackers provide an estimated location of the eye-gaze fixations every few milliseconds. However, this estimated data is found to be very noisy. As such, commercial eye-trackers are unable to accurately track lines while reading. In this paper we propose several statistical models to bridge the commercial gaze tracker outputs and eye-gaze patterns while reading. We then employ hidden Markov models to parametrize these statistical models and to accurately detect the line being read. The proposed approach is shown to yield an improvement of over 20% in line detection accuracy.',\n",
       " 'When viewing stereoscopic displays, people may not always be able to stay exactly in front of the display. It is known that viewing stereoscopic display from different vertical angles lead to different visual discomfort. However, the effects of horizontal viewing angle on stereoscopic visual discomfort have been rarely investigated, especially for household stereoscopic displays. In this study, subjects were required to view a stereoscopic display from various horizontal viewing angles, and assessed their visual discomfort during viewing. The visual stimuli have various amount of disparities: positive disparity, negative disparity or zero disparity. Results showed that the visual discomfort changes with horizontal viewing angle, and greater angles generally lead to more serious visual discomfort. Furthermore, the relationship between visual discomfort and horizontal viewing angle can be approximately expressed by a quadratic function.',\n",
       " 'The rise of machine learning has brought closer scrutiny to intelligent systems, leading to calls for greater transparency and explainable algorithms. We explore the effects of transparency on user perceptions of a working intelligent system for emotion detection. In exploratory Study 1, we observed paradoxical effects of transparency which improves perceptions of system accuracy for some participants while reducing accuracy perceptions for others. In Study 2, we test this observation using mixed methods, showing that the apparent transparency paradox can be explained by a mismatch between participant expectations and system predictions. We qualitatively examine this process, indicating that transparency can undermine user confidence by causing users to fixate on flaws when they already have a model of system operation. In contrast transparency helps if users lack such a model. Finally, we revisit the notion of transparency and suggest design considerations for building safe and successful machine learning systems based on our insights.',\n",
       " \"We present the concept of X-Vision, an enhanced Augmented Reality (AR)-based visualization tool, with the real-time sensing capability in a tagged environment. We envision that this type of a tool will enhance the user-environment interaction and improve the productivity in factories, smart-spaces, home & office environments, maintenance/facility rooms and operation theatres, etc. In this paper, we describe the design of this visualization system built upon combining the object's pose information estimated by the depth camera and the object's ID & physical attributes captured by the RFID tags. We built a physical prototype of the system demonstrating the projection of 3D holograms of the objects encoded with sensed information like water-level and temperature of common office/household objects. The paper also discusses the quality metrics used to compare the pose estimation algorithms for robust reconstruction of the object's 3D data.\",\n",
       " \"Digitally presenting physiological signals as biofeedback to users raises awareness of both body and mind. This paper describes the effectiveness of conveying a physiological signal often overlooked for communication: breathing. We present the design and development of digital breathing patterns and their evaluation along three output modalities: visual, audio, and haptic. We also present Breeze, a wearable pendant placed around the neck that measures breathing and sends biofeedback in real-time. We evaluated how the breathing patterns were interpreted in a fixed environment and gathered qualitative data on the wearable device's design. We found that participants intentionally modified their own breathing to match the biofeedback, as a technique for understanding the underlying emotion. Our results describe how the features of the breathing patterns and the feedback modalities influenced participants' perception. We include guidelines and suggested use cases, such as Breeze being used by loved ones to increase connectedness and empathy.\",\n",
       " 'Even if only the acoustic channel is considered, human communication is highly multi-modal. Non-lexical cues provide a variety of information such as emotion or agreement. The ability to process such cues is highly relevant for spoken dialog systems, especially in assistance systems. In this paper we focus on the recognition of non-lexical confirmations such as \"mhm\", as they enhance the system\\'s ability to accurately interpret human intent in natural communication. The architecture uses a Support Vector Machine to detect confirmations based on acoustic features. In a systematic comparison, several feature sets were evaluated for their performance on a corpus of human-agent interaction in a setting with naive users including elderly and cognitively impaired people. Our results show that using stacked formants as features yield an accuracy of 84% outperforming regular formants and MFCC or pitch based features for online classification.',\n",
       " 'This paper presents an open source tool for testing the recognition accuracy of Chinese handwriting input methods. The tool consists of two modules, namely the PC and Android mobile client. The PC client reads handwritten samples in the computer, and transfers them individually to the Android client in accordance with the socket communication protocol. After the Android client receives the data, it simulates the handwriting on screen of client device, and triggers the corresponding handwriting recognition method. The recognition accuracy is recorded by the Android client. We present the design principles and describe the implementation of the test platform. We construct several test datasets for evaluating different handwriting recognition systems, and conduct an objective and comprehensive test using six Chinese handwriting input methods with five datasets. The test results for the recognition accuracy are then compared and analyzed.',\n",
       " 'Experts in different domains rely increasingly on simulation models of complex processes to reach insights, make decisions, and plan future projects. These models are often used to study possible trade-offs, as experts try to optimise multiple conflicting objectives in a single investigation. Understanding all the model intricacies, however, is challenging for a single domain expert. We propose a simple approach to support multiple experts when exploring complex model results. First, we reduce the model exploration space, then present the results on a shared interactive surface, in the form of a scatterplot matrix and linked views. To explore how multiple experts analyse trade-offs using this setup, we carried out an observational study focusing on the link between expertise and insight generation during the analysis process. Our results reveal the different exploration strategies and multi-storyline approaches that domain experts adopt during trade-off analysis, and inform our recommendations for collaborative model exploration systems.',\n",
       " \"The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams' challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by industry practitioners and solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address industry practitioners' needs.\",\n",
       " \"Most of today's scientific research relies on computers and software not only for administrational tasks, but also for processing scientific information. Examples of such computer-aided research are the analysis of experimental data or the simulation of phenomena based on theoretical models. With the rapid increase of computational power, scientific software has integrated more and more complex scientific knowledge in a black-box fashion. As a consequence, its users do not know, and don't even have a chance of finding out, which models or assumptions their computations are based on. The black-box nature of scientific software has thereby become a major cause of mistakes. The present work starts with an analysis of this situation from the point of view of human-computer interaction in scientific research. It identifies the key role of digital scientific notations at the human-computer interface, and describes a proof-of-concept implementation of such a digital scientific notation for scientific models formulated as mathematical equations.\",\n",
       " 'Virtual laboratories are the new online educational trend for communicating to students practical skills of science. In this paper we report on a comparison of techniques for familiarizing distance learning students with a 3D virtual biology laboratory, in order to prepare them for their microscopy experiment in their physical wet lab. Initial training for these students was provided at a distance, via Skype. Their progress was assessed through Pre and Post-tests and compared to those of students who opted to only prepare for their wet lab using the conventional face-to-face educational method, which was provided for all students. Our results provide preliminary answers to questions such as whether the incorporation of a virtual lab in the educational process will improve the quality of distance learning education and whether a virtual lab can be a valuable educational supplement to students enrolled in laboratory courses on Biology.',\n",
       " \"We consider a crowdsourcing platform where workers' responses to questions posed by a crowdsourcer are used to determine the hidden state of a multi-class labeling problem. As workers may be unreliable, we propose to perform sequential questioning in which the questions posed to the workers are designed based on previous questions and answers. We propose a Partially-Observable Markov Decision Process (POMDP) framework to determine the best questioning strategy, subject to the crowdsourcer's budget constraint. As this POMDP formulation is in general intractable, we develop a suboptimal approach based on a $q$-ary Ulam-R\\\\'enyi game. We also propose a sampling heuristic, which can be used in tandem with standard POMDP solvers, using our Ulam-R\\\\'enyi strategy. We demonstrate through simulations that our approaches outperform a non-sequential strategy based on error correction coding and which does not utilize workers' previous responses.\",\n",
       " 'Current machine algorithms for analysis of unstructured data typically show low accuracies due to the need for human-like intelligence. Conversely, though humans are much better than machine algorithms on analyzing unstructured data, they are unpredictable, slower and can be erroneous or even malicious as computing agents. Therefore, a hybrid platform that can intelligently orchestrate machine and human computing resources would potentially be capable of providing significantly better benefits compared to either type of computing agent in isolation. In this paper, we propose a new hybrid human-machine computing platform with integrated service level objectives (SLO) management for complex tasks that can be decomposed into a dependency graph where nodes represent subtasks. Initial experimental results are highly encouraging. To the best of our knowledge, ours is the first work that attempts to design such a hybrid human-machine computing platform with support for addressing the three SLO parameters of accuracy, budget and completion time.',\n",
       " 'In this article, we explore the availability of head-mounted display (HMD) devices which can be coupled in a seamless way with P300-based brain-computer interfaces (BCI) using electroencephalography (EEG). The P300 is an event-related potential appearing about 300ms after the onset of a stimulation. The recognition of this potential on the ongoing EEG requires the knowledge of the exact onset of the stimuli. In other words, the stimulations presented in the HMD must be perfectly synced with the acquisition of the EEG signal. This is done through a process called tagging. The tagging must be performed in a reliable and robust way so as to guarantee the recognition of the P300 and thus the performance of the BCI. An HMD device should also be able to render images fast enough to allow an accurate perception of the stimulations, and equally to not perturb the acquisition of the EEG signal. In addition, an affordable HMD device is needed for both research and entertainment purposes. In this study, we selected and tested two HMD configurations.',\n",
       " 'Nowadays, the development of Web applications supporting distributed user interfaces (DUI) is straightforward. However, it is still hard to find Web sites supporting this kind of user interaction. Although studies on this field have demonstrated that DUI would improve the user experience, users are not massively empowered to manage these kinds of interactions. In this setting, we propose to move the responsibility of distributing both the UI and user interaction, from the application (a Web application) to the client (the Web browser), giving also rise to inter-application interaction distribution. This paper presents a platform for client-side DUI, built on the foundations of Web augmentation and End User Development. The idea is to empower end users to apply an augmentation layer over existing Web applications, considering both frequent use and opportunistic DUI requirements. In this work, we present the architecture and a prototype tool supporting this approach and illustrate the incorporation of some DUI features through case studies.',\n",
       " \"Virtual museums aim to promote access to cultural artifacts. However, they often face the challenge of getting audiences to read and understand a large amount of information in an uncontrolled online environment. Inspired by successful practices in physical museums, we investigated the possible use of guiding questions to engage audiences in virtual museums. To this end, we first identified how to construct questions that are likely to attract audiences through domain expert interviews and mining cultural-related posts in a popular question and answer community. Then in terms of the proactive level for attracting users' attention, we designed two mechanisms to interactively prompt questions: active and passive. Through an online experiment with 150 participants, we showed that having interactive guiding questions encourages browsing and improves content comprehension. We discuss reasons why they are useful by conducting another qualitative comparison and obtained insights about the influence of question category and interaction mechanism.\",\n",
       " 'Critical infrastructure is vulnerable to a broad range of hazards. Timely and effective recovery of critical infrastructure after extreme events is crucial. However, critical infrastructure disaster recovery planning is complicated and involves both domain- and user-centered characteristics and complexities. Recovery planning currently uses few quantitative computer-based tools and instead largely relies on expert judgment. Simulation modeling can simplify domain-centered complexities but not the human factors. Conversely, human-centered design places end-users at the center of design. We discuss the benefits of combining simulation modeling with human-centered design and refer it as human-centered simulation modeling. Human-centered simulation modeling has the capability to make recovery planning simpler and more understandable for critical infrastructure and emergency management experts and other recovery planning decision-makers.',\n",
       " 'Feedback tools help people to monitor information about themselves to improve their health, sustainability practices, or personal well-being. Yet reasoning about personal data (e.g., pedometer counts, blood pressure readings, or home electricity consumption) to gain a deep understanding of your current practices and how to change can be challenging with the data alone. We integrate quantitative feedback data within a personal digital calendar; this approach aims to make the feedback data readily accessible and more comprehensible. We report on an eight-week field study of an on-calendar visualization tool. Results showed that a personal calendar can provide rich context for people to reason about their feedback data. The on-calendar visualization enabled people to quickly identify and reason about regular patterns and anomalies. Based on our results, we also derived a model of the behavior feedback process that extends existing technology adoption models. With that, we reflected on potential barriers for the ongoing use of feedback tools.',\n",
       " 'Although information workers may complain about meetings, they are an essential part of their work life. Consequently, busy people spend a significant amount of time scheduling meetings. We present Calendar.help, a system that provides fast, efficient scheduling through structured workflows. Users interact with the system via email, delegating their scheduling needs to the system as if it were a human personal assistant. Common scheduling scenarios are broken down using well-defined workflows and completed as a series of microtasks that are automated when possible and executed by a human otherwise. Unusual scenarios fall back to a trained human assistant who executes them as unstructured macrotasks. We describe the iterative approach we used to develop Calendar.help, and share the lessons learned from scheduling thousands of meetings during a year of real-world deployments. Our findings provide insight into how complex information tasks can be broken down into repeatable components that can be executed efficiently to improve productivity.',\n",
       " \"This is the preprint version of our paper on 3rd International Workshop on Virtual and Augmented Assistive Technology (VAAT) at IEEE Virtual Reality 2015 (VR2015). An assistive training tool for rehabilitation of dysphonic patients is designed and developed according to the practical clinical needs. The assistive tool employs a space flight game as the attractive logic part, and microphone arrays as input device, which is getting rid of ambient noise by setting a specific orientation. The therapist can guide the patient to play the game as well as the voice training simultaneously side by side, while not interfere the patient voice. The voice information can be recorded and extracted for evaluating the long-time rehabilitation progress. This paper outlines a design science approach for the development of an initial useful software prototype of such a tool, considering 'Intuitive', 'Entertainment', 'Incentive' as main design factors.\",\n",
       " 'Linear and non-linear measures of heart rate variability (HRV) are widely investigated as non-invasive indicators of health. Stress has a profound impact on heart rate, and different meditation techniques have been found to modulate heartbeat rhythm. This paper aims to explore the process of identifying appropriate metrices from HRV analysis for sonification. Sonification is a type of auditory display involving the process of mapping data to acoustic parameters. This work explores the use of auditory display in aiding the analysis of HRV leveraged by unsupervised machine learning techniques. Unsupervised clustering helps select the appropriate features to improve the sonification interpretability. Vocal synthesis sonification techniques are employed to increase comprehension and learnability of the processed data displayed through sound. These analyses are early steps in building a real-time sound-based biofeedback training system.',\n",
       " 'In video production, inserting B-roll is a widely used technique to enrich the story and make a video more engaging. However, determining the right content and positions of B-roll and actually inserting it within the main footage can be challenging, and novice producers often struggle to get both timing and content right. We present B-Script, a system that supports B-roll video editing via interactive transcripts. B-Script has a built-in recommendation system trained on expert-annotated data, recommending users B-roll position and content. To evaluate the system, we conducted a within-subject user study with 110 participants, and compared three interface variations: a timeline-based editor, a transcript-based editor, and a transcript-based editor with recommendations. Users found it easier and were faster to insert B-roll using the transcript-based interface, and they created more engaging videos when recommendations were provided.',\n",
       " 'To use social media for health-related analysis, one key step is the detection of health-related labels for users. But unlike transient conditions like flu, social media users are less vocal about chronic conditions such as obesity, as users might not tweet \"I\\'m still overweight\". As, however, obesity-related conditions such as diabetes, heart disease, osteoarthritis, and even cancer are on the rise, this obese-or-not label could be one of the most useful for studies in public health.\\n  In this paper we investigate the feasibility of using profile pictures to infer if a user is overweight or not. We show that this is indeed possible and further show that the fraction of labeled-as-overweight users is higher in U.S. counties with higher obesity rates. Going from public to individual health analysis, we then find differences both in behavior and social networks, for example finding users labeled as overweight to have fewer followers.',\n",
       " 'A large number of statistical decision problems in the social sciences and beyond can be framed as a (contextual) multi-armed bandit problem. However, it is notoriously hard to develop and evaluate policies that tackle these types of problem, and to use such policies in applied studies. To address this issue, this paper introduces StreamingBandit, a Python web application for developing and testing bandit policies in field studies. StreamingBandit can sequentially select treatments using (online) policies in real time. Once StreamingBandit is implemented in an applied context, different policies can be tested, altered, nested, and compared. StreamingBandit makes it easy to apply a multitude of bandit policies for sequential allocation in field experiments, and allows for the quick development and re-use of novel policies. In this article, we detail the implementation logic of StreamingBandit and provide several examples of its use.',\n",
       " 'High performance computing (HPC) has driven collaborative science discovery for decades. Exascale computing platforms, currently in the design stage, will be deployed around 2022. The next generation of supercomputers is expected to utilize radically different computational paradigms, necessitating fundamental changes in how the community of scientific users will make the most efficient use of these powerful machines. However, there have been few studies of how scientists work with exascale or close-to-exascale HPC systems. Time as a metaphor is so pervasive in the discussions and valuation of computing within the HPC community that it is worthy of close study. We utilize time as a lens to conduct an ethnographic study of scientists interacting with HPC systems. We build upon recent CSCW work to consider temporal rhythms and collective time within the HPC sociotechnical ecosystem and provide considerations for future system design.',\n",
       " 'Appearance-based gaze estimation methods that only require an off-the-shelf camera have significantly improved but they are still not yet widely used in the human-computer interaction (HCI) community. This is partly because it remains unclear how they perform compared to model-based approaches as well as dominant, special-purpose eye tracking equipment. To address this limitation, we evaluate the performance of state-of-the-art appearance-based gaze estimation for interaction scenarios with and without personal calibration, indoors and outdoors, for different sensing distances, as well as for users with and without glasses. We discuss the obtained findings and their implications for the most important gaze-based applications, namely explicit eye input, attentive user interfaces, gaze-based user modelling, and passive eye monitoring. To democratise the use of appearance-based gaze estimation and interaction in HCI, we finally present OpenGaze (www.opengaze.org), the first software toolkit for appearance-based gaze estimation and interaction.',\n",
       " \"This paper introduces an Internet of Things (IoT)-based data acquisition and monitoring scheme for insulin pumps. The proposed work employs embedded system hardware (Keil LPC1768-board) for data acquisition and monitoring. The hardware is used as an abstract layer between the insulin pump and the cloud. Diabetes data are secured before they are sent to the cloud for storage. Each patient's record is digitally signed using a secure hash algorithm mechanism. The proposed work will protect the patient's records from being breached from unauthorized entities, and authenticates them from improper modifications. The design is tested and verified using $\\\\mu$Vision studio, the Keil board mentioned above, and an ALARIS 8100 infusion pump. Moreover, a test case for a real cloud example is presented with the help of the Center of Computationally Assisted System and Technology. This center provided the infrastructure service to test our work.\",\n",
       " 'When inspecting information visualizations under time critical settings, such as emergency response or monitoring the heart rate in a surgery room, the user only has a small amount of time to view the visualization \"at a glance\". In these settings, it is important to provide a quantitative measure of the visualization to understand whether or not the visualization is too \"complex\" to accurately judge at a glance. This paper proposes Pixel Approximate Entropy (PAE), which adapts the approximate entropy statistical measure commonly used to quantify regularity and unpredictability in time-series data, as a measure of visual complexity for line charts. We show that PAE is correlated with user-perceived chart complexity, and that increased chart PAE correlates with reduced judgement accuracy. We also find that the correlation between PAE values and participants\\' judgment increases when the user has less time to examine the line charts.',\n",
       " \"UI design languages, such as Google's Material Design, make applications both easier to develop and easier to learn by providing a set of standard UI components. Nonetheless, it is hard to assess the impact of design languages in the wild. Moreover, designers often get stranded by strong-opinionated debates around the merit of certain UI components, such as the Floating Action Button and the Navigation Drawer. To address these challenges, this short paper introduces a method for measuring the impact of design languages and informing design debates through analyzing a dataset consisting of view hierarchies, screenshots, and app metadata for more than 9,000 mobile apps. Our data analysis shows that use of Material Design is positively correlated to app ratings, and to some extent, also the number of installs. Furthermore, we show that use of UI components vary by app category, suggesting a more nuanced view needed in design debates.\",\n",
       " 'Chatbot has become an important solution to rapidly increasing customer care demands on social media in recent years. However, current work on chatbot for customer care ignores a key to impact user experience - tones. In this work, we create a novel tone-aware chatbot that generates toned responses to user requests on social media. We first conduct a formative research, in which the effects of tones are studied. Significant and various influences of different tones on user experience are uncovered in the study. With the knowledge of effects of tones, we design a deep learning based chatbot that takes tone information into account. We train our system on over 1.5 million real customer care conversations collected from Twitter. The evaluation reveals that our tone-aware chatbot generates as appropriate responses to user requests as human agents. More importantly, our chatbot is perceived to be even more empathetic than human agents.',\n",
       " 'How should an AI-based explanation system explain an agent\\'s complex behavior to ordinary end users who have no background in AI? Answering this question is an active research area, for if an AI-based explanation system could effectively explain intelligent agents\\' behavior, it could enable the end users to understand, assess, and appropriately trust (or distrust) the agents attempting to help them. To provide insights into this question, we turned to human expert explainers in the real-time strategy domain, \"shoutcaster\", to understand (1) how they foraged in an evolving strategy game in real time, (2) how they assessed the players\\' behaviors, and (3) how they constructed pertinent and timely explanations out of their insights and delivered them to their audience. The results provided insights into shoutcasters\\' foraging strategies for gleaning information necessary to assess and explain the players; a characterization of the types of implicit questions shoutcasters answered; and implications for creating explanations by using the patterns',\n",
       " 'In this paper, we propose a new interface for virtual reality headset: a touchpad in front of the headset. To demonstrate the feasibility of the front touch interface, we built a prototype device, explored VR UI design space expansion, and performed various user studies. We started with preliminary tests to see how intuitively and accurately people can interact with the front touchpad. Then, we further experimented various user interfaces such as a binary selection, a typical menu layout, and a keyboard. Two-Finger and Drag-n-Tap were also explored to find the appropriate selection technique. As a low-cost, light-weight, and in low power budget technology, a touch sensor can make an ideal interface for mobile headset. Also, front touch area can be large enough to allow wide range of interaction types such as multi-finger interactions. With this novel front touch interface, we paved a way to new virtual reality interaction methods.',\n",
       " 'Display advertising normally charges advertisers for every single ad impression. Specifically, if an ad in a webpage has been loaded in the browser, an ad impression is counted. However, due to the position and size of the ad slot, lots of ads are actually not viewed but still measured as impressions and charged. These fraud ad impressions indeed undermine the efficacy of display advertising. A perfect ad impression viewability measurement should match what the user has really viewed with a short memory. In this paper, we conduct extensive investigations on display ad impression viewability measurements on dimensions of ad creative displayed pixel percentage and exposure time to find which measurement provides the most accurate ad impression counting. The empirical results show that the most accurate measurement counts one ad impression if more than 75% of the ad creative pixels have been exposed for at least 2 continuous seconds.',\n",
       " 'Building practitioners (architects, engineers, energy managers) are showing a growing interest in the design of more energy efficient and livable buildings. The best way to predict how a building will behave regarding energy consumption and thermal comfort is to use a dynamic simulation tool. However, the use of this kind of tools is difficult on a daily basis practice due to the heuristic and exploratory nature of the architectural design process. To deal with this difficulty, the University of Coimbra and three companies have been working on the development of a prototype design aiding tool, specifically devoted to the space planning phase of building design, under the project GerAPlanO (Automatic Generation of Architecture Floor plans with Energy Optimization). This project aims to combine the capabilities of design generation techniques, thermal assessment programs, and design optimization methods to provide assistance to decision makers. This paper presents the overall concept, as well as the current status of development of this tool.',\n",
       " 'For more then a decade the term User Experience (UX) has been highly debated and defined in many ways. However, often UX remains as a vague concept and it may be hard to understand the very nature of it. In this paper we aimed at providing a better understanding of this concept. We explored the multi-faceted UX literature, reviewing the current state-of- the-art knowledge and emphasizing the multi-dimensional nature of the concept. Based on the literature review we built a conceptual framework of UX using the elements that are linked to it and reported in different studies. To show the potential use of the framework, we examined the UX delivered by different phone applications on different mobile devices using the elements in the framework. Several interesting insights have been obtained in terms of how the phone applications deliver different UX. Our study opens up a promising line of investigating the contemporary meaning of UX.',\n",
       " 'This article proposes a multistage framework for time series analysis of user activity on touch sensitive surfaces in noisy environments. Here multiple methods are put together in multi stage framework; including moving average, moving median, linear regression, kernel density estimation, partial differential equations and Kalman filter. The proposed three stage filter consisting of partial differential equation based denoising, Kalman filter and moving average method provides ~25% better noise reduction than other methods according to Mean Squared Error (MSE) criterion in highly noise susceptible environments. Apart from synthetic data, we also obtained real world data like hand writing, finger/stylus drags etc. on touch screens in the presence of high noise such as unauthorized charger noise or display noise and validated our algorithms. Furthermore, the proposed algorithm performs qualitatively better than the existing solutions for touch panels of the high end hand held devices available in the consumer electronics market qualitatively.',\n",
       " \"With respect to machine operation tasks, the experiences from different skill level operators, especially novices, can provide worthy understanding about the manner in which they perceive the operational environment and formulate knowledge to deal with various operation situations. In this study, we describe the operator's behaviors by utilizing the relations among their head, hand, and operation location (hotspot) during the operation. A total of 40 experiences associated with a sewing machine operation task performed by amateur operators was recorded via a head-mounted RGB-D camera. We examined important features of operational behaviors in different skill level operators and confirmed their correlation to the difficulties of the operation steps. The result shows that the pure-gazing behavior is significantly reduced when the operator's skill improved. Moreover, the hand-approaching duration and the frequency of attention movement before operation are strongly correlated to the operational difficulty in such machine operating environments.\",\n",
       " \"The trend towards mobile devices usage has put more than ever the Web as a ubiquitous platform where users perform all kind of tasks. In some cases, users access the Web with 'native' mobile applications developed for well-known sites, such as LinkedIn, Facebook, Twitter, etc. These native applications might offer further (e.g. location-based) functionalities to their users in comparison with their corresponding Web sites, because they were developed with mobile features in mind. However, most Web applications have not this native mobile counterpart and users access them using browsers in the mobile device. Users might eventually want to add mobile features on these Web sites even though those features were not supported originally. In this paper we present a novel approach to allow end users to augment their preferred Web sites with mobile features. This end-user approach is supported by a framework for mobile Web augmentation that we describe in the paper. We also present a set of supporting tools and a validation experiment with end users.\",\n",
       " \"The objective of the system presented in this paper is to give users tactile feedback while walking in a virtual world through an anthropomorphic finger motion interface. We determined that the synchrony between the first person perspective and proprioceptive information together with the motor activity of the user's fingers are able to induce an illusionary feeling that is equivalent to the sense of ownership of the invisible avatar's legs. Under this condition, the perception of the ground under the virtual avatar's foot is felt through the user's fingertip. The experiments indicated that using our method the scale of the tactile perception of the texture roughness was extended and that the enlargement ratio was proportional to the avatar's body (foot) size. In order to display the target tactile perception to the users, we have to control only the virtual avatar's body (foot) size and the roughness of the tactile texture. Our results suggest that in terms of tactile perception fingers can be a replacement for legs in locomotion interfaces.\",\n",
       " 'As the internet of things (IoT) has integrated physical and digital technologies, designing for multiple sensory media (mulsemedia) has become more attainable. Designing technology for multiple senses has the capacity to improve virtual realism, extend our ability to process information, and more easily transfer knowledge between physical and digital environments. HCI researchers are beginning to explore the viability of integrating multimedia into virtual experiences, however research has yet to consider whether mulsemedia truly enhances realism, immersion and knowledge transfer. My work developing StreamBED, a VR training platform to train citizen science water monitors plans to consider the role of mulsemedia in immersion and learning goals. Future findings about the role of mulsemedia in learning contexts will potentially allow learners to experience, connect to, learn from spaces that are impossible to experience firsthand.',\n",
       " 'Predicting human performance in interaction tasks allows designers or developers to understand the expected performance of a target interface without actually testing it with real users. In this work, we present a deep neural net to model and predict human performance in performing a sequence of UI tasks. In particular, we focus on a dominant class of tasks, i.e., target selection from a vertical list or menu. We experimented with our deep neural net using a public dataset collected from a desktop laboratory environment and a dataset collected from hundreds of touchscreen smartphone users via crowdsourcing. Our model significantly outperformed previous methods on these datasets. Importantly, our method, as a deep model, can easily incorporate additional UI attributes such as visual appearance and content semantics without changing model architectures. By understanding about how a deep learning model learns from human behaviors, our approach can be seen as a vehicle to discover new patterns about human behaviors to advance analytical modeling.',\n",
       " 'Interaction methods based on computer-vision hold the potential to become the next powerful technology to support breakthroughs in the field of human-computer interaction. Non-invasive vision-based techniques permit unconventional interaction methods to be considered, including use of movements of the face and head for intentional gestural control of computer systems. Facial gesture interfaces open new possibilities for assistive input technologies. This chapter gives an overview of research aimed at developing vision-based head and face-tracking interfaces. This work has important implications for future assistive input devices. To illustrate this concretely we describe work from our own research in which we developed two vision-based facial feature tracking algorithms for human computer interaction and assistive input. Evaluation forms a critical component of this research and we provide examples of new quantitative evaluation tasks as well as the use of model real-world applications for the qualitative evaluation of new interaction styles.',\n",
       " \"We evaluate the performance and usability of mouse-based, touch-based, and tangible interaction for manipulating objects in a 3D virtual environment. This comparison is a step toward a better understanding of the limitations and benefits of these existing interaction techniques, with the ultimate goal of facilitating the integration of different 3D data exploration environments into a single interaction continuum. For this purpose we analyze participants' performance in 3D manipulation using a docking task. We measured completion times, docking precision, as well as subjective criteria such as fatigue, workload, and preference. Our results show that the three input modalities provide similar levels of precision but require different interaction times. We also discuss our qualitative observations as well as people's preferences and put our findings into context of the practical application domain of 3D data analysis environments.\",\n",
       " 'Several changes occur in the brain in response to voluntary and involuntary activities performed by a person. The ability to retrieve data from the brain within a time space provides basis for in-depth analyses that offer insight on what changes occur in the brain during its decision making processes. In this work, we present the technical description and software implementation of an electroencephalographic (EEG) based intelligent communication system. We use EEG dry sensors to read brain waves data in real-time with which we compute the likelihood that a voluntary eye blink has been made by a person and use the decision to trigger buttons on a user interface in order to produce text using a modification of the T9 algorithm. Our results indicate that EEG-based technology can be effectively applied in facilitating speech for people with severe speech and muscular disabilities, providing a foundation for future work in the area.',\n",
       " 'Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice are yet to benefit from the capacity of VEs. In this paper, we examine this perplexity from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theatre-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.',\n",
       " \"Alphanumeric text entry is a challenge for Virtual Reality (VR) applications. VR enables new capabilities, impossible in the real world, such as an unobstructed view of the keyboard, without occlusion by the user's physical hands. Several hand representations have been proposed for typing in VR on standard physical keyboards. However, to date, these hand representations have not been compared regarding their performance and effects on presence for VR text entry. Our work addresses this gap by comparing existing hand representations with minimalistic fingertip visualization. We study the effects of four hand representations (no hand representation, inverse kinematic model, fingertip visualization using spheres and video inlay) on typing in VR using a standard physical keyboard with 24 participants. We found that the fingertip visualization and video inlay both resulted in statistically significant lower text entry error rates compared to no hand or inverse kinematic model representations. We found no statistical differences in text entry speed.\",\n",
       " 'We present a crowdsourcing workflow to collect image annotations for visually similar synthetic categories without requiring experts. In animals, there is a direct link between taxonomy and visual similarity: e.g. a collie (type of dog) looks more similar to other collies (e.g. smooth collie) than a greyhound (another type of dog). However, in synthetic categories such as cars, objects with similar taxonomy can have very different appearance: e.g. a 2011 Ford F-150 Supercrew-HD looks the same as a 2011 Ford F-150 Supercrew-LL but very different from a 2011 Ford F-150 Supercrew-SVT. We introduce a graph based crowdsourcing algorithm to automatically group visually indistinguishable objects together. Using our workflow, we label 712,430 images by ~1,000 Amazon Mechanical Turk workers; resulting in the largest fine-grained visual dataset reported to date with 2,657 categories of cars annotated at 1/20th the cost of hiring experts.',\n",
       " 'This study examines the acceptance of technology and behavioral intention to use learning management systems (LMS). In specific, the aim of this research is to examine whether students ultimately accept and use educational learning systems such as e-class and the impact of behavioral intention on their decision to use them. An extended version of technology acceptance model has been proposed and used by employing the System Usability Scale to measure perceived ease of use. 345 university students participated in the study and the data analysis was based on partial least squares method. The results were confirmed in most of the research hypotheses. In particular, social norm, system access and self-efficacy significantly affect behavioral intention to use. As a result, it is suggested that e-learning developers and stakeholders should focus on these factors to increase acceptance and effectiveness of learning management systems.',\n",
       " 'In this paper we present Organic Primitives, an enabling toolbox that expands upon the library of input-output devices in HCI and facilitates the design of interactions with organic, fluid-based systems. We formulated color, odor and shape changing material primitives which act as sensor-actuators that convert pH signals into human-readable outputs. Food-grade organic molecules anthocyanin, vanillin, and chitosan were employed as dopants to synthesize materials which output a spectrum of colors, degrees of shape deformation, and switch between odorous and non-odorous states. We evaluated the individual output properties of our sensor-actuators to assess the rate, range, and reversibility of the changes as a function of pH 2-10. We present a design space with techniques for enhancing the functionality of the material primitives, and offer passive and computational methods for controlling the material interfaces. Finally, we explore applications enabled by Organic Primitives under four contexts: environmental, cosmetic, edible, and interspecies.',\n",
       " \"The number of informal caregivers for family members with Alzheimer's Disease (AD) is rising dramatically in the United States. AD caregivers disproportionately experience numerous health problems and are often isolated with little support. An active lifestyle can help prevent and mitigate physical and psychological health concerns amongst AD caregivers. Research has demonstrated how pervasive exergames can encourage physical activity (PA) in the general population, yet little work has explored how these tools can address the significant PA barriers that AD caregivers face. To identify opportunities for design, we conducted semi-structured interviews and participatory design sessions with 14 informal caregivers of family members with AD. Our findings characterize how becoming an AD caregiver profoundly impacts one's ability to be active, perspectives on being active, and the ways that exergames might best support this population. We discuss implications for design and how our findings challenge existing technological approaches to PA promotion.\",\n",
       " 'This study investigates the effect of a physical robot taking the role of a teacher or exercise partner in a language learning exercise. In order to investigate this, an application was developed enabling a 2:nd language learning vocabulary exercise in three different conditions. In the first condition the learner would receive tutoring from a disembodied voice, in the second condition the tutor would be embodied by an animated avatar on a computer screen, and in the final condition the tutor was a physical robotic head with a 3D animated face mask. A Russian language vocabulary exercise with 15 subjects was conducted. None of the subjects reported any Russian language skills prior to the exercises. Each subject were taught a set of 9 words in each of the three conditions during a practice phase, and were then asked to recall the words in a test phase. Results show that the recall of the words practiced with the physical robot were significantly higher than that of the words practiced with the avatar on the screen or with the disembodied voice.',\n",
       " 'This paper presents SAM, a modular and extensible JavaScript framework for self-adapting menus on webpages. SAM allows control of two elementary aspects for adapting web menus: (1) the target policy, which assigns scores to menu items for adaptation, and (2) the adaptation style, which specifies how they are adapted on display. By decoupling them, SAM enables the exploration of different combinations independently. Several policies from literature are readily implemented, and paired with adaptation styles such as reordering and highlighting. The process - including user data logging - is local, offering privacy benefits and eliminating the need for server-side modifications. Researchers can use SAM to experiment adaptation policies and styles, and benchmark techniques in an ecological setting with real webpages. Practitioners can make websites self-adapting, and end-users can dynamically personalise typically static web menus.',\n",
       " 'Heuristic inspections are often carried out in a rather restrictive manner in the sense that they often address one or two of User Experience aspects. These two generally being: usability and \"user experience\". This fails to consider UX as it should be [considered]: through a holistic approach. Thus, we suggest to go beyond that by opting for what we have called an Integrative Heu-ristic Inspection that takes into account issues of: accessibility, usability, emotions \\\\& motivation and persuasion, and that aims to simplify the overflow of recommendations UX professionals are faced with nowadays. We illustrate our proposal by a case study carried out on an insurance prospecting tablet application. We analyzed the results of the inspection separately for each dimension as well as combined across dimensions. Implications for a reflection on the struc-turing of the criteria for a general criteria-based approach in UX are discussed.',\n",
       " 'We propose a novel method to implement an optical see-through head mounted display which renders real aerial images with a wide viewing angle, called an Air Mounted Eyepiece (AME). To achieve the AMD design, we employ an off-the-shelf head mounted display and Transmissive Mirror Device (TMD) which is usually used in aerial real imaging systems. In the proposed method, we replicate the function of the head mounted display (HMD) itself, which is used in the air by using the TMD and presenting a real image of eyepiece in front of the eye. Moreover, it can realize a wide viewing angle 3D display by placing a virtual lens in front of the eye without wearing an HMD. In addition to enhancing the experience of mixed reality and augmented reality, our proposed method can be used as a 3D imaging method for use in other applications such as in automobiles and desktop work. We aim to contribute to the field of human-computer interaction and the research on eyepiece interfaces by discussing the advantages and the limitations of this near-eye optical system.',\n",
       " 'Multiple Sclerosis (MS) is an unpredictable, often disabling disease that can adversely affect any body function; this often requires persons with MS to be active patients who are able to self-manage. There are currently thousands of health applications available but it is unknown how many concern MS. We conducted a systematic review of all MS apps present in the most popular app stores (iTunes and Google Play store) on June 2016 to identify all relevant MS apps. After discarding non-MS related apps and duplicates, only a total of 25 MS apps were identified. App description contents and features were explored to assess target audience, functionalities, and developing entities. The vast majority of apps were focused on disease and treatment information with disease management being a close second. This is the first study that reviews MS apps and it highlights an interesting gap in the current repertoire of MS mHealth resources.',\n",
       " \"Mobile applications and on-body devices are becoming increasingly ubiquitous tools for physical activity tracking. We propose utilizing a self-tracker's habits to support continuous prediction of whether they will reach their daily step goal, thus enabling a variety of potential persuasive interventions. Our aim is to improve the prediction by leveraging historical data and other qualitative (motivation for using the systems, location, gender) and, quantitative (age) features. We have collected datasets from two activity tracking platforms (Moves and Fitbit) and aim to check if the model we derive from one is generalizable over the other. In the following paper we establish a pipeline for extracting the data and formatting it for modeling. We discuss the approach we took and our findings while selecting the features and classification models for the dataset. We further discuss the notion of generalizability of the model across different types of dataset and the probable inclusion of non standard features to further improve the model's accuracy.\",\n",
       " 'Machine learning advances have afforded an increase in algorithms capable of creating art, music, stories, games, and more. However, it is not yet well-understood how machine learning algorithms might best collaborate with people to support creative expression. To investigate how practicing designers perceive the role of AI in the creative process, we developed a game level design tool for Super Mario Bros.-style games with a built-in AI level designer. In this paper we discuss our design of the Morai Maker intelligent tool through two mixed-methods studies with a total of over one-hundred participants. Our findings are as follows: (1) level designers vary in their desired interactions with, and role of, the AI, (2) the AI prompted the level designers to alter their design practices, and (3) the level designers perceived the AI as having potential value in their design practice, varying based on their desired role for the AI.',\n",
       " 'The increasing pervasiveness of voice assistants in the home poses several privacy threats, including the stealthy recording of conversations. Little is known about user mental models of such threats, levels of comfort with their presence, and perceptions of potential interventions to eliminate them. In this paper, we present the design and prototyping of two such interventions as technology probes, \"Obfuscator\" and \"Blackout.\" We also present our findings from a field study that used both prototypes to better understand user mental models, preferences, and perceptions of privacy threats surrounding voice assistants. The findings from the field study have revealed several themes, including the importance of the aesthetics, ease-of-use, and form factor of the prototypes, all superseding the privacy-preserving features of the intervention. We discuss the design and research implications of our findings for the development of future interventions, the design of voice assistants, and our understanding of user privacy perceptions of voice assistants.',\n",
       " 'We present SigniFYI-CDN, an inspection method built from previously proposed methods combining Semiotic Engineering and the Cognitive Dimensions of Notations. Compared to its predecessors, SigniFYI-CDN simplifies procedural steps and supports them with more analytic scaffolds. It is especially fit for the study of interaction with technologies where notations are created and used by various people, or by a single person in various, and potentially distant, occasions. In such cases, notations may serve several purposes, like (mutual) comprehension, recall, coordination, negotiation, and documentation. We illustrate SigniFYI-CDN with highlights from the evaluation of a computer tool that supports qualitative data analysis. Our contribution is a simpler tool for researchers and practitioners to probe the power of combined communicability and usability analysis of interaction with increasingly complex data-intensive applications.',\n",
       " \"Asynchronous interfaces allow users to concurrently issue requests while existing ones are processed. While it is widely used to support non-blocking input when there is latency, it's not clear if people can make use of asynchrony as the data is updating, since the UI updates dynamically and the changes can be hard to interpret. Interactive data visualization presents an interesting context for studying the effects of asynchronous interfaces, since interactions are frequent, task latencies can vary widely, and results often require interpretation.\\n  In this paper, we study the effects of introducing asynchrony into interactive visualizations, under different latencies, and with different tasks. We observe that traditional asynchronous interfaces, where results update in place, induce users to wait for the result before interacting, not taking advantage of the asynchronous rendering of the results. However, when results are rendered cumulatively over the recent history, users perform asynchronous interactions and get faster task completion times.\",\n",
       " 'Researchers, technology reviewers, and governmental agencies have expressed concern that automation may necessitate the introduction of added displays to indicate vehicle intent in vehicle-to-pedestrian interactions. An automated online methodology for obtaining communication intent perceptions for 30 external vehicle-to-pedestrian display concepts was implemented and tested using Amazon Mechanic Turk. Data from 200 qualified participants was quickly obtained and processed. In addition to producing a useful early-stage evaluation of these specific design concepts, the test demonstrated that the methodology is scalable so that a large number of design elements or minor variations can be assessed through a series of runs even on much larger samples in a matter of hours. Using this approach, designers should be able to refine concepts both more quickly and in more depth than available development resources typically allow. Some concerns and questions about common assumptions related to the implementation of vehicle-to-pedestrian displays are posed.',\n",
       " 'Virtual Reality (VR) becomes accessible to mimic a \"real-like\" world now. People who have a VR experience usually can be impressed by the immersive feeling, they might consider themselves are actually existed in the VR space. Self-consciousness is important for people to identify their own characters in VR space, and illusory ownership can help people to \"build\" their \"bodies\". The rubber hand illusion can convince us a fake hand made by rubber is a part of our bodies under certain circumstances. Researches about autoscopic phenomena extend this illusory to the so-called full body illusion. We conducted 3 type of experiments to study the illusory ownership in VR space as it shows in Figure 1, and we learned: Human body must receive the synchronized visual signal and somatosensory stimulus at the same time; The visual signal must be the first person perceptive; the subject and the virtual body needs to be the same height as much as possible. All these illusory ownerships accompanied by the body temperature decreases, where the body is stimulated.',\n",
       " 'Mobile devices with touch keyboards have become ubiquitous, but text entry on these devices remains slow and errorprone. Understanding touch patterns during text entry could be useful in designing robust error-correction algorithms for soft keyboards. In this paper, we present an analysis of text input behaviors on a soft QWERTY keyboard in three different text entry postures: index finger only, one thumb, and two thumb. Our work expands on the work of [1] by considering the entire surface area of digit contact with the smartphone keyboard, rather than interpreting each touch as a single point. To do this, we captured touch areas for every key in a lab study with 8 participants and calculated offsets, error rates, and size measurements. We then repeated the original experiment described in [1] and showed that significant differences exist when basing offset calculations on touch area compared to touch points for two postures.',\n",
       " 'In this paper we present a fully autonomous and intrinsically motivated robot usable for HRI experiments. We argue that an intrinsically motivated approach based on the Predictive Information formalism, like the one presented here, could provide us with a pathway towards autonomous robot behaviour generation, that is capable of producing behaviour interesting enough for sustaining the interaction with humans and without the need for a human operator in the loop. We present a possible reactive baseline behaviour for comparison for future research. Participants perceive the baseline and the adaptive, intrinsically motivated behaviour differently. In our exploratory study we see evidence that participants perceive an intrinsically motivated robot as less intelligent than the reactive baseline behaviour. We argue that is mostly due to the high adaptation rate chosen and the design of the environment. However, we also see that the adaptive robot is perceived as more warm, a factor which carries more weight in interpersonal interaction than competence.',\n",
       " \"This paper presents a study on mutual speech variation influences in a human-computer setting. The study highlights behavioral patterns in data collected as part of a shadowing experiment, and is performed using a novel end-to-end platform for studying phonetic variation in dialogue. It includes a spoken dialogue system capable of detecting and tracking the state of phonetic features in the user's speech and adapting accordingly. It provides visual and numeric representations of the changes in real time, offering a high degree of customization, and can be used for simulating or reproducing speech variation scenarios. The replicated experiment presented in this paper along with the analysis of the relationship between the human and non-human interlocutors lays the groundwork for a spoken dialogue system with personalized speaking style, which we expect will improve the naturalness and efficiency of human-computer interaction.\",\n",
       " 'Conceptual design relies on extensive manipulation of morphological properties of real or virtual objects.This study investigates the nature of the perceptual information that could be retrieved from different representation modalities to reproduce structural properties of a complex object by drawing . The abstract and complex object (tensegrity simplex) was presented to two study populations (design experts/architects and non-experts) in three different representation modalities (2D image view explored visually only, digital 3D model explored visually using a computer mouse, the real object explored visually and manually. After viewing and exploring, observers had to draw the most critical parts of the structure by hand into a 2D reference frame. The results reveal a considerable performance advantage of digital 3D model exploration compared with real-world 3D object manipulation in the expert population.The results are discussed in terms of the specific nature of morphological cues made available through the different representation modalities.',\n",
       " 'Algorithms for data visualizations are essential tools for transforming data into useful narratives. Unfortunately, very few visualization algorithms can handle the large datasets of many real-world scenarios. In this study, we address the visualization of these datasets as a Multi-Objective Optimization Problem. We propose mQAPViz, a divide-and-conquer multi-objective optimization algorithm to compute large-scale data visualizations. Our method employs the Multi-Objective Quadratic Assignment Problem (mQAP) as the mathematical foundation to solve the visualization task at hand. The algorithm applies advanced sampling techniques originating from the field of machine learning and efficient data structures to scale to millions of data objects. The algorithm allocates objects onto a 2D grid layout. Experimental results on real-world and large datasets demonstrate that mQAPViz is a competitive alternative to existing techniques.',\n",
       " 'Constructive approaches to visualization authoring have been shown to offer advantages such as providing options for flexible outputs, scaffolding and ideation of new data mappings, personalized exploration of data, as well as supporting data understanding and literacy. However, visualization authoring tools based on a constructive approach do not scale well to larger datasets. As construction often involves manipulating small pieces of data and visuals, it requires a significant amount of time, effort, and repetitive steps. We present ReConstructor, an authoring tool in which a visualization is constructed by instantiating its structural and functional components through four interaction elements (objects, modifiers, activators, and tools). This design preserves most of the benefits of a constructive process while avoiding scalability issues by allowing designers to propagate individual mapping steps to all the elements of a visualization. We also discuss the perceived benefits of our approach and propose avenues for future research in this area.',\n",
       " \"This study investigated the essential of meaningful automated feedback for programming assignments. Three different types of feedback were tested, including (a) What's wrong - what test cases were testing and which failed, (b) Gap - comparisons between expected and actual outputs, and (c) Hint - hints on how to fix problems if test cases failed. 46 students taking a CS2 participated in this study. They were divided into three groups, and the feedback configurations for each group were different: (1) Group One - What's wrong, (2) Group Two - What's wrong + Gap, (3) Group Three - What's wrong + Gap + Hint. This study found that simply knowing what failed did not help students sufficiently, and might stimulate system gaming behavior. Hints were not found to be impactful on student performance or their usage of automated feedback. Based on the findings, this study provides practical guidance on the design of automated feedback.\",\n",
       " 'Computing devices such as laptops, tablets and mobile phones have become part of our daily lives. End users increasingly know more and more information about these devices. Further, more technically savvy end users know how such devices are being built and know how to choose one over the others. However, we cannot say the same about the Internet of Things (IoT) products. Due to its infancy nature of the marketplace, end users have very little idea about IoT products. To address this issue, we developed a method, a crowdsourced peer learning activity, supported by an online platform (OLYMPUS) to enable a group of learners to learn IoT products space better. We conducted two different user studies to validate that our tool enables better IoT education. Our method guide learners to think more deeply about IoT products and their design decisions. The learning platform we developed is open source and available for the community.',\n",
       " 'Data is omnipresent in the modern, digital world and a significant number of people need to make sense of data as part of their everyday social and professional life. Therefore, together with the rise of data, the design of graphical representations has gained importance and attention. Yet, although a large body of procedural knowledge about effective visualization exists, the quality of representations is often reported to be poor, proposedly because these guidelines are scattered, unstructured and sometimes perceived as contradictive. Therefore, this paper describes a literature research addressing these problems. The research resulted in the collection and structuring of 81 guidelines and 34 underlying propositions, as well as in the derivation of 7 foundational principles about graphical representation design, called the \"Physics of Diagrams\", which are illustrated with concrete, practical examples throughout the paper.',\n",
       " 'The need for data preservation and reproducible research is widely recognized in the scientific community. Yet, researchers often struggle to find the motivation to contribute to data repositories and to use tools that foster reproducibility. In this paper, we explore possible uses of gamification to support reproducible practices in High Energy Physics. To understand how gamification can be effective in research tools, we participated in a workshop and performed interviews with data analysts. We then designed two interactive prototypes of a research preservation service that use contrasting gamification strategies. The evaluation of the prototypes showed that gamification needs to address core scientific challenges, in particular the fair reflection of quality and individual contribution. Through thematic analysis, we identified four themes which describe perceptions and requirements of gamification in research: Contribution, Metrics, Applications and Scientific practice. Based on these, we discuss design implications for gamification in science.',\n",
       " 'Crisis situations are characterised by their sudden occurrence and an unclear information situation. In that context, social media platforms have become a highly utilised resource for collective information gathering to fill these gaps. However, there are indications that not only humans, but also social bots are active on these platforms during crisis situations. Although identifying the impact of social bots during extreme events seems to be a highly relevant topic, research remains sparse. To fill this research gap, we started a bigger project in analysing the influence of social bots during crisis situations. As a part of this project, we initially conducted a case study on the Manchester Bombing 2017 and analysed the social bot activity. Our results indicate that mainly benign bots are active during crisis situations. While the quantity of the bot accounts is rather low, their tweet activity indicates a high influence.',\n",
       " 'We propose that safe, beautiful, fulfilling vehicle HMI design must start from a rigorous consideration of minimalist design. Modern vehicles are changing from mechanical machines to mobile computing devices, similar to the change from landline phones to smartphones. We propose the approach of \"designing toward minimalism\", where we ask \"why?\" rather than \"why not?\" in choosing what information to display to the driver. We demonstrate this approach on an HMI case study of displaying vehicle speed. We first show that vehicle speed is what 87.6% of people ask for. We then show, through an online study with 1,038 subjects and 22,950 videos, that humans can estimate ego-vehicle speed very well, especially at lower speeds. Thus, despite believing that we need this information, we may not. In this way, we demonstrate a systematic approach of questioning the fundamental assumptions of what information is essential for vehicle HMI.',\n",
       " 'Digitalization offers chances as well as risks for industrial companies. This article describes how the area of Mixed Reality, with its manifestations Augmented and Virtual Reality, can support industrial applications in the age of digitalization. Starting from a historical perspective on Augmented and Virtual Reality, this article surveys recent developments in the domain of Mixed Reality, relevant for industrial use cases.\\n  ---\\n  Die Digitalisierung bietet f\\\\\"ur Industrieunternehmen neue Chancen, stellt diese jedoch auch vor Herausforderungen. Dieser Artikel beleuchtet wie das Gebiet der vermischten Realit\\\\\"at mit seinen Auspr\\\\\"agungen der erweiterten Realit\\\\\"at und der virtuellen Realit\\\\\"at f\\\\\"ur industriellen Anwendungen im Zeitalter der Digitalisierung Vorteile schaffen kann. Ausgehend von einer historischen Betrachtung, werden aktuelle Entwicklungen auf dem Gebiet der erweiterten und virtuellen Realit\\\\\"at diskutiert.',\n",
       " \"Telepresence is a necessity for present time as we can't reach everywhere and also it is useful in saving human life at dangerous places. A robot, which could be controlled from a distant location, can solve these problems. This could be via communication waves or networking methods. Also controlling should be in real time and smooth so that it can actuate on every minor signal in an effective way. This paper discusses a method to control a robot over the network from a distant location. The robot was controlled by hand gestures which were captured by the live camera. A DSP board TMS320DM642EVM was used to implement image pre-processing and fastening the whole system. PCA was used for gesture classification and robot actuation was done according to predefined procedures. Classification information was sent over the network in the experiment. This method is robust and could be used to control any kind of robot over distance.\",\n",
       " 'Understanding complex user behaviour under various conditions, scenarios and journeys can be fundamental to the improvement of the user-experience for a given system. Predictive models of user reactions, responses -- and in particular, emotions -- can aid in the design of more intuitive and usable systems. Building on this theme, the preliminary research presented in this paper correlates events and interactions in an online social network against user behaviour, focusing on personality traits. Emotional context and tone is analysed and modelled based on varying types of sentiments that users express in their language using the IBM Watson Developer Cloud tools. The data collected in this study thus provides further evidence towards supporting the hypothesis that analysing and modelling emotions, sentiments and personality traits provides valuable insight into improving the user experience of complex social computer systems.',\n",
       " 'Human Computer Interaction (HCI) has been redefined in this era. People want to interact with their devices in such a way that has physical significance in the real world, in other words, they want ergonomic input devices. In this paper, we propose a new method of interaction with computing devices having a consumer grade camera, that uses two colored markers (red and green) worn on tips of the fingers to generate desired hand gestures, and for marker detection and tracking we used template matching with kalman filter. We have implemented all the usual system commands, i.e., cursor movement, right click, left click, double click, going forward and backward, zoom in and out through different hand gestures. Our system can easily recognize these gestures and give corresponding system commands. Our system is suitable for both desktop devices and devices where touch screen is not feasible like large screens or projected screens.',\n",
       " 'Dimensionality reduction is a common method for analyzing and visualizing high-dimensional data. However, reasoning dynamically about the results of a dimensionality reduction is difficult. Dimensionality-reduction algorithms use complex optimizations to reduce the number of dimensions of a dataset, but these new dimensions often lack a clear relation to the initial data dimensions, thus making them difficult to interpret. Here we propose a visual interaction framework to improve dimensionality-reduction based exploratory data analysis. We introduce two interaction techniques, forward projection and backward projection, for dynamically reasoning about dimensionally reduced data. We also contribute two visualization techniques, prolines and feasibility maps, to facilitate the effective use of the proposed interactions. We apply our framework to PCA and autoencoder-based dimensionality reductions. Through data-exploration examples, we demonstrate how our visual interactions can improve the use of dimensionality reduction in exploratory data analysis.',\n",
       " 'Unrecognized hazards increase the likelihood of workplace fatalities and injuries substantially. However, recent research has demonstrated that a large proportion of hazards remain unrecognized in dynamic construction environments. Recent studies have suggested a strong correlation between viewing patterns of workers and their hazard recognition performance. Hence, it is important to study and analyze the viewing patterns of workers to gain a better understanding of their hazard recognition performance. The objective of this exploratory research is to explore hazard recognition as a visual search process to identifying various visual search factors that affect the process of hazard recognition. Further, the study also proposes a framework to develop a vision based tool capable of recording and analyzing viewing patterns of construction workers and generate feedback for personalized training and proactive safety management.',\n",
       " \"In a pointing task with time constraints, it was only possible to predict the user's error rate when pointing to a stationary target. This study presents a novel model for predicting pointing error rates regardless of the target motion. The model assumes that in the last submovement of the pointing trajectory just before the click, the timing to activate the button is anticipated by the user's internal clock decoding the temporal cues present in the relative movement between the cursor and the target. Then, based on the recent theory of temporal pointing, the model can predict the user's pointing error rate with a high R2 for both stationary (0.993) and moving targets (0.986) by analyzing the kinematic characteristics of the last submovement. In addition, empirical parameters obtained from the model fit succeeded in revealing differences in the cognitive characteristics of experts and novices in first-person shooter games.\",\n",
       " 'Research Objects (ROs) are semantically enhanced aggregations of resources associated to scientific experiments, such as data, provenance of these data, the scientific workflow used to run the experiment, intermediate results, logs and the interpretation of the results. As the number of ROs increases, it is becoming difficult to find ROs to be used, reused or re-purposed. New search and retrieval techniques are required to find the most appropriate ROs for a given researcher, paying attention to provide an intuitive user interface. In this paper we show CollabSpheres, a user interface that provides a new visual metaphor to find ROs by means of a recommendation system that takes advantage of the social aspects of ROs. The experimental evaluation of this tool shows that users perceive high values of usability, user satisfaction, usefulness and ease of use. From the analysis of these results we argue that users perceive the simplicity, intuitiveness and cleanness of this tool, as well as this tool increases collaboration and reuse of research objects.',\n",
       " \"Deaf individuals face great challenges in today's society. It can be very difficult to be able to understand different forms of media without a sense of hearing. Many videos and movies found online today are not captioned, and even fewer have a supporting video with an interpreter. Also, even with a supporting interpreter video provided, information is still lost due to the inability to look at both the video and the interpreter simultaneously. To alleviate this issue, we came up with a tool called closed interpreting. Similar to closed captioning, it will be displayed with an online video and can be toggled on and off. However, the closed interpreter is also user-adjustable. Settings, such as interpreter size, transparency, and location, can be adjusted. Our goal with this study is to find out what deaf and hard of hearing viewers like about videos that come with interpreters, and whether the adjustability is beneficial.\",\n",
       " 'Multimodal simulations augment the presentation of abstract concepts facilitating theoretical models understanding and learning. Most simulations only engage two of our five senses: sight and hearing. If we employ additional sensory communication channels in simulations, we may gain a deeper understanding of illustrated concepts by increasing the communication bandwidth and providing alternative perspectives. We implemented the sense of touch in 3D simulations to teach important concepts in introductory physics. Specifically, we developed a visual/haptic simulation for friction. We prove that interactive 3D haptic simulations, if carefully developed and deployed, are useful in engaging students and allowing them to understand concepts faster. We hypothesize that large scale deployment of such haptic-based simulators in science laboratories is now possible due to the advancements in haptic software and hardware technology.',\n",
       " 'Drones are a versatile platform for both amateur and professional photographers, enabling them to capture photos that are impossible to shoot with ground-based cameras. However, when guided by inexperienced pilots, they have a high incidence of collisions, crashes, and poorly framed photographs. This paper presents an intelligent user interface for photographing objects that is robust against navigation errors and reliably collects high quality photographs. By retaining the human in the loop, our system is faster and more selective than purely autonomous UAVs that employ simple coverage algorithms. The intelligent user interface operates in multiple modes, allowing the user to either directly control the quadcopter or fly in a semi-autonomous mode around a target object in the environment. To evaluate the interface, users completed a data set collection task in which they were asked to photograph objects from multiple views. Our sketchbased control paradigm facilitated task completion, reduced crashes, and was favorably reviewed by the participants.',\n",
       " 'In conditionally automated vehicles, drivers can engage in secondary activities while traveling to their destination. However, drivers are required to appropriately respond, in a limited amount of time, to a take-over request when the system reaches its functional boundaries. In this context, Virtual Reality systems represent a promising training and learning tool to properly familiarize drivers with the automated vehicle and allow them to interact with the novel equipment involved. In this study, the effectiveness of an Head-Mounted display (HMD)-based training program for acquiring interaction skills in automated cars was compared to a user manual and a fixed-base simulator. Results show that the training system affects the take-over performances evaluated in a test drive in a high-end driving simulator. Moreover, self-reported measures indicate that the HMD-based training is preferred with respect to the other systems.',\n",
       " 'We study the performance and user experience of two popular mainstream text entry devices, desktop keyboards and touchscreen keyboards, for use in Virtual Reality (VR) applications. We discuss the limitations arising from limited visual feedback, and examine the efficiency of different strategies of use. We analyze a total of 24 hours of typing data in VR from 24 participants and find that novice users are able to retain about 60% of their typing speed on a desktop keyboard and about 40-45\\\\% of their typing speed on a touchscreen keyboard. We also find no significant learning effects, indicating that users can transfer their typing skills fast into VR. Besides investigating baseline performances, we study the position in which keyboards and hands are rendered in space. We find that this does not adversely affect performance for desktop keyboard typing and results in a performance trade-off for touchscreen keyboard typing.',\n",
       " \"Although the live music entertainment sector does not directly fuel the current debate on automation, it might harbor positions that resonate with it. In this paper we study a prototype software application helping DJs and VJs to accurately manage and even automate the synchronization of visuals with music during amateur or professional live performance. The goal of the study was to unravel VJs' and DJs' ambivalent positions about this software. We preliminarily investigated VJs' and DJs' perception of their sector of activity with seven face-to-face interviews and an online survey (N = 102); then, we asked DJs and VJs (N = 25) for their opinions about our prototype software application. Four core controversies were identified in their answers, along with a set of arguments mobilized to take side on them. The advantages of focusing on ambivalence and argumentation when studying users' response to new media are discussed.\",\n",
       " 'The advent of abundant on-board sensors and electronic devices in vehicles populates the paradigm of participatory sensing to harness crowd-sourced data gathering for intelligent transportation applications, such as distance-to-empty prediction and eco-routing. While participatory sensing can provide diverse driving data, there lacks a systematic study of effective utilization of the data for personalized prediction. There are considerable challenges on how to interpolate the missing data from a sparse dataset, which often arises from participatory sensing. This paper presents and compares various approaches for personalized vehicle energy consumption prediction, including a blackbox framework that identifies driver/vehicle/environment-dependent factors and a collaborative filtering approach based on matrix factorization. Furthermore, a case study of distance-to-empty prediction for electric vehicles by participatory sensing data is conducted and evaluated empirically, which shows that our approaches can significantly improve the prediction accuracy.',\n",
       " 'Cartograms are maps in which areas of geographic regions (countries, states) appear in proportion to some variable of interest (population, income). Cartograms are popular visualizations for geo-referenced data that have been used for over a century and that make it possible to gain insight into patterns and trends in the world around us. Despite the popularity of cartograms and the large number of cartogram types, there are few studies evaluating the effectiveness of cartograms in conveying information. Based on a recent task taxonomy for cartograms, we evaluate four major different types of cartograms: contiguous, non-contiguous, rectangular, and Dorling cartograms. Specifically, we evaluate the effectiveness of these cartograms by quantitative performance analysis, as well as by subjective preferences. We analyze the results of our study in the context of some prevailing assumptions in the literature of cartography and cognitive science. Finally, we make recommendations for the use of different types of cartograms for different tasks and settings.',\n",
       " 'Tactile enhanced multimedia is generated by synchronizing traditional multimedia clips, to generate hot and cold air effect, with an electric heater and a fan. This objective is to give viewers a more realistic and immersing feel of the multimedia content. The response to this enhanced multimedia content (mulsemedia) is evaluated in terms of the appreciation/emotion by using human brain signals. We observe and record electroencephalography (EEG) data using a commercially available four channel MUSE headband. A total of 21 participants voluntarily participated in this study for EEG recordings. We extract frequency domain features from five different bands of each EEG channel. Four emotions namely: happy, relaxed, sad, and angry are classified using a support vector machine in response to the tactile enhanced multimedia. An increased accuracy of 76:19% is achieved when compared to 63:41% by using the time domain features. Our results show that the selected frequency domain features could be better suited for emotion classification in mulsemedia studies.',\n",
       " 'Many people struggle to control their use of digital devices. However, our understanding of the design mechanisms that support user self-control remains limited. In this paper, we make two contributions to HCI research in this space: first, we analyse 367 apps and browser extensions from the Google Play, Chrome Web, and Apple App stores to identify common core design features and intervention strategies afforded by current tools for digital self-control. Second, we adapt and apply an integrative dual systems model of self-regulation as a framework for organising and evaluating the design features found. Our analysis aims to help the design of better tools in two ways: (i) by identifying how, through a well-established model of self-regulation, current tools overlap and differ in how they support self-control; and (ii) by using the model to reveal underexplored cognitive mechanisms that could aid the design of new tools.',\n",
       " \"The purpose of this research is to see the application of modified TAM model by entering the experiential variable as a moderation variable to see one's intention in the use of technology especially internet banking. Data obtained through the distribution of questionnaires to customers. The study population is bank customers registered as users of internet banking services. The sample selection used a simple random sampling technique. Hypothesis testing using Partial Least Square (PLS) method through AMOS program. The results showed that the proposed five hypotheses, two significant and three insignificant. Perceived ease of use is significantly related to perceived usefulness. Per-ceived usefulness is not significantly related to intention to use. Perceived ease of use is significantly related to intention to use moderated by experience and not significantly correlated with intention to use moderated by the Experience.\",\n",
       " 'An important problem for HCI researchers is to estimate the parameter values of a cognitive model from behavioral data. This is a difficult problem, because of the substantial complexity and variety in human behavioral strategies. We report an investigation into a new approach using approximate Bayesian computation (ABC) to condition model parameters to data and prior knowledge. As the case study we examine menu interaction, where we have click time data only to infer a cognitive model that implements a search behaviour with parameters such as fixation duration and recall probability. Our results demonstrate that ABC (i) improves estimates of model parameter values, (ii) enables meaningful comparisons between model variants, and (iii) supports fitting models to individual users. ABC provides ample opportunities for theoretical HCI research by allowing principled inference of model parameter values and their uncertainty.',\n",
       " \"Multiplayer online battle arena games provide an excellent opportunity to study team performance. When designing a team, players must negotiate a \\\\textit{proficiency-congruency dilemma} between selecting roles that best match their experience and roles that best complement the existing roles on the team. We adopt a mixed-methods approach to explore how users negotiate this dilemma. Using data from \\\\textit{League of Legends}, we define a similarity space to operationalize team design constructs about role proficiency, generality, and congruency. We collect publicly available data from 3.36 million users to test the influence of these constructs on team performance. We also conduct focus groups with novice and elite players to understand how players' team design practices vary with expertise. We find that player proficiency increases team performance more than team congruency. These findings have implications for players, designers, and theorists about how to recommend team designs that jointly prioritize individuals' expertise and teams' compatibility.\",\n",
       " \"Today's competition between the professional eSports teams is so strong that in-depth analysis of players' performance literally crucial for creating a powerful team. There are two main approaches to such an estimation: obtaining features and metrics directly from the in-game data or collecting detailed information about the player including data on his/her physical training. While the correlation between the player's skill and in-game data has already been covered in many papers, there are very few works related to analysis of eSports athlete's skill through his/her physical behavior. We propose the smart chair platform which is to collect data on the person's behavior on the chair using an integrated accelerometer, a gyroscope and a magnetometer. We extract the important game events to define the players' physical reactions to them. The obtained data are used for training machine learning models in order to distinguish between the low-skilled and high-skilled players. We extract and figure out the key features during the game and discuss the results.\",\n",
       " \"The usability of small devices such as smartphones or interactive watches is often hampered by the limited size of command vocabularies. This paper is an attempt at better understanding how finger identification may help users invoke commands on touch screens, even without recourse to multi-touch input. We describe how finger identification can increase the size of input vocabularies under the constraint of limited real estate, and we discuss some visual cues to communicate this novel modality to novice users. We report a controlled experiment that evaluated, over a large range of input-vocabulary sizes, the efficiency of single-touch command selections with vs. without finger identification. We analyzed the data not only in terms of traditional time and error metrics, but also in terms of a throughput measure based on Shannon's theory, which we show offers a synthetic and parsimonious account of users' performance. The results show that the larger the input vocabulary needed by the designer, the more promising the identification of individual fingers.\",\n",
       " 'The relation between performance and stress is described by the Yerkes-Dodson Law but varies significantly between individuals. This paper describes a method for determining the individual optimal performance as a function of physiological signals. The method is based on attention and reasoning tests of increasing complexity under monitoring of three physiological signals: Galvanic Skin Response (GSR), Heart Rate (HR), and Electromyogram (EMG). Based on the test results with 15 different individuals, we first show that two of the signals, GSR and HR, have enough discriminative power to distinguish between relax and stress periods. We then show a positive correlation between the complexity level of the tests and the GSR and HR signals, and we finally determine the optimal performance point as the signal level just before a performance decrease. We also discuss the differences among signals depending on the type of test.',\n",
       " \"Using highly interactive systems like computer games requires a lot of visual activity and eye movements. Eye movements are best characterized by visual fixation - periods of time when the eyes stay relatively still over an object. We analyzed the distributions of fixation duration of professional athletes, amateur and newbie players. We show that the analysis of fixation durations can be used to deduce the skill level in computer game players. Highly skilled gaming performance is characterized by more variability in fixation durations and by bimodal fixation duration distributions suggesting the presence of two fixation types in high skill gamers. These fixation types were identified as ambient (automatic spatial processing) and focal (conscious visual processing). The analysis of computer gamers' skill level via the analysis of fixation durations may be used in developing adaptive interfaces and in interface design.\",\n",
       " 'User interfaces provide an interactive window between physical and virtual environments. A new concept in the field of human-computer interaction is a soft user interface; a compliant surface that facilitates touch interaction through deformation. Despite the potential of these interfaces, they currently lack a signal processing framework that can efficiently extract information from their deformation. Here we present OrbTouch, a device that uses statistical learning algorithms, based on convolutional neural networks, to map deformations from human touch to categorical labels (i.e., gestures) and touch location using stretchable capacitor signals as inputs. We demonstrate this approach by using the device to control the popular game Tetris. OrbTouch provides a modular, robust framework to interpret deformation in soft media, laying a foundation for new modes of human computer interaction through shape changing solids.',\n",
       " \"Researchers and experts are taking efforts in delivering an optimal user experience from a long time. Computer interfaces are being developed to keep user 'in the flow' as well as for making users more connected to the real world wile using virtual environment. Developing ubiquitous user interfaces for novices and experts at the same time is crucial work for interaction designers. This paper molds the designing approach of user interfaces in bit different parameters by reviewing the existing literature and proposing a different way to develop a smart user interface to make user more familiar with the design and to keep user 'in the flow'. Contextually proximate approach (CPA) will help users to minimize their feeling of insecurity as designing process includes local resources of users to develop the user interfaces. These various resources and parameters are explained further in the paper by giving different examples.\",\n",
       " \"The rise of increasingly more powerful chatbots offers a new way to collect information through conversational surveys, where a chatbot asks open-ended questions, interprets a user's free-text responses, and probes answers when needed. To investigate the effectiveness and limitations of such a chatbot in conducting surveys, we conducted a field study involving about 600 participants. In this study, half of the participants took a typical online survey on Qualtrics and the other half interacted with an AI-powered chatbot to complete a conversational survey. Our detailed analysis of over 5200 free-text responses revealed that the chatbot drove a significantly higher level of participant engagement and elicited significantly better quality responses in terms of relevance, depth, and readability. Based on our results, we discuss design implications for creating AI-powered chatbots to conduct effective surveys and beyond.\",\n",
       " \"We propose, in this paper, a model of continuous use of corporate collaborative KMS. Companies do not always have the guaranty that their KMS will be continuously used. This statement can constitute an important obstacle for knowledge management processes. Our work is based on the analysis of classical models for initial and continuous use of technologies. We also analyse the regulation concept and explain how it is valuable to support a continuous use of KMS. We observed that awareness may be a regulation means that allows taking this problem into account. Awareness is a concept, which has been profusely used to improve user experience in collaborative environments. It is an important element for regulation of activity. In our model, we assume that one can integrate awareness in information systems to positively influence beliefs about them. The final objective of our work is to refine some concepts to fit the particularities of collaborative KMS and to propose an awareness regulation process using the traces of the users' interactions with the systems.\",\n",
       " \"Many personal devices have transitioned from visual-controlled interfaces to speech-controlled interfaces to reduce costs and interactive friction, supported by the rapid growth in capabilities of speech-controlled interfaces, e.g., Amazon Echo or Apple's Siri. A consequence is that people who are deaf or hard of hearing (DHH) may be unable to use these speech-controlled devices. We show that deaf speech has a high error rate compared to hearing speech, in commercial speech-controlled interfaces. Deaf speech had approximately a 78% word error rate (WER) compared to a hearing speech 18% WER. Our findings show that current speech-controlled interfaces are not usable by DHH people. Based on our findings, significant advances in speech recognition software or alternative approaches will be needed for deaf use of speech-controlled interfaces. We show that current speech-controlled interfaces are not usable by DHH people.\",\n",
       " 'Multi-modality is an important feature of sensor based activity recognition. In this work, we consider two inherent characteristics of human activities, the spatially-temporally varying salience of features and the relations between activities and corresponding body part motions. Based on these, we propose a multi-agent spatial-temporal attention model. The spatial-temporal attention mechanism helps intelligently select informative modalities and their active periods. And the multiple agents in the proposed model represent activities with collective motions across body parts by independently selecting modalities associated with single motions. With a joint recognition goal, the agents share gained information and coordinate their selection policies to learn the optimal recognition model. The experimental results on four real-world datasets demonstrate that the proposed model outperforms the state-of-the-art methods.',\n",
       " 'This paper presents an autoethnography of my experiences living without a mobile phone. What started as an experiment motivated by a personal need to reduce stress, has resulted in two voluntary mobile phone breaks spread over nine years (i.e., 2002-2008 and 2014-2017). Conducting this autoethnography is the means to assess if the lack of having a phone has had any real impact in my life. Based on formative and summative analyses, four meaningful units or themes were identified (i.e., social relationships, everyday work, research career, and location and security), and judged using seven criteria for successful ethnography from existing literature. Furthermore, I discuss factors that allow me to make the choice of not having a mobile phone, as well as the relevance that the lessons gained from not having a mobile phone have on the lives of people who are involuntarily disconnected from communication infrastructures.',\n",
       " 'This research proposes Finger Based Technique (FBT) for non-visual touch screen device interaction designed for blind users. Based on the proposed technique, the blind user can access virtual keys based on finger holding positions. Three different models have been proposed. They are Single Digit Finger-Digit Input (FDI), Double Digit FDI for digital text entry, and Finger-Text Input (FTI) for normal text entry. All the proposed models were implemented with voice feedback while enabling touch as the input gesture. The models were evaluated with 7 blind participants with Samsung Galaxy S2 apparatus. The results show that Single Digit FDI is substantially faster and more accurate than Double Digit FDI and iPhone voice-over. FTI also looks promising for text entry. Our study also reveals 11 accessible regions to place widgets for quick access by blind users in flat touch screen based smartphones. Identification of these accessible regions will promote dynamic interactions for blind users and serve as a usability design framework for touch screen applications.',\n",
       " \"Entry-level crowd work is often reported to pay less than minimum wage. While this may be appropriate or even necessary, due to various legal, economic, and pragmatic factors, some Requesters and workers continue to question this status quo. To promote further discussion on the issue, we survey Requesters and workers whether they would support restricting tasks to require minimum wage pay. As a form of design activism, we confronted workers with this dilemma directly by posting a dummy Mechanical Turk task which told them that they could not work on it because it paid less than their local minimum wage, and we invited their feedback. Strikingly, for those workers expressing an opinion, two-thirds of Indians favored the policy while two-thirds of Americans opposed it. Though a majority of Requesters supported minimum wage pay, only 20\\\\% would enforce it. To further empower Requesters, and to ensure that effort or ignorance are not barriers to change, we provide a simple public API to make it easy to find a worker's local minimum wage by his/her IP address.\",\n",
       " \"Currently, adaptive voice applications supported by voice assistants (VA) are very popular (i.e., Alexa skills and Google Home Actions). Under this circumstance, how to design and evaluate these voice interactions well is very important. In our study, we developed a voice crawler to collect responses from 100 most popular Alexa skills under 10 different categories and evaluated these responses to find out how they comply with 8 selected design guidelines published by Amazon. Our findings show that basic commands support are the most followed ones while those related to personalised interaction are relatively less. There also exists variation in design guidelines compliance across different skill categories. Based on our findings and real skill examples, we offer suggestions for new guidelines to complement the existing ones and propose agendas for future HCI research to improve voice applications' user experiences.\",\n",
       " \"Reproducibility should be a cornerstone of scientific research and is a growing concern among the scientific community and the public. Understanding how to design services and tools that support documentation, preservation and sharing is required to maximize the positive impact of scientific research. We conducted a study of user attitudes towards systems that support data preservation in High Energy Physics, one of science's most data-intensive branches. We report on our interview study with 12 experimental physicists, studying requirements and opportunities in designing for research preservation and reproducibility. Our findings suggest that we need to design for motivation and benefits in order to stimulate contributions and to address the observed scalability challenge. Therefore, researchers' attitudes towards communication, uncertainty, collaboration and automation need to be reflected in design. Based on our findings, we present a systematic view of user needs and constraints that define the design space of systems supporting reproducible practices.\",\n",
       " 'This study proposes middleware, GameControllerizer, that allows users to combine the processes of Internet of Things (IoT) devices, Web services, and applications of Artificial Intelligence (AI), and to convert them into game control operations to augment existing digital games. The system facilitates easy trial-and-error development of new forms of entertainment and the configuration of gamification by enabling the use of diverse devices and sources of information as inputs to games. GameControllerizer consists of a visual programming element that uses the Node-RED tool to allow users to program easily to convert diverse formats of information into inputs to games, and contains a game input emulation element whereby hardware- and software-based emulation generates inputs for gaming devices. Evidence of the usefulness of the system was provided by a performance assessment and the proposal of a variety of use cases.',\n",
       " \"Dynamic Difficulty Adjustment (DDA) is a mechanism used in video games that automatically tailors the individual gaming experience to match an appropriate difficulty setting. This is generally achieved by removing pre-defined difficulty tiers such as Easy, Medium and Hard; and instead concentrates on balancing the gameplay to match the challenge to the individual's abilities. The work presented in this paper examines the implementation of DDA in a custom survival game developed by the author, namely Colwell's Castle Defence. The premise of this arcade-style game is to defend a castle from hordes of oncoming enemies. The AI system that we developed adjusts the enemy spawn rate based on the current performance of the player. Specifically, we read the Player Health and Gate Health at the end of each level and then assign the player with an appropriate difficulty tier for the proceeding level. We tested the impact of our technique on thirty human players and concluded, based on questionnaire feedback, that enabling the technique led to more enjoyable gameplay.\",\n",
       " \"Even though many approaches have been proposed for entity resolution (ER), it remains very challenging to find one with quality guarantees. To this end, we proposea risk-aware HUman-Machine cOoperation framework for ER, denoted by r-HUMO. Built on the existing HUMO framework, r-HUMO similarly enforces both precision and recall levels by partitioning an ER workload between the human and the machine. However, r-HUMO is the first solution to optimize the process of human workload selection from a risk perspective. It iteratively selects human workload based on real-time risk analysis on human-labeled results as well as prespecified machine metrics. In this paper,we first introduce the r-HUMO framework and then present the risk analysis technique to prioritize the instances for manual labeling. Finally,we empirically evaluate r-HUMO's performance on real data. Our extensive experiments show that r-HUMO is effective in enforcing quality guarantees,and compared with the state-of-the-art alternatives, it can achieve better quality control with reduced human cost.\",\n",
       " \"Machine learning is the capacity of a computational system to learn structures from datasets in order to make predictions on newly seen data. Such an approach offers a significant advantage in music scenarios in which musicians can teach the system to learn an idiosyncratic style, or can break the rules to explore the system's capacity in unexpected ways. In this chapter we draw on music, machine learning, and human-computer interaction to elucidate an understanding of machine learning algorithms as creative tools for music and the sonic arts. We motivate a new understanding of learning algorithms as human-computer interfaces. We show that, like other interfaces, learning algorithms can be characterised by the ways their affordances intersect with goals of human users. We also argue that the nature of interaction between users and algorithms impacts the usability and usefulness of those algorithms in profound ways. This human-centred view of machine learning motivates our concluding discussion of what it means to employ machine learning as a creative tool.\",\n",
       " \"In recent years, configuration problems have drawn tremendous attention because of their increasing prevalence and their big impact on system availability. We believe that many of these problems are attributable to today's configuration interfaces that have not evolved to accommodate the enormous shift of the system administrator group. Plain text files, as the de facto configuration interfaces, assume administrators' understanding of the system under configuration. They ask administrators to directly edit the corresponding entries with little guidance or assistance. However, this assumption no longer holds for todays administrator group which has expanded greatly to include non- and semi-professional administrators. In this paper, we provide an HCI view of today's configuration problems, and articulate system configuration as a new HCI problem. Moreover, we present the top obstacles to correctly and efficiently configuring software systems, and most importantly their implications on the design and implementation of new-generation configuration interfaces.\",\n",
       " 'Design Thinking workshops are used by companies to help generate new ideas for technologies and products by engaging subjects in exercises to understand their users\\' wants and become more empathetic towards their needs. The \"aha moment\" experienced during these thought-provoking, step outside the yourself activities occurs when a group of persons iterate over several problems and converge upon a solution that will fit seamlessly everyday life. With the increasing use and cost of Design workshops being offered, it is important that technology be developed that can help identify empathy and its onset in humans. This position paper presents an approach to modeling empathy using Gaussian mixture models and heart rate and skin conductance. This paper also presents an updated approach to Design Thinking that helps to ensure participants are thinking outside of their own race\\'s, culture\\'s, or other affiliations\\' motives.',\n",
       " \"This work addresses our research on driving skill modeling using artificial neural networks for haptic assistance. In this paper, we present a haptic driving training simulator with performance-based, error-corrective haptic feedback. One key component of our simulator is the ability to learn an optimized driving skill model from the driving data of expert drivers. To this end, we obtain a model utilizing artificial neural networks to extract a desired movement of a steering wheel and an accelerator pedal based on the experts' prediction. Then, we can deliver haptic assistance based on a driver's performance error which is a difference between a current and the desired movement. We validate the performance of our framework in two respective user experiments recruiting expert/novice drivers to show the feasibility and applicability of facilitating neural networks for performance-based haptic driving skill transfer.\",\n",
       " 'The uncertainty and variability of underwater environment propose the request to control underwater robots in real time and dynamically, especially in the scenarios where human and robots need to work collaboratively in the field. However, the underwater environment imposes harsh restrictions on the application of typical control and communication methods. Considering that gestures are a natural and efficient interactive way for human, we, utilizing convolution neural network, implement a real-time gesture-based recognition system, who can recognize 50 kinds of gestures from images captured by one normal monocular camera, and apply this recognition system in human and underwater robot interaction. We design A Flexible and Extendable Interaction Scheme (AFEIS) through which underwater robots can be programmed in situ underwater by human operators using customized gesture-based sign language. This paper elaborates the design of gesture recognition system and AFEIS, and presents our field trial results when applying this system and scheme on underwater robots.',\n",
       " 'The F-layout was introduced in 1955 and eventually enforced as a national standard as a replacement to the popular QWERTY keyboard layout in Turkey. In a more recent work, another alternative (E-layout) was developed for Turkish language and argued to be faster and more comfortable than the F-layout. However, there has not been any empirical evidence favouring any of these layouts so far. To fill this research gap in the literature, we have employed a hybrid model and conducted both between-subjects and within-subjects user experiments with twelve freshmen majoring in computer engineering. The experimental results show that there is no significant difference between learning percentage of these two layouts but the completion time of typing a trial passage with the F-layout is significantly lower than the E-layout. The F-layout has also a significantly lower physical demand score, as revealed by the subjective assessments of participants. Based on our user survey data, we also discuss some possible reasons of F-keyboard limited prevalence among Turkish users.',\n",
       " \"Mental Imagery based Brain-Computer Interfaces (MI-BCI) are a mean to control digital technologies by performing MI tasks alone. Throughout MI-BCI use, human supervision (e.g., experimenter or caregiver) plays a central role. While providing emotional and social feedback, people present BCIs to users and ensure smooth users' progress with BCI use. Though, very little is known about the influence experimenters might have on the results obtained. Such influence is to be expected as social and emotional feedback were shown to influence MI-BCI performances. Furthermore, literature from different fields showed an experimenter effect, and specifically of their gender, on experimental outcome. We assessed the impact of the interaction between experi-menter and participant gender on MI-BCI performances and progress throughout a session. Our results revealed an interaction between participants gender, experimenter gender and progress over runs. It seems to suggest that women experimenters may positively influence partici-pants' progress compared to men experimenters.\",\n",
       " 'Driving simulators can be used to test vehicle designs earlier, prior to building physical prototypes. One area of particular interest is winter testing since testing is limited to specific times of year and specific regions in the world. To ensure that the simulator is fit for purpose, an objective assessment is required. In this study a simulator and real world comparison was performed with three simulator configurations (standard, no steering torque, no motion) to assess the ability of a utility triplet of analyses to be able to quantify the differences between the real world and the different simulator configurations. The results suggest that the utility triplet is effective in measuring the differences in simulator configurations and that the developed Virtual Sweden environment achieved rather good behavioural fidelity in the sense of preserving absolute levels of many measures of behaviour. The main limitation in the simulated environment seemed to be the poor match of the dynamic lateral friction limit on snow and ice when compared to the real world.',\n",
       " 'Human thermal comfort measurement plays a critical role in giving feedback signals for building energy efficiency. A non-invasive measuring method based on subtleness magnification and deep learning (NIDL) was designed to achieve a comfortable, energy efficient built environment. The method relies on skin feature data, e.g., subtle motion and texture variation, and a 315-layer deep neural network for constructing the relationship between skin features and skin temperature. A physiological experiment was conducted for collecting feature data (1.44 million) and algorithm validation. The non-invasive measurement algorithm based on a partly-personalized saturation temperature model (NIPST) was used for algorithm performance comparisons. The results show that the mean error and median error of the NIDL are 0.4834 Celsius and 0.3464 Celsius which is equivalent to accuracy improvements of 16.28% and 4.28%, respectively.',\n",
       " 'Semi-supervised learning is crucial for alleviating labelling burdens in people-centric sensing. However, human-generated data inherently suffer from distribution shift in semi-supervised learning due to the diverse biological conditions and behavior patterns of humans. To address this problem, we propose a generic distributionally robust model for semi-supervised learning on distributionally shifted data. Considering both the discrepancy and the consistency between the labeled data and the unlabeled data, we learn the latent features that reduce person-specific discrepancy and preserve task-specific consistency. We evaluate our model in a variety of people-centric recognition tasks on real-world datasets, including intention recognition, activity recognition, muscular movement recognition and gesture recognition. The experiment results demonstrate that the proposed model outperforms the state-of-the-art methods.',\n",
       " 'Intelligent agents such as Alexa, Siri, and Google Assistant are now built into streaming TV systems, allowing people to use voice input to navigate the increasingly complex set of apps available on a TV. However, these systems typically support a narrow range of control- and search-oriented commands, and do not support deeper recommendation or exploration queries. To learn about how people interact with a recommendation-oriented voice-controlled TV, we use research through design methods to explore an early prototype movie recommendation system where the only input modality is voice. We describe in-depth qualitative research sessions with 11 participants. We contribute implications for designers of voice-controlled TV: mitigating the drawbacks of voice-only interactions, navigating the tension between expressiveness and efficiency, and building voice-driven recommendation interfaces that facilitate exploration.',\n",
       " 'We present a novel multi-modal bio-sensing platform capable of integrating multiple data streams for use in real-time applications. The system is composed of a central compute module and a companion headset. The compute node collects, time-stamps and transmits the data while also providing an interface for a wide range of sensors including electroencephalogram, photoplethysmogram, electrocardiogram, and eye gaze among others. The companion headset contains the gaze tracking cameras. By integrating many of the measurements systems into an accessible package, we are able to explore previously unanswerable questions ranging from open-environment interactions to emotional response studies. Though some of the integrated sensors are designed from the ground-up to fit into a compact form factor, we validate the accuracy of the sensors and find that they perform similarly to, and in some cases better than, alternatives.',\n",
       " \"Distributed, parallel crowd workers can accomplish simple tasks through workflows, but teams of collaborating crowd workers are necessary for complex goals. Unfortunately, a fundamental condition for effective teams - familiarity with other members - stands in contrast to crowd work's flexible, on-demand nature. We enable effective crowd teams with Huddler, a system for workers to assemble familiar teams even under unpredictable availability and strict time constraints. Huddler utilizes a dynamic programming algorithm to optimize for highly familiar teammates when individual availability is unknown. We first present a field experiment that demonstrates the value of familiarity for crowd teams: familiar crowd teams doubled the performance of ad-hoc (unfamiliar) teams on a collaborative task. We then report a two-week field deployment wherein Huddler enabled crowd workers to convene highly familiar teams in 18 minutes on average. This research advances the goal of supporting long-term, team-based collaborations without sacrificing the flexibility of crowd work.\",\n",
       " 'Human-computer interaction (HCI) studies the design and use of interfaces and interactive systems. HCI has been adopted successfully in modern commercial products. Recently, its use for promoting social good and pursuing sustainability, known as sustainable HCI, has begun to receive wide attention. Conventionally, scientists and decision-makers apply top-down approaches to lead research activities that engage lay people in facilitating sustainability, such as saving energy. We introduce an alternative framework, Community Citizen Science (CCS), to closely connect research and social issues by empowering communities to produce scientific knowledge, represent their needs, address their concerns, and advocate for impact. CCS advances the current science-oriented concept to a deeper level that aims to sustain community engagement when researchers are no longer involved after the intervention of interactive systems.',\n",
       " \"Knowing who is in one's vicinity is key to managing privacy in everyday environments, but is challenging for people with visual impairments. Wearable cameras and other sensors may be able to detect such information, but how should this complex visually-derived information be conveyed in a way that is discreet, intuitive, and unobtrusive? Motivated by previous studies on the specific information that visually impaired people would like to have about their surroundings, we created three medium-fidelity prototypes: 1) a 3D printed model of a watch to convey tactile information; 2) a smartwatch app for haptic feedback; and 3) a smartphone app for audio feedback. A usability study with 14 participants with visual impairments identified a range of practical issues (e.g., speed of conveying information) and design considerations (e.g., configurable privacy bubble) for conveying privacy feedback in real-world contexts.\",\n",
       " 'The application of mobile computing is currently altering patterns of our behavior to a greater degree than perhaps any other invention. In combination with the introduction of BLE (Bluetooth Low Energy) and similar technologies enabling context-awareness, designers are today finding themselves empowered to build experiences and facilitate interactions with our physical surroundings in ways not possible before. The aim of this thesis is to present a research project, currently underway at the University of Cambridge, which is dealing with implementation of a BLE system into a museum environment. By assessing the technology, describing the design decisions as well as presenting a qualitative evaluation, this paper seeks to provide insight into some of the challenges and possible solutions connected to the process of developing ubiquitous BLE computing systems for public spaces. The project outcome revealed the potential use of BLE to engage whole new groups of audiences as well as made me argue in favor of a more seamful approach to the design of these systems.',\n",
       " 'Making decisions about what clinical tasks to prepare for is multi-factored, and especially challenging in intensive care environments where resources must be balanced with patient needs. Electronic health records (EHRs) are a rich data source, but are task-agnostic and can be difficult to use as summarizations of patient needs for a specific task, such as \"could this patient need a ventilator tomorrow?\" In this paper, we introduce ClinicalVis, an open-source EHR visualization-based prototype system for task-focused design evaluation of interactions between healthcare providers (HCPs) and EHRs. We situate ClinicalVis in a task-focused proof-of-concept design study targeting these interactions with real patient data. We conduct an empirical study of 14 HCPs, and discuss our findings on usability, accuracy, preference, and confidence in treatment decisions. We also present design implications that our findings suggest for future EHR interfaces, the presentation of clinical data for task-based planning, and evaluating task-focused HCP/EHR interactions in practice.',\n",
       " 'Invasive species are a major cause of ecological damage and commercial losses. A current problem spreading in North America and Europe is the vinegar fly Drosophila suzukii. Unlike other Drosophila, it infests non-rotting and healthy fruits and is therefore of concern to fruit growers, such as vintners. Consequently, large amounts of data about infestations have been collected in recent years. However, there is a lack of interactive methods to investigate this data. We employ ensemble-based classification to predict areas susceptible to infestation by D. suzukii and bring them into a spatio-temporal context using maps and glyph-based visualizations. Following the information-seeking mantra, we provide a visual analysis system Drosophigator for spatio-temporal event prediction, enabling the investigation of the spread dynamics of invasive species. We demonstrate the usefulness of this approach in two use cases.',\n",
       " 'In this paper we focus on the International Journal of Human-Computer Studies (IJHCS) as a domain of analysis, to gain insights about its evolution in the past 50 years and what this evolution tells us about the research landscape associated with the journal. To this purpose we use techniques from the field of Science of Science and analyse the relevant scholarly data to identify a variety of phenomena, including significant geopolitical patterns, the key trends that emerge from a topic-centric analysis, and the insights that can be drawn from an analysis of citation data. Because the area of Human-Computer Interaction (HCI) has always been a central focus for IJHCS, we also include in the analysis the CHI conference, which is the premiere scientific venue in HCI. Analysing both venues provides more data points to our study and allows us to consider two alternative viewpoints on the evolution of HCI research.',\n",
       " 'Internet of Things (IoT) systems are bundles of networked sensors and actuators that are deployed in an environment and act upon the sensory data that they receive. These systems, especially consumer electronics, have two main cooperating components: a device and a mobile app. The unique combination of hardware and software in IoT systems presents challenges that are lesser known to mainstream software developers. They might require innovative solutions to support the development and integration of such systems. In this paper, we analyze more than 90,000 reviews of ten IoT devices and their corresponding apps and extract the issues that users encountered while using these systems. Our results indicate that issues with connectivity, timing, and updates are particularly prevalent in the reviews. Our results call for a new software-hardware development framework to assist the development of reliable IoT systems.',\n",
       " \"Navigation applications are becoming ubiquitous in our daily navigation experiences. With the intention to circumnavigate congested roads, their route guidance always follows the basic assumption that drivers always want the fastest route. However, it is unclear how their recommendations are followed and what factors affect their adoption. We present the results of a semi-structured qualitative study with 17 drivers, mostly from the Philippines and Japan. We recorded their daily commutes and occasional trips, and inquired into their navigation practices, route choices and on-the-fly decision-making. We found that while drivers choose a recommended route in urgent situations, many still preferred to follow familiar routes. Drivers deviated because of a recommendation's use of unfamiliar roads, lack of local context, perceived driving unsuitability, and inconsistencies with realized navigation experiences. Our findings and implications emphasize their personalization needs, and how the right amount of algorithmic sophistication can encourage behavioral adaptation.\",\n",
       " 'Effective data analysis ideally requires the analyst to have high expertise as well as high knowledge of the data. Even with such familiarity, manually pursuing all potential hypotheses and exploring all possible views is impractical. We present DataSite, a proactive visual analytics system where the burden of selecting and executing appropriate computations is shared by an automatic server-side computation engine. Salient features identified by these automatic background processes are surfaced as notifications in a feed timeline. DataSite effectively turns data analysis into a conversation between analyst and computer, thereby reducing the cognitive load and domain knowledge requirements. We validate the system with a user study comparing it to a recent visualization recommendation system, yielding significant improvement, particularly for complex analyses that existing analytics systems do not support well.',\n",
       " 'This paper describes \"ARbis Pictus\" --a novel system for immersive language learning through dynamic labeling of real-world objects in augmented reality. We describe a within-subjects lab-based study (N=52) that explores the effect of our system on participants learning nouns in an unfamiliar foreign language, compared to a traditional flashcard-based approach. Our results show that the immersive experience of learning with virtual labels on real-world objects is both more effective and more enjoyable for the majority of participants, compared to flashcards. Specifically, when participants learned through augmented reality, they scored significantly better by 7% (p=0.011) on productive recall tests performed same-day, and significantly better by 21% (p=0.001) on 4-day delayed productive recall post tests than when they learned using the flashcard method. We believe this result is an indication of the strong potential for language learning in augmented reality, particularly because of the improvement shown in sustained recall compared to the traditional approach.',\n",
       " 'One of the most crucial issues in data mining is to model human behaviour in order to provide personalisation, adaptation and recommendation. This usually involves implicit or explicit knowledge, either by observing user interactions, or by asking users directly. But these sources of information are always subject to the volatility of human decisions, making utilised data uncertain to a particular extent. In this contribution, we elaborate on the impact of this human uncertainty when it comes to comparative assessments of different data mining approaches. In particular, we reveal two problems: (1) biasing effects on various metrics of model-based prediction and (2) the propagation of uncertainty and its thus induced error probabilities for algorithm rankings. For this purpose, we introduce a probabilistic view and prove the existence of those problems mathematically, as well as provide possible solution strategies. We exemplify our theory mainly in the context of recommender systems along with the metric RMSE as a prominent example of precision quality measures.',\n",
       " 'This paper outlines the development and testing of a novel, feedback-enabled attention allocation aid (AAAD), which uses real-time physiological data to improve human performance in a realistic sequential visual search task. Indeed, by optimizing over search duration, the aid improves efficiency, while preserving decision accuracy, as the operator identifies and classifies targets within simulated aerial imagery. Specifically, using experimental eye-tracking data and measurements about target detectability across the human visual field, we develop functional models of detection accuracy as a function of search time, number of eye movements, scan path, and image clutter. These models are then used by the AAAD in conjunction with real time eye position data to make probabilistic estimations of attained search accuracy and to recommend that the observer either move on to the next image or continue exploring the present image. An experimental evaluation in a scenario motivated from human supervisory control in surveillance missions confirms the benefits of the AAAD.',\n",
       " \"Automatic detection of emotion has the potential to revolutionize mental health and wellbeing. Recent work has been successful in predicting affect from unimodal electrocardiogram (ECG) data. However, to be immediately relevant for real-world applications, physiology-based emotion detection must make use of ubiquitous photoplethysmogram (PPG) data collected by affordable consumer fitness trackers. Additionally, applications of emotion detection in healthcare settings will require some measure of uncertainty over model predictions. We present here a Bayesian deep learning model for end-to-end classification of emotional valence, using only the unimodal heartbeat time series collected by a consumer fitness tracker (Garmin V\\\\'ivosmart 3). We collected a new dataset for this task, and report a peak F1 score of 0.7. This demonstrates a practical relevance of physiology-based emotion detection `in the wild' today.\",\n",
       " \"Pedestrian's road crossing behaviour is one of the important aspects of urban dynamics that will be affected by the introduction of autonomous vehicles. In this study we introduce DeepSurvival, a novel framework for estimating pedestrian's waiting time at unsignalized mid-block crosswalks in mixed traffic conditions. We exploit the strengths of deep learning in capturing the nonlinearities in the data and develop a cox proportional hazard model with a deep neural network as the log-risk function. An embedded feature selection algorithm for reducing data dimensionality and enhancing the interpretability of the network is also developed. We test our framework on a dataset collected from 160 participants using an immersive virtual reality environment. Validation results showed that with a C-index of 0.64 our proposed framework outperformed the standard cox proportional hazard-based model with a C-index of 0.58.\",\n",
       " \"Personality affect the way someone feels or acts. This paper examines the effect of personality traits, as operationalized by the Big-five questionnaire, on the number, type, and severity of the identified usability issues, physiological signals (skin conductance), and subjective emotional ratings (valence-arousal).Twenty-four users interacted with a web service and then participated in a retrospective thinking aloud session. Results revealed that the number of usability issues is significantly affected by the Openness trait. Emotional Stability significantly affects the type of reported usability issues. Problem severity is not affected by any trait. Valence ratings are significantly affected by Conscientiousness, whereas Agreeableness, Emotional Stability and Openness significantly affect arousal ratings. Finally, Openness has a significant effect on the number of detected peaks in user's skin conductance.\",\n",
       " 'Effective task management is essential to successful team collaboration. While the past decade has seen considerable innovation in systems that track and manage group tasks, these innovations have typically been outside of the principal communication channels: email, instant messenger, and group chat. Teams formulate, discuss, refine, assign, and track the progress of their collaborative tasks over electronic communication channels, yet they must leave these channels to update their task-tracking tools, creating a source of friction and inefficiency. To address this problem, we explore how bots might be used to mediate task management for individuals and teams. We deploy a prototype bot to eight different teams of information workers to help them create, assign, and keep track of tasks, all within their main communication channel. We derived seven insights for the design of future bots for coordinating work.',\n",
       " 'Proactive decision support (PDS) helps in improving the decision making experience of human decision makers in human-in-the-loop planning environments. Here both the quality of the decisions and the ease of making them are enhanced. In this regard, we propose a PDS framework, named RADAR, based on the research in Automated Planning in AI, that aids the human decision maker with her plan to achieve her goals by providing alerts on: whether such a plan can succeed at all, whether there exist any resource constraints that may foil her plan, etc. This is achieved by generating and analyzing the landmarks that must be accomplished by any successful plan on the way to achieving the goals. Note that, this approach also supports naturalistic decision making which is being acknowledged as a necessary element in proactive decision support, since it only aids the human decision maker through suggestions and alerts rather than enforcing fixed plans or decisions. We demonstrate the utility of the proposed framework through search-and-rescue examples in a fire-fighting domain.',\n",
       " 'Objective: Effective collaboration between machines and clinicians requires flexible data structures to represent medical processes and clinical practice guidelines. Such a data structure could enable effective turn-taking between human and automated components of a complex treatment, accurate on-line monitoring of clinical treatments (for example to detect medical errors), or automated treatment systems (such as future medical robots) whose overall treatment plan is understandable and auditable by human experts.\\n  Materials and Methods: Behavior trees (BTs) emerged from video game development as a graphical language for modeling intelligent agent behavior. BTs have several properties which are attractive for modeling medical procedures including human-readability, authoring tools, and composability.\\n  Results: This paper will illustrate construction of BTs for exemplary medical procedures and clinical protocols.\\n  Discussion and Conclusion: Behavior Trees thus form a useful, and human authorable/readable bridge between clinical practice guidelines and AI systems.',\n",
       " \"Android is Google's latest open source software platform for mobile devices which has already attained enormous popularity. The purpose of this paper is to describe the development of mobile application for shopping mall using Android platform. A prototype was developed for the shoppers of Bashundhara Shopping Mall of Bangladesh. This prototype will serve as a framework for any such applications (apps). The paper presents a practical demonstration of how to integrate shops' information, such as names, categories, locations, descriptions, floor layout and so forth, with map module via an android application. A summary of survey results of the related literature and projects have also been included. Critical Evaluation of the prototype along with future research and development plan has been revealed. The paper will serve as a guideline for the researchers and developers to introduce and develop similar apps.\",\n",
       " \"The increasing prevalence of Virtual Reality technologies as a platform for gaming and video playback warrants research into how to best apply the current state of the art to challenges in data visualization. Many current VR systems are noncollaborative, while data analysis and visualization is often a multi-person process. Our goal in this paper is to address the technical and user experience challenges that arise when creating VR environments for collaborative data visualization. We focus on the integration of multiple tracking systems and the new interaction paradigms that this integration can enable, along with visual design considerations that apply specifically to collaborative network visualization in virtual reality. We demonstrate a system for collaborative interaction with large 3D layouts of Twitter friend/follow networks. The system is built by combining a 'Holojam' architecture (multiple GearVR Headsets within an OptiTrack motion capture stage) and Perception Neuron motion suits, to offer an untethered, full-room multi-person visualization experience.\",\n",
       " \"In this work, we address the symptoms of cognitive depletion as they relate to generalized knowledge workers. We unify previous findings within a single analytical model of cognitive depletion. Our purpose is to develop a model that will help us predict when a person has reached a sufficient state of cognitive depletion such that taking a break or some other restorative action will benefit both his or her own wellbeing and the quality of his or her performance. We provide a definition of each symptom in our model as well as the effect it would have on a knowledge worker's ability to work productively. We discuss methods to detect each symptom that do not require self assessment. Understanding symptoms of cognitive depletion provides the ability to support human knowledge workers by reducing the stress involved with cognitive and work overload while maintaining or improving the quality of their performance.\",\n",
       " 'Pluridisciplinar convergence is a major problem that had emerged with Human-Artefact Systems and so-called \" Augmented Humanity \" as academical fields and even more as technical fields. Problems come mainly from the juxtaposition of two very different types of system, a biological one and an artificial one. Thus, conceiving and designing the multiple couplings between them has become a major difficulty. Some came with reductionnist solutions to answer these problems but since we know that a biological system and a technical system are different, this approach is limited from its beginning. Using a specifically designed questionnaire and statistical analysis we determined how specialists (medical practitioners, ergonomists and engineers) in the domain conceive themselves what is a Human-Artifact System and how they relate to existent traditions and showed that some of them relate to the integrativist views.',\n",
       " 'In this paper, we present an abstract model of visualization and inference processes and describe an information-theoretic measure for optimizing such processes. In order to obtain such an abstraction, we first examined six classes of workflows in data analysis and visualization, and identified four levels of typical visualization components, namely disseminative, observational, analytical and model-developmental visualization. We noticed a common phenomenon at different levels of visualization, that is, the transformation of data spaces (referred to as alphabets) usually corresponds to the reduction of maximal entropy along a workflow. Based on this observation, we establish an information-theoretic measure of cost-benefit ratio that may be used as a cost function for optimizing a data visualization process. To demonstrate the validity of this measure, we examined a number of successful visualization processes in the literature, and showed that the information-theoretic measure can mathematically explain the advantages of such processes over possible alternatives.',\n",
       " 'Conversational agents promise conversational interaction but fail to deliver. Efforts often emulate functional rules from human speech, without considering key characteristics that conversation must encapsulate. Given its potential in supporting long-term human-agent relationships, it is paramount that HCI focuses efforts on delivering this promise. We aim to understand what people value in conversation and how this should manifest in agents. Findings from a series of semi-structured interviews show people make a clear dichotomy between social and functional roles of conversation, emphasising the long-term dynamics of bond and trust along with the importance of context and relationship stage in the types of conversations they have. People fundamentally questioned the need for bond and common ground in agent communication, shifting to more utilitarian definitions of conversational qualities. Drawing on these findings we discuss key challenges for conversational agent design, most notably the need to redefine the design parameters for conversational agent interaction.',\n",
       " 'This paper studies the concept of color semantics by modeling a dataset of magazine cover designs, evaluating the model via crowdsourcing, and demonstrating several prototypes that facilitate color-related design tasks. We investigate a probabilistic generative modeling framework that expresses semantic concepts as a combination of color and word distributions $-$color-word topics. We adopt an extension to Latent Dirichlet Allocation (LDA) topic modeling called LDA-dual to infer a set of color-word topics over a corpus of 2,654 magazine covers spanning 71 distinct titles and 12 genres. While LDA models text documents as distributions over word topics, we model magazine covers as distributions over color-word topics. The results of our crowdsourced experiments confirm that the model is able to successfully discover the associations between colors and linguistic concepts. Finally, we demonstrate several simple prototypes that apply the learned model to color palette recommendation, design example retrieval, image retrieval, image color selection, and image recoloring.',\n",
       " 'In this study, we developed a method to estimate the relationship between stimulation current and volatility during isometric contraction. In functional electrical stimulation (FES), joints are driven by applying voltage to muscles. This technology has been used for a long time in the field of rehabilitation, and recently application oriented research has been reported. However, estimation of the relationship between stimulus value and exercise capacity has not been discussed to a great extent. Therefore, in this study, a human muscle model was estimated using the transfer function estimation method with fast Fourier transform. It was found that the relationship between stimulation current and force exerted could be expressed by a first-order lag system. In verification of the force estimate, the ability of the proposed model to estimate the exerted force under steady state response was found to be good.',\n",
       " 'Result diversification is an important aspect in query events, web-based search, facility location and other applications. To satisfy more users in event-based social networks(EBSNs), search result diversification in an event that covers as many user intents as possible. Most existing result diversification algorithms recognize an user may search for information by issuing the different query as much as possible. In this paper, we leverage many different users in a same event such that satisfy the maximum benefit of users, where users want to participate in an event that s/he did not know any users, for example, blind date, Greek and other activities. To solve this problem, we devise an effective greedy heuristic method and integrate simulated annealing techniques to optimize the algorithm performance. In particular, the Greedy algorithm is more effective but less efficient than Integrate Simulated Annealing in most cases. Finally, we conduct extensive experiments on real and synthetic datasets which verify the efficiency and effectiveness of our proposed algorithms.',\n",
       " \"Room-level air conditioners (also referred as ACs) consume a significant proportion of total energy in residential and small-scale commercial buildings. In a typical AC, occupants specify their comfort requirements by manually setting the desired temperature on the thermostat. Though commercial thermostats (such as Tado) provide basic energy-saving features, they neither consider the influence of external factors (such as weather) to set the thermostat temperature nor offer advanced features such as monitoring the fitness level of AC. In this paper, we discuss grey-box modeling techniques to enhance existing thermostats for energy-efficient control of the ACs and provide actionable and corrective feedback to the users. Our study indicates that the enhancements can reduce occupants' discomfort by 23% when maximising the user experience, and reduce AC energy consumption by 26% during the power-saving mode.\",\n",
       " 'Though detection systems have been developed to identify obscene content such as pornography and violence, artificial intelligence is simply not good enough to fully automate this task yet. Due to the need for manual verification, social media companies may hire internal reviewers, contract specialized workers from third parties, or outsource to online labor markets for the purpose of commercial content moderation. These content moderators are often fully exposed to extreme content and may suffer lasting psychological and emotional damage. In this work, we aim to alleviate this problem by investigating the following question: How can we reveal the minimum amount of information to a human reviewer such that an objectionable image can still be correctly identified? We design and conduct experiments in which blurred graphic and non-graphic images are filtered by human moderators on Amazon Mechanical Turk (AMT). We observe how obfuscation affects the moderation experience with respect to image classification accuracy, interface usability, and worker emotional well-being.',\n",
       " 'Crowd-powered conversational assistants have been shown to be more robust than automated systems, but do so at the cost of higher response latency and monetary costs. A promising direction is to combine the two approaches for high quality, low latency, and low cost solutions. In this paper, we introduce Evorus, a crowd-powered conversational assistant built to automate itself over time by (i) allowing new chatbots to be easily integrated to automate more scenarios, (ii) reusing prior crowd answers, and (iii) learning to automatically approve response candidates. Our 5-month-long deployment with 80 participants and 281 conversations shows that Evorus can automate itself without compromising conversation quality. Crowd-AI architectures have long been proposed as a way to reduce cost and latency for crowd-powered systems; Evorus demonstrates how automation can be introduced successfully in a deployed system. Its architecture allows future researchers to make further innovation on the underlying automated components in the context of a deployed open domain dialog system.',\n",
       " 'Many different approaches for estimating the Interaction Quality (IQ) of Spoken Dialogue Systems have been investigated. While dialogues clearly have a sequential nature, statistical classification approaches designed for sequential problems do not seem to work better on automatic IQ estimation than static approaches, i.e., regarding each turn as being independent of the corresponding dialogue. Hence, we analyse this effect by investigating the subset of temporal features used as input for statistical classification of IQ. We extend the set of temporal features to contain the system and the user view. We determine the contribution of each feature sub-group showing that temporal features contribute most to the classification performance. Furthermore, for the feature sub-group modeling the temporal effects with a window, we modify the window size increasing the overall performance significantly by +15.69%.',\n",
       " 'The performance of brain-computer interfaces (BCIs) improves with the amount of available training data, the statistical distribution of this data, however, varies across subjects as well as across sessions within individual subjects, limiting the transferability of training data or trained models between them. In this article, we review current transfer learning techniques in BCIs that exploit shared structure between training data of multiple subjects and/or sessions to increase performance. We then present a framework for transfer learning in the context of BCIs that can be applied to any arbitrary feature space, as well as a novel regression estimation method that is specifically designed for the structure of a system based on the electroencephalogram (EEG). We demonstrate the utility of our framework and method on subject-to-subject transfer in a motor-imagery paradigm as well as on session-to-session transfer in one patient diagnosed with amyotrophic lateral sclerosis (ALS), showing that it is able to outperform other comparable methods on an identical dataset.',\n",
       " 'The number of applications on online mobile application stores is increasing at a rapid rate. Smart-phones are used by a wide range of people varying in age, and also in the ability to use a smart phone. With the increasing dependency on smart-phones, the paper aims to determine whether the popular applications on Google Play, the official store for Android applications, can be used by people with vision impairment. The accessibility of the applications was tested using an external keyboard, and TalkBack, an accessibility tool developed by Google. It was found that several popular applications on the store were not designed keeping accessibility in mind. It was observed that there exists a weak positive relationship between the popularity of the application and its accessibility. A framework is proposed that can be used by developers to improve the accessibility of an application. The paper also discusses the programming aspects to be considered while developing an Android application, so that the application can be used by sighted as well as visually impaired users.',\n",
       " 'Natural language programming is a promising approach to enable end users to instruct new tasks for intelligent agents. However, our formative study found that end users would often use unclear, ambiguous or vague concepts when naturally instructing tasks in natural language, especially when specifying conditionals. Existing systems have limited support for letting the user teach agents new concepts or explaining unclear concepts. In this paper, we describe a new multi-modal domain-independent approach that combines natural language programming and programming-by-demonstration to allow users to first naturally describe tasks and associated conditions at a high level, and then collaborate with the agent to recursively resolve any ambiguities or vagueness through conversations and demonstrations. Users can also define new procedures and concepts by demonstrating and referring to contents within GUIs of existing mobile apps. We demonstrate this approach in PUMICE, an end-user programmable agent that implements this approach. A lab study with 10 users showed its usability.',\n",
       " \"Children under 11 are often regarded as too young to comprehend the implications of online privacy. Perhaps as a result, little research has focused on younger kids' risk recognition and coping. Such knowledge is, however, critical for designing efficient safeguarding mechanisms for this age group. Through 12 focus group studies with 29 children aged 6-10 from UK schools, we examined how children described privacy risks related to their use of tablet computers and what information was used by them to identify threats. We found that children could identify and articulate certain privacy risks well, such as information oversharing or revealing real identities online; however, they had less awareness with respect to other risks, such as online tracking or game promotions. Our findings offer promising directions for supporting children's awareness of cyber risks and the ability to protect themselves online.\",\n",
       " 'In this paper we describe the requirements and early system design for a smart conversational agent that can assist older adults in the reminiscence process. The practice of reminiscence has well documented benefits for the mental, social and emotional well-being of older adults. However, the technology support, valuable in many different ways, is still limited in terms of need of co-located human presence, data collection capabilities, and ability to support sustained engagement, thus missing key opportunities to improve care practices, facilitate social interactions, and bring the reminiscence practice closer to those with less opportunities to engage in co-located sessions with a (trained) companion. We discuss conversational agents and cognitive services as the platform for building the next generation of reminiscence applications, and introduce the concept application of a smart reminiscence agent.',\n",
       " 'We investigate crowdsourcing algorithms for finding the top-quality item within a large collection of objects with unknown intrinsic quality values. This is an important problem with many relevant applications, for example in networked recommendation systems. The core of the algorithms is that objects are distributed to crowd workers, who return a noisy and biased evaluation. All received evaluations are then combined, to identify the top-quality object. We first present a simple probabilistic model for the system under investigation. Then, we devise and study a class of efficient adaptive algorithms to assign in an effective way objects to workers. We compare the performance of several algorithms, which correspond to different choices of the design parameters/metrics. In the simulations we show that some of the algorithms achieve near optimal performance for a suitable setting of the system parameters.',\n",
       " 'Big data often has emergent structure that exists at multiple levels of abstraction, which are useful for characterizing complex interactions and dynamics of the observations. Here, we consider multiple levels of abstraction via a multiresolution geometry of data points at different granularities. To construct this geometry we define a time-inhomogeneous diffusion process that effectively condenses data points together to uncover nested groupings at larger and larger granularities. This inhomogeneous process creates a deep cascade of intrinsic low pass filters in the data that are applied in sequence to gradually eliminate local variability while adjusting the learned data geometry to increasingly coarser resolutions. We provide visualizations to exhibit our method as a \"continuously-hierarchical\" clustering with directions of eliminated variation highlighted at each step. The utility of our algorithm is demonstrated via neuronal data condensation, where the constructed multiresolution data geometry uncovers the organization, grouping, and connectivity between neurons.',\n",
       " 'Taller and sleeker smartphone devices are becoming the new norm. More screen space and very responsive touchscreens have made for enjoyable experiences available to us at all times. However, after years of interacting with smaller, portable devices, we still try to use these large smartphones on the go, and do not want to change how, where, and when we interact with them. The older devices were easier to use with one hand, when mobile. Now, with bigger devices, users have trouble accessing all parts of the screen with one hand. We need to recognize the limitations in usability due to these large screens. We must start designing user interfaces that are more conducive to one hand usage, which is the preferred way of interacting with the phone. This paper introduces Adaptive App Design, a design methodology that promotes dynamic and adaptive interfaces for one handed usage. We present a novel method of recognizing which hand the user is interacting with and suggest how to design friendlier interfaces for them by presenting a set of design guidelines for this methodology.',\n",
       " 'Users of Virtual Reality (VR) systems often experience vection, the perception of self-motion in the absence of any physical movement. While vection helps to improve presence in VR, it often leads to a form of motion sickness called cybersickness. Cybersickness is a major deterrent to large scale adoption of VR.\\n  Prior work has discovered that changing vection (changing the perceived speed or moving direction) causes more severe cybersickness than steady vection (walking at a constant speed or in a constant direction). Based on this idea, we try to reduce the cybersickness caused by character movements in a First Person Shooter (FPS) game in VR. We propose Rotation Blurring (RB), uniformly blurring the screen during rotational movements to reduce cybersickness. We performed a user study to evaluate the impact of RB in reducing cybersickness. We found that the blurring technique led to an overall reduction in sickness levels of the participants and delayed its onset. Participants who experienced acute levels of cybersickness benefited significantly from this technique.',\n",
       " \"We describe a case study with the participation of a Danish veteran suffering from post-traumatic stress disorder (PTSD). As part of psychotherapeutic treatment the participant and therapist have used our novel technique for instrumenting self-tracking of select aspects of subjective experience using a one-button wearable device. The instrumentation system is described along with the specific self-track- ing protocol which defined the participant's self-tracking of a single symptom, namely the occurrences of a bodily experienced precursor to hyperarousal. Results from the case study demonstrate how self-tracking data on a single symptom collected by a patient can provide valuable input to the therapeutic process. Specifically, it facilitated identification of crucial details otherwise unavailable from the clinical assessment and even became decisive in disentangling different symptoms and their causes.\",\n",
       " 'Urban air pollution has been linked to various human health considerations, including cardiopulmonary diseases. Communities who suffer from poor air quality often rely on experts to identify pollution sources due to the lack of accessible tools. Taking this into account, we developed Smell Pittsburgh, a system that enables community members to report odors and track where these odors are frequently concentrated. All smell report data are publicly accessible online. These reports are also sent to the local health department and visualized on a map along with air quality data from monitoring stations. This visualization provides a comprehensive overview of the local pollution landscape. Additionally, with these reports and air quality data, we developed a model to predict upcoming smell events and send push notifications to inform communities. Our evaluation of this system demonstrates that engaging residents in documenting their experiences with pollution odors can help identify local air pollution patterns, and can empower communities to advocate for better air quality.',\n",
       " 'Mobile users today interact with a variety of mobile device types including smartphones, tablets, smartwatches, and others. However research on mobile device type substitution has been limited in several respects including a lack of detailed and robust analyses. Therefore, in this work we study mobile device type substitution through analysis of multidevice usage data from a large US-based user panel. Specifically, we use regression analysis over paired user groups to test five device type substitution hypotheses. We find that both tablets and PCs are partial substitutes for smartphones with tablet and PC ownership decreasing smartphone usage by about 12.5 and 13 hours/month respectively. Additionally, we find that tablets and PCs also prompt about 20 and 57 hours/month respectively of additional (non-substituted) usage. We also illustrate significant inter-user diversity in substituted and additional usage. Overall, our results can help in understanding the relative positioning of different mobile device types and in parameterizing higher level mobile ecosystem models.',\n",
       " \"The increasing application of social and human-enabled systems in people's daily life from one side and from the other side the fast growth of mobile and smart phones technologies have resulted in generating tremendous amount of data, also referred to as big data, and a need for analyzing these data, i.e., big data analytics. Recently a trend has emerged to incorporate human computing power into big data analytics to solve some shortcomings of existing big data analytics such as dealing with semi or unstructured data. Including crowd into big data analytics creates some new challenges such as security, privacy and availability issues.\\n  In this paper study hybrid human-machine big data analytics and propose a framework to study these systems from crowd involvement point of view. We identify some open issues in the area and propose a set of research directions for the future of big data analytics area.\",\n",
       " 'Usability and user experience (UX) issues are often not well emphasized and addressed in open source software (OSS) development. There is an imperative need for supporting OSS communities to collaboratively identify, understand, and fix UX design issues in a distributed environment. In this paper, we provide an initial step towards this effort and report on an exploratory study that investigated how the OSS communities currently reported, discussed, negotiated, and eventually addressed usability and UX issues. We conducted in-depth qualitative analysis of selected issue tracking threads from three OSS projects hosted on GitHub. Our findings indicated that discussions about usability and UX issues in OSS communities were largely influenced by the personal opinions and experiences of the participants. Moreover, the characteristics of the community may have greatly affected the focus of such discussion.',\n",
       " 'Tactile perception plays an important role in medical simulation and training, specifically in surgery. The surgeon must feel organic tissue hardness, evaluate anatomical structures, measure tissue properties, and apply appropriate force control actions for safe tissue manipulation. Development of novel cost effective haptic-based simulators and their introduction in the minimally invasive surgery learning cycle can absorb the learning curve for residents. Receiving pre-training in a core set of surgical skills can reduce skill acquisition time and risks. We present the development of a cost-effective visuo-haptic simulator for the liver tissue, designed to improve practice-based education in minimally invasive surgery. Such systems can positively affect the next generations of learners by enhancing their knowledge in connection with real-life situations while they train in mandatory safe conditions.',\n",
       " \"In brain machine interfaces (BMI) that are used to control motor rehabilitation devices there is the need to process the monitored brain signals with the purpose of recognizing patient's intentions to move his hands or limbs and reject artifact and noise superimposed on these signals. This kind of processing has to take place within time limits imposed by the on-line control requirements of such devices. A widely-used algorithm is the Second Order Blind Identification (SOBI) independent component analysis (ICA) algorithm. This algorithm, however, presents long processing time and therefor it not suitable for use in the brain-based control of rehabilitation devices. A rework of this algorithm that is presented in this paper and based on SCHUR decomposition results to significantly reduced processing time. This new algorithm is quite appropriate for use in brain-based control of rehabilitation devices.\",\n",
       " \"Our work aims to generate visualizations to enable meta-analysis of analytic provenance and aid better understanding of analysts' strategies during exploratory text analysis. We introduce ProvThreads, a visual analytics approach that incorporates interactive topic modeling outcomes to illustrate relationships between user interactions and the data topics under investigation. ProvThreads uses a series of continuous analysis paths called topic threads to demonstrate both topic coverage and the progression of an investigation over time. As an analyst interacts with different pieces of data during the analysis, interactions are logged and used to track user interests in topics over time. A line chart shows different amounts of interest in multiple topics over the duration of the analysis. We discuss how different configurations of ProvThreads can be used to reveal changes in focus throughout an analysis.\",\n",
       " 'We present AirCode, a technique that allows the user to tag physically fabricated objects with given information. An AirCode tag consists of a group of carefully designed air pockets placed beneath the object surface. These air pockets are easily produced during the fabrication process of the object, without any additional material or postprocessing. Meanwhile, the air pockets affect only the scattering light transport under the surface, and thus are hard to notice to our naked eyes. But, by using a computational imaging method, the tags become detectable. We present a tool that automates the design of air pockets for the user to encode information. AirCode system also allows the user to retrieve the information from captured images via a robust decoding algorithm. We demonstrate our tagging technique with applications for metadata embedding, robotic grasping, as well as conveying object affordances.',\n",
       " 'Processes of urban planning, urban design and architecture are inherently tangible, iterative and collaborative. Nevertheless, the majority of tools in these fields offer virtual environments and single user experience. This paper presents CityScopeAR: a computational-tangible mixed-reality platform designed for collaborative urban design processes. It portrays the evolution of the tool and presents an overview of the history and limitations of notable CAD and TUI platforms. As well, it depicts the development of a distributed networking system between TUIs and CityScopeAR, as a key in design collaboration. It shares the potential advantage of broad and decentralized community-engagement process using such tools. Finally, this paper demonstrates several real-world tests and deployments of CityScopeAR and proposes a path to future integration of AR/MR devices in urban design and public participation.',\n",
       " \"Recently, great progress has been made in virtual reality(VR) research and application. However, virtual reality faces a big problem since its appearance, i.e. discomfort (nausea, stomach awareness, etc). Discomfort can be relieved by increasing hardware (sensor, cpu and display) speed. But this will increase cost. This paper gives another low cost solution. The phenomenon of cybersickness is explained with the control theory: discomfort arises if feedback scene differs from expectation, so it can be relieved by disturbing feedback loop in human brain. A hardware platform is build to test this explanation. The VR display on a Samsung S6 is blurred while head movement is detected. The effect is evaluated by comparing responses to the Simulated Sickness Questionnaire (SSQ) between a control and experimental condition. Experimental results show that the new method can ease discomfort remarkably with little extra cost. As a result, VR may be used more widely in teaching (like foreign language, medicine). It's also reasonable to expect likewise merits in other VR applications.\",\n",
       " \"The increasing availability and diversity of virtual reality (VR) applications highlighted the importance of their usability. Function-oriented VR applications posed new challenges that are not well studied in the literature. Moreover, user feedback becomes readily available thanks to modern software engineering tools, such as app stores and open source platforms. Using Firefox Reality as a case study, we explored the major types of VR usability issues raised in these platforms. We found that 77% of usability feedbacks can be mapped to Nielsen's heuristics while few were mappable to VR-specific heuristics. This result indicates that Nielsen's heuristics could potentially help developers address the usability of this VR application in its early development stage. This work paves the road for exploring tools leveraging the community effort to promote the usability of function-oriented VR applications.\",\n",
       " \"This workshop invites researchers and practitioners to participate in exploring behavioral change support intelligent transportation applications. We welcome submissions that explore intelligent transportation systems (ITS), which interact with travelers in order to persuade them or nudge them towards sustainable transportation behaviors and decisions. Emerging opportunities including the use of data and information generated by ITS and users' mobile devices in order to render personalized, contextualized and timely transport behavioral change interventions are in our focus. We invite submissions and ideas from domains of ITS including, but not limited to, multi-modal journey planners, advanced traveler information systems and in-vehicle systems. The expected outcome will be a deeper understanding of the challenges and future research directions with respect to behavioral change support through ITS.\",\n",
       " \"Augmented Reality (AR) and Mobile Augmented Reality (MAR) applications have gained much research and industry attention these days. The mobile nature of MAR applications limits users' interaction capabilities such as inputs, and haptic feedbacks. This survey reviews current research issues in the area of human computer interaction for MAR and haptic devices. The survey first presents human sensing capabilities and their applicability in AR applications. We classify haptic devices into two groups according to the triggered sense: cutaneous/tactile: touch, active surfaces, and mid-air, kinesthetic: manipulandum, grasp, and exoskeleton. Due to the mobile capabilities of MAR applications, we mainly focus our study on wearable haptic devices for each category and their AR possibilities. To conclude, we discuss the future paths that haptic feedbacks should follow for MAR applications and their challenges.\",\n",
       " \"Mobile applications (a.k.a., apps), which facilitate a large variety of tasks on mobile devices, have become indispensable in our everyday lives. Accomplishing a task may require the user to navigate among various apps. Unlike Web pages that are inherently interconnected through hyperlinks, mobile apps are usually isolated building blocks, and the lack of direct links between apps has largely compromised the efficiency of task completion. In this paper, we present the first in-depth empirical study of inter-app navigation behaviors of smartphone users based on a comprehensive dataset collected through a sizable user study over three months. We propose a model to distinguish informational pages and transitional pages, based on which a large number of inter-app navigation are identified. We reveal that developing 'tunnels' between of isolated apps has a huge potential to reduce the cost of navigation. Our analysis provides various practical implications on how to improve app-navigation experiences from both the operating system's perspective and the developer's perspective.\",\n",
       " 'On Kickstarter only 36% of crowdfunding campaigns successfully raise sufficient funds for their projects. In this paper, we explore the possibility of redistribution of crowdfunding donations to increase the chances of success. We define several intuitive redistribution policies and, using data from a real crowdfunding platform, LaunchGood, we assess the potential improvement in campaign fundraising success rates. We find that an aggressive redistribution scheme can boost campaign success rates from 37% to 79%, but such choice-agnostic redistribution schemes come at the cost of disregarding donor preferences. Taking inspiration from offline giving societies and donor clubs, we build a case for choice preserving redistribution schemes that strike a balance between increasing the number of successful campaigns and respecting giving preference. We find that choice-preserving redistribution can easily achieve campaign success rates of 48%. Finally, we discuss the implications of these different redistribution schemes for the various stakeholders in the crowdfunding ecosystem.',\n",
       " \"To choose restaurants and coffee shops, people are increasingly relying on social-networking sites. In a popular site such as Foursquare or Yelp, a place comes with descriptions and reviews, and with profile pictures of people who frequent them. Descriptions and reviews have been widely explored in the research area of data mining. By contrast, profile pictures have received little attention. Previous work showed that people are able to partly guess a place's ambiance, clientele, and activities not only by observing the place itself but also by observing the profile pictures of its visitors. Here we further that work by determining which visual cues people may have relied upon to make their guesses; showing that a state-of-the-art algorithm could make predictions more accurately than humans at times; and demonstrating that the visual cues people relied upon partly differ from those of the algorithm.\",\n",
       " 'The lack of certain types of geographic data prevents the development of location-aware technologies in a number of important domains. One such type of \"unmapped\" geographic data is space usage rules (SURs), which are defined as geographically-bound activity restrictions (e.g. \"no dogs\", \"no smoking\", \"no fishing\", \"no skateboarding\"). Researchers in the area of human-computer interaction have recently begun to develop techniques for the automated mapping of SURs with the aim of supporting activity planning systems (e.g. one-touch \"Can I Smoke Here?\" apps, SUR-aware vacation planning tools). In this paper, we present a novel SUR mapping technique - SPtP - that outperforms state-of-the-art approaches by 30% for one of the most important components of the SUR mapping pipeline: associating a point observation of a SUR (e.g. a \\'no smoking\\' sign) with the corresponding polygon in which the SUR applies (e.g. the nearby park or the entire campus on which the sign is located). This paper also contributes a series of new SUR benchmark datasets to help further research in this area.',\n",
       " 'Most window management systems support multitasking by allowing users to open, resize, position, and switch between application windows. Although multitasking has become a way of life for most knowledge workers, our current understanding of how users use window management features to switch between multiple tasks---which may comprise multiple application windows---is limited. In this paper, we present a study providing an in-depth analysis of how task switching is supported in Windows 7. As part of analysis, we developed an interface-agnostic classification of common task switching operations supported by window managers which can be used to quantify the time spent on each constituting action. Our study shows that task switching is a time intensive activity and highlights the dominant actions that contribute to task switch time. Furthermore, our classification highlights the specific operations that are optimized by more recent and experimental window managers and allows identifying opportunities for design that could further reduce the overhead of switching between tasks.',\n",
       " 'Visualizations of tabular data are widely used; understanding their effectiveness in different task and data contexts is fundamental to scaling their impact. However, little is known about how basic tabular data visualizations perform across varying data analysis tasks and data attribute types. In this paper, we report results from a crowdsourced experiment to evaluate the effectiveness of five visualization types --- Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart --- across ten common data analysis tasks and three data attribute types using two real-world datasets. We found the effectiveness of these visualization types significantly varies across task and data attribute types, suggesting that visualization design would benefit from considering context dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different tasks. We finally train a decision tree on the data we collected to drive a recommender, showcasing how to effectively engineer experimental user data into practical visualization systems.',\n",
       " 'Advertisements (ads) often include strongly emotional content to leave a lasting impression on the viewer. This work (i) compiles an affective ad dataset capable of evoking coherent emotions across users, as determined from the affective opinions of five experts and 14 annotators; (ii) explores the efficacy of convolutional neural network (CNN) features for encoding emotions, and observes that CNN features outperform low-level audio-visual emotion descriptors upon extensive experimentation; and (iii) demonstrates how enhanced affect prediction facilitates computational advertising, and leads to better viewing experience while watching an online video stream embedded with ads based on a study involving 17 users. We model ad emotions based on subjective human opinions as well as objective multimodal features, and show how effectively modeling ad emotions can positively impact a real-life application.',\n",
       " \"While smartphones are widely used for web browsing, also other novel devices like Smart TVs become increasingly popular. Yet, current interfaces do not cater for the newly available devices beyond touch and small screens, if at all for the latter. Particularly search engines--today's entry points of the WWW--must ensure their interfaces are easy to use on any web-enabled device. We report on a survey that investigated (1) users' perception and usage of current search interfaces, and (2) their expectations towards current and future search interfaces. Users are mostly satisfied with desktop and mobile search, but seem to be skeptical towards web search with novel devices and input modalities. Hence, we derive REFOCUS--a novel set of requirements for current and future search interfaces, which shall address the demand for improvement of novel web search and has been validated by 12 dedicated experts.\",\n",
       " 'Understanding the needs of a variety of distinct user groups is vital in designing effective, desirable dialogue systems that will be adopted by the largest possible segment of the population. Despite the increasing popularity of dialogue systems in both mobile and home formats, user studies remain relatively infrequent and often sample a segment of the user population that is not representative of the needs of the potential user population as a whole. This is especially the case for users who may be more reluctant adopters, such as older adults.\\n  In this paper we discuss the results of a recent user study performed over a large population of age 50 and over adults in the Midwestern United States that have experience using a variety of commercial dialogue systems. We show the common preferences, use cases, and feature gaps identified by older adult users in interacting with these systems. Based on these results, we propose a new, robust user modeling framework that addresses common issues facing older adult users, which can then be generalized to the wider user population.',\n",
       " 'The past, present and future are not fundamental properties of Minkowski spacetime. It has been suggested that they are properties of a class of information gathering and utilizing systems (IGUSs).The past, present and future are psychologically created phenomena not actually properties of spacetime. A human is a model IGUS robot. We develop a way to establish that the past, present, and future do not follow from the laws of physics by constructing robots that process information differently and therefore experience different nows (presents). We construct a customized virtual reality (VR) system which allows an observer to switch between present and past. This robot (human with VR system) can experience immersion in the immediate past ad libitum. Being able to actually construct an IGUS that has the same present at two different coordinates along the worldline lends support to the IGUS hypothesis.',\n",
       " \"Parallel Coordinates are a popular data visualization technique for multivariate data. Dating back to as early as 1880 PC are nearly as old as John Snow's famous cholera outbreak map of 1855, which is frequently regarded as a historic landmark for modern data visualization. Numerous extensions have been proposed to address integrity, scalability and readability. We make a new case to employ PC on conditional data, where additional dimensions are only unfolded if certain criteria are met in an observation. Compared to standard PC which operate on a flat set of dimensions the ontology of our input to Conditional Parallel Coordinates is of hierarchical nature. We therefore briefly review related work around hierarchical PC using aggregation or nesting techniques. Our contribution is a visualization to seamlessly adapt PC for conditional data under preservation of intuitive interaction patterns to select or highlight polylines. We conclude with intuitions on how to operate CPC on two data sets: an AutoML hyperparameter search log, and session results from a conversational agent.\",\n",
       " 'The purpose of this study is to provide an accessibility measure of web-pages, in order to draw disabled users to the pages that have been designed to be ac-cessible to them. Our approach is based on the theory of belief functions, using data which are supplied by reports produced by automatic web content assessors that test the validity of criteria defined by the WCAG 2.0 guidelines proposed by the World Wide Web Consortium (W3C) organization. These tools detect errors with gradual degrees of certainty and their results do not always converge. For these reasons, to fuse information coming from the reports, we choose to use an information fusion framework which can take into account the uncertainty and imprecision of infor-mation as well as divergences between sources. Our accessibility indicator covers four categories of deficiencies. To validate the theoretical approach in this context, we propose an evaluation completed on a corpus of 100 most visited French news websites, and 2 evaluation tools. The results obtained illustrate the interest of our accessibility indicator.',\n",
       " \"Crowdsourcing is the primary means to generate training data at scale, and when combined with sophisticated machine learning algorithms, crowdsourcing is an enabler for a variety of emergent automated applications impacting all spheres of our lives. This paper surveys the emerging field of formally reasoning about and optimizing open-ended crowdsourcing, a popular and crucially important, but severely understudied class of crowdsourcing---the next frontier in crowdsourced data management. The underlying challenges include distilling the right answer when none of the workers agree with each other, teasing apart the various perspectives adopted by workers when answering tasks, and effectively selecting between the many open-ended operators appropriate for a problem. We describe the approaches that we've found to be effective for open-ended crowdsourcing, drawing from our experiences in this space.\",\n",
       " 'Mobile is taking center stage and becoming the device of preference for all aspects of communication because of our increasingly on the go lifestyles. With this the demands on mobile capability to execute increasingly complex operations are also on the rise. However, despite improvements in device computing power in the last couple of years a mobile device continues to have limitations. Mobile driven everyday use cases are increasingly raising expectations that rest on mobile technologies that are still evolving. A number of fragmented approaches and solutions have been created that address various requirements unique to mobility, however there is a lack of a single framework that serves as a unifying reference for industry and solution architectures. The paper addresses this concern through the specification of a comprehensive reference framework for mobility that is generic and vendor neutral.',\n",
       " 'Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.',\n",
       " \"Advanced driver assistance systems have successfully reduced drivers' workloads and increased safety. On the other hand, the excessive use of such systems can impede the development of driving skills. However, there exist collaborative driver assistance systems, including shared and cooperative controls, which can promote effective collaboration between an assistance system and a human operator under appropriate system settings. Given an effective collaboration setup, we address the goal of simultaneously developing or maintaining driving skills while reducing workload. As there has been a paucity of research on such systems and their methodologies, we discuss a methodology applying shared and cooperative controls by considering related concepts in the skill training field. Reverse parking assisted by haptic shared control is presented as a means of increasing performance during assistance, while skill improvement following assistance is used to demonstrate the possibility of simultaneous achievement of driver assistance through the reduction of workload and skill improvement.\",\n",
       " 'In this paper we propose a computational design tool that al-lows end-users to create advanced quadrotor trajectories witha variety of application scenarios in mind. Our algorithm al-lows novice users to create quadrotor based use-cases withoutrequiring deep knowledge in either quadrotor control or theunderlying constraints of the target domain. To achieve thisgoal we propose an optimization-based method that gener-ates feasible trajectories which can be flown in the real world.Furthermore, the method incorporates high-level human ob-jectives into the planning of flight trajectories. An easy touse 3D design tool allows for quick specification and edit-ing of trajectories as well as for intuitive exploration of theresulting solution space. We demonstrate the utility of our ap-proach in several real-world application scenarios, includingaerial-videography, robotic light-painting and drone racing.',\n",
       " \"Executive coaching has been drawing more and more attention for developing corporate managers. While conversing with managers, coach practitioners are also required to understand internal states of coachees through objective observations. In this paper, we present REsCUE, an automated system to aid coach practitioners in detecting unconscious behaviors of their clients. Using an unsupervised anomaly detection algorithm applied to multimodal behavior data such as the subject's posture and gaze, REsCUE notifies behavioral cues for coaches via intuitive and interpretive feedback in real-time. Our evaluation with actual coaching scenes confirms that REsCUE provides the informative cues to understand internal states of coachees. Since REsCUE is based on the unsupervised method and does not assume any prior knowledge, further applications beside executive coaching are conceivable using our framework.\",\n",
       " 'Current exergaming sensors and inertial systems attached to sports equipment or the human body can provide quantitative information about the movement or impact e.g. with the ball. However, the scope of these technologies is not to qualitatively assess sports technique at a personalised level, similar to a coach during training or replay analysis. The aim of this paper is to demonstrate a novel approach to automate identification of tennis swings executed with erroneous technique without recorded ball impact. The presented spatiotemporal transformations relying on motion gradient vector flow and polynomial regression with RBF classifier, can identify previously unseen erroneous swings (84.5-94.6%). The presented solution is able to learn from a small dataset and capture two subjective swing-technique assessment criteria from a coach. Personalised and flexible assessment criteria required for players of diverse skill levels and various coaching scenarios were demonstrated by assigning different labelling criteria for identifying similar spatiotemporal patterns of tennis swings.',\n",
       " \"A multitude of web and desktop applications are now widely available in diverse human languages. This paper explores the design issues that are specifically relevant for multilingual users. It reports on the continued studies of Information System (IS) issues and users' behaviour across cross-cultural and transnational boundaries. Taking the BBC website as a model that is internationally recognised, usability tests were conducted to compare different versions of the website. The dependant variables derived from the questionnaire were analysed (via descriptive statistics) to elucidate the multilingual UI design issues. Using Principal Component Analysis (PCA), five de-correlated variables were identified which were then used for hypotheses tests. A modified version of Herzberg's Hygiene-motivational Theory about the Workplace was applied to assess the components used in the website. Overall, it was concluded that the English versions of the website gave superior usability results and this implies the need for deeper study of the problems in usability of the translated versions.\",\n",
       " 'This paper introduces Wisture, a new online machine learning solution for recognizing touch-less dynamic hand gestures on a smartphone. Wisture relies on the standard Wi-Fi Received Signal Strength (RSS) using a Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN), thresholding filters and traffic induction. Unlike other Wi-Fi based gesture recognition methods, the proposed method does not require a modification of the smartphone hardware or the operating system, and performs the gesture recognition without interfering with the normal operation of other smartphone applications.\\n  We discuss the characteristics of Wisture, and conduct extensive experiments to compare its performance against state-of-the-art machine learning solutions in terms of both accuracy and time efficiency. The experiments include a set of different scenarios in terms of both spatial setup and traffic between the smartphone and Wi-Fi access points (AP). The results show that Wisture achieves an online recognition accuracy of up to 94% (average 78%) in detecting and classifying three hand gestures.',\n",
       " 'Conversion optimization means designing a web interface so that as many users as possible take a desired action on it, such as register or purchase. Such design is usually done by hand, testing one change at a time through A/B testing, or a limited number of combinations through multivariate testing, making it possible to evaluate only a small fraction of designs in a vast design space. This paper describes Sentient Ascend, an automatic conversion optimization system that uses evolutionary optimization to create effective web interface designs. Ascend makes it possible to discover and utilize interactions between the design elements that are difficult to identify otherwise. Moreover, evaluation of design candidates is done in parallel online, i.e. with a large number of real users interacting with the system. A case study on an existing media site shows that significant improvements (i.e. over 43%) are possible beyond human design. Ascend can therefore be seen as an approach to massively multivariate conversion optimization, based on a massively parallel interactive evolution.',\n",
       " \"Traditionally, evaluation studies in information visualization have measured effectiveness by assessing performance time and accuracy. More recently, there has been a concerted effort to understand aspects beyond time and errors. In this paper we study enjoyment, which, while arguably not the primary goal of visualization, has been shown to impact performance and memorability. Different models of enjoyment have been proposed in psychology, education and gaming; yet there is no standard approach to evaluate and measure enjoyment in visualization. In this paper we relate the flow model of Csikszentmihalyi to Munzner's nested model of visualization evaluation and previous work in the area. We suggest that, even though previous papers tackled individual elements of flow, in order to understand what specifically makes a visualization enjoyable, it might be necessary to measure all specific elements.\",\n",
       " 'An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed -- Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.',\n",
       " 'We present Lemotif. Lemotif generates a motif for your emotional life. You tell Lemotif a little bit about your day -- what were salient events or aspects and how they made you feel. Lemotif will generate a lemotif -- a creative abstract visual depiction of your emotions and their sources. Over time, Lemotif can create visual motifs to capture a summary of your emotional states over arbitrary periods of time -- making patterns in your emotions and their sources apparent, presenting opportunities to take actions, and measure their effectiveness. The underlying principles in Lemotif are that the lemotif should (1) separate out the sources of the emotions, (2) depict these sources visually, (3) depict the emotions visually, and (4) have a creative aspect to them. We verify via human studies that each of these factors contributes to the proposed lemotifs being favored over corresponding baselines.',\n",
       " 'The electroencephalogram (EEG) is the most widely used input for brain computer interfaces (BCIs), and common spatial pattern (CSP) is frequently used to spatially filter it to increase its signal-to-noise ratio. However, CSP is a supervised filter, which needs some subject-specific calibration data to design. This is time-consuming and not user-friendly. A promising approach for shortening or even completely eliminating this calibration session is transfer learning, which leverages relevant data or knowledge from other subjects or tasks. This paper reviews three existing approaches for incorporating transfer learning into CSP, and also proposes a new transfer learning enhanced CSP approach. Experiments on motor imagery classification demonstrate their effectiveness. Particularly, our proposed approach achieves the best performance when the number of target domain calibration samples is small.',\n",
       " 'Traditional instrument learning is time-consuming. It begins with learning music notation and necessitates layers of sophistication and abstraction. Haptic interfaces open another door to the music world for the vast majority of beginners when traditional training methods are not effective. However, existing haptic interfaces can only deal with specially designed pieces with great restrictions on performance duration and pitch range due to the fact that not all performance motions could be guided haptically for most instruments. Our system breaks such restrictions using a semi-haptic interface. For the first time, the pitch range of the haptically learned pieces goes beyond an octave (with the fingering motion covers most of the possible choices) and the duration of learned pieces cover a whole phrase. This significant change leads to a more realistic instrument learning process. Experiments show that our semi-haptic interface is effective as long as learners are not \"tone deaf.\" Using our prototype device, the learning rate is about 30% faster compared to learning from videos.',\n",
       " 'Developing applications for interactive space is different from developing cross-platform applications for personal computing. Input, output, and architectural variations in each interactive space introduce big overhead in terms of cost and time for developing, deploying and maintaining applications for interactive spaces. Often, these applications become on-off experience tied to the deployed spaces. To alleviate this problem and enable rapid responsive space design applications similar to responsive web design, we present CELIO application development framework for interactive spaces. The framework is micro services based and neatly decouples application and design specifications from hardware and architecture specifications of an interactive space. In this paper, we describe this framework and its implementation details. Also, we briefly discuss the use cases developed using this framework.',\n",
       " 'The ever evolving informatics technology has gradually bounded human and computer in a compact way. Understanding user behavior becomes a key enabler in many fields such as sedentary-related healthcare, human-computer interaction (HCI) and affective computing. Traditional sensor-based and vision-based user behavior analysis approaches are obtrusive in general, hindering their usage in realworld. Therefore, in this article, we first introduce WiFi signal as a new source instead of sensor and vision for unobtrusive user behaviors analysis. Then we design BeSense, a contactless behavior analysis system leveraging signal processing and computational intelligence over WiFi channel state information (CSI). We prototype BeSense on commodity low-cost WiFi devices and evaluate its performance in realworld environments. Experimental results have verified its effectiveness in recognizing user behaviors.',\n",
       " \"The purpose of this paper is to investigate the applicability of applying gamification approach on the physiotherapy rehabilitation. A new developing game called JCave was designed and developed for the prove of concept. The propose game target the children from six to twelve years of age who need physical therapy in their upper limbs. JCave is a finite and multilevel single-player 3D video game. The player's role is to collect jewels from a cave and increase his/her score by performing physical therapy exercises. The game uses Xbox360 Kinect as a motion capture camera to observe gestures and track the child. Automatic gesture recognition algorithms are implemented for elbow flexion-extension exercises and shoulder flexion, which are the active range of motion (AROM) exercises for both the right and left arms. The JCave game is implemented using Unity3D and Blender to design 3D model objects.\",\n",
       " \"Designing a user interface for military situation awareness presents challenges for managing information in a useful and usable manner. We present an integrated set of functions for the presentation of and interaction with information for a mobile augmented reality application for military applications. Our research has concentrated on four areas. We filter information based on relevance to the user (in turn based on location), evaluate methods for presenting information that represents entities occluded from the user's view, enable interaction through a top-down map view metaphor akin to current techniques used in the military, and facilitate collaboration with other mobile users and/or a command center. In addition, we refined the user interface architecture to conform to requirements from subject matter experts. We discuss the lessons learned in our work and directions for future research.\",\n",
       " 'Security incidents such as targeted distributed denial of service (DDoS) attacks on power grids and hacking of factory industrial control systems (ICS) are on the increase. This paper unpacks where emerging security risks lie for the industrial internet of things, drawing on both technical and regulatory perspectives. Legal changes are being ushered by the European Union (EU) Network and Information Security (NIS) Directive 2016 and the General Data Protection Regulation 2016 (GDPR) (both to be enforced from May 2018). We use the case study of the emergent smart energy supply chain to frame, scope out and consolidate the breadth of security concerns at play, and the regulatory responses. We argue the industrial IoT brings four security concerns to the fore, namely: appreciating the shift from offline to online infrastructure; managing temporal dimensions of security; addressing the implementation gap for best practice; and engaging with infrastructural complexity. Our goal is to surface risks and foster dialogue to avoid the emergence of an Internet of Insecure Industrial Things',\n",
       " \"Visual attention is highly fragmented during mobile interactions, but the erratic nature of attention shifts currently limits attentive user interfaces to adapting after the fact, i.e. after shifts have already happened. We instead study attention forecasting -- the challenging task of predicting users' gaze behaviour (overt visual attention) in the near future. We present a novel long-term dataset of everyday mobile phone interactions, continuously recorded from 20 participants engaged in common activities on a university campus over 4.5 hours each (more than 90 hours in total). We propose a proof-of-concept method that uses device-integrated sensors and body-worn cameras to encode rich information on device usage and users' visual scene. We demonstrate that our method can forecast bidirectional attention shifts and predict whether the primary attentional focus is on the handheld mobile device. We study the impact of different feature sets on performance and discuss the significant potential but also remaining challenges of forecasting user attention during mobile interactions.\",\n",
       " \"Nowadays Digital Personal Assistants (DPA) become more and more popular. DPAs help to increase quality of life especially for elderly or disabled people. In this paper we develop an open source DPA and smart home system as a 3-rd party extension to show the functionality of the assistant. The system is designed to use the DPA as a learning platform for engineers to provide them with the opportunity to create and test their own hypothesis. The DPA is able to recognize users' commands in natural language and transform it to the set of machine commands that can be used to control different 3rd-party application. We use smart home system as an example of such 3rd-party. We demonstrate that the system is able to control home appliances, like lights, or to display information about the current state of the home, like temperature, through a dialogue between a user and the Digital Personal Assistant.\",\n",
       " 'The combination of clinical and personal health and wellbeing data can tell us much about our behaviors, risks and overall status. The way this data is visualized may affect our understanding of our own health. To study this effect, we conducted a small experiment with 30 participants in which we presented a holistic overview of the health and wellbeing of two modeled individuals, one of them with metabolic syndrome. We used an insight-based methodology to assess the effectiveness of the visualizations. The results show that adequate visualization of holistic health data helps users without medical background to better understand the overall health situation and possible health risks related to lifestyles. Furthermore, we found that the application of insight-based methodology in the health and wellbeing domain remains unexplored and additional research and methodology development are needed.',\n",
       " 'Fully immersive virtual reality (VR) has the potential to improve neurosurgical planning. For example, it may offer 3D visualizations of relevant anatomical structures with complex shapes, such as blood vessels and tumors. However, there is a lack of research tools specifically tailored for this area. We present a research framework for VR neurosurgery based on open-source tools and preliminary evaluation results. We showcase the potential of such a framework using clinical data of two patients and research data of one subject. As a first step toward practical evaluations, two certified senior neurosurgeons positively assessed the usefulness of the VR visualizations using head-mounted displays. The methods and findings described in our study thus provide a foundation for research and development aiming at versatile and user-friendly VR tools for improving neurosurgical planning and training.',\n",
       " 'Humanness is core to speech interface design. Yet little is known about how users conceptualise perceptions of humanness and how people define their interaction with speech interfaces through this. To map these perceptions n=21 participants held dialogues with a human and two speech interface based intelligent personal assistants, and then reflected and compared their experiences using the repertory grid technique. Analysis of the constructs show that perceptions of humanness are multidimensional, focusing on eight key themes: partner knowledge set, interpersonal connection, linguistic content, partner performance and capabilities, conversational interaction, partner identity and role, vocal qualities and behavioral affordances. Through these themes, it is clear that users define the capabilities of speech interfaces differently to humans, seeing them as more formal, fact based, impersonal and less authentic. Based on the findings, we discuss how the themes help to scaffold, categorise and target research and design efforts, considering the appropriateness of emulating humanness.',\n",
       " \"The smartification of industries is marked by the development of cyber-physical systems, interfaces, and intelligent software featuring knowledge models, empirical real-time data, and feedback-loops. This brings up new requirements and challenges for HMI design and industrial labor. Social sciences can contribute to such engineering projects with their perspectives, concepts and knowledge. Hence, we claim that, in addition to following their own intellectual curiosities, the social sciences can and should contribute to such projects in terms of an 'applied' science, helping to foster interdisciplinary collaboration and providing toolkits and devices for what we call 'interdisciplinary diplomacy'. We illustrate the benefits of such an approach, support them with selected examples of our involvement in such an engineering project and propose using methods as diplomatic devices and concepts as social theory plug-ins. The article ends with an outlook and reflection on the remaining issue of whether and in how far such 'applied' and critical social science can or should be integrated.\",\n",
       " 'This paper is concerned with the design and implementation of an innovative user support system in the frame of an open educational environment. The environment adapted is ModelsCreator (MC), an educational system supporting learning through modelling activities. The pupils typical interaction with the system was modelled us-ing Bayesian Belief Networks (BBN). This model has been used in ModelsCreator to build an adaptive help system providing the most useful guidelines according to the current state of interaction. A brief description of the system and an overview of application of Bayesian techniques to educational systems is presented together with discussion about the process of building of the Bayesian Network derived from actual student interaction data. A preliminary evaluation of the developed prototype indicates that the proposed approach produces systems with promising performance.',\n",
       " 'In this position paper, we present ideas about creating a next generation framework towards an adaptive interface for data communication and visualisation systems. Our objective is to develop a system that accepts large data sets as inputs and provides user-centric, meaningful visual information to assist owners to make sense of their data collection. The proposed framework comprises four stages: (i) the knowledge base compilation, where we search and collect existing state-ofthe-art visualisation techniques per domain and user preferences; (ii) the development of the learning and inference system, where we apply artificial intelligence techniques to learn, predict and recommend new graphic interpretations (iii) results evaluation; and (iv) reinforcement and adaptation, where valid outputs are stored in our knowledge base and the system is iteratively tuned to address new demands. These stages, as well as our overall vision, limitations and possible challenges are introduced in this article. We also discuss further extensions of this framework for other knowledge discovery tasks.',\n",
       " 'Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization systems that utilizes state-of-the-art natural language processing techniques to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and special utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram special utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem and a formal user study.',\n",
       " 'Sensory feedback is the fundamental driving force behind motor control and learning. However, the technology for low-cost and efficient sensory feedback remains a big challenge during stroke rehabilitation, and for prosthetic designs. Here we show that a low-cost accelerometer mounted on the finger can provide accurate decoding of many daily life materials during touch. We first designed a customized touch analysis system that allowed us to present different materials for touch by human participants, while controlling for the contact force and touch speed. Then, we collected data from six participants, who touched seven daily life materials-plastic, cork, wool, aluminum, paper, denim, cotton. We use linear sparse logistic regression and show that the materials can be classified from accelerometer recordings with an accuracy of 88% across materials and participants within 7 seconds of touch.',\n",
       " 'The course description provided by instructors is an important piece of information as it defines what is expected from the instructor and what he/she is going to deliver during a particular course. One of the key components of a course description is the Learning Outcomes section. The contents of this section are used by program managers who are tasked to compare and match two different courses during the development of Transfer Agreements between different institutions. This research introduces the development of visual tools for understanding the two different courses and making comparisons. We designed methods to extract the text from a course description document, developed an algorithm to perform semantic analysis, and displayed the results in a web interface. We are able to achieve the intermediate results of the research which includes extracting, analyzing and visualizing the data.',\n",
       " 'Critical human-machine interfaces are present in many systems including avionics systems and medical devices. Use error is a concern in these systems both in terms of hardware panels and input devices, and the software that drives the interfaces. Guaranteeing safe usability, in terms of  buttons, knobs and displays is now a key element in the overall safety of the system. New integrated development environments (IDEs) based on formal methods technologies have been developed by the research community to support the design and analysis of high-confidence human-machine interfaces. To date, little work has focused on the comparison of these particular types of formal IDEs. This paper compares and evaluates two state-of-the-art toolkits: CIRCUS, a model-based development and analysis tool based on Petri net extensions, and PVSio-web, a prototyping toolkit based on the PVS theorem proving system.',\n",
       " 'The use of complex machine learning models can make systems opaque to users. Machine learning research proposes the use of post-hoc explanations. However, it is unclear if they give users insights into otherwise uninterpretable models. One minimalistic way of explaining image classifications by a deep neural network is to show only the areas that were decisive for the assignment of a label. In a pilot study, 20 participants looked at 14 of such explanations generated either by a human or the LIME algorithm. For explanations of correct decisions, they identified the explained object with significantly higher accuracy (75.64% vs. 18.52%). We argue that this shows that explanations can be very minimalistic while retaining the essence of a decision, but the decision-making contexts that can be conveyed in this manner is limited. Finally, we found that explanations are unique to the explainer and human-generated explanations were assigned 79% higher trust ratings. As a starting point for further studies, this work shares our first insights into quality criteria of post-hoc explanations.',\n",
       " '\"Social sensing\" is a form of crowd-sourcing that involves systematic analysis of digital communications to detect real-world events. Here we consider the use of social sensing for observing natural hazards. In particular, we present a case study that uses data from a popular social media platform (Twitter) to detect and locate flood events in the UK. In order to improve data quality we apply a number of filters (timezone, simple text filters and a naive Bayes `relevance\\' filter) to the data. We then use place names in the user profile and message text to infer the location of the tweets. These two steps remove most of the irrelevant tweets and yield orders of magnitude more located tweets than we have by relying on geo-tagged data. We demonstrate that high resolution social sensing of floods is feasible and we can produce high-quality historical and real-time maps of floods using Twitter.',\n",
       " 'This paper proposes the usage of \\\\emph{visualisation widgets} for exploratory search with \\\\emph{sentiment} as a facet. Starting from specific design goals for depiction of ambivalence in sentiment, two visualization widgets were implemented: \\\\emph{scatter plot} and \\\\emph{parallel coordinates}. Those widgets were evaluated against a text baseline in a small-scale usability study with exploratory tasks using Wikipedia as dataset. The study results indicate that users spend more time browsing with scatter plots in a positive way. A post-hoc analysis of individual differences in behavior revealed that when considering two types of users, \\\\emph{explorers} and \\\\emph{achievers}, engagement with scatter plots is positive and significantly greater \\\\textit{when users are explorers}. We discuss the implications of these findings for sentiment-based exploratory search and personalised user interfaces.',\n",
       " 'Gestalt theory has provided perceptual science with a conceptual framework which has inspired researchers ever since, taking the field of perceptual organization into the 21st century. This opinion article discusses the importance of rules of perceptual organization for the testing and design of visual interface technology. It is argued that major Gestalt principles, such as the law of good continuation or the principle of Praegnanz (suggested translation: salience), taken as examples here, are important to our understanding of visual image processing by a human observer. Perceptual integration of contrast information across collinear space, and the organization of objects in the 2D image plane into figure and ground are of a particular importance here. Visual interfaces for image-guided surgery illustrate the criticality of these two types of perceptual processes for reliable decision making and action. It is concluded that Gestalt theory continues to generate powerful concepts and insights for perceptual science placed within the context of major technological challenges of today.',\n",
       " 'This paper presents a pilot study on developing an instrument to predict the quality of e-commerce websites. The 8C model was adopted as the reference model of the heuristic evaluation. Each dimension of the 8C was mapped into a set of quantitative website elements, selected websites were scraped to get the quantitative website elements, and the score of each dimension was calculated. A software was developed in PHP for the experiments. In the training process, 10 experiments were conducted and quantitative analyses were regressively conducted between the experiments. The conversion rate was used to verify the heuristic evaluation of an e-commerce website after each experiment. The results showed that the mapping revisions between the experiments improved the performance of the evaluation instrument, therefore the experiment process and the quantitative mapping revision guideline proposed was on the right track. The software resulted from the experiment 10 can serve as the aimed e-commerce website evaluation instrument. The experiment results and the future work have been discussed.',\n",
       " \"The exponential growth in smartphone adoption is contributing to the availability of vast amounts of human behavioral data. This data enables the development of increasingly accurate data-driven user models that facilitate the delivery of personalized services which are often free in exchange for the use of its customers' data. Although such usage conventions have raised many privacy concerns, the increasing value of personal data is motivating diverse entities to aggressively collect and exploit the data. In this paper, we unfold profiling scenarios around mobile HTTP(S) traffic, focusing on those that have limited but meaningful segments of the data. The capability of the scenarios to profile personal information is examined with real user data, collected in-the-wild from 61 mobile phone users for a minimum of 30 days. Our study attempts to model heterogeneous user traits and interests, including personality, boredom proneness, demographics, and shopping interests. Based on our modeling results, we discuss various implications to personalization, privacy, and personal data rights.\",\n",
       " 'We examine a class of techniques for 3D object manipulation on mobile devices, in which the device\\'s physical motion is applied to 3D objects displayed on the device itself. This \"local coupling\" between input and display creates specific challenges compared to manipulation techniques designed for monitor-based or immersive virtual environments. Our work focuses specifically on the mapping between device motion and object motion. We review existing manipulation techniques and introduce a formal description of the main mappings under a common notation. Based on this notation, we analyze these mappings and their properties in order to answer crucial usability questions. We first investigate how the 3D objects should move on the screen, since the screen also moves with the mobile device during manipulation. We then investigate the effects of a limited range of manipulation and present a number of solutions to overcome this constraint. This work provides a theoretical framework to better understand the properties of locally-coupled 3D manipulation mappings based on mobile device motion.',\n",
       " \"We present a new approach for improving the friendliness and warmth of a virtual agent in an AR environment by generating appropriate movement characteristics. Our algorithm is based on a novel data-driven friendliness model that is computed using a user-study and psychological characteristics. We use our model to control the movements corresponding to the gaits, gestures, and gazing of friendly virtual agents (FVAs) as they interact with the user's avatar and other agents in the environment. We have integrated FVA agents with an AR environment using with a Microsoft HoloLens. Our algorithm can generate plausible movements at interactive rates to increase the social presence. We also investigate the perception of a user in an AR setting and observe that an FVA has a statistically significant improvement in terms of the perceived friendliness and social presence of a user compared to an agent without the friendliness modeling. We observe an increment of 5.71% in the mean responses to a friendliness measure and an improvement of 4.03% in the mean responses to a social presence measure.\",\n",
       " \"To allow non-designers' involvement in design projects new methods are needed. Co-design gives the same opportunity to all the multidisciplinary participants to co-create ideas simultaneously. Nevertheless, current co-design processes involving such users tend to limit their contribution to the proposal of basic design ideas only through brainstorming. The co-design approach needs to be enhanced by a properly suited representational ecosystem supporting active participation and by conscious use of structured verbal exchanges giving awareness of the creative process. In this respect, we developed two social virtual reality co-design systems, and a co-design verbal exchange methodology to favour participants' awareness of the co-creative process. By using such representations and verbal exchanges, participants could co-create with more ease by benefiting from being informed of the process and from the collective immersion, empowering their participation. This paper presents the rationale behind this approach of using Social VR in co-design and the feedback of three co-design workshops.\",\n",
       " \"This demo takes the form of a challenge to the IJCAI community. A physical vault, secured by a 4-digit code, will be placed in the demo area. The author will publicly open the vault by entering the code on a touch-based interface, and as many times as requested. The challenge to the IJCAI participants will be to crack the code, open the vault, and collect its content. The interface is based on previous work on calibration-free interactive systems that enables a user to start instructing a machine without the machine knowing how to interpret the user's actions beforehand. The intent and the behavior of the human are simultaneously learned by the machine. An online demo and videos are available for readers to participate in the challenge. An additional interface using vocal commands will be revealed on the demo day, demonstrating the scalability of our approach to continuous input signals.\",\n",
       " 'The development and design of visualization solutions that are truly usable is essential for ensuring both their adoption and effectiveness. User-centered design principles, which focus on involving users throughout the entire development process, are well suited for visualization and have been shown to be effective in numerous information visualization endeavors. In this paper, we report a two year long collaboration with combustion scientists that, by applying these design principles, generated multiple results including an in situ visualization technique and a post hoc probability distribution function (PDF) exploration tool. Furthermore, we examine the importance of user-centered design principles and describe lessons learned over the design process in an effort to aid others who also seek to work with scientists for developing effective and usable scientific visualization solutions.',\n",
       " \"As pre-diagnostic technologies are becoming increasingly accessible, using them to improve the quality of care available to dementia patients and their caregivers is of increasing interest. Specifically, we aim to develop a tool for non-invasively assessing task performance in a simple gaming application. To address this, we have developed Caregiver Assessment using Smart Gaming Technology (CAST), a mobile application that personalizes a traditional word scramble game. Its core functionality uses a Fuzzy Inference System (FIS) optimized via a Genetic Algorithm (GA) to provide customized performance measures for each user of the system. With CAST, we match the relative level of difficulty of play using the individual's ability to solve the word scramble tasks. We provide an analysis of the preliminary results for determining task difficulty, with respect to our current participant cohort.\",\n",
       " 'We present an analytic provenance data repository that can be used to study human analysis activity, thought processes, and software interaction with visual analysis tools during exploratory data analysis. We conducted a series of user studies involving exploratory data analysis scenario with textual and cyber security data. Interactions logs, think-alouds, videos and all coded data in this study are available online for research purposes. Analysis sessions are segmented in multiple sub-task steps based on user think-alouds, video and audios captured during the studies. These analytic provenance datasets can be used for research involving tools and techniques for analyzing interaction logs and analysis history. By providing high-quality coded data along with interaction logs, it is possible to compare algorithmic data processing techniques to the ground-truth records of analysis history.',\n",
       " 'Brain computer interfaces (BCI) provide a direct communication link between the brain and a computer or other external devices. They offer an extended degree of freedom either by strengthening or by substituting human peripheral working capacity and have potential applications in various fields such as rehabilitation, affective computing, robotics, gaming and artificial intelligence. Significant research efforts on a global scale have delivered common platforms for technology standardization and help tackle highly complex and nonlinear brain dynamics and related feature extraction and classification challenges. Psycho-neurophysiological phenomena and their impact on brain signals impose another challenge for BCI researchers to transform the technology from laboratory experiments to plug-and-play daily life. This review summarizes progress in BCI field and highlights critical challenges.',\n",
       " 'This paper outlines ongoing dissertation research located in the intersection of science fiction, human-computer interaction and computer science. Through an interdisciplinary perspective, drawing from fields such as human-computer interaction, film theory and studies of science and technology, qualitative and quantitative content analysis techniques are used to contextually analyze expressions of science fiction in peer-reviewed computer science research repositories, such as the ACM or IEEE Xplore Digital Libraries. This paper concisely summarizes and introduces the relationship of science fiction and computer science research and presents the research questions, aims and implications in addition to prior work and study methodology. In the latter part of this work-in-progress report, preliminary results, current limitations, future work and a post-dissertation trajectory are outlined.',\n",
       " 'We introduce and evaluate a novel approach for detecting smooth pursuit eye movements that increases the number of distinguishable targets and is more robust against false positives. Being natural and calibration-free, Pursuits has been gaining popularity in the past years. At the same time, current implementations show poor performance when more than eight on-screen targets are being used, thus limiting its applicability. Our approach (1) leverages the slope of a regression line, and (2) introduces a minimum signal duration that improves both the new and the traditional detection method. After introducing the approach as well as the implementation, we compare it to the traditional correlation-based Pursuits detection method. We tested the approach up to 24 targets and show that, if accepting a similar error rate, nearly twice as many targets can be distinguished compared to state of the art. For fewer targets, accuracy increases significantly. We believe our approach will enable more robust pursuit-based user interfaces, thus making it valuable for both researchers and practitioners.',\n",
       " \"In this exploratory study we assessed how attitudes of children with autism spectrum disorder (ASD) towards robots together with children's autism-related social impairments are linked to indicators of children's preference of an interaction with a robot over an interaction with a person. We found that children with ASD have overall positive attitudes towards robots and that they often prefer interacting with a robot than with a person. Several of children's attitudes were linked to children's longer gazes towards a robot compared to a person. Autism-related social impairments were linked to more repetitive and stereotyped behaviors and to a shorter gaze duration in the interaction with the robot compared to the person. These preliminary results contribute to better understand factors that might help determine sub-groups of children with ASD for whom robots could be particularly useful.\",\n",
       " \"With the increasing complexity of modern industrial automatic and robotic systems, an increasing burden is put on the operators, who are requested to supervise and interact with such complex systems, typically under challenging and stressful conditions. To overcome this issue, it is necessary to adopt a responsible approach based on the anthropocentric design methodology, such that machines adapt to the humans capabilities. Moving along these lines, a methodological approach called MATE was introduced in [1], which consists in devising complex automatic or robotic solutions that measure current operator's status, adapting the interaction accordingly, and providing her/him with proper training to improve the interaction and learn lacking skills and expertise. In this paper we propose an evaluation and validation procedure to guarantee the achievement of the requirements of a MATE system.\",\n",
       " 'People from all over the world use social media to share thoughts and opinions about events, and understanding what people say through these channels has been of increasing interest to researchers, journalists, and marketers alike. However, while automatically generated summaries enable people to consume large amounts of data efficiently, they do not provide the context needed for a viewer to fully understand an event. Narrative structure can provide templates for the order and manner in which this data is presented to create stories that are oriented around narrative elements rather than summaries made up of facts. In this paper, we use narrative theory as a framework for identifying the links between social media content. To do this, we designed crowdsourcing tasks to generate summaries of events based on commonly used narrative templates. In a controlled study, for certain types of events, people were more emotionally engaged with stories created with narrative structure and were also more likely to recommend them to others compared to summaries created without narrative structure.',\n",
       " 'AI and advanced automation are involved in almost all aspects of our life. In such systems, human responsibility for outcomes becomes equivocal. We analyze the descriptive abilities of a newly developed responsibility quantification model (ResQu) to predict actual human responsibility and perceptions of responsibility in HCI. In two laboratory experiments, participants performed an aided decision task. We compared the theoretical responsibility values to the actual responsibility a person took on and to the subjectively perceived responsibility. The ResQu model predictions were strongly correlated with the measured and the subjective responsibility. However, observed values differed from the model predictions, as less knowledgeable participants overestimated their own capabilities and assumed greater-than-optimal responsibility. The results demonstrate the value of the ResQu model as a descriptive model, considering some systematic deviations. It can be used to aid system design and guide policy and legal decisions regarding human responsibility in events involving intelligent systems.',\n",
       " 'Understanding validity of user behaviour in Virtual Environments (VEs) is critical as they are increasingly being used for serious Health and Safety applications such as predicting human behaviour and training in hazardous situations. This paper presents a comparative study exploring user behaviour in VE-based fire evacuation and investigates whether this is affected by the addition of thermal and olfactory simulation. Participants (N=43) were exposed to a virtual fire in an office building. Quantitative and qualitative analyses of participant attitudes and behaviours found deviations from those we would expect in real life (e.g. pre-evacuation actions), but also valid behaviours like fire avoidance. Potentially important differences were found between multisensory and audiovisual-only conditions (e.g. perceived urgency). We conclude VEs have significant potential in safety-related applications, and that multimodality may afford additional uses in this context, but the identified limitations of behavioural validity must be carefully considered to avoid misapplication of the technology.',\n",
       " 'Social participation is known to bring great benefits to the health and well-being of people as they age. From being in contact with others to engaging in group activities, keeping socially active can help slow down the effects of age-related declines, reduce risks of loneliness and social isolation and even mortality in old age. There are unfortunately a variety of barriers that make it difficult for older adults to engage in social activities in a regular basis. In this chapter, we give an overview of the challenges to social participation and discuss how technology can help overcome these barriers and promote participation in social activities. We examine two particular research threads and designs, exploring ways in which technology can support co-located and virtual participation: i) an application that motivates the virtual participation in group training programs, and ii) a location-based game that supports co-located intergenerational ICT training classes. We discuss the effectiveness and limitations of various design choices in the two use cases and outline the lessons learned',\n",
       " 'Single-switch scanning systems allow nonspeaking individuals with motor disabilities to communicate by triggering a single switch (e.g., raising an eye brow). A problem with current single-switch scanning systems is that while they result in reasonable performance in noiseless conditions, for instance via simulation or tests with able-bodied users, they fail to accurately model the noise sources that are introduced when a non-speaking individual with motor disabilities is triggering the switch in a realistic use context. To help assist the development of more noise-resilient single-switch scanning systems we have developed a mathematical model of scanning systems which incorporates extensive noise modelling. Our model includes an improvement to the standard scanning method, which we call fast-scan, which we show via simulation can be more suitable for certain users of scanning systems.',\n",
       " 'Multimodal features play a key role in wearable sensor based Human Activity Recognition (HAR). Selecting the most salient features adaptively is a promising way to maximize the effectiveness of multimodal sensor data. In this regard, we propose a \"collect fully and select wisely (Fullie and Wiselie)\" principle as well as a dual-stream recurrent convolutional attention model, Recurrent Attention and Activity Frame (RAAF), to improve the recognition performance. We first collect modality features and the relations between each pair of features to generate activity frames, and then introduce an attention mechanism to select the most prominent regions from activity frames precisely. The selected frames not only maximize the utilization of valid features but also reduce the number of features to be computed effectively. We further analyze the hyper-parameters, accuracy, interpretability, and annotation dependency of the proposed model based on extensive experiments. The results show that RAAF achieves competitive performance on two benchmarked datasets and works well in real life scenarios.',\n",
       " \"The movement of a user's face, easily detected by a smartphone's front camera, is an underexploited input modality for mobile interactions. We introduce three sets of face-engaged interaction techniques for augmenting the traditional mobile inputs, which leverages the combination of the head movements with touch gestures and device motions, all sensed via the phone's built-in sensors. We systematically present the space of design considerations for mobile interactions using one or more of the three input modalities (i.e., touch, motion, and head). The additional affordances of the proposed techniques expand the mobile interaction vocabulary, and can facilitate unique usage scenarios such as one-hand or touch-free interaction. An initial evaluation was conducted and users had positive reactions to the new techniques, indicating the promise of an intuitive and convenient user experience.\",\n",
       " 'The main aim of this paper is to discuss how the combination of Web 2.0, social media and geographic technologies can provide opportunities for learning and new forms of participation in an urban design studio. This discussion is mainly based on our recent findings from two experimental urban design studio setups as well as former research and literature studies. In brief, the web platform enabled us to extend the learning that took place in the design studio beyond the studio hours, to represent the design information in novel ways and allocate multiple communication forms. We found that the student activity in the introduced web platform was related to their progress up to a certain extent. Moreover, the students perceived the platform as a convenient medium and addressed it as a valuable resource for learning. This study should be conceived as a continuation of a series of our Design Studio 2.0 experiments which involve the exploitation of opportunities provided by novel socio-geographic information and communication technologies for the improvement of the design learning processes.',\n",
       " 'Sketching and natural languages are effective communication media for interactive applications. We introduce Sketchforme, the first neural-network-based system that can generate sketches based on text descriptions specified by users. Sketchforme is capable of gaining high-level and low-level understanding of multi-object sketched scenes without being trained on sketched scene datasets annotated with text descriptions. The sketches composed by Sketchforme are expressive and realistic: we show in our user study that these sketches convey descriptions better than human-generated sketches in multiple cases, and 36.5% of those sketches are considered to be human-generated. We develop multiple interactive applications using these generated sketches, and show that Sketchforme can significantly improve language learning applications and support intelligent language-based sketching assistants.',\n",
       " 'Existing research highlight the myriad of benefits realized when technology is sufficiently democratized and made accessible to non-technical or novice users. However, democratizing complex technologies such as artificial intelligence (AI) remains hard. In this work, we draw on theoretical underpinnings from the democratization of innovation, in exploring the design of maker kits that help introduce novice users to complex technologies. We report on our work designing TJBot: an open source cardboard robot that can be programmed using pre-built AI services. We highlight principles we adopted in this process (approachable design, simplicity, extensibility and accessibility), insights we learned from showing the kit at workshops (66 participants) and how users interacted with the project on GitHub over a 12-month period (Nov 2016 - Nov 2017). We find that the project succeeds in attracting novice users (40% of users who forked the project are new to GitHub) and a variety of demographics are interested in prototyping use cases such as home automation, task delegation, teaching and learning.',\n",
       " \"The blocks editor, such as the editor in Scratch, is widely applied for visual programming languages (VPL) nowadays. Despite it's friendly for non-programmers, it exists three main limitations while displaying block codes: (1) the readability, (2) the program structure, and (3) the re-use. To cope with these issues, we introduce a novel formatting tool, block shelves, into the editor for organizing blocks. A user could utilize shelves to constitute a user-defined structure for the VPL projects. Based on the experiment results, block shelves improves the block code navigating and searching significantly. Besides, for achieving code re-use, users could use shelf export/import to share/re-use their block codes between projects in the file format of eXtensible Markup Language (xml.) All functions were demonstrated on MIT App inventor 2, while all modifications were made in Google Blockly.\",\n",
       " 'Many user studies of home automation, as the most familiar representative of the Internet of Things, have shown the difficulty of developing technology that users understand and like. It helps to state requirements as largely-independent features, but features are not truly independent, so this incurs the cost of managing and explaining feature interactions. We propose to compose features at runtime, resolving their interactions by means of priority. Although the basic idea is simple, its details must be designed to make users comfortable by balancing manual and automatic control. On the technical side, its details must be designed to allow meaningful separation of features and maximum generality. As evidence that our composition mechanism achieves its goals, we present three substantive examples of home automation, and the results of a user study to investigate comprehension of feature interactions. A survey of related work shows that this proposal occupies a sensible place in a design space whose dimensions include actuator type, detection versus resolution strategies, and modularity.',\n",
       " \"This paper presents a novel game prototype that uses music and motion detection as preventive medicine for the elderly. Given the aging populations around the globe, and the limited resources and staff able to care for these populations, eHealth solutions are becoming increasingly important, if not crucial, additions to modern healthcare and preventive medicine. Furthermore, because compliance rates for performing physical exercises are often quite low in the elderly, systems able to motivate and engage this population are a necessity. Our prototype uses music not only to engage listeners, but also to leverage the efficacy of music to improve mental and physical wellness. The game is based on a memory task to stimulate cognitive function, and requires users to perform physical gestures to mimic the playing of different musical instruments. To this end, the Microsoft Kinect sensor is used together with a newly developed gesture detection module in order to process users' gestures. The resulting prototype system supports both cognitive functioning and physical strengthening in the elderly.\",\n",
       " 'A graphical user interface (GUI) represents the most common option for interacting with computer systems. However, according to the literature system administrators often favor command line interfaces (CLIs). The goal of our work is to investigate which interfaces system administrators prefer, and which they actually utilize in their daily tasks. We collected experiences and opinions from 300 system administrators with the help of an online survey. All our respondents are system administrators, who work or have worked with firewalls. Our results show that only 32% of the respondents prefer CLIs for managing firewalls, while the corresponding figure is 60% for GUIs. We report the mentioned strengths and limitations of each interface and the tasks for which they are utilized by the system administrators. Based on these results, we provide design recommendations for firewall interfaces.',\n",
       " 'In this paper we consider the neuroscientific theory of the Bayesian brain in the light of adaptive web systems and content personalisation. In particular, we elaborate on neural mechanisms of human decision-making and the origin of lacking reliability of user feedback, often denoted as noise or human uncertainty. To this end, we first introduce an adaptive model of cognitive agency in which populations of neurons provide an estimation for states of the world. Subsequently, we present various so-called decoder functions with which neuronal activity can be translated into quantitative decisions. The interplay of the underlying cognition model and the chosen decoder function leads to different model-based properties of decision processes. The goal of this paper is to promote novel user models and exploit them to naturally associate users to different clusters on the basis of their individual neural characteristics and thinking patterns. These user models might be able to turn the variability of user behaviour into additional information for improving web personalisation and its experience.',\n",
       " \"Sharkzor is a web application for machine-learning assisted image sort and summary. Deep learning algorithms are leveraged to infer, augment, and automate the user's mental model. Initially, images uploaded by the user are spread out on a canvas. The user then interacts with the images to impute their mental model into the application's algorithmic underpinnings. Methods of interaction within Sharkzor's user interface and user experience support three primary user tasks; triage, organize and automate. The user triages the large pile of overlapping images by moving images of interest into proximity. The user then organizes said images into meaningful groups. After interacting with the images and groups, deep learning helps to automate the user's interactions. The loop of interaction, automation, and response by the user allows the system to quickly make sense of large amounts of data.\",\n",
       " \"Traditional relative pointing devices such as mice and trackpads are unsuitable for pointing at distant displays, because they encumber the users by requiring either a flat surface to operate on or being held by two hands. Past research has examined many new pointing methods, but few could surpass the speed and accuracy of mice and trackpads. This paper introduces a new pointing system that is developed based on HTC Vive, a relatively low-cost virtual reality system, and proposes two methods of combining absolute and relative pointing. The proposed methods were compared against single-mode pointing methods (i.e., pure absolute pointing and pure relative pointing) in a Fitts' law study. The results show that with only a short period of practice, one hybrid pointing technique enabled faster and more accurate pointing than both single-mode pointing techniques, which included a trackpad.\",\n",
       " 'Distant pointing is still not efficient, accurate or flexible enough for many applications, although many researchers have focused on it. To improve upon distant pointing, we propose MPP3D, which is especially suitable for high-resolution displays. MPP3D uses two dimensions of hand positioning to move a pointer, and it also uses the third dimension to adjust the precision of the movement. Based on the idea of MPP3D, we propose four techniques which combine two ways of mapping and two techniques for precision adjustment. We further provide three types of mapping scheme and visual feedback for each technique. The potential of the proposed techniques was investigated through experimentation. The results show that these techniques were competent for usual computer operations with a cursor, and the adjustment for pointing precision was beneficial for both pointing efficiency and accuracy.',\n",
       " 'In recent times, there have been significant advancements in utilizing the sensing capabilities of mobile devices for developing applications. The primary objective has been to enhance the way a user interacts with the application by making it effortless and convenient. This paper explores the capabilities of using Brain Computer Interfaces (BCI), an evolving subset of Human Computer Interaction (HCI) paradigms, to control mobile devices. We present a comprehensive survey of the state-of-the-art in this area, discussing the challenges and limitations in using BCI for mobile applications. Further we propose possible modalities that in future can benefit with BCI applications. This paper consolidates research directions being pursued in this domain, and draws conclusions on feasibility and benefits of using BCI systems effectively augmented to the mobile application development domain.',\n",
       " \"Software tools for generating digital sound often present users with high-dimensional, parametric interfaces, that may not facilitate exploration of diverse sound designs. In this paper, we propose to investigate artificial agents using deep reinforcement learning to explore parameter spaces in partnership with users for sound design. We describe a series of user-centred studies to probe the creative benefits of these agents and adapting their design to exploration. Preliminary studies observing users' exploration strategies with parametric interfaces and testing different agent exploration behaviours led to the design of a fully-functioning prototype, called Co-Explorer, that we evaluated in a workshop with professional sound designers. We found that the Co-Explorer enables a novel creative workflow centred on human-machine partnership, which has been positively received by practitioners. We also highlight varied user exploration behaviors throughout partnering with our system. Finally, we frame design guidelines for enabling such co-exploration workflow in creative digital applications.\",\n",
       " 'In this paper, we discuss the generation of symbols (and alphabets) based on specific user requirements (medium, priorities, type of information that needs to be conveyed). A framework for the generation of alphabets is proposed, and its use for the generation of a shorthand writing system is explored. We discuss the possible use of machine learning and genetic algorithms to gather inputs for generation of such alphabets and for optimization of already generated ones. The alphabets generated using such methods may be used in very different fields, from the creation of synthetic languages and constructed scripts to the creation of sensible commands for multimodal interaction through Human-Computer Interfaces, such as mouse gestures, touchpads, body gestures, eye-tracking cameras, and brain-computing Interfaces, especially in applications for elderly care and people with disabilities.',\n",
       " 'Collaborative sensemaking requires that analysts share their information and insights with each other, but this process of sharing runs the risks of prematurely focusing the investigation on specific suspects. To address this tension, we propose and test an interface for collaborative crime analysis that aims to make analysts more aware of their sensemaking processes. We compare our sensemaking translucence interface to a standard interface without special sensemaking features in a controlled laboratory study. We found that the sensemaking translucence interface significantly improved clue finding and crime solving performance, but that analysts rated the interface lower on subjective measures than the standard interface. We conclude that designing for distributed sensemaking requires balancing task performance vs. user experience and real-time information sharing vs. data accuracy.',\n",
       " 'Engagement is a vital metric in the advertising industry and its automatic estimation has huge commercial implications. This work presents a basic and simple framework for engagement estimation using EEG (electroencephalography) data specifically recorded while watching advertisement videos, and is meant to be a first step in a promising line of research. The system combines recent advances in low cost commercial Brain-Computer Interfaces with modeling user engagement in response to advertisement videos. We achieve an F1 score of nearly 0.7 for a binary classification of high and low values of self-reported engagement from multiple users. This study illustrates the possibility of seamless engagement measurement in the wild when interacting with media using a non invasive and readily available commercial EEG device. Performing engagement measurement via implicit tagging in this manner with a direct feedback from physiological signals, thus requiring no additional human effort, demonstrates a novel and potentially commercially relevant application in the area of advertisement video analysis.',\n",
       " 'The \"gig economy\" has transformed the ways in which people work, but in many ways these markets stifle the growth of workers and the autonomy and protections that workers have grown to expect. We explored the viability of a \"worker centric peer economy\"--a system wherein workers benefit as well as consumers-- and conducted ethnographic field work across fields ranging from domestic labor to home health care. We discovered seven facets that system designers ought to consider when designing a labor market for \"gig workers,\" consisting principally of the following: constructive feedback, assigning work fairly, managing customer expectations, protecting vulnerable workers, reconciling worker identities, assessing worker qualifications, & communicating worker quality. We discuss these considerations and provide guidance toward the design of a mutually beneficial market for gig workers.',\n",
       " 'Fall is one of the major health threats and obstacles to independent living for elders, timely and reliable fall detection is crucial for mitigating the effects of falls. In this paper, leveraging the fine-grained Channel State Information (CSI) and multi-antenna setting in commodity WiFi devices, we design and implement a real-time, non-intrusive, and low-cost indoor fall detector, called Anti-Fall. For the first time, the CSI phase difference over two antennas is identified as the salient feature to reliably segment the fall and fall-like activities, both phase and amplitude information of CSI is then exploited to accurately separate the fall from other fall-like activities. Experimental results in two indoor scenarios demonstrate that Anti-Fall consistently outperforms the state-of-the-art approach WiFall, with 10% higher detection rate and 10% less false alarm rate on average.',\n",
       " 'Opportunistic affect sensing offers unprecedented potential for capturing spontaneous affect ubiquitously, obviating biases inherent in the laboratory setting. Facial expression and voice are two major affective displays, however most affect sensing systems on smartphone avoid them due to extensive power requirement. Encouragingly, due to the recent advent of low-power DSP (Digital Signal Processing) co-processor and GPU (Graphics Processing Unit) technology, audio and video sensing are becoming more feasible. To properly evaluate opportunistically captured facial expression and voice, contextual information about the dynamic audio-visual stimuli needs to be inferred. This paper discusses recent advances of affect sensing on the smartphone and identifies the key barriers and potential solutions of implementing opportunistic and context-aware affect sensing on smartphone platforms.',\n",
       " 'Recently, we developed a dynamic distributed end-to-end vehicle routing system (E2ECAV) using a network of intelligent intersections and level 5 CAVs (Djavadian & Farooq, 2018). The case study of the downtown Toronto Network showed that E2ECAV has the ability to maximize throughput and reduce travel time up to 40%. However, the efficiency of these new technologies relies on the acceptance of users in adapting to them and their willingness to give control fully or partially to CAVs. In this study a stated preference laboratory experiment is designed employing Virtual Reality Immersive Environment (VIRE) driving simulator to evaluate the behavioral response of drivers to E2ECAV. The aim is to investigate under what conditions drivers are more willing to adapt. The results show that factors such as locus of control, congestion level and ability to multi-task have significant impact.',\n",
       " 'Medical non-adherence increasingly is recognized as a major medical health problem. Approximately 50% of patients do not take their medications as prescribed and such poor adherence has been shown to result in complications, death, and increased health care costs. This problem becomes even more significant for patients with chronic illness and those who need to take medications lifetime, like transplant patients. Studies show that one-half of rejection episodes and 15% of graft losses happen due to immunosuppression medications non-adherence. This article explores factors that have an impact on non-compliant behavior among transplant patients: patient factors, illness factor, therapeutic regimen factors. Using user-centered design thinking approach a set of hypotheses are defined and discussed strategies to enhance adherence by using mobile technology and gamification techniques.',\n",
       " 'The main challenges in large-scale people tracking are the recognition of people density in a specific area and tracking the people flow path. To address these challenges, we present SenseFlow, a lightweight people tracking system. SenseFlow utilises off-the-shelf devices which sniff probe requests periodically polled by user\\'s smartphones in a passive manner. We demonstrate the feasibility of SenseFlow by building a proof-of-concept prototype and undertaking extensive evaluations in real-world settings. We deploy the system in one laboratory to study office hours of researchers, a crowded public area in city to evaluate the scalability and performance \"in the wild\", and four classrooms in the university to monitor the number of students. We also evaluate SenseFlow with varying walking speeds and different models of smartphones to investigate the people flow tracking performance.',\n",
       " \"An implicit association test is a human psychological test used to measure subconscious associations. While widely recognized by psychologists as an effective tool in measuring attitudes and biases, the validity of the results can be compromised if a subject does not follow the instructions or attempts to manipulate the outcome. Compared to previous work, we collect training data using a more generalized methodology. We train a variety of different classifiers to identify a participant's first attempt versus a second possibly compromised attempt. To compromise the second attempt, participants are shown their score and are instructed to change it using one of five randomly selected deception methods. Compared to previous work, our methodology demonstrates a more robust and practical framework for accurately identifying a wide variety of deception techniques applicable to the IAT.\",\n",
       " 'When people browse online news, small thumbnail images accompanying links to articles attract their attention and help them to decide which articles to read. As an increasing proportion of online news can be construed as data journalism, we have witnessed a corresponding increase in the incorporation of visualization in article thumbnails. However, there is little research to support alternative design choices for visualization thumbnails, which include resizing, cropping, simplifying, and embellishing charts appearing within the body of the associated article. We therefore sought to better understand these design choices and determine what makes a visualization thumbnail inviting and interpretable. This paper presents our findings from a survey of visualization thumbnails collected online and from conversations with data journalists and news graphics designers. Our study reveals that there exists an uncharted design space, one that is in need of further empirical study. Our work can thus be seen as a first step toward providing structured guidance on how to design thumbnails for data stories.',\n",
       " 'Teaching and advocating data visualization are among the most important activities in the visualization community. With growing interest in data analysis from business and science professionals, data visualization courses attract students across different disciplines. However, comprehensive visualization training requires students to have a certain level of proficiency in programming, a requirement that imposes challenges on both teachers and students. With recent developments in visualization tools, we have managed to overcome these obstacles by teaching a wide range of visualization and supporting tools. Starting with GUI-based visualization tools and data analysis with Python, students put visualization knowledge into practice with increasing amounts of programming. At the end of the course, students can design and implement visualizations with D3 and other programming-based visualization tools. Throughout the course, we continuously collect student feedback and refine the teaching materials. This paper documents our teaching methods and considerations when designing the teaching materials.',\n",
       " \"When studying human-technology interaction systems, researchers thrive to achieve intuitiveness and facilitate the people's life through a thoughtful and in-depth study of several components of the application system that supports some particular business communication with customers. Particularly in the healthcare field, some requirements such as clarity, transparency, efficiency, and speed in transmitting information to patients and or healthcare professionals might mean an important increase in the well-being of the patient and productivity of the healthcare professional. In this work, the authors study the difficulties patients frequently have when communicating with pharmacists. In addition to a statistical study of a survey conducted with more than two hundred frequent pharmacy customers, we propose an IT solution for better communication between patients and pharmacists.\",\n",
       " 'Data visualization and interaction with large data sets is known to be essential and critical in many businesses today, and the same applies to research and teaching, in this case, when exploring large and complex mathematical objects. GAP is a computer algebra system for computational discrete algebra with an emphasis on computational group theory. The existing XGAP package for GAP works exclusively on the X Window System. It lacks abstraction between its mathematical and graphical cores, making it difficult to extend, maintain, or port. In this paper, we present Francy, a graphical semantics package for GAP. Francy is responsible for creating a representational structure that can be rendered using many GUI frameworks independent from any particular programming language or operating system. Building on this, we use state of the art web technologies that take advantage of an improved REPL environment, which is currently under development for GAP. The integration of this project with Jupyter provides a rich graphical environment full of features enhancing the usability and accessibility of GAP.',\n",
       " \"The advent of the digital pathology has introduced new avenues of diagnostic medicine. Among them, crowdsourcing has attracted researchers' attention in the recent years, allowing them to engage thousands of untrained individuals in research and diagnosis. While there exist several articles in this regard, prior works have not collectively documented them. We, therefore, aim to review the applications of crowdsourcing in human pathology in a semi-systematic manner. We firstly, introduce a novel method to do a systematic search of the literature. Utilizing this method, we, then, collect hundreds of articles and screen them against a pre-defined set of criteria. Furthermore, we crowdsource part of the screening process, to examine another potential application of crowdsourcing. Finally, we review the selected articles and characterize the prior uses of crowdsourcing in pathology.\",\n",
       " \"While visual comparison of directed acyclic graphs (DAGs) is commonly encountered in various disciplines (e.g., finance, biology), knowledge about humans' perception of graph similarity is currently quite limited. By graph similarity perception we mean how humans perceive commonalities and differences in graphs and herewith come to a similarity judgment. As a step toward filling this gap the study reported in this paper strives to identify factors which influence the similarity perception of DAGs. In particular, we conducted a card-sorting study employing a qualitative and quantitative analysis approach to identify 1) groups of DAGs that are perceived as similar by the participants and 2) the reasons behind their choice of groups. Our results suggest that similarity is mainly influenced by the number of levels, the number of nodes on a level, and the overall shape of the graph.\",\n",
       " \"Smart devices have become common place in many homes, and these devices can be utilized to provide support for people with mental or physical deficits. Voice-controlled assistants are a class of smart device that collect a large amount of data in the home. In this work we present Echo SCraper and ClAssifier of Persons (ESCAPE), an open source software for the extraction of Amazon Echo interaction data, and speaker recognition on that data. We show that ESCAPE is able to extract data from a voice-controlled assistant and classify with accuracy who is talking, based on a small number of labeled audio data. Using ESCAPE to extract interactions recorded over 3 months in the first author's home yields a rich dataset of transcribed audio recordings. Our results demonstrate that using this software the Amazon Echo can be used to study participants in a naturalistic setting with minimal intrusion. We also discuss the potential for usage of voice-controlled devices together with ESCAPE to understand how diseases affect individuals, and how these data can be used to monitor disease processes in general.\",\n",
       " 'The human visual color response is driven by specialized cells called cones, which exist in three types, viz. R, G, and B. Software is developed to simulate how color images are displayed for different types of color blindness. Specified the default color deficiency associated with a user, it generates a preview of the rainbow (in the visible range, from red to violet) and shows up, side by side with a colorful image provided as input, the display correspondent colorblind. The idea is to provide an image processing after image acquisition to enable a better perception ofcolors by the color blind. Examples of pseudo-correction are shown for the case of Protanopia (red blindness). The system is adapted into a screen of an i-pad or a cellphone in which the colorblind observe the camera, the image processed with color detail previously imperceptible by his naked eye. As prospecting, wearable computer glasses could be manufactured to provide a corrected image playback. The approach can also provide augmented reality for human vision by adding the UV or IR responses as a new feature of Google Glass.',\n",
       " \"In the last decade, the effects of interruptions through mobile notifications have been extensively researched in the field of Human-Computer Interaction. Breakpoints in tasks and activities, cognitive load, and personality traits have all been shown to correlate with individuals' interruptibility. However, concepts that explain interruptibility in a broader sense are needed to provide a holistic understanding of its characteristics. In this paper, we build upon the theory of social roles to conceptualize and investigate the correlation between individuals' private and work-related smartphone usage and their interruptibility. Through our preliminary study with four participants over 11 weeks, we found that application sequences on smartphones correlate with individuals' private and work roles. We observed that participants engaged in these roles tend to follow specific interruptibility strategies - integrating, combining, or segmenting private and work-related engagements. Understanding these strategies breaks new ground for attention and interruption management systems in ubiquitous computing.\",\n",
       " 'Intelligent systems and advanced automation are involved in information collection and evaluation, in decision-making and in the implementation of chosen actions. In such systems, human responsibility becomes equivocal. Understanding human responsibility is particularly important when intelligent autonomous systems can harm people, as with autonomous vehicles or, most notably, with Advanced Weapon Systems (AWS). Using Information Theory, we develop a responsibility quantification (ResQu) model of human involvement in intelligent automated systems and demonstrate its applications on decisions regarding AWS. The analysis reveals that human comparative responsibility is often low, even when major functions are allocated to the human. Thus, broadly stated policies of keeping humans in the loop and having meaningful human control are misleading and cannot truly direct decisions on how to involve humans in intelligent systems and advanced automation. Our responsibility model can guide system design decisions and can aid policy and legal decisions regarding human responsibility in intelligent systems.',\n",
       " \"Interactive visual analytic systems enable users to discover insights from complex data. Users can express and test hypotheses via user interaction, leveraging their domain expertise and prior knowledge to guide and steer the analytic models in the system. For example, semantic interaction techniques enable systems to learn from the user's interactions and steer the underlying analytic models based on the user's analytical reasoning. However, an open challenge is how to not only steer models based on the dimensions or features of the data, but how to add dimensions or attributes to the data based on the domain expertise of the user. In this paper, we present a technique for inferring and appending dimensions onto the dataset based on the prior expertise of the user expressed via user interactions. Our technique enables users to directly manipulate a spatial organization of data, from which both the dimensions of the data are weighted, and also dimensions created to represent the prior knowledge the user brings to the system. We describe this technique and demonstrate its utility via a use case.\",\n",
       " 'The present paper reports the results of testing first year students of Informatics on their algorithmic skills and knowledge transfer abilities in spreadsheet environments. The selection of students plays a crucial role in the project. On the one hand, they have officially finished their spreadsheet training - they know everything - while on the other hand, they do not need any training, since they are digital natives, to whom digital skills are assigned by birth. However, we found that the students had serious difficulties in solving the spreadsheet problems presented: so low were their results that it allowed us to form broad tendencies. Considering computational thinking, algorithmic skills, and knowledge transfer abilities, it is clear that those students performed better who used algorithm-based, multilevel array formulas instead of problem specific, unconnected built-in functions. Furthermore, we can conclude that students, regardless of their birth date and digital generation assigned to them, are in great need of official, high-mathability, algorithm-based training with expert teachers.',\n",
       " \"We present an end-to-end voice-based conversational agent that is able to engage in naturalistic multi-turn dialogue and align with the interlocutor's conversational style. The system uses a series of deep neural network components for speech recognition, dialogue generation, prosodic analysis and speech synthesis to generate language and prosodic expression with qualities that match those of the user. We conducted a user study (N=30) in which participants talked with the agent for 15 to 20 minutes, resulting in over 8 hours of natural interaction data. Users with high consideration conversational styles reported the agent to be more trustworthy when it matched their conversational style. Whereas, users with high involvement conversational styles were indifferent. Finally, we provide design guidelines for multi-turn dialogue interactions using conversational style adaptation.\",\n",
       " 'A significant body of research in Artificial Intelligence (AI) has focused on generating stories automatically, either based on prior story plots or input images. However, literature has little to say about how users would receive and use these stories. Given the quality of stories generated by modern AI algorithms, users will nearly inevitably have to edit these stories before putting them to real use. In this paper, we present the first analysis of how human users edit machine-generated stories. We obtained 962 short stories generated by one of the state-of-the-art visual storytelling models. For each story, we recruited five crowd workers from Amazon Mechanical Turk to edit it. Our analysis of these edits shows that, on average, users (i) slightly shortened machine-generated stories, (ii) increased lexical diversity in these stories, and (iii) often replaced nouns and their determiners/articles with pronouns. Our study provides a better understanding on how users receive and edit machine-generated stories,informing future researchers to create more usable and helpful story generation systems.',\n",
       " \"Traditional approaches for ensuring high quality crowdwork have failed to achieve high-accuracy on difficult problems. Aggregating redundant answers often fails on the hardest problems when the majority is confused. Argumentation has been shown to be effective in mitigating these drawbacks. However, existing argumentation systems only support limited interactions and show workers general justifications, not context-specific arguments targeted to their reasoning.\\n  This paper presents Cicero, a new workflow that improves crowd accuracy on difficult tasks by engaging workers in multi-turn, contextual discussions through real-time, synchronous argumentation. Our experiments show that compared to previous argumentation systems which only improve the average individual worker accuracy by 6.8 percentage points on the Relation Extraction domain, our workflow achieves 16.7 percentage point improvement. Furthermore, previous argumentation approaches don't apply to tasks with many possible answers; in contrast, Cicero works well in these cases, raising accuracy from 66.7% to 98.8% on the Codenames domain.\",\n",
       " 'Web portals have served as an excellent medium to facilitate user centric services for organizations irrespective of the type, size, and domain of operation. The objective of these portals has been to deliver a plethora of services such as information dissemination, transactional services, and customer feedback. Therefore, the design of a web portal is crucial in order that it is accessible to a wide range of user community irrespective of age group, physical abilities, and level of literacy. In this paper, we have studied the compliance of WCAG 2.0 by three different categories of Indian web sites which are most frequently accessed by a large section of user community. We have provided a quantitative evaluation of different aspects of accessibility which we believe can pave the way for better design of web sites by taking care of the deficiencies inherent in the web portals.',\n",
       " \"Recently, researchers started using cognitive load in various settings, e.g., educational psychology, cognitive load theory, or human-computer interaction. Cognitive load characterizes a tasks' demand on the limited information processing capacity of the brain. The widespread adoption of eye-tracking devices led to increased attention for objectively measuring cognitive load via pupil dilation. However, this approach requires a standardized data processing routine to reliably measure cognitive load. This technical report presents CEP-Web, an open source platform to providing state of the art data processing routines for cleaning pupillary data combined with a graphical user interface, enabling the management of studies and subjects. Future developments will include the support for analyzing the cleaned data as well as support for Task-Evoked Pupillary Response (TEPR) studies.\",\n",
       " 'Automatic recognition of the quality of movement in human beings is a challenging task, given the difficulty both in defining the constraints that make a movement correct, and the difficulty in using noisy data to determine if these constraints were satisfied. This paper presents a method for the detection of deviations from the correct form in movements from physical therapy routines based on Hidden Markov Models, which is compared to Dynamic Time Warping. The activities studied include upper an lower limbs movements, the data used comes from a Kinect sensor. Correct repetitions of the activities of interest were recorded, as well as deviations from these correct forms. The ability of the proposed approach to detect these deviations was studied. Results show that a system based on HMM is much more likely to determine if a certain movement has deviated from the specification.',\n",
       " 'Due to the prevalence of online services in modern society, such as internet banking and social media, it is important for users to have an understanding of basic security measures in order to keep themselves safe online. However, users often do not know how to make their online interactions secure, which demonstrates an educational need in this area. Gamification has grown in popularity in recent years and has been used to teach people about a range of subjects. This paper presents an exploratory study investigating the use of gamification techniques to educate average users about password security, with the aim of raising overall security awareness. To explore the impact of such techniques, a role-playing quiz application (RPG) was developed for the Android platform to educate users about password security. Results gained from the work highlighted that users enjoyed learning via the use of the password application, and felt they benefitted from the inclusion of gamification techniques. Future work seeks to expand the prototype into a full solution, covering a range of security awareness issues.',\n",
       " 'Mobile money can facilitate financial inclusion in developing countries, which usually have high mobile phone use and steady remittance activity. Many countries in Latin America meet the minimum technological requirements to use mobile money, however, the adoption in this region is relatively low. This paper investigates the different factors that lead people in Latin America to distrust and therefore not adopt mobile money. For this purpose, we analyzed 27 mobile money applications on the market and investigated the perceptions that people in Latin America have of such interfaces. From our study, we singled out the interface features that have the greatest influence in user adoption in developing countries. We identified that for the Latin America market it is crucial to create mobile applications that allow the user to visualize and understand the workflow through which their money is traveling to recipients. We examined the significance of these findings in the design of future mobile money applications that can effectively improve the use of electronic financial transactions in Latin America.',\n",
       " \"Today, eye trackers are extensively used in user interface evaluations. However, it's still hard to analyze and interpret eye tracking data from the aesthetic point of view. To find quantitative links between eye movements and aesthetic experience, we tracked 30 observers' initial landings for 40 web pages (each displayed for 3 seconds). The web pages were also rated based on the observers' subjective aesthetic judgments. Shannon entropy was introduced to analyze the eye-tracking data. The result shows that the heatmap entropy (visual attention entropy, VAE) is highly correlated with the observers' aesthetic judgements of the web pages. Its improved version, relative VAE (rVAE), has a more significant correlation with the perceived aesthetics. (r=-0.65, F= 26.84, P$<$0.0001). This single metric alone can distinguish between good- and bad-looking pages with an approximate 85\\\\% accuracy. Further investigation reveals that the performance of both VAE and rVAE became stable after 1 second. The curves indicate that their performances could be better, if the tracking time was extended beyond 3 seconds.\",\n",
       " 'Virtual Learning Environments (VLEs) are spaces designed to educate students remotely via online platforms. Although traditional VLEs such as iSocial have shown promise in educating students, they offer limited immersion that diminishes learning effectiveness. This paper outlines a virtual reality learning environment (VRLE) over a high-speed network, which promotes educational effectiveness and efficiency via our creation of flexible content and infrastructure which meet established VLE standards with improved immersion. This paper further describes our implementation of multiple learning modules developed in High Fidelity, a \"social VR\" platform. Our experiment results show that the VR mode of content delivery better stimulates the generalization of lessons to the real world than non-VR lessons and provides improved immersion when compared to an equivalent desktop version.',\n",
       " 'In the early stages of designing graphical user interfaces (GUIs), the look (appearance) can be easily presented by sketching, but the feel (interactive behaviors) cannot, and often requires an accompanying description of how it works (Myers et al. 2008). We propose to use crowdsourcing to augment early sketches with interactive behaviors generated, used, and reused by collective \"wizards-of-oz\" as opposed to a single wizard as in prior work (Davis et al. 2007). This demo presents an extension of Apparition (Lasecki et al. 2015), a crowd-powered prototyping tool that allows end users to create functional GUIs using speech and sketch. In Apparition, crowd workers collaborate in real-time on a shared canvas to refine the user-requested sketch interactively, and with the assistance of the end users. Our demo extends this functionality to let crowd workers \"demonstrate\" the canvas changes that are needed for a behavior and refine their demonstrations to improve the fidelity of interactive behaviors. The system then lets workers \"remix\" these behaviors to make creating future behaviors more efficient.',\n",
       " 'Today, more and more open data statistics are published by governments, statistical offices and organizations like the United Nations, The World Bank or Eurostat. This data is freely available and can be consumed by end users in interactive visualizations. However, additional information is needed to enable laymen to interpret these statistics in order to make sense of the raw data. In this paper, we present an approach to combine open data statistics with historical events. In a user interface we have integrated interactive visualizations of open data statistics with a timeline of thematically appropriate historical events from Wikipedia. This can help users to explore statistical data in several views and to get related events for certain trends in the timeline. Events include links to Wikipedia articles, where details can be found and the search process can be continued. We have conducted a user study to evaluate if users can use the interface intuitively, if relations between trends in statistics and historical events can be found and if users like this approach for their exploration process.',\n",
       " \"Based on the previously proposed concept Understanding Tree, this paper introduces two concepts: Understanding Graph and Understanding Map, and explores their potential applications. Understanding Graph and Understanding Map can be deemed as special cases of mind map, semantic network, or concept map. The two main differences are: Firstly, the data sources for constructing Understanding Map and Understanding Graph are distinctive and simple. Secondly, the relations between concepts in Understanding Graph and Understanding Map are monotonous. Based on their characteristics, applications of them include quantitatively measuring a concept's complexity degree, quantitatively measuring a concept's importance degree in a domain, and computing an optimized learning sequence for comprehending a concept etc. Further study involves evaluating their performances in these applications.\",\n",
       " 'Promotion of healthy habits help maintain and improve people health, reduce disease risks, and manage chronic illness. Regular healthy activities like walking, exercising, healthy eating, drinking water or taking medication on time require forming the new habits. Gamification techniques are promising in promoting healthy behaviors and delivering health promotion information. However, using gaming elements such as badges, leader boards, health-related challenges in mobile applications to motivate and engage people to change health behavior is quite new. In this exploratory study, we aimed to assess how game mechanics and dynamics influence formation of a habit through the mobile application. Results indicate the different level of user engagement depending on the presence of gamification elements and suggest that there is value in adding game elements to the user experience.',\n",
       " 'Texture is an essential property of physical objects that affects aesthetics, usability, and functionality. However, designing and applying textures to 3D objects with existing tools remains difficult and time-consuming; it requires proficient 3D modeling skills. To address this, we investigated an auto-completion approach for efficient texture creation that automates the tedious, repetitive process of applying texture while allowing flexible customization. We developed techniques for users to select a target surface, sketch and manipulate a texture with 2D drawings, and then generate 3D printable textures onto an arbitrary curved surface. In a controlled experiment our tool sped texture creation by 80% over conventional tools, a performance gain that is higher with more complex target surfaces. This result confirms that auto-completion is powerful for creating 3D textures.',\n",
       " \"In this paper we introduce a paradigm for completing complex tasks from wearable devices by leveraging crowdsourcing, and demonstrate its validity for academic writing. We explore this paradigm using a collaborative authoring system, called WearWrite, which is designed to enable authors and crowd workers to work together using an Android smartwatch and Google Docs to produce academic papers, including this one. WearWrite allows expert authors who do not have access to large devices to contribute bits of expertise and big picture direction from their watch, while freeing them of the obligation of integrating their contributions into the overall document. Crowd workers on desktop computers actually write the document. We used this approach to write several simple papers, and found it was effective at producing reasonable drafts. However, the workers often needed more structure and the authors more context. WearWrite addresses these issues by focusing workers on specific tasks and providing select context to authors on the watch. We demonstrate the system's feasibility by writing this paper using it.\",\n",
       " 'We present a Virtual Reality (VR) application for labeling and handling point cloud data sets. A series of room-scale point clouds are recorded as a video sequence using a Microsoft Kinect. The data can be played and paused, and frames can be skipped just like in a video player. The user can walk around and inspect the data while it is playing or paused. Using the tracked hand-held controller, the user can select and label individual parts of the point cloud. The points are highlighted with a color when they are labeled. With a tracking algorithm, the labeled points can be tracked from frame to frame to ease the labeling process. Our sample data is an RGB point cloud recording of two people juggling with pins. Here, the user can select and label, for example, the juggler pins as shown in Figure 1. Each juggler pin is labeled with various colors to indicate di erent labels.',\n",
       " 'Human computation games (HCGs) are a crowdsourcing approach to solving computationally-intractable tasks using games. In this paper, we describe the need for generalizable HCG design knowledge that accommodates the needs of both players and tasks. We propose a formal representation of the mechanics in HCGs, providing a structural breakdown to visualize, compare, and explore the space of HCG mechanics. We present a methodology based on small-scale design experiments using fixed tasks while varying game elements to observe effects on both the player experience and the human computation task completion. Finally we discuss applications of our framework using comparisons of prior HCGs and recent design experiments. Ultimately, we wish to enable easier exploration and development of HCGs, helping these games provide meaningful player experiences while solving difficult problems.',\n",
       " \"We present Eventful, a system for producing news reports of local events using remote and locative crowd workers. The system recruits and guides novice crowd workers as they perform the roles of field reporter, curator, or writer. Field reporters attend the events in person, and use Eventful's mobile web app to get a personalized mission, submit content, and receive feedback. Missions include tasks such as taking a photo, and asking a question to an attendee. In parallel, remote curators approve, reject, and give real-time feedback on the content collected by field reporters. Finally, writers put together a report by mashing up and tweaking the content approved by the curators. We used Eventful to produce a news report for each of the six local events we decided to cover as we piloted the system. The process was typically completed under an hour and costing under $150 USD.\",\n",
       " 'Task-based, rather than vehicle-based, control architectures have been shown to provide superior performance in certain human supervisory control missions. These results motivate the need for the development of robust, reliable usability metrics to aid in creating interfaces for use in this domain. To this end, we conduct a pilot usability study of a particular task-based supervisory control interface called the Research Environment for Supervisory Control of Heterogenous Unmanned Vehicles (RESCHU). In particular, we explore the use of eye-tracking metrics as an objective means of evaluating the RESCHU interface and providing guidance in improving usability. Our main goals for this study are to 1) better understand how eye-tracking can augment standard usability metrics, 2) formulate initial models of operator behavior, and 3) identify interesting areas of future research.',\n",
       " \"Here, we introduce a new data visualization and exploration method, TMAP (tree-map), which exploits locality sensitive hashing, Kruskal's minimum-spanning-tree algorithm, and a multilevel multipole-based graph layout algorithm to represent large and high dimensional data sets as a tree structure, which is readily understandable and explorable. Compared to other data visualization methods such as t-SNE or UMAP, TMAP increases the size of data sets that can be visualized due to its significantly lower memory requirements and running time and should find broad applicability in the age of big data. We exemplify TMAP in the area of cheminformatics with interactive maps for 1.16 million drug-like molecules from ChEMBL, 10.1 million small molecule fragments from FDB17, and 131 thousand 3D- structures of biomolecules from the PDB Databank, and to visualize data from literature (GUTENBERG data set), cancer biology (PANSCAN data set) and particle physics (MiniBooNE data set). TMAP is available as a Python package. Installation, usage instructions and application examples can be found at http://tmap.gdb.tools.\",\n",
       " 'Humans use a host of signals to infer the emotional state of others. In general, computer systems that leverage signals from multiple modalities will be more robust and accurate in the same task. We present a multimodal affect and context sensing platform. The system is composed of video, audio and application analysis pipelines that leverage ubiquitous sensors (camera and microphone) to log and broadcast emotion data in real-time. The platform is designed to enable easy prototyping of novel computer interfaces that sense, respond and adapt to human emotion. This paper describes the different audio, visual and application processing components and explains how the data is stored and/or broadcast for other applications to consume. We hope that this platform helps advance the state-of-the-art in affective computing by enabling development of novel human-computer interfaces.',\n",
       " \"In virtual reality games, players dive into fictional environments and can experience a compelling and immersive world. State-of-the-art VR systems allow for natural and intuitive navigation through physical walking. However, the tracking space is still limited, and viable alternatives \\\\comm{or extensions }are required to reach further virtual destinations. Our work focuses on the exploration of vast open worlds -- an area where existing local navigation approaches such as the arc-based teleport are not ideally suited and world-in-miniature techniques potentially reduce presence. We present a novel alternative for open environments: Our idea is to equip players with the ability to switch from first-person to a third-person bird's eye perspective on demand. From above, players can command their avatar and initiate travels over large distance. Our evaluation reveals a significant increase in spatial orientation while avoiding cybersickness and preserving presence, enjoyment, and competence. We summarize our findings in a set of comprehensive design guidelines to help developers integrate our technique.\",\n",
       " \"Collaboration is built on trust, and establishing trust with a creative Artificial Intelligence is difficult when the decision process or internal state driving its behaviour isn't exposed. When human musicians improvise together, a number of extra-musical cues are used to augment musical communication and expose mental or emotional states which affect musical decisions and the effectiveness of the collaboration. We developed a collaborative improvising AI drummer that communicates its confidence through an emoticon-based visualisation. The AI was trained on musical performance data, as well as real-time skin conductance, of musicians improvising with professional drummers, exposing both musical and extra-musical cues to inform its generative process. Uni- and bi-directional extra-musical communication with real and false values were tested by experienced improvising musicians. Each condition was evaluated using the FSS-2 questionnaire, as a proxy for musical engagement. The results show a positive correlation between extra-musical communication of machine internal state and human musical engagement.\",\n",
       " 'The way that design is being taught is continuously changing under the pressure of the transition from analogical to digital environments. This becomes even more important as the novelty and the alleged superiority of the digital world is used as a marketing tool by competing universities. Even though in some fields of application this approach is desirable, some particular aspects of teaching design and architecture make this transition debatable. The advantages of drawing on blackboards over drawing on whiteboard surfaces in regards of line aesthetic and expression possibilities were previously identified, along with the complementary necessary features for improvement. This study showcases a proof of concept in digitally augmenting a blackboard surface. The system allows the capturing, processing and making real time projections of images over the blackboard surface as trace references. Such a hybrid system, along with providing support for design and architecture related presentations and discussions could also mediate the contradictory relation towards technology that students and teachers have.',\n",
       " \"In a contemporary world, people become dependent on electronic devices. Technologies help to clarification and structure life in many ways to meet the need of the children oriented requirements. The children suffering from disabilities (e.g. autism) has desperate needs for elucidation and structures their life. MumIES is a research based system facilitates to support and manage their living. This paper works on MumIES system to evaluate usability of the system in extraordinary environment for extraordinary people. The paper shows from the survey observation users need supporting tools to access the children's potential and challenges and to give the full support to overcome disabilities. Usability evaluation has been considered one of the key challenges to MumIES system. The paper represents analysis, design of usability studies for the extraordinary user in environment.\",\n",
       " '\"Does placing workers together based on their personality give better performance results in cooperative crowdsourcing settings, compared to non-personality based crowd team formation?\" In this work we examine the impact of personality compatibility on the effectiveness of crowdsourced team work. Using a personality-based group dynamics approach, we examine two main types of personality combinations (matching and crashing) on two main types of tasks (collaborative and competitive). Our experimental results show that personality compatibility significantly affects the quality of the team\\'s final outcome, the quality of interactions and the emotions experienced by the team members. The present study is the first to examine the effect of personality over team result in crowdsourcing settings, and it has practical implications for the better design of crowdsourced team work.',\n",
       " \"The assumptions we make about a dialogue partner's knowledge and communicative ability (i.e. our partner models) can influence our language choices. Although similar processes may operate in human-machine dialogue, the role of design in shaping these models, and their subsequent effects on interaction are not clearly understood. Focusing on synthesis design, we conduct a referential communication experiment to identify the impact of accented speech on lexical choice. In particular, we focus on whether accented speech may encourage the use of lexical alternatives that are relevant to a partner's accent, and how this is may vary when in dialogue with a human or machine. We find that people are more likely to use American English terms when speaking with a US accented partner than an Irish accented partner in both human and machine conditions. This lends support to the proposal that synthesis design can influence partner perception of lexical knowledge, which in turn guide user's lexical choices. We discuss the findings with relation to the nature and dynamics of partner models in human machine dialogue.\",\n",
       " 'Around-device interaction promises to extend the input space of mobile and wearable devices beyond the common but restricted touchscreen. So far, most around-device interaction approaches rely on instrumenting the device or the environment with additional sensors. We believe, that the full potential of ordinary cameras, specifically user-facing cameras, which are integrated in most mobile devices today, are not used to their full potential, yet. We To this end, we present a novel approach for extending the input space around unmodified mobile devices using built-in front-facing cameras of unmodified handheld devices. Our approach estimates hand poses and gestures through reflections in sunglasses, ski goggles or visors. Thereby, GlassHands creates an enlarged input space, rivaling input reach on large touch displays. We discuss the idea, its limitations and future work.',\n",
       " 'With the fast development of network information technology, more and more people are immersed in the virtual community environment brought by the network, ignoring the social interaction in real life. The consequent urban autism problem has become more and more serious. Promoting offline communication between people \" and \"eliminating loneliness through emotional communication between pet robots and breeders\" to solve this problem, and has developed a design called \"Tom\". \"Tom\" is a smart pet robot with a pet robot-based social mechanism Called \"Tom-Talker\". The main contribution of this paper is to propose a social mechanism called \"Tom-Talker\" that encourages users to socialize offline. And \"Tom-Talker\" also has a corresponding reward mechanism and a friend recommendation algorithm. It also proposes a pet robot named \"Tom\" with an emotional interaction algorithm to recognize users\\' emotions, simulate animal emotions and communicate emotionally with use s. This paper designs experiments and analyzes the results. The results show that our pet robots have a good effect on solving urban autism problems.',\n",
       " 'The Q-method has been utilized over time in various areas, including information systems. In this study, we used a systematic mapping to illustrate how the Q-method was applied within Information Systems (IS) community and proposing towards the integration of Q-method into the Design Sciences Research (DSR) process as a tool for future research DSR-based IS studies. In this mapping study, we collected peer-reviewed journals from Basket-of-Eight journals and the digital library of the Association for Information Systems (AIS). Then we grouped the publications according to the process of DSR, and different variables for preparing Q-method from IS publications. We found that the potential of the Q-methodology can be used to support each main research stage of DSR processes and can serve as the useful tool to evaluate a system in the IS topic of system analysis and design',\n",
       " 'The creation and support of Embodied Conversational Agents (ECAs) has been quite challenging, as features required might not be straight-forward to implement and to integrate in a single application. Furthermore, ECAs as desktop applications present drawbacks for both developers and users; the former have to develop for each device and operating system and the latter must install additional software, limiting their widespread use. In this paper we demonstrate how recent advances in web technologies show promising steps towards capable web-based ECAs, through some off-the-shelf technologies, in particular, the Web Speech API, Web Audio API, WebGL and Web Workers. We describe their integration into a simple fully functional web-based 3D ECA accessible from any modern device, with special attention to our novel work in the creation and support of the embodiment aspects.',\n",
       " 'In traditional usability studies, researchers talk to users of tools to understand their needs and challenges. Insights gained via such interviews offer context, detail, and background. Due to costs in time and money, we are beginning to see a new form of tool interrogation that prioritizes scale, cost, and breadth by utilizing existing data from online forums. In this case study, we set out to apply this method of using online forum data to a specific issue---challenges that users face with Excel spreadsheets. Spreadsheets are a versatile and powerful processing tool if used properly. However, with versatility and power come errors, from both users and the software, which make using spreadsheets less effective. By scraping posts from the website Reddit, we collected a dataset of questions and complaints about Excel. Specifically, we explored and characterized the issues users were facing with spreadsheet software in general, and in particular, as resulting from a large amount of data in their spreadsheets. We discuss the implications of our findings on the design of next-generation spreadsheet software.',\n",
       " 'Knowledge workers, such as scientists, journalists, or consultants, adaptively seek, gather, and consume information. These processes are often inefficient as existing user interfaces provide limited possibilities to combine information from various sources and different formats into a common knowledge representation. In this paper, we present the concept of an information collage (IC) -- a web browser extension combining manual spatial organization of gathered information fragments and automatic text analysis for interactive content exploration and expressive visual summaries. We used IC for case studies with knowledge workers from different domains and longer-term field studies over a period of one month. We identified three different ways how users collect and structure information and provide design recommendations how to support these observed usage strategies.',\n",
       " 'This work compares user collaboration with conversational personal assistants vs. teams of expert chatbots. Two studies were performed to investigate whether each approach affects accomplishment of tasks and collaboration costs. Participants interacted with two equivalent financial advice chatbot systems, one composed of a single conversational adviser and the other based on a team of four experts chatbots. Results indicated that users had different forms of experiences but were equally able to achieve their goals. Contrary to the expected, there were evidences that in the teamwork situation that users were more able to predict agent behavior better and did not have an overhead to maintain common ground, indicating similar collaboration costs. The results point towards the feasibility of either of the two approaches for user collaboration with conversational agents.',\n",
       " 'The automated detection of corrosion from images (i.e., photographs) or video (i.e., drone footage) presents significant advantages in terms of corrosion monitoring. Such advantages include access to remote locations, mitigation of risk to inspectors, cost savings and monitoring speed. The automated detection of corrosion requires deep learning to approach human level artificial intelligence (A.I.). The training of a deep learning model requires intensive image labelling, and in order to generate a large database of labelled images, crowd sourced labelling via a dedicated website was sought. The website (corrosiondetector.com) permits any user to label images, with such labelling then contributing to the training of a cloud based A.I. model - with such a cloud-based model then capable of assessing any fresh (or uploaded) image for the presence of corrosion. In other words, the website includes both the crowd sourced training process, but also the end use of the evolving model. Herein, the results and findings from the website (corrosiondetector.com) over the period of approximately one month, are reported.',\n",
       " 'Even though speech-emotion recognition (SER) has been receiving much attention as research topic, there are still some disputes about which vocal features can identify certain emotion. Emotion expression is also known to be differed according to the cultural backgrounds that make it important to study SER specific to the culture where the language belongs to. Furthermore, only a few studies addresses the SER in Indonesian which what this study attempts to explore. In this study, we extract simple features from 3420 voice data gathered from 38 participants. The features are compared by means of linear mixed effect model which shows that people who are in emotional and non-emotional state can be differentiated by their speech duration. Using SVM and speech duration as input feature, we achieve 76.84% average accuracy in classifying emotional and non-emotional speech.',\n",
       " 'Low-quality results have been a long-standing problem on microtask crowdsourcing platforms, driving away requesters and justifying low wages for workers. To date, workers have been blamed for low-quality results: they are said to make as little effort as possible, do not pay attention to detail, and lack expertise. In this paper, we hypothesize that requesters may also be responsible for low-quality work: they launch unclear task designs that confuse even earnest workers, under-specify edge cases, and neglect to include examples. We introduce prototype tasks, a crowdsourcing strategy requiring all new task designs to launch a small number of sample tasks. Workers attempt these tasks and leave feedback, enabling the re- quester to iterate on the design before publishing it. We report a field experiment in which tasks that underwent prototype task iteration produced higher-quality work results than the original task designs. With this research, we suggest that a simple and rapid iteration cycle can improve crowd work, and we provide empirical evidence that requester \"quality\" directly impacts result quality.',\n",
       " \"Like sighted people, visually impaired people want to share photographs on social networking services, but find it difficult to identify and select photos from their albums. We aimed to address this problem by incorporating state-of-the-art computer-generated descriptions into Facebook's photo-sharing feature. We interviewed 12 visually impaired participants to understand their photo-sharing experiences and designed a photo description feature for the Facebook mobile application. We evaluated this feature with six participants in a seven-day diary study. We found that participants used the descriptions to recall and organize their photos, but they hesitated to upload photos without a sighted person's input. In addition to basic information about photo content, participants wanted to know more details about salient objects and people, and whether the photos reflected their personal aesthetics. We discuss these findings from the lens of self-disclosure and self-presentation theories and propose new computer vision research directions that will better support visual content sharing by visually impaired people.\",\n",
       " 'In building on theories of Computer-Mediated Communication (CMC), Human-Robot Interaction, and Media Psychology (i.e. Theory of Affective Bonding), the current paper proposes an explanation of how over time, people experience the mediated or simulated aspects of the interaction with a social robot. In two simultaneously running loops, a more reflective process is balanced with a more affective process. If human interference is detected behind the machine, Robot-Mediated Communication commences, which basically follows CMC assumptions; if human interference remains undetected, Human-Robot Communication comes into play, holding the robot for an autonomous social actor. The more emotionally aroused a robot user is, the more likely they develop an affective relationship with what actually is a machine. The main contribution of this paper is an integration of Computer-Mediated Communication, Human-Robot Communication, and Media Psychology, outlining a full-blown theory of robot communication connected to friendship formation, accounting for communicative features, modes of processing, as well as psychophysiology.',\n",
       " 'We consider a smart home or smart office environment with a number of IoT devices connected and passing data between one another. The footprints of the data transferred can provide valuable information about the devices, which can be used to (a) identify the IoT devices and (b) in case of failure, to identify the correct replacements for these devices. In this paper, we generate the embeddings for IoT devices in a smart home using Word2Vec, and explore the possibility of having a similar concept for IoT devices, aka IoT2Vec. These embeddings can be used in a number of ways, such as to find similar devices in an IoT device store, or as a signature of each type of IoT device. We show results of a feasibility study on the CASAS dataset of IoT device activity logs, using our method to identify the patterns in embeddings of various types of IoT devices in a household.',\n",
       " 'This paper describes the development of a real-time Human-Robot Interaction (HRI) system for a service robot based on 3D human activity recognition and human-like decision mechanism. The Human-Robot Interactive (HRI) system, which allows one person to interact with a service robot using natural body language, collects sequences of 3D skeleton joints comprising rich human movement information about the user via Microsoft Kinect. This information is used to train a three-layer Long-Short-Term Memory (LSTM) network for human action recognition. The robot understands user intent based on an online LSTM network test, and responds to the user via movements of the robotic arm or chassis. Furthermore, the human-like decision mechanism is also fused into this process, which allows the robot to instinctively decide whether to interrupt the current task according to task priority. The framework of the overall system is established on the Robot Operating System (ROS) platform. The real-life activity interaction between our service robot and the user was conducted to demonstrate the effectiveness of developed HRI system.',\n",
       " 'The identification of intentionally delivered commands is a challenge in Brain Computer Interfaces (BCIs) based on Sensory-Motor Rhythms (SMR). It is of fundamental importance that BCI systems controlling a robotic device (i.e., upper limb prosthesis) are capable of detecting if the user is in the so called Intentional Non-Control (INC) state (i.e., holding the prosthesis in a given position). In this work, we propose a novel approach based on the entropy of the Electroencephalogram (EEG) signals to provide a continuous identification of motion intention. Results from ten healthy subjects suggest that the proposed system can be used for reliably predicting motion in real-time at a framerate of 8 Hz with $80\\\\% \\\\pm 5\\\\%$ of accuracy. Moreover, motion intention can be detected more than 1 second before muscular activation with an average accuracy of $76\\\\% \\\\pm 11\\\\%$.',\n",
       " 'Sleep deprivation is a public health issue. A lack of sleep not only harms our body immune systems but also degrades their capacity to maintain cognitive skills. Awareness of sleep deprivation has not been widely investigated in work-based wellness programmes. In the study, the project carried out with nine participants from a local manufacture company to raise that awareness. The common causes of sleep deprivation have been identified the through the deployment of probes and the interviews. The research generated design concepts of smart IoT workplace to track and share daytime sleep-related activities. Through the bottom up and co-design methods, participants give the points of considering the use of sleep data from different power relationship perspective, includes the unexpected use of sleep for fatigue risk management and evaluation of employee performance.',\n",
       " 'Human behavior recognition has been considered as a core technology that can facilitate variety of applications. However, accurate detection and recognition of human behavior is still a big challenge that attracts a lot of research efforts. Recent advances in the wireless technology (e.g., Wi-Fi Channel State Information, i.e., CSI) enable a new behavior recognition paradigm, which is able to recognize behaviors in a device-free and non-intrusive manner. In this article, we first provide an overview of the basics of Wi-Fi CSI based behavior recognition. Afterwards, we classify related applications into three-granularity: signals, actions and activities, and then provide some insights for designing new schemes. Finally, we conclude by discussing the challenges, possible solutions to these challenges and some open issues involved in CSI based behavior recognition.',\n",
       " 'The Python--elsA user interface of the elsA cfd (Computational Fluid Dynamics) software has been developed to allow users to specify simulations with confidence, through a global context of description objects grouped inside scripts. The software main features are generated documentation, context checking and completion, and helpful error management. Further developments have used this foundation as a coupling framework, allowing (thanks to the descriptive approach) the coupling of external algorithms with the cfd solver in a simple and abstract way, leading to more success in complex simulations. Along with the description of the technical part of the interface, we try to gather the salient points pertaining to the psychological viewpoint of user experience (ux). We point out the differences between user interfaces and pure data management systems such as cgns.',\n",
       " 'Graph exploration and editing are still mostly considered independently and systems to work with are not designed for todays interactive surfaces like smartphones, tablets or tabletops. When developing a system for those modern devices that supports both graph exploration and graph editing, it is necessary to 1) identify what basic tasks need to be supported, 2) what interactions can be used, and 3) how to map these tasks and interactions. This technical report provides a list of basic interaction tasks for graph exploration and editing as a result of an extensive system review. Moreover, different interaction modalities of interactive surfaces are reviewed according to their interaction vocabulary and further degrees of freedom that can be used to make interactions distinguishable are discussed. Beyond the scope of graph exploration and editing, we provide an approach for finding and evaluating a mapping from tasks to interactions, that is generally applicable. Thus, this work acts as a guideline for developing a system for graph exploration and editing that is specifically designed for interactive surfaces.',\n",
       " \"Design for Voice User Interfaces (VUIs) has become more relevant in recent years due to the enormous advances of speech technologies and their growing presence in our everyday lives. Although modern VUIs still present interaction issues, reports indicate they are being adopted by people with different disabilities and having a positive impact. For the first author's PhD research project, an ethnographic study is currently being carried out in a local charity that provides support and services to people with visual impairments. The purpose is to understand people's competencies and practices, and how these are, or could be, related to voice technologies (assistive technology and mainstream VUIs). Through direct observation and contextual interviews, we aim to investigate the problems and solutions they encounter and the ways they cope with particular situations.\",\n",
       " \"The exponential growth of popularity of multimedia has led to needs for user-centric adaptive applications that manage multimedia content more effectively. Implicit analysis, which examines users' perceptual experience of multimedia by monitoring physiological or behavioral cues, has potential to satisfy such demands. Particularly, physiological signals categorized into cerebral physiological signals (electroencephalography, functional magnetic resonance imaging, and functional near-infrared spectroscopy) and peripheral physiological signals (heart rate, respiration, skin temperature, etc.) have recently received attention along with notable development of wearable physiological sensors. In this paper, we review existing studies on physiological signal analysis exploring perceptual experience of multimedia. Furthermore, we discuss current trends and challenges.\",\n",
       " 'Human-machine systems required a deep understanding of human behaviors. Most existing research on action recognition has focused on discriminating between different actions, however, the quality of executing an action has received little attention thus far. In this paper, we study the quality assessment of driving behaviors and present WiQ, a system to assess the quality of actions based on radio signals. This system includes three key components, a deep neural network based learning engine to extract the quality information from the changes of signal strength, a gradient based method to detect the signal boundary for an individual action, and an activitybased fusion policy to improve the recognition performance in a noisy environment. By using the quality information, WiQ can differentiate a triple body status with an accuracy of 97%, while for identification among 15 drivers, the average accuracy is 88%. Our results show that, via dedicated analysis of radio signals, a fine-grained action characterization can be achieved, which can facilitate a large variety of applications, such as smart driving assistants.',\n",
       " \"Bias is a common problem in today's media, appearing frequently in text and in visual imagery. Users on social media websites such as Twitter need better methods for identifying bias. Additionally, activists --those who are motivated to effect change related to some topic, need better methods to identify and counteract bias that is contrary to their mission. With both of these use cases in mind, in this paper we propose a novel tool called UnbiasedCrowd that supports identification of, and action on bias in visual news media. In particular, it addresses the following key challenges (1) identification of bias; (2) aggregation and presentation of evidence to users; (3) enabling activists to inform the public of bias and take action by engaging people in conversation with bots. We describe a preliminary study on the Twitter platform that explores the impressions that activists had of our tool, and how people reacted and engaged with online bots that exposed visual bias. We conclude by discussing design and implication of our findings for creating future systems to identify and counteract the effects of news bias.\",\n",
       " 'In mixed reality, real objects can be used to interact with virtual objects. However, unlike in the real world, real objects do not encounter any opposite reaction force when pushing against virtual objects. The lack of reaction force during manipulation prevents users from perceiving the mass of virtual objects. Although this could be addressed by equipping real objects with force-feedback devices, such a solution remains complex and impractical.In this work, we present a technique to produce an illusion of mass without any active force-feedback mechanism. This is achieved by simulating the effects of this reaction force in a purely visual way. A first study demonstrates that our technique indeed allows users to differentiate light virtual objects from heavy virtual objects. In addition, it shows that the illusion is immediately effective, with no prior training. In a second study, we measure the lowest mass difference (JND) that can be perceived with this technique. The effectiveness and ease of implementation of our solution provides an opportunity to enhance mixed reality interaction at no additional cost.',\n",
       " \"The paper describes a novel social network-based open educational resource for learning foreign languages in real time from native speakers, based on the predefined teaching materials. This virtual learning platform, named i2istudy, eliminates misunderstanding by providing prepared and predefined scenarios, enabling the participants to understand each other and, as a consequence, to communicate freely. The system allows communication through the real time video and audio feed. In addition to establishing the communication, it tracks the student progress and allows rating the instructor, based on the learner's experience. The system went live in April 2014, and had over six thousand active daily users, with over 40,000 total registered users. Currently monetization is being added to the system, and time will show how popular the system will become in the future.\",\n",
       " \"This study investigates the use of accelerometer data from a smart watch to infer an individual's emotional state. We present our preliminary findings on a user study with 50 participants. Participants were primed either with audio-visual (movie clips) or audio (classical music) to elicit emotional responses. Participants then walked while wearing a smart watch on one wrist and a heart rate strap on their chest. Our hypothesis is that the accelerometer signal will exhibit different patterns for participants in response to different emotion priming. We divided the accelerometer data using sliding windows, extracted features from each window, and used the features to train supervised machine learning algorithms to infer an individual's emotion from their walking pattern. Our discussion includes a description of the methodology, data collected, and early results.\",\n",
       " 'The paper describes creation and development of the educational online communication platform for teaching and learning foreign languages. The system is based on the time bank principle, allowing users to teach others their native tongue along with taking foreign language lessons. The system is based on the WebRTC technology, allowing users to access synchronized teaching materials along with seeing and hearing each other. The platform is free for the users with implemented gamification mechanics to motivate them. It is based on the freemium model, where the main functions are provided free of charged with some premium features. The paper describes studies associated with user involvement in the learning/teaching process. The hypothesis whether two previously unfamiliar individuals could communicate with each other using a foreign language, based on the developed system algorithms, was tested. System virality, where new users are attracted by the existing users was also studied, along with user motivation for viral behavior. Relationships between monetization, virality and user involvement were also considered.',\n",
       " 'Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.',\n",
       " 'Driver assistance systems, also called automated driving systems, allow drivers to immerse themselves in non-driving-related tasks. Unfortunately, drivers may not trust the automated driving system, which prevents either handing over the driving task or fully focusing on the secondary task. We assert that enhancing situational awareness can increase trust in automation. Situational awareness should increase trust and lead to better secondary task performance. This study manipulated situational awareness by providing them with different types of information: the control condition provided no information to the driver, the low condition provided a status update, while the high condition provided a status update and a suggested course of action. Data collected included measures of trust, trusting behavior, and task performance through surveys, eye-tracking, and heart rate data. Results show that situational awareness both promoted and moderated the impact of trust in the automated vehicle, leading to better secondary task performance. This result was evident in measures of self-reported trust and trusting behavior.',\n",
       " 'The domestic environment is a key area for the design and deployment of autonomous systems. Yet research indicates their adoption is already being hampered by a variety of critical issues including trust, privacy and security. This paper explores how potential users relate to the concept of autonomous systems in the home and elaborates further points of friction. It makes two contributions. One methodological, focusing on the use of provocative utopian and dystopian scenarios of future autonomous systems in the home. These are used to drive an innovative workshop-based approach to breaching experiments, which surfaces the usually tacit and unspoken background expectancies implicated in the organisation of everyday life that have a powerful impact on the acceptability of future and emerging technologies. The other contribution is substantive, produced through participants efforts to repair the incongruity or \"reality disjuncture\" created by utopian and dystopian visions, and highlights the need to build social as well as computational accountability into autonomous systems, and to enable coordination and control.',\n",
       " 'Providing reinforcement learning agents with informationally rich human knowledge can dramatically improve various aspects of learning. Prior work has developed different kinds of shaping methods that enable agents to learn efficiently in complex environments. All these methods, however, tailor human guidance to agents in specialized shaping procedures, thus embodying various characteristics and advantages in different domains. In this paper, we investigate the interplay between different shaping methods for more robust learning performance. We propose an adaptive shaping algorithm which is capable of learning the most suitable shaping method in an on-line manner. Results in two classic domains verify its effectiveness from both simulated and real human studies, shedding some light on the role and impact of human factors in human-robot collaborative learning.',\n",
       " 'Ubiquitous technology platforms have been created to track and improve health and fitness; similar technologies can help individuals monitor and reduce their carbon footprints. This paper proposes CarbonKit, a platform combining technology, markets, and incentives to empower and reward people for reducing their carbon footprint. We argue that a goal-and-reward behavioral feedback loop can be combined with the Big Data available from tracked activities, apps, and social media to make CarbonKit an integral part of individuals daily lives. CarbonKit comprises five modules that link personal carbon tracking, health and fitness, social media, and economic incentives. Protocols for safeguarding security, privacy and individuals control over their own data are essential to the design of the CarbonKit. Initially CarbonKit would operate on a voluntary basis, but such a system can also serve as part of a mandatory region-wide initiative. We use the example of the British Columbia to illustrate the regulatory framework and participating stakeholders that would be required to support the CarbonKit in specific jurisdictions.',\n",
       " 'Smart phone and tablet usage has sharply increased for the last decade. While entering test on these devices, virtual keyboards are generally used instead of conventional hardware keyboards. In this study, a new problem which is two-finger keyboard layout problem and solution approach is presented for increasing user test entrance performance, especially on virtual keyboards. Defined two-finger keyboard layout problem is modeled as Quadratic Assignment Problem. Because of combinatorial structure of the problem a genetic algorithm is developed. Its result is given to mathematical model as initial solution for finding better solutions with mathematical model. Proposed approach is applied on Turkish language. The new two finger keyboard layout for Turkish language is compared with F and QWERTY keyboard layouts based on certain performance measurement techniques.',\n",
       " 'We describe a new visualization tool, dubbed HCMapper, that visually helps to compare a pair of dendrograms computed on the same dataset by displaying multiscale partition-based layered structures. The dendrograms are obtained by hierarchical clustering techniques whose output reflects some hypothesis on the data and HCMapper is specifically designed to grasp at first glance both whether the two compared hypotheses broadly agree and the data points on which they do not concur. Leveraging juxtaposition and explicit encodings, HCMapper focus on two selected partitions while displaying coarser ones in context areas for understanding multiscale structure and eventually switching the selected partitions. HCMapper utility is shown through the example of testing whether the prices of credit default swap financial time series only undergo correlation. This use case is detailed in the supplementary material as well as experiments with code on toy-datasets for reproducible research. HCMapper is currently released as a visualization tool on the DataGrapple time series and clustering analysis platorm at www.datagrapple.com.',\n",
       " 'Accurate hand pose estimation at joint level has several uses on human-robot interaction, user interfacing and virtual reality applications. Yet, it currently is not a solved problem. The novel deep learning techniques could make a great improvement on this matter but they need a huge amount of annotated data. The hand pose datasets released so far present some issues that make them impossible to use on deep learning methods such as the few number of samples, high-level abstraction annotations or samples consisting in depth maps. In this work, we introduce a multiview hand pose dataset in which we provide color images of hands and different kind of annotations for each, i.e the bounding box and the 2D and 3D location on the joints in the hand. Besides, we introduce a simple yet accurate deep learning architecture for real-time robust 2D hand pose estimation.',\n",
       " 'In presence of multiple clustering solutions for the same dataset, a clustering ensemble approach aims to yield a single clustering of the dataset by achieving a consensus among the input clustering solutions. The goal of this consensus is to improve the quality of clustering. It has been seen that there are some image clustering tasks that cannot be easily solved by computer. But if these images can be outsourced to the general people (crowd workers) to group them based on some similar features, and opinions are collected from them, then this task can be managed in an efficient manner and time effective way. In this work, the power of crowd has been used to annotate the images so that multiple clustering solutions can be obtained from them and thereafter a Markov chain based ensemble method is introduced to make a consensus of multiple clustering solutions.',\n",
       " 'Taking a picture has been traditionally a one-persons task. In this paper we present a novel system that allows multiple mobile devices to work collaboratively in a synchronized fashion to capture a panorama of a highly dynamic scene, creating an entirely new photography experience that encourages social interactions and teamwork. Our system contains two components: a client app that runs on all participating devices, and a server program that monitors and communicates with each device. In a capturing session, the server collects in realtime the viewfinder images of all devices and stitches them on-the-fly to create a panorama preview, which is then streamed to all devices as visual guidance. The system also allows one camera to be the host and to send direct visual instructions to others to guide camera adjustment. When ready, all devices take pictures at the same time for panorama stitching. Our preliminary study suggests that the proposed system can help users capture high quality panoramas with an enjoyable teamwork experience.\\n  A demo video of the system in action is provided at http://youtu.be/PwQ6k_ZEQSs.',\n",
       " \"Student learning activity in MOOCs can be viewed from multiple perspectives. We present a new organization of MOOC learner activity data at a resolution that is in between the fine granularity of the clickstream and coarse organizations that count activities, aggregate students or use long duration time units. A detailed access trajectory (DAT) consists of binary values and is two dimensional with one axis that is a time series, e.g. days and the other that is a chronologically ordered list of a MOOC component type's instances, e.g. videos in instructional order. Most popular MOOC platforms generate data that can be organized as detailed access trajectories (DATs).We explore the value of DATs by conducting four empirical mini-studies. Our studies suggest DATs contain rich information about students' learning behaviors and facilitate MOOC learning analyses.\",\n",
       " \"In this demo paper, we introduce LogCanvas, a platform for user search history visualisation. Different from the existing visualisation tools, LogCanvas focuses on helping users re-construct the semantic relationship among their search activities. LogCanvas segments a user's search history into different sessions and generates a knowledge graph to represent the information exploration process in each session. A knowledge graph is composed of the most important concepts or entities discovered by each search query as well as their relationships. It thus captures the semantic relationship among the queries. LogCanvas offers a session timeline viewer and a snippets viewer to enable users to re-find their previous search results efficiently. LogCanvas also provides a collaborative perspective to support a group of users in sharing search results and experience.\",\n",
       " 'The monthly Bureau of Labor Statistics Employment Situation Report is widely anticipated by economists, journalists, and politicians as it is used to forecast the economic condition of the United States. The report has broad impact on public and corporate economic confidence; however, the online access to this data employs outdated techniques, using a PDF format containing solely text and fixed tabular information. Creating an interactive interface for dynamic discovery on the BLS website could elicit more dialogue between the public and government spheres, drawing more traffic to government websites and triggering greater civic engagement. Our work suggests that the implementation of interactive visual analysis techniques to enable dynamic discovery leads to rapid interpretation of data as well as provides the means to explore the data for further insights. This paper presents two inspirational prototypes: a dashboard of interactive visualizations and an interactive time series explorer, allowing for temporal and spatial analyses and enabling users to combine data sets to create their own customized visualization.',\n",
       " 'Studies have shown that children can be exposed to smart devices at a very early age. This has important implications on research in children-computer interaction, children online safety and early education. Many systems have been built based on such research. In this work, we present multiple techniques to automatically detect the presence of a child on a smart device, which could be used as the first step on such systems. Our methods distinguish children from adults based on behavioral differences while operating a touch-enabled modern computing device. Behavioral differences are extracted from data recorded by the touchscreen and built-in sensors. To evaluate the effectiveness of the proposed methods, a new data set has been created from 50 children and adults who interacted with off-the-shelf applications on smart phones. Results show that it is possible to achieve 99% accuracy and less than 0.5% error rate after 8 consecutive touch gestures using only touch information or 5 seconds of sensor reading. If information is used from multiple sensors, then only after 3 gestures, similar performance could be achieved.',\n",
       " 'Buses are the primary means of public transportation in the city of Rio de Janeiro, carrying around 100 million passengers every month. Recently, real-time GPS coordinates of all operating public buses has been made publicly available - roughly 1 million GPS entries each captured each day. In an initial study, we observed that a substantial number of buses follow trajectories that do not follow the expected behavior. In this paper, we present RioBusData, a tool that helps users identify and explore, through different visualizations, the behavior of outlier trajectories. We describe how the system automatically detects these outliers using a Convolutional Neural Network (CNN) and we also discuss a series of case studies which show how RioBusData helps users better understand not only the flow and service of outlier buses but also the bus system as a whole.',\n",
       " 'In this position paper, we frame the field of Visual Musicology by providing an overview of well-established musicological sub-domains and their corresponding analytic and visualization tasks. To foster collaborative, interdisciplinary research, we discuss relevant data and domain characteristics. We give a description of the problem space, as well as the design space of musicology and discuss how existing problem-design mappings or solutions from other fields can be transferred to musicology. We argue that, through methodology transfer, established methods can be exploited to solve current musicological problems and show exemplary mappings from analytics fields related to text, geospatial, time-series, and other high-dimensional data to musicology. Finally, we point out open challenges, discuss research gaps, and highlight future research opportunities.',\n",
       " \"This paper draws from literature and our experience of conducting Wizard-of-Oz (WoZ) studies using natural language, conversational user interfaces (CUIs) in the automotive domain. These studies have revealed positive effects of using in-vehicle CUIs on issues such as: cognitive demand/workload, passive task-related fatigue, trust, acceptance and environment engagement. A nascent set of human-centred design guidelines that have emerged is presented. These are based on the analysis of users' behaviour and the positive benefits observed, and aim to make interactions with an in-vehicle agent interlocutor safe, effective, engaging and enjoyable, while confirming with users' expectations. The guidelines can be used to inform the design of future in-vehicle CUIs or applied experimentally using WoZ methodology, and will be evaluated and refined in ongoing work.\",\n",
       " \"We report from the Do Not Disturb Challenge where 30 volunteers disabled notification alerts for 24 hours across all devices. The effect of the absence of notifications on the participants was isolated through an experimental study design: we compared self-reported feedback from the day without notifications against a baseline day. The evidence indicates that notifications have locked us in a dilemma: without notifications, participants felt less distracted and more productive. But, they also felt no longer able to be as responsive as expected, which made some participants anxious. And, they felt less connected with one's social group. In contrast to previous reports, about two third of the participants expressed the intention to change how they manage notifications. Two years later, half of the participants are still following through with their plans.\",\n",
       " 'This paper positions and explores the topic of image-based personality test. Instead of responding to text-based questions, the subjects will be provided a set of \"choose-your-favorite-image\" visual questions. With the image options of each question belonging to the same concept, the subjects\\' personality traits are estimated by observing their preferences of images under several unique concepts. The solution to design such an image-based personality test consists of concept-question identification and image-option selection. We have presented a preliminary framework to regularize these two steps in this exploratory study. A demo version of the designed image-based personality test is available at http://www.visualbfi.org/. Subjective as well as objective evaluations have demonstrated the feasibility of image-based personality test in limited questions.',\n",
       " 'Cursor tracking data contains information about website visitors which may provide new ways to understand visitors and their needs. This paper presents an Amazon Mechanical Turk study where participants were tracked as they used modified variants of the Wikipedia and BBC News websites. Participants were asked to complete reading and information-finding tasks. The results showed that it was possible to differentiate between users reading content and users looking for information based on cursor data. The effects of website aesthetics, user interest and cursor hardware were also analysed which showed it was possible to identify hardware from cursor data, but no relationship between cursor data and engagement was found. The implications of these results, from the impact on web analytics to the design of experiments to assess user engagement, are discussed.',\n",
       " \"Modern security operations centers (SOCs) employ a variety of tools for intrusion detection, prevention, and widespread log aggregation and analysis. While research efforts are quickly proposing novel algorithms and technologies for cyber security, access to actual security personnel, their data, and their problems are necessarily limited by security concerns and time constraints. To help bridge the gap between researchers and security centers, this paper reports results of semi-structured interviews of 13 professionals from five different SOCs including at least one large academic, research, and government organization. The interviews focused on the current practices and future desires of SOC operators about host-based data collection capabilities, what is learned from the data, what tools are used, and how tools are evaluated. Questions and the responses are organized and reported by topic. Then broader themes are discussed. Forest-level takeaways from the interviews center on problems stemming from size of data, correlation of heterogeneous but related data sources, signal-to-noise ratio of data, and analysts' time.\",\n",
       " 'In research and practice into the accessibility of digital games, much of the work has focused on how to make games accessible to people with disa- bilities. With an increasing number of people with disabilities playing main- stream commercial games, it is important that we understand who they are and how they play in order to take a more user-centered approach as this field grows. We conducted a demographic survey of 230 players with disabilities and found that they play mainstream digital games using a variety of assistive tech- nologies, use accessibility options such as key remapping and subtitles, and they identify themselves as gamers who play digital games as their primary hobby. This gives us a richer picture of players with disabilities and indicates that there are opportunities to begin to look at accessible player experiences (APX) in games.',\n",
       " 'We investigate grasping of rigid objects in unilateral robot-assisted minimally invasive surgery (RAMIS) in this paper. We define a human-centered transparency that quantifies natural action and perception in RAMIS. We demonstrate this human-centered transparency analysis for different values of gripper scaling - the scaling between the grasp aperture of the surgeon-side manipulator and the aperture of the surgical instrument grasper. Thirty-one participants performed teleoperated grasping and perceptual assessment of rigid objects in one of three gripper scaling conditions (fine, normal, and quick, trading off precision and responsiveness). Psychophysical analysis of the variability of maximal grasping aperture during prehension and of the reported size of the object revealed that in normal and quick (but not in the fine) gripper scaling conditions, teleoperated grasping with our system was similar to natural grasping, and therefore, human-centered transparent. We anticipate that using motor control and psychophysics for human-centered optimizing of teleoperation control will eventually improve the usability of RAMIS.',\n",
       " \"One of the main benefits of a wrist-worn computer is its ability to collect a variety of physiological data in a minimally intrusive manner. Among these data, electrodermal activity (EDA) is readily collected and provides a window into a person's emotional and sympathetic responses. EDA data collected using a wearable wristband are easily influenced by motion artifacts (MAs) that may significantly distort the data and degrade the quality of analyses performed on the data if not identified and removed. Prior work has demonstrated that MAs can be successfully detected using supervised machine learning algorithms on a small data set collected in a lab setting. In this paper, we demonstrate that unsupervised learning algorithms perform competitively with supervised algorithms for detecting MAs on EDA data collected in both a lab-based setting and a real-world setting comprising about 23 hours of data. We also find, somewhat surprisingly, that incorporating accelerometer data as well as EDA improves detection accuracy only slightly for supervised algorithms and significantly degrades the accuracy of unsupervised algorithms.\",\n",
       " 'We consider worker skill estimation for the single-coin Dawid-Skene crowdsourcing model. In practice, skill-estimation is challenging because worker assignments are sparse and irregular due to the arbitrary and uncontrolled availability of workers. We formulate skill estimation as a rank-one correlation-matrix completion problem, where the observed components correspond to observed label correlations between workers. We show that the correlation matrix can be successfully recovered and skills are identifiable if and only if the sampling matrix (observed components) does not have a bipartite connected component. We then propose a projected gradient descent scheme and show that skill estimates converge to the desired global optima for such sampling matrices. Our proof is original and the results are surprising in light of the fact that even the weighted rank-one matrix factorization problem is NP-hard in general. Next, we derive sample complexity bounds in terms of spectral properties of the signless Laplacian of the sampling matrix. Our proposed scheme achieves state-of-art performance on a number of real-world datasets.',\n",
       " \"The SmartAbility Android Application recommends Assistive Technology (AT) for people with reduced physical ability, by focusing on the actions (abilities) that can be performed independently. The Application utilises built-in sensor technologies in Android devices to detect user abilities, including head and limb movements, speech and blowing. The Application was evaluated by 18 participants with varying physical conditions and assessed through the System Usability Scale (SUS) and NASA Task Load Index (TLX). The Application achieved a SUS score of 72.5 (indicating 'Good Usability') with low levels of Temporal Demand and Frustration and medium levels of Mental Demand, Physical Demand and Effort. It is anticipated that the SmartAbility Application will be disseminated to the AT domain, to improve quality of life for people with reduced physical ability.\",\n",
       " 'Extensible 3D (X3D) modeling language is one of the leading Web3D technologies. Despite the rich functionality, the language does not currently provide tools for rapid development of conventional graphical user interfaces (GUIs). Every X3D author is responsible for building from primitives a purpose specific set of required interface components, often for a single use. We address the challenge of creating consistent, efficient, interactive, and visually appealing GUIs by proposing the X3D User Interface (X3DUI) library. This library includes a wide range of cross-compatible X3D widgets, equipped with configurable appearance and behavior. With this library, we attempt to standardize the GUI construction across various X3D-driven projects, and improve the usability, compatibility, adaptability, readability, and flexibility of many existing applications.',\n",
       " \"The adoption of intelligent systems creates opportunities as well as challenges for medical work. On the positive side, intelligent systems have the potential to compute complex data from patients and generate automated diagnosis recommendations for doctors. However, medical professionals often perceive such systems as black boxes and, therefore, feel concerned about relying on system generated results to make decisions. In this paper, we contribute to the ongoing discussion of explainable artificial intelligence (XAI) by exploring the concept of explanation from a human-centered perspective. We hypothesize that medical professionals would perceive a system as explainable if the system was designed to think and act like doctors. We report a preliminary interview study that collected six medical professionals' reflection of how they interact with data for diagnosis and treatment purposes. Our data reveals when and how doctors prioritize among various types of data as a central part of their diagnosis process. Based on these findings, we outline future directions regarding the design of XAI systems in the medical context.\",\n",
       " \"With a number of cheap commercial dry EEG kits available today, it is possible to look at user attention driven scenarios for interaction with the web browser. Using EEG to determine the user's attention level is preferable to using methods such as gaze tracking or time spent on the webpage. In this paper we use the attention level in three different ways. First, as a control mechanism, to control user interface elements such as menus or buttons. Second, to make the web browser responsive to the current attention level. Third, as a means for the web developer to control the user experience based on the level of attention paid by the user, thus creating attention sensitive websites. We present implementation details for each of these, using the NeuroSky MindWave sensor. We also explore issues in the system, and possibility of an EEG based web standard.\",\n",
       " \"This paper brings into discussion some of the most relevant technological challenges involving haptic systems in medical education. One of these challenges is choosing the suitable haptic hardware, API or framework for developing a visuo-haptic e-Learning system. The decision is based on several criteria such as the multimodal resources needed by the software system, compatibility with haptic devices and the dynamic configuration of the scene. Another challenge is related to the software system reactivity in conjunction with the user's actions. The immediate haptic feedback from the virtual models, together with the synchronization of the rendered haptic and visual cues seen by the users are essential for enhancing the user's learning ability. Visuo-haptic simulation facilitates accurate training scenarios of medical protocols and surgical processes.\",\n",
       " \"The sizes of compressed images depend on their spatial resolution (number of pixels) and on their color resolution (number of color quantization levels). We introduce DaltonQuant, a new color quantization technique for image compression that cloud services can apply to images destined for a specific user with known color vision deficiencies. DaltonQuant improves compression in a user-specific but reversible manner thereby improving a user's network bandwidth and data storage efficiency. DaltonQuant quantizes image data to account for user-specific color perception anomalies, using a new method for incremental color quantization based on a large corpus of color vision acuity data obtained from a popular mobile game. Servers that host images can revert DaltonQuant's image requantization and compression when those images must be transmitted to a different user, making the technique practical to deploy on a large scale. We evaluate DaltonQuant's compression performance on the Kodak PC reference image set and show that it improves compression by an additional 22%-29% over the state-of-the-art compressors TinyPNG and pngquant.\",\n",
       " 'StreamBED is an embodied VR training for citizen scientists to make qualitative stream assessments. Early findings garnered positive feedback about training qualitative assessment using a virtual representation of different stream spaces, but presented field-specific challenges; novice biologists had trouble interpreting qualitative protocols, and needed substantive guidance to look for and interpret environment cues. In order to address these issues in the redesign, this work uses research through design (RTD) methods to consider feedback from expert stream biologists, firsthand stream monitoring experience, discussions with education and game designers, and feedback from a low fidelity prototype. The qualitative findings found that training should facilitate personal narratives, maximize realism, and should use social dynamics to scaffold learning.',\n",
       " 'Technology has become an essential part in every aspect of our lives. However the key to a successful implementation of a technology depends on the acceptance by the general public. In order to increase the acceptance various approaches can be applied. In this paper, we will examine the human-robot emotional interaction by investigating the capabilities of a developed low-resolution RGB-LED display in the context of artificial emotions. We are focusing on four of the most representative human emotions which include happiness, anger, sadness and fear. We will work with colors and dynamic light patterns which are supposed to evoke various associations. In an experiment, the use these patterns as expressions of emotions are validated. The results of the conducted study show that some of the considered basic emotions can be recognized by human observers.',\n",
       " 'These days mobile devices like phones or tablets are very common among people of all age. They are connected with network and provide seamless communications through internet or cellular services. These devices can be a big help for the people who are not able to communicate properly and even in emergency conditions. A disabled person who is not able to speak or a person who speak a different language, these devices can be a boon for them as understanding, translating and speaking systems for these people. This chapter discusses a portable android based hand sign recognition system which can be used by disabled people. This chapter shows a part of on-going project. Computer Vision based techniques were used for image analysis and PCA was used after image tokenizer for recognition. This method was tested with webcam results to make system more robust.',\n",
       " 'We examine the utility of implicit user behavioral signals captured using low-cost, off-the-shelf devices for anonymous gender and emotion recognition. A user study designed to examine male and female sensitivity to facial emotions confirms that females recognize (especially negative) emotions quicker and more accurately than men, mirroring prior findings. Implicit viewer responses in the form of EEG brain signals and eye movements are then examined for existence of (a) emotion and gender-specific patterns from event-related potentials (ERPs) and fixation distributions and (b) emotion and gender discriminability. Experiments reveal that (i) Gender and emotion-specific differences are observable from ERPs, (ii) multiple similarities exist between explicit responses gathered from users and their implicit behavioral signals, and (iii) Significantly above-chance ($\\\\approx$70%) gender recognition is achievable on comparing emotion-specific EEG responses-- gender differences are encoded best for anger and disgust. Also, fairly modest valence (positive vs negative emotion) recognition is achieved with EEG and eye-based features.',\n",
       " 'Today\\'s conversational agents are restricted to simple standalone commands. In this paper, we present Iris, an agent that draws on human conversational strategies to combine commands, allowing it to perform more complex tasks that it has not been explicitly designed to support: for example, composing one command to \"plot a histogram\" with another to first \"log-transform the data\". To enable this complexity, we introduce a domain specific language that transforms commands into automata that Iris can compose, sequence, and execute dynamically by interacting with a user through natural language, as well as a conversational type system that manages what kinds of commands can be combined. We have designed Iris to help users with data science tasks, a domain that requires support for command combination. In evaluation, we find that data scientists complete a predictive modeling task significantly faster (2.6 times speedup) with Iris than a modern non-conversational programming environment. Iris supports the same kinds of commands as today\\'s agents, but empowers users to weave together these commands to accomplish complex goals.',\n",
       " 'Recent years have seen a sharp increase in the use of open source projects by common novice users; Open Source Software (OSS) is thus no longer a reserved arena for software developers and computer gurus. Although user-centered designs are gaining popularity in OSS, usability is still not considered as one of the prime objectives in many design scenarios. In this paper, we analyze industry users perception of usability factors, including understandability, learnability, operability and attractiveness, on OSS usability. The research model of this empirical study establishes the relationship between the key usability factors and OSS usability from industrial perspective. In order to conduct the study, a data set of 105 industry users is included. The results of the empirical investigation indicate the significance of the key factors for OSS usability.',\n",
       " \"Information architecture forms the foundation of users' navigation experience. Open card sorting is a widely-used method to create information architectures based on users' groupings of the content. However, little is known about the method's cross-study reliability: Does it produce consistent content groupings for similar profile participants involved in different card sort studies? This paper presents an empirical evaluation of the method's cross-study reliability. Six card sorts involving 140 participants were conducted: three open sorts for a travel website, and three for an eshop. Results showed that participants provided highly similar card sorting data for the same content. A rather high agreement of the produced navigation schemes was also found. These findings provide support for the cross-study reliability of the open card sorting method.\",\n",
       " 'Older users population is rapidly increasing all over the World. Presently, we observe efforts in the human-computer interaction domain aiming to improve life quality of age 65 and over through the use of mobile apps. Nonetheless, these efforts focus primary on interface and interaction de- sign. Little work has focused on the study of motivation to use and adherence to, of elderly to technology. Developing specific design guidelines for this population is relevant, however it should be parallel to the study of desire of elderly to embrace specific technology in their life. Designers should not be limited to technology design but consider as well how to fully convey the value that technology can bring to the lives of the users and motivate adoption. This position paper discusses techniques that might nudge elderly towards the use of new technology.',\n",
       " 'In this paper we present the evaluation process for Barbarossa, a pervasive role playing game. Barbarossa involves an invitational (preparatory) and a main execution phase. The former is freely available though Google Play store and may be played anytime/ anywhere. The latter defines three inter-dependent player roles acted by players who need to collaborate in a treasure hunting game. The eligibility of players for participating in the main game phase is restricted among those ranked relatively high in the invitational phase. Herein, we investigate the impact of the invitational game mode on the players overall game experience. The main hypothesis tested is that game awareness (gained from participating in a preliminary game phase) may serve as a means for recruiting the most suitable subjects for user trials on pervasive game research prototypes.',\n",
       " 'Crowdsourcing platforms enable companies to propose tasks to a large crowd of users. The workers receive a compensation for their work according to the serious of the tasks they managed to accomplish. The evaluation of the quality of responses obtained from the crowd remains one of the most important problems in this context. Several methods have been proposed to estimate the expertise level of crowd workers. We propose an innovative measure of expertise assuming that we possess a dataset with an objective comparison of the items concerned. Our method is based on the definition of four factors with the theory of belief functions. We compare our method to the Fagin distance on a dataset from a real experiment, where users have to assess the quality of some audio recordings. Then, we propose to fuse both the Fagin distance and our expertise measure.',\n",
       " 'We compiled a demo application and collected a motion database of more than 10,000 smartphone users to produce a health risk model trained on physical activity streams. We turned to adversarial domain adaptation and employed the UK Biobank dataset of motion data, augmented by a rich set of clinical information as the source domain to train the model using a deep residual convolutional neuron network (ResNet). The model risk score is a biomarker of ageing, since it was predictive of lifespan and healthspan (as defined by the onset of specified diseases), and was elevated in groups associated with life-shortening lifestyles, such as smoking. We ascertained the target domain performance in a smaller cohort of the mobile application that included users who were willing to share answers to a short questionnaire related to their disease and smoking status. We thus conclude that the proposed pipeline combining deep convolutional and Domain Adversarial neuron networks (DANN) is a powerful tool for disease risk and lifestyle-associated hazard assessment from mobile motion sensors that are transferable across devices and populations.',\n",
       " 'Social Virtual Reality based Learning Environments (VRLEs) such as vSocial render instructional content in a three-dimensional immersive computer experience for training youth with learning impediments. There are limited prior works that explored attack vulnerability in VR technology, and hence there is a need for systematic frameworks to quantify risks corresponding to security, privacy, and safety (SPS) threats. The SPS threats can adversely impact the educational user experience and hinder delivery of VRLE content. In this paper, we propose a novel risk assessment framework that utilizes attack trees to calculate a risk score for varied VRLE threats with rate and duration of threats as inputs. We compare the impact of a well-constructed attack tree with an adhoc attack tree to study the trade-offs between overheads in managing attack trees, and the cost of risk mitigation when vulnerabilities are identified. We use a vSocial VRLE testbed in a case study to showcase the effectiveness of our framework and demonstrate how a suitable attack tree formalism can result in a more safer, privacy-preserving and secure VRLE system.',\n",
       " \"Exploration has been one of the greatest challenges in reinforcement learning (RL), which is a large obstacle in the application of RL to robotics. Even with state-of-the-art RL algorithms, building a well-learned agent often requires too many trials, mainly due to the difficulty of matching its actions with rewards in the distant future. A remedy for this is to train an agent with real-time feedback from a human observer who immediately gives rewards for some actions. This study tackles a series of challenges for introducing such a human-in-the-loop RL scheme. The first contribution of this work is our experiments with a precisely modeled human observer: binary, delay, stochasticity, unsustainability, and natural reaction. We also propose an RL method called DQN-TAMER, which efficiently uses both human feedback and distant rewards. We find that DQN-TAMER agents outperform their baselines in Maze and Taxi simulated environments. Furthermore, we demonstrate a real-world human-in-the-loop RL application where a camera automatically recognizes a user's facial expressions as feedback to the agent while the agent explores a maze.\",\n",
       " 'Powered wheelchair users encounter barriers to their mobility everyday. Entering a building with non barrier-free areas can massively impact the user mobility related activities. There are a few commercial devices and some experimental that can climb stairs using for instance adaptive wheels with joints or caterpillar drive. These systems rely on the use for sensing and control. For safe automated obstacle crossing, a robust and environment invariant detection of the surrounding is necessary. Radar may prove to be a suitable sensor for its capability to handle harsh outdoor environmental conditions. In this paper, we introduce a mirror based two dimensional Frequency-Modulated Continuous-Wave (FMCW) radar scanner for stair detection. A radar image based stair dimensioning approach is presented and tested under laboratory and realistic conditions.',\n",
       " 'The emergence of mobile eye trackers embedded in next generation smartphones or VR displays will make it possible to trace not only what objects we look at but also the level of attention in a given situation. Exploring whether we can quantify the engagement of a user interacting with a laptop, we apply mobile eye tracking in an in-depth study over 2 weeks with nearly 10.000 observations to assess pupil size changes, related to attentional aspects of alertness, orientation and conflict resolution. Visually presenting conflicting cues and targets we hypothesize that it\\'s feasible to measure the allocated effort when responding to confusing stimuli. Although such experiments are normally carried out in a lab, we are able to differentiate between sustained alertness and complex decision making even with low cost eye tracking \"in the wild\". From a quantified self perspective of individual behavioral adaptation, the correlations between the pupil size and the task dependent reaction time and error rates may longer term provide a foundation for modifying smartphone content and interaction to the users perceived level of attention.',\n",
       " 'As aging societies grow, researchers are actively studying care systems concerning the life and diseases of the elderly. Among these diseases, dementia makes it difficult to maintain daily life due to the degradation of cognitive functioning, memory, and reasoning, as well as the ability to perform actions. Moreover, dementia does not have a perfect cure, though therapy and care can slow its onset and provide patients with physical and mental support. In this paper, we developed a projection-based augmented reality system robot that can cover 360 degrees of space. We also propose an application that supports continuous monitoring of dementia patients to address the difficulties they face in daily life. The system is also designed to provide therapy applications, such as entertainment and spatial art, to provide mental care aids for the patients.',\n",
       " 'This paper describes the conception, development and deployment of a novel HCI system for public participation and decision making. This system was applied for the process of allocating refugee accommodation in the City of Hamburg within the FindingPlaces project in 2016. The CityScope a rapid prototyping platform for urban planning and decision making offered a technical solution which was complemented by a workshop process to facilitate effective interaction of multiple participants and stakeholder groups. This paper presents the origins of CS and the evolution of the tangible user interface approach to urban planning and public participation. It further outlines technical features of the system, including custom hardware and software in use, utilization in real time as well as technical constraints and limitations. Special focus is on the adaptation of the CS technology to the specific demands of Hamburg FP project, whose procedures, processes, and results are reflected. The final section analyzes success factors as well as shortcomings of the approach, and indicates further R&D as well as application scenarios for the CS.',\n",
       " 'Science fiction literature, comics, cartoons and, in particular, audio-visual materials, such as science fiction movies and shows, can be a valuable addition in Human-computer interaction (HCI) Education. In this paper, we present an overview of research relative to future directions in HCI Education, distinct crossings of science fiction in HCI and Computer Science teaching and the Framework for 21st Century Learning. Next, we provide examples where science fiction can add to the future of HCI Education. In particular, we argue herein first that science fiction, as tangible and intangible cultural artifact, can serve as a trigger for creativity and innovation and thus, support us in exploring the design space. Second, science fiction, as a means to analyze yet-to-come HCI technologies, can assist us in developing an open-minded and reflective dialogue about technological futures, thus creating a singular base for critical thinking and problem solving. Provided that one is cognizant of its potential and limitations, we reason that science fiction can be a meaningful extension of selected aspects of HCI curricula and research.',\n",
       " 'We present an interactive visualisation tool for recommending travel trajectories. This system is based on new machine learning formulations and algorithms for the sequence recommendation problem. The system starts from a map-based overview, taking an interactive query as starting point. It then breaks down contributions from different geographical and user behavior features, and those from individual points-of-interest versus pairs of consecutive points on a route. The system also supports detailed quantitative interrogation by comparing a large number of features for multiple points. Effective trajectory visualisations can potentially benefit a large cohort of online map users and assist their decision-making. More broadly, the design of this system can inform visualisations of other structured prediction tasks, such as for sequences or trees.',\n",
       " 'Visually impaired people face numerous challenges when it comes to transportation. Not only must they circumvent obstacles while navigating, but they also need access to essential information related to available public transport, up-to-date weather forecast, and convenient method for booking private taxis. In this paper we introduce Transport Assistant - a voice based assistive technology prototype, built with a goal of leveling the playing field for the visually impaired to solve these problems that they face in their day to day life. Being voice enabled makes it seamlessly integrate into the environment, and can be invoked by saying a hotword - hello assistant. The paper explores this research question, followed by investigating existing technologies, explains the methodology and design, then concludes by presenting the prototype and results.',\n",
       " 'We investigate to what extent mobile use patterns can predict -- at the moment it is posted -- whether a notification will be clicked within the next 10 minutes. We use a data set containing the detailed mobile phone usage logs of 279 users, who over the course of 5 weeks received 446,268 notifications from a variety of apps. Besides using classical gradient-boosted trees, we demonstrate how to make continual predictions using a recurrent neural network (RNN). The two approaches achieve a similar AUC of ca. 0.7 on unseen users, with a possible operation point of 50% sensitivity and 80% specificity considering all notification types (an increase of 40% with respect to a probabilistic baseline). These results enable automatic, intelligent handling of mobile phone notifications without the need for user feedback or personalization. Furthermore, they showcase how forego feature-extraction by using RNNs for continual predictions directly on mobile usage logs. To the best of our knowledge, this is the first work that leverages mobile sensor data for continual, context-aware predictions of interruptibility using deep neural networks.',\n",
       " 'The distinct abilities of older adults to interact with computers has motivated a wide range of contributions in the the form of design guidelines for making technologies usable and accessible for the elderly population. However, despite the growing effort by the research community, the adoption of guidelines by developers and designers has been scant or not properly translated into more accessible interaction systems. In this paper we explore this issue by reporting on a qualitative outcomes of a systematic review of 204 research-derived design guidelines for touchscreen applications. We report first on the different definitions of \"elderly\" and assess the reliability, organization and accessibility of the guidelines. Then we present our early attempt at facilitating the reporting and access of such guidelines to researchers and practitioners.',\n",
       " 'Email is one of the most successful computer applications yet devised. Communication features in email, however, have remained relatively static in years. We investigate one way of expanding email functionality without modifying the existing email infrastructure. We introduce email late bound content, a simple and generalizable technique that defers message content binding through image lazy-loading. Parts of an email are converted into external images embedded in HTML code snippets, making it so that email clients will defer the image download (i.e. content binding) until the moment users open the email. This late bound content allows email senders and third party services to update delivered emails. To illustrate the utilities of late bound content, we present four new example features and discuss the tradeoffs of email content late binding.',\n",
       " 'We model Human-Robot-Interaction (HRI) scenarios as linear dynamical systems and use Model Predictive Control (MPC) with mixed integer constraints to generate human-aware control policies. We motivate the approach by presenting two scenarios. The first involves an assistive robot that aims to maximize productivity while minimizing the human\\'s workload, and the second involves a listening humanoid robot that manages its eye contact behavior to maximize \"connection\" and minimize social \"awkwardness\" with the human during the interaction. Our simulation results show that the robot generates useful behaviors as it finds control policies to minimize the specified cost function. Further, we implement the second scenario on a humanoid robot and test the eye contact scenario with 48 human participants to demonstrate and evaluate the desired controller behavior. The humanoid generated 25% more eye contact when it was told to maximize connection over when it was told to maximize awkwardness. However, despite showing the desired behavior, there was no statistical difference between the participant\\'s perceived connection with the humanoid.',\n",
       " 'Designing 3D User Interfaces (UI) requires adequate evaluation tools to ensure good usability and user experience. While many evaluation tools are already available and widely used, existing approaches generally cannot provide continuous and objective measures of usa-bility qualities during interaction without interrupting the user. In this paper, we propose to use brain (with ElectroEncephaloGraphy) and physiological (ElectroCardioGraphy, Galvanic Skin Response) signals to continuously assess the mental effort made by the user to perform 3D object manipulation tasks. We first show how this mental effort (a.k.a., mental workload) can be estimated from such signals, and then measure it on 8 participants during an actual 3D object manipulation task with an input device known as the CubTile. Our results suggest that monitoring workload enables us to continuously assess the 3DUI and/or interaction technique ease-of-use. Overall, this suggests that this new measure could become a useful addition to the repertoire of available evaluation tools, enabling a finer grain assessment of the ergonomic qualities of a given 3D user interface.',\n",
       " 'Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.',\n",
       " 'This research investigates the variety and complexity of situational impairment events (SIE) that are being experienced by users of smartphone technology of all abilities. The authors have created a classification system to help describe the different types of SIE as well as differentiate a certain subgroup of events that were identified as severely constraining. Continuing research examined workarounds that users deploy when attempting to complete a mobile I/O transaction in the presence of an SIE, as well as social/cultural barriers to attempting mobile interaction that users recognize but do not always follow. The ultimate goal of this research arc would be the creation of guidelines to assist mobile designers and researchers in the accounting of SIE and perhaps different design considerations for those events deemed severely constraining.',\n",
       " 'Data credibility is a crucial issue in mobile crowd sensing (MCS) and, more generally, people-centric Internet of Things (IoT). Prior work takes approaches such as incentive mechanism design and data mining to address this issue, while overlooking the power of crowds itself, which we exploit in this paper. In particular, we propose a cross validation approach which seeks a validating crowd to verify the data credibility of the original sensing crowd, and uses the verification result to reshape the original sensing dataset into a more credible posterior belief of the ground truth. Following this approach, we design a specific cross validation mechanism, which integrates four sampling techniques with a privacy-aware competency-adaptive push (PACAP) algorithm and is applicable to time-sensitive and quality-critical MCS applications. It does not require redesigning a new MCS system but rather functions as a lightweight \"plug-in\", making it easier for practical adoption. Our results demonstrate that the proposed mechanism substantially improves data credibility in terms of both reinforcing obscure truths and scavenging hidden truths.',\n",
       " 'Despite the enormous interest in emotion classification from speech, the impact of noise on emotion classification is not well understood. This is important because, due to the tremendous advancement of the smartphone technology, it can be a powerful medium for speech emotion recognition in the outside laboratory natural environment, which is likely to incorporate background noise in the speech. We capitalize on the current breakthrough of Recurrent Neural Network (RNN) and seek to investigate its performance for emotion classification from noisy speech. We particularly focus on the recently proposed Gated Recurrent Unit (GRU), which is yet to be explored for emotion recognition from speech. Experiments conducted with speech compounded with eight different types of noises reveal that GRU incurs an 18.16% smaller run-time while performing quite comparably to the Long Short-Term Memory (LSTM), which is the most popular Recurrent Neural Network proposed to date. This result is promising for any embedded platform in general and will initiate further studies to utilize GRU to its full potential for emotion recognition on smartphones.',\n",
       " \"This study explores the use of natural language to give instructions that might be interpreted by Internet of Things (IoT) devices in a domestic `smart home' environment. We start from the proposition that reminders can be considered as a type of end-user programming, in which the executed actions might be performed either by an automated agent or by the author of the reminder. We conducted an experiment in which people wrote sticky notes specifying future actions in their home. In different conditions, these notes were addressed to themselves, to others, or to a computer agent.We analyse the linguistic features and strategies that are used to achieve these tasks, including the use of graphical resources as an informal visual language. The findings provide a basis for design guidance related to end-user development for the Internet of Things.\",\n",
       " 'In recent years, the rapid development of neuroimaging technology has been providing many powerful tools for cognitive neuroscience research. Among them, the functional magnetic resonance imaging (fMRI), which has high spatial resolution, acceptable temporal resolution, simple calibration, and short preparation time, has been widely used in brain research. Compared with the electroencephalogram (EEG), real-time fMRI-based brain computer interface (rtfMRI-BCI) not only can perform decoding analysis across the whole brain to control external devices, but also allows a subject to voluntarily self-regulate specific brain regions. This paper reviews the basic architecture of rtfMRI-BCI, the emerging machine learning based data analysis approaches (also known as multi-voxel pattern analysis), and the applications and recent advances of rtfMRI-BCI.',\n",
       " \"Explanations given by automation are often used to promote automation adoption. However, it remains unclear whether explanations promote acceptance of automated vehicles (AVs). In this study, we conducted a within-subject experiment in a driving simulator with 32 participants, using four different conditions. The four conditions included: (1) no explanation, (2) explanation given before or (3) after the AV acted and (4) the option for the driver to approve or disapprove the AV's action after hearing the explanation. We examined four AV outcomes: trust, preference for AV, anxiety and mental workload. Results suggest that explanations provided before an AV acted were associated with higher trust in and preference for the AV, but there was no difference in anxiety and workload. These results have important implications for the adoption of AVs.\",\n",
       " 'Mobile proactive tourist recommender systems can support tourists by recommending the best choice depending on different contexts related to herself and the environment. In this paper, we propose to utilize wearable sensors to gather health information about a tourist and use them for recommending tourist activities. We discuss a range of wearable devices, sensors to infer physiological conditions of the users, and exemplify the feasibility using a popular self-quantification mobile app. Our main contribution then comprises a data model to derive relations between the parameters measured by the wearable sensors, such as heart rate, body temperature, blood pressure, and use them to infer the physiological condition of a user. This model can then be used to derive classes of tourist activities that determine which items should be recommended.',\n",
       " 'We present a novel, real-time algorithm, EVA, for generating virtual agents with various perceived emotions. Our approach is based on using Expressive Features of gaze and gait to convey emotions corresponding to happy, sad, angry, or neutral. We precompute a data-driven mapping between gaits and their perceived emotions. EVA uses this gait emotion association at runtime to generate appropriate walking styles in terms of gaits and gaze. Using the EVA algorithm, we can simulate gaits and gazing behaviors of hundreds of virtual agents in real-time with known emotional characteristics. We have evaluated the benefits in different multi-agent VR simulation environments. Our studies suggest that the use of expressive features corresponding to gait and gaze can considerably increase the sense of presence in scenarios with multiple virtual agents.',\n",
       " 'The analysis of the collaborative learning process is one of the growing fields of education research, which has many different analytic solutions. In this paper, we provided a new solution to improve automated collaborative learning analyses using deep neural networks. Instead of using self-reported questionnaires, which are subject to bias and noise, we automatically extract group-working information by object recognition results using Mask R-CNN method. This process is based on detecting the people and other objects from pictures and video clips of the collaborative learning process, then evaluate the mobile learning performance using the collaborative indicators. We tested our approach to automatically evaluate the group-work collaboration in a controlled study of thirty-three dyads while performing an anatomy body painting intervention. The results indicate that our approach recognizes the differences of collaborations among teams of treatment and control groups in the case study. This work introduces new methods for automated quality prediction of collaborations among human-human interactions using computer vision techniques.',\n",
       " 'Insights into social phenomenon can be gleaned from trends and patterns in corpora of documents associated with that phenomenon. Recent years have witnessed the use of computational techniques, mostly based on keywords, to analyze large corpora for these purposes. In this paper, we extend these techniques to incorporate semantic features. We introduce Doris, an interactive exploration tool that combines semantic features with information retrieval techniques to enable exploration of document corpora corresponding to the social phenomenon. We discuss the semantic techniques and describe an implementation on a corpus of United States (US) presidential speeches. We illustrate, with examples, how the ability to combine syntactic and semantic features in a visualization helps researchers more easily gain insights into the underlying phenomenon.',\n",
       " 'The wide spread of mobile devices in the consumer market has posed a number of new issues in the design of internet applications and their user interfaces. In particular, applications need to adapt their interaction modalities to different portable devices. In this paper we address the problem of defining models and techniques for designing internet based applications that automatically adapt to different mobile devices. First, we define a formal model that allows for specifying the interaction in a way that is abstract enough to be decoupled from the presentation layer, which is to be adapted to different contexts. The model is mainly based on the idea of describing the user interaction in terms of elementary actions. Then, we provide a formal device characterization showing how to effectively implements the AIUs in a multidevice context.',\n",
       " 'We propose a system to deliver dynamic guidance in drawing, sketching and handwriting tasks via an electromagnet moving underneath a high refresh rate pressure sensitive tablet. The system allows the user to move the pen at their own pace and style and does not take away control. The system continously and iteratively measures the pen motion and adjusts magnet position and power according to the user input in real-time via a receding horizon optimal control formulation. The optimization is based on a novel approximate electromagnet model that is fast enough for use in real-time methods, yet provides very good fit to experimental data. Using a closed-loop time-free approach allows for error-correcting behavior, gently pulling the user back to the desired trajectory rather than pushing or pulling the pen to a continuously advancing setpoint. Our experimental results show that the system can control the pen position with a very low dispersion of 2.8mm (+/-0.8mm). An initial user study indicates that it significantly increases accuracy of users drawing a variety of shapes and that this improvement increases with complexity of the shape.',\n",
       " \"AI systems are being deployed to support human decision making in high-stakes domains. In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI's inferences. A successful partnership requires that the human develops insights into the performance of the AI system, including its failures. We study the influence of updates to an AI system in this setting. While updates can increase the AI's predictive performance, they may also lead to changes that are at odds with the user's prior experiences and confidence in the AI's inferences, hurting therefore the overall team performance. We introduce the notion of the compatibility of an AI update with prior user experience and present methods for studying the role of compatibility in human-AI teams. Empirical results on three high-stakes domains show that current machine learning algorithms do not produce compatible updates. We propose a re-training objective to improve the compatibility of an update by penalizing new errors. The objective offers full leverage of the performance/compatibility tradeoff, enabling more compatible yet accurate updates.\",\n",
       " 'Virtual reality (VR) is an important new technology that is fun-damentally changing the way people experience entertainment and education content. Due to the fact that most currently available VR products are one size fits all, the accessibility of the content design and user interface design, even for healthy children is not well understood. It requires more research to ensure that children can have equally good user compared to adults in VR. In our study, we seek to explore accessibility of locomotion in VR between healthy adults and minors along both objective and subjective dimensions. We performed a user experience experiment where subjects completed a simple task of moving and touching underwater animals in VR using one of four different locomotion modalities, as well as real-world walking without wearing VR headsets as the baseline. Our results show that physical body movement that mirrors real-world movement exclusively is the least preferred by both adults and minors. However, within the different modalities of controller assisted locomotion there are variations between adults and minors for preference and challenge levels.',\n",
       " 'Development of the majority of the leading web services and software products today is generally guided by data-driven decisions based on evaluation that ensures a steady stream of updates, both in terms of quality and quantity. Large internet companies use online evaluation on a day-to-day basis and at a large scale. The number of smaller companies using A/B testing in their development cycle is also growing. Web development across the board strongly depends on quality of experimentation platforms. In this tutorial, we overview state-of-the-art methods underlying everyday evaluation pipelines at some of the leading Internet companies. Software engineers, designers, analysts, service or product managers --- beginners, advanced specialists, and researchers --- can learn how to make web service development data-driven and do it effectively.',\n",
       " 'Cartograms are maps in which areas of geographic regions (countries, states) appear in proportion to some variable of interest (population, income). Cartograms are popular visualizations for geo-referenced data that have been around for over a century. Newspapers, magazines, textbooks, blogs, and presentations frequently employ cartograms to show voting results, popularity, and in general, geographic patterns. Despite the popularity of cartograms and the large number of cartogram variants, there are very few studies evaluating the effectiveness of cartograms in conveying information. In order to design cartograms as a useful visualization tool and to be able to compare the effectiveness of cartograms generated by different methods, we need to study the nature of information conveyed and the specific tasks that can be performed on cartograms. In this paper we consider a set of cartogram visualization tasks, based on standard taxonomies from cartography and information visualization. We then propose a cartogram task taxonomy that can be used to organize not only the tasks considered here but also other tasks that might be added later.',\n",
       " 'The future scenarios often associated with Internet of Things (IoT) oscillate between the peril of IoT for the future of humanity and the promises for an ever-connected and efficient future. Such a dichotomous positioning creates problems not only for expanding the field of application of the technology, but also ensuring ethical and responsible design and production. As part of VirtEU (Values and Ethics in Innovation for Responsible Technology in Europe) (EU Horizon 2020 FP7), we have conducted ethnographic research into the main hubs of IoT in Europe, such as London, Amsterdam, Barcelona and Belgrade, with developers and designers of IoT to identify the challenges they face in their day-to-day work. In this paper, we focus on the IoT and the ethical imaginaries explore the practical challenges IoT developers face when they are designing, producing and marketing IoT technologies. We argue that top-down ethical frameworks that overlook the situated capabilities of developers or the solutionist approaches that treat ethical issues as technical problems are unlikely to provide an alternative to the dichotomous imaginary for the future.',\n",
       " 'Context-aware applications process context information to support users in their daily tasks and routines. These applications can adapt their functionalities by aggregating context information through machine-learning and data processing algorithms, supporting users with recommendations or services based on their current needs. In the last years, smartphones have been used in the field of context-awareness due to their embedded sensors and various communication interfaces such as Bluetooth, WiFi, NFC or cellular. However, building context-aware applications for smartphones can be a challenging and time-consuming task. In this paper, we describe an ontology-based reasoning framework to create context-aware applications. The framework is based on an ontology as well as micro-services to aggregate, process and represent context information.',\n",
       " 'Aerial robots are becoming popular among general public, and with the development of artificial intelligence (AI), there is a trend to equip aerial robots with a natural user interface (NUI). Hand/arm gestures are an intuitive way to communicate for humans, and various research works have focused on controlling an aerial robot with natural gestures. However, the techniques in this area are still far from mature. Many issues in this area have been poorly addressed, such as the principles of choosing gestures from the design point of view, hardware requirements from an economic point of view, considerations of data availability, and algorithm complexity from a practical perspective. Our work focuses on building an economical monocular system particularly designed for gesture-based piloting of an aerial robot. Natural arm gestures are mapped to rich target directions and convenient fine adjustment is achieved. Practical piloting scenarios, hardware cost and algorithm applicability are jointly considered in our system design. The entire system is successfully implemented in an aerial robot and various properties of the system are tested.',\n",
       " 'Recent trend of touch-screen devices produces an accessibility barrier for visually impaired people. On the other hand, these devices come with sensors such as accelerometer. This calls for new approaches to human computer interface (HCI). In this study, our aim is to find an alternative approach to classify 20 different hand gestures captured by iPhone 3GS\\'s built-in accelerometer and make high accuracy on user-independent classifications using Dynamic Time Warping (DTW) with dynamic warping window sizes. 20 gestures with 1,100 gesture data are collected from 15 normal-visioned people. This data set is used for training. Experiment-1 based on this data set produced an accuracy rate of 96.7~\\\\%. In order for visually impaired people to use the system, a gesture recognition based \"talking\" calculator is implemented. In Experiment-2, 4 visually impaired end-users used the calculator and obtained 95.5~\\\\% accuracy rate among 17 gestures with 720 gesture data totally. Contributions of the techniques to the end result is also investigated. Dynamic warping window size is found to be the most effective one. The data and the code is available.',\n",
       " 'Mobile agents that can leverage help from humans can potentially accomplish more complex tasks than they could entirely on their own. We develop \"Help, Anna!\" (HANNA), an interactive photo-realistic simulator in which an agent fulfills object-finding tasks by requesting and interpreting natural languageand-vision assistance. An agent solving tasks in a HANNA environment can leverage simulated human assistants, called ANNA (Automatic Natural Navigation Assistants), which, upon request, provide natural language and visual instructions to direct the agent towards the goals. To address the HANNA problem, we develop a memory-augmented neural agent that hierarchically models multiple levels of decision-making, and an imitation learning algorithm that teaches the agent to avoid repeating past mistakes while simultaneously predicting its own chances of making future progress. Empirically, our approach is able to ask for help more effectively than competitive baselines and, thus, attains higher task success rate on both previously seen and previously unseen environments. We publicly release code and data at https://github.com/khanhptnk/hanna .',\n",
       " 'This paper explores a design study of a smartphone enabled meet-up app meant to inspire engagement in community innovation. Community hubs such as co-working spaces, incubators, and maker spaces attract community members with diverse interests. This paper presents these spaces as a design opportunity for an application that helps host community-centered meet-ups in smart and connected communities. Our design study explores three scenarios of use, inspired by previous literature, for organizing meet-ups and compares them by surveying potential users. Based on the results of our survey, we propose several design implications and implement them in the Community Animator geosocial networking application, which identifies nearby individuals that are willing to chat or perform community-centered activities. We present the results of both our survey and our prototype, discuss our design goals, and provide design implications for civic-minded, geosocial networking applications. Our contribution in this work is the development process, proposed design of a mobile application to support community-centered meet-ups, and insights for future work.',\n",
       " \"Traditional employment usually provides mechanisms for workers to improve their skills to access better opportunities. However, crowd work platforms like Amazon Mechanical Turk (AMT) generally do not support skill development (i.e., becoming faster and better at work). While researchers have started to tackle this problem, most solutions are dependent on experts or requesters willing to help. However, requesters generally lack the necessary knowledge, and experts are rare and expensive. To further facilitate crowd workers' skill growth, we present Crowd Coach, a system that enables workers to receive peer coaching while on the job. We conduct a field experiment and real world deployment to study Crowd Coach in the wild. Hundreds of workers used Crowd Coach in a variety of tasks, including writing, doing surveys, and labeling images. We find that Crowd Coach enhances workers' speed without sacrificing their work quality, especially in audio transcription tasks. We posit that peer coaching systems hold potential for better supporting crowd workers' skill development while on the job. We finish with design implications from our research.\",\n",
       " 'In this paper we describe the design and validation of a virtual fitness environment aiming at keeping older adults physically and socially active. We target particularly older adults who are socially more isolated, physically less active, and with less chances of training in a gym. The virtual fitness environment, namely Gymcentral, was designed to enable and motivate older adults to follow personalised exercises from home, with a (heterogeneous) group of remote friends and under the remote supervision of a Coach. We take the training activity as an opportunity to create social interactions, by complementing training features with social instruments. Finally, we report on the feasibility and effectiveness of the virtual environment, as well as its effects on the usage and social interactions, from an intervention study in Trento, Italy',\n",
       " \"The work presented in this paper focuses on the improvement of corporate knowledge management systems. For the implementation of such systems, companies deploy can important means for small gains. Indeed, management services often notice very limited use compared to what they actually expect. We present a five-step redesigning approach which takes into account different factors to increase the use of these systems. We use as an example the knowledge sharing platform implemented for the employees of Soci{\\\\'e}t{\\\\'e} du Canal de Provence (SCP). This system was taken into production but very occasionally used. We describe the reasons for this limited use and we propose a design methodology adapted to the context. Promoting the effective use of the system, our approach has been experimented and evaluated with a panel of users working at SCP.\",\n",
       " 'We propose VRGym, a virtual reality testbed for realistic human-robot interaction. Different from existing toolkits and virtual reality environments, the VRGym emphasizes on building and training both physical and interactive agents for robotics, machine learning, and cognitive science. VRGym leverages mechanisms that can generate diverse 3D scenes with high realism through physics-based simulation. We demonstrate that VRGym is able to (i) collect human interactions and fine manipulations, (ii) accommodate various robots with a ROS bridge, (iii) support experiments for human-robot interaction, and (iv) provide toolkits for training the state-of-the-art machine learning algorithms. We hope VRGym can help to advance general-purpose robotics and machine learning agents, as well as assisting human studies in the field of cognitive science.',\n",
       " 'Globally distributed groups require collaborative systems to support their work. Besides being able to support the teamwork, these systems also should promote well-being and maximize the human potential that leads to an engaging system and joyful experience. Designing such system is a significant challenge and requires a thorough understanding of group work. We used the field theory as a lens to view the essential aspects of group motivation and then utilized collaboration personas to analyze the elements of group work. We integrated well-being determinants as engagement factors to develop a group-centered framework for digital collaboration in a global setting. Based on the outcomes, we proposed a conceptual framework to design an engaging collaborative system and recommend system values that can be used to evaluate the system further',\n",
       " 'This position paper takes the first step to attempt to present the initial characterization of HCI research in China. We discuss the current streams and methodologies of Chinese HCI research based on two well-known HCI theories: Micro/Marco-HCI and the Three Paradigms of HCI. We evaluate the discussion with a survey of Chinese publications at CHI 2019, which shows HCI research in China has less attention to Macro-HCI topics and the third paradigms of HCI (Phenomenologically situated Interaction). We then propose future HCI research directions such as paying more attention to Macro-HCI topics and third paradigm of HCI, combining research methodologies from multiple HCI paradigms, including emergent users who have less access to technology, and addressing the cultural dimensions in order to provide better technical solutions and support.',\n",
       " 'From a computational viewpoint, emotions continue to be intriguingly hard to understand. In research, direct, real-time inspection in realistic settings is not possible. Discrete, indirect, post-hoc recordings are therefore the norm. As a result, proper emotion assessment remains a problematic issue. The Continuously Annotated Signals of Emotion (CASE) dataset provides a solution as it focusses on real-time continuous annotation of emotions, as experienced by the participants, while watching various videos. For this purpose, a novel, intuitive joystick-based annotation interface was developed, that allowed for simultaneous reporting of valence and arousal, that are instead often annotated independently. In parallel, eight high quality, synchronized physiological recordings (1000 Hz, 16-bit ADC) were made of ECG, BVP, EMG (3x), GSR (or EDA), respiration and skin temperature. The dataset consists of the physiological and annotation data from 30 participants, 15 male and 15 female, who watched several validated video-stimuli. The validity of the emotion induction, as exemplified by the annotation and physiological data, is also presented.',\n",
       " \"Online experimentation is at the core of Booking.com's customer-centric product development. While randomised controlled trials are a powerful tool for estimating the overall effects of product changes on business metrics, they often fall short in explaining the mechanism of change. This becomes problematic when decision-making depends on being able to distinguish between the direct effect of a treatment on some outcome variable and its indirect effect via a mediator variable. In this paper, we demonstrate the need for mediation analyses in online experimentation, and use simulated data to show how these methods help identify and estimate direct causal effect. Failing to take into account all confounders can lead to biased estimates, so we include sensitivity analyses to help gauge the robustness of estimates to missing causal factors.\",\n",
       " '3D gaze information is important for scene-centric attention analysis but accurate estimation and analysis of 3D gaze in real-world environments remains challenging. We present a novel 3D gaze estimation method for monocular head-mounted eye trackers. In contrast to previous work, our method does not aim to infer 3D eyeball poses but directly maps 2D pupil positions to 3D gaze directions in scene camera coordinate space. We first provide a detailed discussion of the 3D gaze estimation task and summarize different methods, including our own. We then evaluate the performance of different 3D gaze estimation approaches using both simulated and real data. Through experimental validation, we demonstrate the effectiveness of our method in reducing parallax error, and we identify research challenges for the design of 3D calibration procedures.',\n",
       " 'The health effects of air pollution have been subject to intense study in recent decades. Exposure to pollutants such as airborne particulate matter and ozone has been associated with increases in morbidity and mortality, especially with regards to respiratory and cardiovascular diseases. Unfortunately, individuals do not have readily accessible methods by which to track their exposure to pollution. This paper proposes how pollution parameters like CO, NO2, O3, PM2.5, PM10 and SO2 can be monitored for respiratory and cardiovascular personalized health during outdoor exercise events. Using location tracked activities, we synchronize them to public data sets of pollution sensors. For improved accuracy in estimation, we use heart rate data to understand breathing volume mapped with the local air quality sensors via constant GPS tracking.',\n",
       " 'The increasing accessibility of data provides substantial opportunities for understanding user behaviors. Unearthing anomalies in user behaviors is of particular importance as it helps signal harmful incidents such as network intrusions, terrorist activities, and financial frauds. Many visual analytics methods have been proposed to help understand user behavior-related data in various application domains. In this work, we survey the state of art in visual analytics of anomalous user behaviors and classify them into four categories including social interaction, travel, network communication, and transaction. We further examine the research works in each category in terms of data types, anomaly detection techniques, and visualization techniques, and interaction methods. Finally, we discuss the findings and potential research directions.',\n",
       " \"In 2013, scholars laid out a framework for a sustainable, ethical future of crowd work, recommending career ladders so that crowd work can lead to career advancement and more economic mobility. Five years later, we consider this vision in the context of Amazon Mechanical Turk (AMT). To understand how workers currently view their experiences on AMT, and how they publicly present and share these experiences in their professional lives, we conducted a survey study with workers on AMT (n=98). The survey we administered included a combination of multiple choice, binary, and open-ended (short paragraph) items gauging Turkers' perceptions of their experiences on AMT within the context of their broader work experience and career goals. This work extends existing understandings of who crowd workers are and why they crowd work by seeking to better understand how crowd work factors into Turkers' professional profiles, and how we can subsequently better support crowd workers in their career advancement. Our survey results can inform the design of better tools to empower crowd workers in their professional development both inside and outside of AMT.\",\n",
       " 'We live in a world where data generation is omnipresent. Innovations in computer hardware in the last few decades coupled with increasingly reliable connectivity among them have fueled this phenomenon. We are constantly creating and consuming data across digital devices of varying form factors. Leveraging huge quantities of data involves making interpretations from it. However, interpreting data is still a difficult task. We need data analysts to help make decisions. These experts apply their domain knowledge, understanding of the problem space and numerical analysis to draw inferences from the data in order to support decision making. Existing tools and techniques for interference serve users making decisions with hard constraints. Consumer systems are often built to support exploratory data analysis in mind rather than sense making.',\n",
       " 'The 9th Semantic Ambient Media Experience (SAME) proceedings where based on the academic contributions to a two day workshop that was held at Curtin University, Perth, WA, Australia. The symposium was held to discuss visualisation, emerging media, and user-experience from various angles. The papers of this workshop are freely available through http://www.ambientmediaassociation.org/Journal under open access as provided by the International Ambient Media Association (iAMEA) Ry. iAMEA is hosting the international open access journal entitled \"International Journal on Information Systems and Management in Creative eMedia\", and the series entitled \"International Series on Information Systems and Management in Creative eMedia\". For any further information, please visit the website of the Association: http://www.ambientmediaassociation.org.',\n",
       " \"The virtuality continuum describes the degrees of positive virtuality under the umbrella term mixed reality. Besides adding virtual information within a mixed environment, diminished reality aims at reducing real world information. Mann defined the term mediated reality (MR), which also considered diminished reality, but without the possibility to describe different degrees of fusion between a mixed and a diminished reality. That is why this work defines the new term blending entropy that captures the relations between a mixed and a diminished reality. The blending entropy is based on the information density of the mediated reality and the actual area the user has to comprehend, which is named perceptual frustum. We describe the blending entropy's twodimensional dependencies and detail important points in the blending entropy's space.\",\n",
       " 'With rapid development of computer techniques, the human computer interaction scenarios are becoming more and more frequent. The development history of the human-computer interaction is from a person to adapt to the computer to the computer and continually adapt to the rapid development. Facing the process of human-computer interaction, information system daily operation to produce huge amounts of data, how to ensure human-computer interaction interface clear, generated data safe and reliable, has become a problem to be solved in the world of information. To deal with the challenging, we propose the information security enhancement approaches and the core applications on HCI systems. Through reviewing the other state-of-the-art methods, we propose the data encryption system to deal with the issues that uses mixed encryption system to make full use of the symmetric cipher algorithm encryption speed and encryption intensity is high while the encryption of large amounts of data efficiently. Our method could enhance the general safety of the HCI system, the experimental result verities the feasibility and general robustness of our approach.',\n",
       " \"With the proliferation of social networking sites (SNSs) such as Facebook and Google+, investigating the impact of SNSs on our lives has become an important research area in recent years. Though SNS usage plays a key role in connecting people with friends and families from distant places, SNSs also bring concern for families. We focus on imbalance SNS usage, i.e., an individual remains busy in using SNSs when her family member is expecting to spend time with her. More specifically, we investigate the cause and pattern of imbalance SNS usage and how the emotion of family members may become affected, if they use SNSs in an imbalanced way in a regular manner. This paper is the first attempt to identify the relationship between an individual's imbalance SNS usage and the emotion of her family member in the context of a developing country.\",\n",
       " \"Many personal devices have transitioned from visual-controlled interfaces to speech-controlled interfaces to reduce device costs and interactive friction. This transition has been hastened by the increasing capabilities of speech-controlled interfaces, e.g., Amazon Echo or Apple's Siri. A consequence is that people who are deaf or hard of hearing (DHH) may be unable to use these speech-controlled devices. We show that deaf speech has a high error rate compared to hearing speech, in commercial speech-controlled interfaces. Deaf speech had approximately a 78% word error rate (WER) compared to a hearing speech 18% WER. Our findings show that current speech-controlled interfaces are not usable by deaf and hard of hearing people. Therefore, it might be wise to pursue other methods for deaf persons to deliver natural commands to computers.\",\n",
       " 'Good quality sleep is essential for good health and sleep monitoring becomes a vital research topic. This paper provides a low cost, continuous and contactless WiFi-based vital signs (breathing and heart rate) monitoring method. In particular, we set up the antennas based on Fresnel diffraction model and signal propagation theory, which enhances the detection of weak breathing/heartbeat motion. We implement a prototype system using the off-shelf devices and a real-time processing system to monitor vital signs in real time. The experimental results indicate the accurate breathing rate and heart rate detection performance. To the best of our knowledge, this is the first work to use a pair of WiFi devices and omnidirectional antennas to achieve real-time individual breathing rate and heart rate monitoring in different sleeping postures.',\n",
       " 'Fallback authentication is the backup authentication method used when the primary authentication method (e.g., passwords, fingerprints, etc.) fails. Currently, widely-deployed fallback authentication methods (e.g., security questions, email resets, and SMS resets) suffer from documented security and usability flaws that threaten the security of accounts. These flaws motivate us to design and study Geographical Security Questions (GeoSQ), a system for fallback authentication. GeoSQ is an Android application that utilizes autobiographical location data for fallback authentication. We performed security and usability analyses of GeoSQ through an in-person two-session lab study (n=36,18 pairs). Our results indicate that GeoSQ exceeds the security of its counterparts, while its usability (specifically login time) has room for improvement.',\n",
       " 'Immigrants usually are pro-social towards their hometowns and try to improve them. However, the lack of trust in their government can drive immigrants to work individually. As a result, their pro-social activities are usually limited in impact and scope. This paper studies the interface factors that ease collaborations between immigrants and their home governments. We specifically focus on Mexican immigrants in the US who want to improve their rural communities. We identify that for Mexican immigrants having clear workflows of how their money flows and a sense of control over this workflow is important for collaborating with their government. Based on these findings, we create a blockchain based system for building trust between governments and immigrants. We finish by discussing design implications of our work and future directions.',\n",
       " 'Assessing and understanding intelligent agents is a difficult task for users that lack an AI background. A relatively new area, called \"Explainable AI,\" is emerging to help address this problem, but little is known about how users would forage through information an explanation system might offer. To inform the development of Explainable AI systems, we conducted a formative study, using the lens of Information Foraging Theory, into how experienced users foraged in the domain of StarCraft to assess an agent. Our results showed that participants faced difficult foraging problems. These foraging problems caused participants to entirely miss events that were important to them, reluctantly choose to ignore actions they did not want to ignore, and bear high cognitive, navigation, and information costs to access the information they needed.',\n",
       " 'Many data mining approaches aim at modelling and predicting human behaviour. An important quantity of interest is the quality of model-based predictions, e.g. for finding a competition winner with best prediction performance.\\n  In real life, human beings meet their decisions with considerable uncertainty. Its assessment and resulting implications for statistically evident evaluation of predictive models are in the main focus of this contribution. We identify relevant sources of uncertainty as well as the limited ability of its accurate measurement, propose an uncertainty-aware methodology for more evident evaluations of data mining approaches, and discuss its implications for existing quality assessment strategies. Specifically, our approach switches from common point-paradigm to more appropriate distribution-paradigm.\\n  This is exemplified in the context of recommender systems and their established metrics of prediction quality. The discussion is substantiated by comprehensive experiments with real users, large-scale simulations, and discussion of prior evaluation campaigns (i.a. Netflix Prize) in the light of human uncertainty aspects.',\n",
       " 'We present a qualitative analysis of interviews with participants in the NoSleep community within Reddit where millions of fans and writers of horror fiction congregate. We explore how the community handled a massive, sudden, and sustained increase in new members. Although existing theory and stories like Usenet\\'s infamous \"Eternal September\" suggest that large influxes of newcomers can hurt online communities, our interviews suggest that NoSleep survived without major incident. We propose that three features of NoSleep allowed it to manage the rapid influx of newcomers gracefully: (1) an active and well-coordinated group of administrators, (2) a shared sense of community which facilitated community moderation, and (3) technological systems that mitigated norm violations. We also point to several important trade-offs and limitations.',\n",
       " \"Bar charts with y-axes that don't begin at zero can visually exaggerate effect sizes, and in turn lead to unjustified or erroneous judgments. However, advice for whether or not to truncate the y-axis can be equivocal for other visualization types, and there is little existing empirical work on how axis truncation impacts judgments. In this paper we present examples of visualizations where this y-axis truncation can be beneficial as well as harmful, depending on the communicative and analytical intent. We also present the results of a series of crowd-sourced experiments in which we examine how y-axis truncation impacts subjective effect size across visualization types, and we explore alternative designs that more directly alert viewers to this truncation. We find that the subjective impact of axis truncation is persistent across visualizations designs, even for viewers that are aware of the presence of truncated axes. We therefore advise against ironclad rules about when y-axes are appropriate, but ask designers to consider the scale of the meaningful effect sizes and variation they intend to communicate, regardless of the visual encoding.\",\n",
       " 'This article presents \"John\", an open-source software designed to help collective free improvisation. It provides generated screen-scores running on distributed, reactive web-browsers. The musicians can then concurrently edit the scores in their own browser. John is used by ONE, a septet playing improvised electro-acoustic music with digital musical instruments (DMI). One of the original features of John is that its design takes care of leaving the musician\\'s attention as free as possible. Firstly, a quick review of the context of screen-based scores will help situate this research in the history of contemporary music notation. Then I will trace back how improvisation sessions led to John\\'s particular \"notational perspective\". A brief description of the software will precede a discussion about the various aspects guiding its design.',\n",
       " \"Cluster analysis has become one of the most exercised research areas over the past few decades in computer science. As a consequence, numerous clustering algorithms have already been developed to find appropriate partitions of a set of objects. Given multiple such clustering solutions, it is a challenging task to obtain an ensemble of these solutions. This becomes more challenging when the ground truth about the number of clusters is unavailable. In this paper, we introduce a crowd-powered model to collect solutions of image clustering from the general crowd and pose it as a clustering ensemble problem with variable number of clusters. The varying number of clusters basically reflects the crowd workers' perspective toward a particular set of objects. We allow a set of crowd workers to independently cluster the images as per their perceptions. We address the problem by finding out centroid of the clusters using an appropriate distance measure and prioritize the likelihood of similarity of the individual cluster sets. The effectiveness of the proposed method is demonstrated by applying it on multiple artificial datasets obtained from crowd.\",\n",
       " 'Recently, with the impact of AJAX a new way of web development techniques have been emerged. Hence, with the help of this model, single-page web application was introduced which can be updated/replaced independently. Today we have a new challenge of building a powerful single-page application using the currently emerged technologies. Gaining an understanding of navigational model and user interface structure of the source application is the first step to successfully build a single- page application. In this paper, it explores not only building powerful single-page application but also Two Dimensional (2D) drawings on images and videos. Moreover, in this research it clearly express the findings on 2D multi-points polygon drawing concepts on client side; real-time data binding in between drawing module on image, video and view pages.',\n",
       " \"The ability to focus one's attention underlies success in many everyday tasks, but voluntary attention cannot be sustained for a long period of time. Several studies indicate that attention training using computer-based exercises can lead to improved attention in children and adults. a major goal of recent research is to create a short (10 minutes) and effective VR Mindfulness meditation particularly designed for regaining or improving sustained attention. In this study, we have created a custom virtually relaxing environment including an archery game with multiple targets. In the experiment, the attention span of 12 adults are tested before and after the virtual reality session by a non-action video game ([19]) score and Muse headband EEG-signals. After the 10-minute virtual reality session participants' game scores increased (according to game experience): for the beginner by 275%, for intermediate by 107%, and for an expert by 17%. For Muse headband data, calm points increased by 250% irrespective of the participants gaming experiences. After the experiment, all participants reported feeling recharged to continue their daily activities.\",\n",
       " \"Virtual Reality (VR) can cause an unprecedented immersion and feeling of presence yet a lot of users experience motion sickness when moving through a virtual environment. Rollercoaster rides are popular in Virtual Reality but have to be well designed to limit the amount of nausea the user may feel. This paper describes a novel framework to get automated ratings on motion sickness using Neural Networks. An application that lets users create rollercoasters directly in VR, share them with other users and ride and rate them is used to gather real-time data related to the in-game behaviour of the player, the track itself and users' ratings based on a Simulator Sickness Questionnaire (SSQ) integrated into the application. Machine learning architectures based on deep neural networks are trained using this data aiming to predict motion sickness levels. While this paper focuses on rollercoasters this framework could help to rate any VR application on motion sickness and intensity that involves camera movement. A new well defined dataset is provided in this paper and the performance of the proposed architectures are evaluated in a comparative study.\",\n",
       " 'Driving under the influence of alcohol is a widespread phenomenon in the US where it is considered a major cause of fatal accidents. In this research we present a novel approach and concept for detecting intoxication from motion differences obtained by the sensors of wearable devices. We formalize the problem of drunkenness detection as a supervised machine learning task, both as a binary classification problem (drunk or sober) and a regression problem (the breath alcohol content level).\\n  In order to test our approach, we collected data from 30 different subjects (patrons at three bars) using Google Glass and the LG G-watch, Microsoft Band, and Samsung Galaxy S4. We validated our results against an admissible breathalyzer used by the police.\\n  A system based on this concept, successfully detected intoxication and achieved the following results: 0.95 AUC and 0.05 FPR, given a fixed TPR of 1.0. Applications based on our system can be used to analyze the free gait of drinkers when they walk from the car to the bar and vice-versa, in order to alert people, or even a connected car and prevent people from driving under the influence of alcohol.',\n",
       " \"We present magnetic fingerprints; an input technique for mobile touchscreen devices that uses a small magnet attached to a user's fingernail in order to differentiate between a normal touch and a magnetic touch. The polarity of the magnet can be used to create different magnetic fingerprints where this technique takes advantage of the rich vocabulary offered by the use of multitouch input. User studies investigate the accuracy of magnetic fingerprint recognition in relation to magnet size, number of magnetic fingerprints used; and size of the touchscreen. Studies found our technique to be limited to using up to two fingerprints non-simultaneously, while achieving a high classification accuracy (95%) but it nearly triples the number of distinguishable multi touch events. Potential useful applications of this technique are presented.\",\n",
       " \"The interactive machine learning (IML) community aims to augment humans' ability to learn and make decisions over time through the development of automated decision-making systems. This interaction represents a collaboration between multiple intelligent systems---humans and machines. A lack of appropriate consideration for the humans involved can lead to problematic system behaviour, and issues of fairness, accountability, and transparency. This work presents a human-centred thinking approach to applying IML methods. This guide is intended to be used by AI practitioners who incorporate human factors in their work. These practitioners are responsible for the health, safety, and well-being of interacting humans. An obligation of responsibility for public interaction means acting with integrity, honesty, fairness, and abiding by applicable legal statutes. With these values and principles in mind, we as a research community can better achieve the collective goal of augmenting human ability. This practical guide aims to support many of the responsible decisions necessary throughout iterative design, development, and dissemination of IML systems.\",\n",
       " 'The Vim text editor is very rich in capabilities and thus complex. This article is a description of Vim and a set of considerations about its usage and design. It results from more than ten years of experience in using Vim for writing and editing various types of documents, e.g. Python, C++, JavaScript, ChucK programs; \\\\LaTeX, Markdown, HTML, RDF, Make and other markup files; % TTM binary files. It is commonplace, in the Vim users and developers communities, to say that it takes about ten years to master (or start mastering) this text editor, and I find that other experienced users have a different view of Vim and that they use a different set of features. Therefore, this document exposes my understandings in order to confront my usage with that of other Vim users. Another goal is to make available a reference document with which new users can grasp a sound overview by reading it and the discussions that it might generate. Also, it should be useful for users of any degree of experience, including me, as a compendium of commands, namespaces and tweaks. Upon feedback, and maturing of my Vim usage, this document might be enhanced and expanded.',\n",
       " 'We present MedCATTrainer an interface for building, improving and customising a given Named Entity Recognition and Linking (NER+L) model for biomedical domain text. NER+L is often used as a first step in deriving value from clinical text. Collecting labelled data for training models is difficult due to the need for specialist domain knowledge. MedCATTrainer offers an interactive web-interface to inspect and improve recognised entities from an underlying NER+L model via active learning. Secondary use of data for clinical research often has task and context specific criteria. MedCATTrainer provides a further interface to define and collect supervised learning training data for researcher specific use cases. Initial results suggest our approach allows for efficient and accurate collection of research use case specific training data.',\n",
       " 'Dark patterns are user interface design choices that benefit an online service by coercing, steering, or deceiving users into making unintended and potentially harmful decisions. We present automated techniques that enable experts to identify dark patterns on a large set of websites. Using these techniques, we study shopping websites, which often use dark patterns to influence users into making more purchases or disclosing more information than they would otherwise. Analyzing ~53K product pages from ~11K shopping websites, we discover 1,818 dark pattern instances, together representing 15 types and 7 broader categories. We examine these dark patterns for deceptive practices, and find 183 websites that engage in such practices. We also uncover 22 third-party entities that offer dark patterns as a turnkey solution. Finally, we develop a taxonomy of dark pattern characteristics that describes the underlying influence of the dark patterns and their potential harm on user decision-making. Based on our findings, we make recommendations for stakeholders including researchers and regulators to study, mitigate, and minimize the use of these patterns.',\n",
       " \"While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance- and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.\",\n",
       " \"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\",\n",
       " 'The migration of robots from the laboratory into sensitive home settings as commercially available therapeutic agents represents a significant transition for information privacy and ethical imperatives. We present new privacy paradigms and apply the Fair Information Practices (FIPs) to investigate concerns unique to the placement of therapeutic robots in private home contexts. We then explore the importance and utility of research ethics as operationalized by existing human subjects research frameworks to guide the consideration of therapeutic robotic users -- a step vital to the continued research and development of these platforms. Together, privacy and research ethics frameworks provide two complementary approaches to protect users and ensure responsible yet robust information sharing for technology development. We make recommendations for the implementation of these principles -- paying particular attention to specific principles that apply to vulnerable individuals (i.e., children, disabled, or elderly persons)--to promote the adoption and continued improvement of long-term, responsible, and research-enabled robotics in private settings.',\n",
       " 'The fact that emotions play a vital role in social interactions, along with the demand for novel human-computer interaction applications, have led to the development of a number of automatic emotion classification systems. However, it is still debatable whether the performance of such systems can compare with human coders. To address this issue, in this study, we present a comprehensive comparison in a speech-based emotion classification task between 138 Amazon Mechanical Turk workers (Turkers) and a state-of-the-art automatic computer system. The comparison includes classifying speech utterances into six emotions (happy, neutral, sad, anger, disgust and fear), into three arousal classes (active, passive, and neutral), and into three valence classes (positive, negative, and neutral). The results show that the computer system outperforms the naive Turkers in almost all cases. Furthermore, the computer system can increase the classification accuracy by rejecting to classify utterances for which it is not confident, while the Turkers do not show a significantly higher classification accuracy on their confident utterances versus unconfident ones.',\n",
       " 'Event data is present in a variety of domains such as electronic health records, daily living activities and web clickstream records. Current visualization methods to explore event data focus on discovering sequential patterns but present limitations when studying time attributes in event sequences. Time attributes are especially important when studying waiting times or lengths of visit in patient flow analysis. We propose a visual analytics methodology that allows the identification of trends and outliers in respect of duration and time of occurrence in event sequences. The proposed method presents event data using a single Sequential and Time Patterns overview. User-driven alignment by multiple events, sorting by sequence similarity and a novel visual encoding of events allows the comparison of time trends across and within sequences. The proposed visualization allows the derivation of findings that otherwise could not be obtained using traditional visualizations. The proposed methodology has been applied to a real-world dataset provided by Sheffield Teaching Hospitals NHS Foundation Trust, for which four classes of conclusions were derived.',\n",
       " \"Personalized Active Learner (PAL) is a wearable system for real-time, personalized, and context-aware health and cognition support. PAL's system consists of a wearable device, mobile app, cloud database, data visualization web app, and machine learning server. PAL's wearable device uses multi-modal sensors (camera, microphone, heart-rate) with on-device machine learning and open-ear audio output to provide real-time and context-aware cognitive, behavioral and psychological interventions. PAL also allows users to track the long-term correlations between their activities and physiological states to make well-informed lifestyle decisions. In this paper, we present and open-source PAL's system so that people can use it for health and cognition support applications. We also open-source three fully-developed example applications using PAL for face-based memory augmentation, contextual language learning, and heart-rate-based psychological support. PAL's flexible, modular and extensible platform combines trends in data-driven medicine, mobile psychology, and cognitive enhancement to support data-driven and empowering health and cognition applications.\",\n",
       " \"Surgeons must accomplish complex technical and intellectual tasks that can generate unexpected and serious challenges with little or no room for error. In the last decade, computer simulations have played an increasing role in surgical training, pre-operative planning, and biomedical research. Specifically, visuo-haptic simulations have been the focus of research to develop advanced e-Learning systems facilitating surgical training. The cost of haptic hardware was reduced through mass scale production and as haptics gained popularity in the gaming industry. Visuo-haptic simulations combine the tactile sense with visual information and provide training scenarios with a high degree of reality. For surgical training, such scenarios can be used as ways to gain, improve, and assess resident and expert surgeons' skills and knowledge.\",\n",
       " \"People naturally bring their prior beliefs to bear on how they interpret the new information, yet few formal models exist for accounting for the influence of users' prior beliefs in interactions with data presentations like visualizations. We demonstrate a Bayesian cognitive model for understanding how people interpret visualizations in light of prior beliefs and show how this model provides a guide for improving visualization evaluation. In a first study, we show how applying a Bayesian cognition model to a simple visualization scenario indicates that people's judgments are consistent with a hypothesis that they are doing approximate Bayesian inference. In a second study, we evaluate how sensitive our observations of Bayesian behavior are to different techniques for eliciting people subjective distributions, and to different datasets. We find that people don't behave consistently with Bayesian predictions for large sample size datasets, and this difference cannot be explained by elicitation technique. In a final study, we show how normative Bayesian inference can be used as an evaluation framework for visualizations, including of uncertainty.\",\n",
       " \"In today's age of pervasive computing and social media people make extensive use of technology for communicating, sharing media and learning. Yet while in the outdoors, on a hike or a trail we find ourselves inept of information about the natural world surrounding us. In this paper I present in detail the design and technological considerations required to build a location based mobile application for learning about the avian taxonomy present in the user's surroundings. It is designed to be a game for better engagement and learning. The application makes suggestions for birds likely to be sighted in the vicinity of the user and requires the user to spot those birds and upload a photograph to the system. If spotted correctly the user scores points. I also discuss some design methods and evaluation approaches for the application.\",\n",
       " 'This paper presents Living Globe, an application for visualization of demo- graphic data supporting the temporal comparison of data from several countries represented on a 3D globe. Living Globe allows the visual exploration of the following demographic data: total population, population density and growth, crude birth and death rates, life expectancy, net migration and population per- centage of different age groups. While offering unexperienced users a default mapping of these data variables into visual variables, Living Globe allows more advanced users to select the mapping, increasing its flexibility. The main aspects of the Living Globe model and prototype are described as well as the evaluation results obtained using heuristic evaluation and usability testing. Some conclusions and ideas for future work are also presented.',\n",
       " 'We present a new approach to achieve tangible object manipulation with a single, fully portable and self-contained device. Our solution is based on the concept of a \"tangible volume\". We turn a tangible object into a handheld fish-tank display. The tangible volume represents a volume of space that can be freely manipulated within a virtual scene. This volume can be positioned onto virtual objects to directly grasp them, and to manipulate them in 3D space. We investigate this concept through two user studies. The first study evaluates the intuitiveness of using a tangible volume for grasping and manipulating virtual objects. The second study evaluates the effects of the limited field of view on spatial awareness. Finally, we present a generalization of this concept to other forms of interaction through the surface of the volume.',\n",
       " \"Both offline and online human behaviors are affected by personality. Of special interests are online games, where players have to impersonate specific roles and their behaviors are extensively tracked by the game. In this paper, we propose to study the relationship between players' personality and game behavior in League of Legends (LoL), one of the most popular Multiplayer Online Battle Arena (MOBA) games. We use linear mixed effects (LME) models to describe relationships between players' personality traits (measured by the Five Factor Model) and two major aspects of the game: the impersonated roles and in-game actions. On the one hand, we study relationships within the game environment by modeling role attributes from match behaviors and vice versa. On the other hand, we analyze the relationship between a player's five personality traits and their game behavior by showing significant correlations between each personality trait and the set of corresponding behaviors. Our findings suggest that personality and behavior are highly entangled and provide a new perspective to understand how personality can affect behavior in role-based online games.\",\n",
       " 'Virtual and augmented reality communication platforms are seen as promising modalities for next-generation remote face-to-face interactions. Our study attempts to explore non-verbal communication features in relation to their conversation context for virtual and augmented reality mediated communication settings. We perform a series of user experiments, triggering nine conversation tasks in 4 settings, each containing corresponding non-verbal communication features. Our results indicate that conversation types which involve less emotional engagement are more likely to be acceptable in virtual reality and augmented reality settings with low-fidelity avatar representation, compared to scenarios that involve high emotional engagement or intellectually difficult discussions. We further systematically analyze and rank the impact of low-fidelity representation of micro-expressions, body scale, head pose, and hand gesture in affecting the user experience in one-on-one conversations, and validate that preserving micro-expression cues plays the most effective role in improving bi-directional conversations in future virtual and augmented reality settings.',\n",
       " \"The increasing use of electronic forms of communication presents new opportunities in the study of mental health, including the ability to investigate the manifestations of psychiatric diseases unobtrusively and in the setting of patients' daily lives. A pilot study to explore the possible connections between bipolar affective disorder and mobile phone usage was conducted. In this study, participants were provided a mobile phone to use as their primary phone. This phone was loaded with a custom keyboard that collected metadata consisting of keypress entry time and accelerometer movement. Individual character data with the exceptions of the backspace key and space bar were not collected due to privacy concerns. We propose an end-to-end deep architecture based on late fusion, named DeepMood, to model the multi-view metadata for the prediction of mood scores. Experimental results show that 90.31% prediction accuracy on the depression score can be achieved based on session-level mobile phone typing dynamics which is typically less than one minute. It demonstrates the feasibility of using mobile phone metadata to infer mood disturbance and severity.\",\n",
       " 'As the uni-cultural studies of website usability have matured, the paucity of cross-cultural studies of usability become increasingly apparent. Moving toward these cross-cultural studies will require the development of a new tool to assess website usability in the context of cultural dimensions. This paper introduces the preliminary results from the first phase of this project and then presents the proposed method for the research in progress that specifically is directed to the development and quantitative evaluation of a measurement scale of a culture sensitive measurement of website usability. The recognition of the need to develop this scale resulted from the identification of culture-related shortcomings of previous measurement tools that have been used widely within the Management of Information Systems (MIS) literature.',\n",
       " 'We present encube $-$ a qualitative, quantitative and comparative visualisation and analysis system, with application to high-resolution, immersive three-dimensional environments and desktop displays. encube extends previous comparative visualisation systems by considering: 1) the integration of comparative visualisation and analysis into a unified system; 2) the documentation of the discovery process; and 3) an approach that enables scientists to continue the research process once back at their desktop. Our solution enables tablets, smartphones or laptops to be used as interaction units for manipulating, organising, and querying data. We highlight the modularity of encube, allowing additional functionalities to be included as required. Additionally, our approach supports a high level of collaboration within the physical environment. We show how our implementation of encube operates in a large-scale, hybrid visualisation and supercomputing environment using the CAVE2 at Monash University, and on a local desktop, making it a versatile solution. We discuss how our approach can help accelerate the discovery rate in a variety of research scenarios.',\n",
       " 'In image-guided surgical tasks, the precision and timing of hand movements depend on the effectiveness of visual cues relative to specific target areas in the surgeons peri-personal space. Two-dimensional (2D) image views of real-world movements are known to negatively affect both constrained (with tool) and unconstrained(no tool) hand movements compared with direct action viewing. Task conditions where virtual 3D would generate and advantage for surgical eye-hand coordination are unclear. Here, we compared effects of 2D and 3D image views on the precision and timing of surgical hand movement trajectories in a simulator environment. Eight novices had to pick and place a small cube on target areas across different trajectory segments in the surgeons peri-personal space, with the dominant hand, with and without a tool, under conditions of: (1) direct (2) 2D fisheye camera and (3) virtual 3D viewing (headmounted). Significant effects of the location of trajectories in the surgeons peri-personal space on movement times and precision were found. Subjects were faster and more precise across specific target locations, depending on the viewing modality.',\n",
       " 'Incorporating accurate physics-based simulation into interactive design tools is challenging. However, adding the physics accurately becomes crucial to several emerging technologies. For example, in virtual/augmented reality (VR/AR) videos, the faithful reproduction of surrounding audios is required to bring the immersion to the next level. Similarly, as personal fabrication is made possible with accessible 3D printers, more intuitive tools that respect the physical constraints can help artists to prototype designs. One main hurdle is the sheer amount of computation complexity to accurately reproduce the real-world phenomena through physics-based simulation. In my thesis research, I develop interactive tools that implement efficient physics-based simulation algorithms for automatic optimization and intuitive user interaction.',\n",
       " 'Since the launch of Google Glass in 2014, smart glasses have mainly been designed to support micro-interactions. The ultimate goal for them to become an augmented reality interface has not yet been attained due to an encumbrance of controls. Augmented reality involves superimposing interactive computer graphics images onto physical objects in the real world. This survey reviews current research issues in the area of human computer interaction for smart glasses. The survey first studies the smart glasses available in the market and afterwards investigates the interaction methods proposed in the wide body of literature. The interaction methods can be classified into hand-held, touch, and touchless input. This paper mainly focuses on the touch and touchless input. Touch input can be further divided into on-device and on-body, while touchless input can be classified into hands-free and freehand. Next, we summarize the existing research efforts and trends, in which touch and touchless input are evaluated by a total of eight interaction goals. Finally, we discuss several key design challenges and the possibility of multi-modal input for smart glasses.',\n",
       " 'In this paper, we present MVC-3D design pattern to develop virtual and augmented (or mixed) reality interfaces that use new types of sensors, modalities and implement specific algorithms and simulation models. The proposed pattern represents the extension of classic MVC pattern by enriching the View component (interactive View) and adding a specific component (Library). The results obtained on the development of augmented reality interfaces showed that the complexity of M, iV and C components is reduced. The complexity increases only on the Library component (L). This helps the programmers to well structure their models even if the interface complexity increases. The proposed design pattern is also used in a design process called MVC-3D in the loop that enables a seamless evolution from initial prototype to the final system.',\n",
       " 'We describe the design and implementation of a vision based interactive entertainment system that makes use of both involuntary and voluntary control paradigms. Unintentional input to the system from a potential viewer is used to drive attention-getting output and encourage the transition to voluntary interactive behaviour. The iMime system consists of a character animation engine based on the interaction metaphor of a mime performer that simulates non-verbal communication strategies, without spoken dialogue, to capture and hold the attention of a viewer. The system was developed in the context of a project studying care of dementia sufferers. Care for a dementia sufferer can place unreasonable demands on the time and attentional resources of their caregivers or family members. Our study contributes to the eventual development of a system aimed at providing relief to dementia caregivers, while at the same time serving as a source of pleasant interactive entertainment for viewers. The work reported here is also aimed at a more general study of the design of interactive entertainment systems involving a mixture of voluntary and involuntary control.',\n",
       " \"Video watching had emerged as one of the most frequent media activities on the Internet. Yet, little is known about how users watch online video. Using two distinct YouTube datasets, a set of random YouTube videos crawled from the Web and a set of videos watched by participants tracked by a Chrome extension, we examine whether and how indicators of collective preferences and reactions are associated with view duration of videos. We show that video view duration is positively associated with the video's view count, the number of likes per view, and the negative sentiment in the comments. These metrics and reactions have a significant predictive power over the duration the video is watched by individuals. Our findings provide a more precise understandings of user engagement with video content in social media beyond view count.\",\n",
       " 'Expertise of annotators has a major role in crowdsourcing based opinion aggregation models. In such frameworks, accuracy and biasness of annotators are occasionally taken as important features and based on them priority of the annotators are assigned. But instead of relying on a single feature, multiple features can be considered and separate rankings can be produced to judge the annotators properly. Finally, the aggregation of those rankings with perfect weightage can be done with an aim to produce better ground truth prediction. Here, we propose a novel weighted rank aggregation method and its efficacy with respect to other existing approaches is shown on artificial dataset. The effectiveness of weighted rank aggregation to enhance quality prediction is also shown by applying it on an Amazon Mechanical Turk (AMT) dataset.',\n",
       " 'Our paper contributes to the literature recommending approaches to make online reviews more credible and representative. We analyze data from four diverse major online retailers and find that verified customers who are prompted (by an email) to write a review, submit, on average, up to 0.5 star higher ratings than self-motivated web reviewers. Moreover, these email-prompted reviews remain stable over time, whereas web reviews exhibit a downward trend. This finding provides support for the existence of social influence and selection biases during the submission of a web review, when social signals are being displayed. In contrast, no information about the current state of the reviews is displayed in the email promptings. Moreover, we find that when a retailer decides to start sending email promptings, the existing population of web reviewers is unaffected both in their volume as well as the characteristics of their submitted reviews. We explore how our combined findings can suggest ways to mitigate various biases that govern online review submissions and help practitioners provide more credible, representative and higher ratings to their customers.',\n",
       " 'When creating 3D city models, selecting relevant visualization techniques is a particularly difficult user interface design task. A first obstacle is that current geodata-oriented tools, e.g. ArcGIS, have limited 3D capabilities and limited sets of visualization techniques. Another important obstacle is the lack of unified description of information visualization techniques for 3D city models. If many techniques have been devised for different types of data or information (wind flows, air quality fields, historic or legal texts, etc.) they are generally described in articles, and not really formalized. In this paper we address the problem of visualizing information in (rich) 3D city models by presenting a model-based approach for the rapid prototyping of visualization techniques. We propose to represent visualization techniques as the composition of graph transformations. We show that these transformations can be specified with SPARQL construction operations over RDF graphs. These specifications can then be used in a prototype generator to produce 3D scenes that contain the 3D city model augmented with data represented using the desired technique.',\n",
       " 'Autism Spectrum Disorder (ASD) is an umbrella term for a wide range of developmental disorders. For the past two decades, researchers proposed the use of various technologies in order to tackle specific symptoms of the disorder. Although there exist many literature reviews about screening, assessment, and rehabilitation of ASD, no comprehensive survey of types of technologies in all defined symptoms of ASD has been presented. Therefore, in this paper a comprehensive survey of previous studies has been presented in which the studies are classified into three main categories, and several sub-categories, and three main technologies. An analysis of the number of studies in each category and sub-category is given to help researchers decide on areas which need further investigation. The analysis show that the majority of studies fall into the software-based systems technology category. Finally, a brief review of studies in each category of ASD is presented for each type of technology. As a result, this paper also helps researchers to obtain an overview of the typical methods of using a specific technology in ASD screening, assessment, and rehabilitation.',\n",
       " 'Non-linear dimensionality reduction (NDR) methods such as LLE and t-SNE are popular with visualization researchers and experienced data analysts, but present serious problems of interpretation. In this paper, we present DimReader, a technique that recovers readable axes from such techniques. DimReader is based on analyzing infinitesimal perturbations of the dataset with respect to variables of interest. The perturbations define exactly how we want to change each point in the original dataset and we measure the effect that these changes have on the projection. The recovered axes are in direct analogy with the axis lines (grid lines) of traditional scatterplots. We also present methods for discovering perturbations on the input data that change the projection the most. The calculation of the perturbations is efficient and easily integrated into programs written in modern programming languages. We present results of DimReader on a variety of NDR methods and datasets both synthetic and real-life, and show how it can be used to compare different NDR methods. Finally, we discuss limitations of our proposal and situations where further research is needed.',\n",
       " 'Maps --- specifically floor plans --- are useful for a variety of tasks from arranging furniture to designating conceptual or functional spaces (e.g., kitchen, walkway). We present a simple algorithm for quickly laying a floor plan (or other conceptual map) onto a SLAM map, creating a one-to-one mapping between them. Our goal was to enable using a floor plan (or other hand-drawn or annotated map) in robotic applications instead of the typical SLAM map created by the robot. We look at two use cases, specifying \"no-go\" regions within a room and locating objects within a scanned room. Although a user study showed no statistical difference between the two types of maps in terms of performance on this spatial memory task, we argue that floor plans are closer to the mental maps people would naturally draw to characterize spaces.',\n",
       " \"Spreadsheet users regularly deal with uncertainty in their data, for example due to errors and estimates. While an insight into data uncertainty can help in making better informed decisions, prior research suggests that people often use informal heuristics to reason with probabilities, which leads to incorrect conclusions. Moreover, people often ignore or simplify uncertainty. To understand how people currently encounter and deal with uncertainty in spreadsheets, we conducted an interview study with 11 spreadsheet users from a range of domains. We found that how people deal with uncertainty is influenced by the role the spreadsheet plays in people's work and the user's aims. Spreadsheets are used as a database, template, calculation tool, notepad and exploration tool. In doing so, participants' aims were to compute and compare different scenarios, understand something about the nature of the uncertainty in their situation, and translate the complexity of data uncertainty into simplified presentations to other people, usually decision-makers. Spreadsheets currently provide limited tools to support these aims, and participants had various workarounds.\",\n",
       " 'Event-related potentials (ERPs) are very small voltage produced by the brain in response to external stimulation. In order to detect and evaluate an ERP in an ongoing electroencephalogram (EEG), it is necessary to tag the EEG with the exact onset time of the stimulus. We define the latency as the delay between the time the tagging command is sent and the detection of the stimulus on the screen. Failing to control sequencing in the tagging pipeline causes problems when interpreting latency, in particular when comparing ERPs generated from stimuli displayed by different systems. In this work, we present number of technical aspects which can influence latency such as the refresh rate of the screen or the display of a stimulus at different screen location. A few propositions are suggested to estimate and correct this latency.',\n",
       " \"In this paper we describe and evaluate a mixed reality system that aims to augment users in task guidance applications by combining automated and unsupervised information collection with minimally invasive video guides. The result is a self-contained system that we call GlaciAR (Glass-enabled Contextual Interactions for Augmented Reality), that operates by extracting contextual interactions from observing users performing actions. GlaciAR is able to i) automatically determine moments of relevance based on a head motion attention model, ii) automatically produce video guidance information, iii) trigger these video guides based on an object detection method, iv) learn without supervision from observing multiple users and v) operate fully on-board a current eyewear computer (Google Glass). We describe the components of GlaciAR together with evaluations on how users are able to use the system to achieve three tasks. We see this work as a first step toward the development of systems that aim to scale up the notoriously difficult authoring problem in guidance systems and where people's natural abilities are enhanced via minimally invasive visual guidance.\",\n",
       " 'Automatic optimization of spoken dialog management policies that are robust to environmental noise has long been the goal for both academia and industry. Approaches based on reinforcement learning have been proved to be effective. However, the numerical representation of dialog policy is human-incomprehensible and difficult for dialog system designers to verify or modify, which limits its practical application. In this paper we propose a novel framework for optimizing dialog policies specified in domain language using genetic algorithm. The human-interpretable representation of policy makes the method suitable for practical employment. We present learning algorithms using user simulation and real human-machine dialogs respectively.Empirical experimental results are given to show the effectiveness of the proposed approach.',\n",
       " \"Game-based technologies and mobile learning aids open up many opportunities for learners; however, evidence-based decisions on their appropriate use are necessary. This explorative study (N = 100) examines the role of game elements in university education using a game-based learning app for mobile devices. The educational goal of the app is to support students in the field of engineering to memorize factual knowledge. The study investigates how the game-based app affects learners' motivation. It analyses the perceived impact and appeal as well as the game elements as an incentive in learners' perception. To realize this aim, the study combines structured methods like questionnaires with semi-structured methods like thinking aloud, game diaries, and interviews. The results indicate that flexible tem-poral and spatial use of the app was an important factor of learners' motivation. The app allowed more spontaneous involvement with the subject matter and the learners took advantage of an improved attitude toward the subject matter. However, only a low impact on intrinsic motivation could be observed. We discuss reasons and present practical implications.\",\n",
       " 'Human perception of surrounding events is strongly dependent on audio cues. Thus, acoustic insulation can seriously impact situational awareness. We present an exploratory study in the domain of assistive computing, eliciting requirements and presenting solutions to problems found in the development of an environmental sound recognition system, which aims to assist deaf and hard of hearing people in the perception of sounds. To take advantage of smartphones computational ubiquity, we propose a system that executes all processing on the device itself, from audio features extraction to recognition and visual presentation of results. Our application also presents the confidence level of the classification to the user. A test of the system conducted with deaf users provided important and inspiring feedback from participants.',\n",
       " 'This research aims to quantify human walking patterns through depth cameras to (1) detect walking pattern changes of a person with and without a motion-restricting device or a walking aid, and to (2) identify distinct walking patterns from different persons of similar physical attributes. Microsoft Kinect devices, often used for video games, were used to provide and track coordinates of 25 different joints of people over time to form a human skeleton. Then multiple machine learning (ML) models were applied to the SE datasets from ten college-age subjects - five males and five females. In particular, ML models were applied to classify subjects into two categories: normal walking and abnormal walking (i.e. with motion-restricting devices). The best ML model (K-nearest neighborhood) was able to predict 97.3% accuracy using 10-fold cross-validation. Finally, ML models were applied to classify five gait conditions: walking normally, walking while wearing the ankle brace, walking while wearing the ACL brace, walking while using a cane, and walking while using a walker. The best ML model was again the K-nearest neighborhood performing at 98.7% accuracy rate.',\n",
       " 'Three-dimension will be a characteristic of future user interfaces, although we are just starting to gain an understanding of how users can navigate and share information within a virtual 3D environment. Three-dimensional graphical user interfaces (3D-GUI) raise many issues of design, metaphor and usability. This research is devoted to designing a 3D-GUI as a front-end tool for a file management system, in this case, for Microsoft Windows\\\\c{opyright} Explorer; as well as evaluating the efficiency of a 3D application. The software design was implemented by extending the Half-Life 3D engine. This extension provides a directory traversal and basic file management functions, like cut, copy, paste, delete, and so on. This paper shows the design and implementation of a real-world application that contains an efficient 3D-GUI.',\n",
       " 'Developers are usually unaware of the impact of code changes to the performance of software systems. Although developers can analyze the performance of a system by executing, for instance, a performance test to compare the performance of two consecutive versions of the system, changing from a programming task to a testing task would disrupt the development flow. In this paper, we propose the use of a city visualization that dynamically provides developers with a pervasive view of the continuous performance of a system. We use an immersive augmented reality device (Microsoft HoloLens) to display our visualization and extend the integrated development environment on a computer screen to use the physical space. We report on technical details of the design and implementation of our visualization tool, and discuss early feedback that we collected of its usability. Our investigation explores a new visual metaphor to support the exploration and analysis of possibly very large and multidimensional performance data. Our initial result indicates that the city metaphor can be adequate to analyze dynamic performance data on a large and non-trivial software system.',\n",
       " 'In situ self-report is widely used in human-computer interaction, ubiquitous computing, and for assessment and intervention in health and wellness. Unfortunately, it remains limited by high burdens. We examine unlock journaling as an alternative. Specifically, we build upon recent work to introduce single slide unlock journaling gestures appropriate for health and wellness measures. We then present the first field study comparing unlock journaling with traditional diaries and notification based reminders in self report of health and wellness measures. We find unlock journaling is less intrusive than reminders, dramatically improves frequency of journaling, and can provide equal or better timeliness. Where appropriate to broader design needs, unlock journaling is thus an overall promising method for in situ self report.',\n",
       " 'Building a deployable PhysiComp that merges form and function typically involves a significant investment of time and skill in digital electronics, 3D modeling and mechanical design. We aim to help designers quickly create prototypes by removing technical barriers in that process. Other methods for constructing PhysiComp prototypes either lack fidelity in representing shape and function or are confined to use in the studio next to a workstation, camera or projector system. Software 3D CAD tools can be used to design the shape but do not provide immediate tactile feedback on fit and feel. In this work, sculpting around 3D printed replicas of electronics combines electronics and form in a fluid design environment. The sculptures are scanned, modified for assembly and then printed on a 3D printer. Using this process, functional prototypes can be created with about 4 hours of focused effort over a day and a half with most of that time spent waiting for the 3D printer. The process lends itself to concurrent exploration of several designs and to rapid iteration. This allows the design process to converge quickly to a PhysiComp that is comfortable and useful.',\n",
       " 'Objectives: This paper presents an up-to-date overview of research performed in the Virtual Reality (VR) environment ranging from definitions, its presence in the various fields, and existing market players and their projects in the VR technology. Further an attempt is made to gain an insight on the psychological mechanism underlying experience in using VR device. Methods: Our literature survey is based on the research articles, analysis of the projects of various companies and their findings for different areas of interest. Findings: In our literature survey we observed that the recent advances in virtual reality enabling technologies have led to variety of virtual devices that facilitate people to interact with the digital world. In fact in the past two decades researchers have tried to integrate reality and VR in the form of intuitive computer interface. Improvements: This has led to variety of potential benefits of VR in many applications such as News, Healthcare, Entertainment, Tourism, Military and Defence etc. However despite the extensive research efforts in creating virtual system environments it is yet to become apparent in normal daily life.',\n",
       " 'Based on a large data set of emoji using behavior collected from smartphone users over the world, this paper investigates gender-specific usage of emojis. We present various interesting findings that evidence a considerable difference in emoji usage by female and male users. Such a difference is significant not just in a statistical sense; it is sufficient for a machine learning algorithm to accurately infer the gender of a user purely based on the emojis used in their messages. In real world scenarios where gender inference is a necessity, models based on emojis have unique advantages over existing models that are based on textual or contextual information. Emojis not only provide language-independent indicators, but also alleviate the risk of leaking private user information through the analysis of text and metadata.',\n",
       " 'Wearable cameras allow people to record their daily activities from a user-centered (First Person Vision) perspective. Due to their favorable location, wearable cameras frequently capture the hands of the user, and may thus represent a promising user-machine interaction tool for different applications. Existent First Person Vision methods handle hand segmentation as a background-foreground problem, ignoring two important facts: i) hands are not a single \"skin-like\" moving element, but a pair of interacting cooperative entities, ii) close hand interactions may lead to hand-to-hand occlusions and, as a consequence, create a single hand-like segment. These facts complicate a proper understanding of hand movements and interactions. Our approach extends traditional background-foreground strategies, by including a hand-identification step (left-right) based on a Maxwell distribution of angle and position. Hand-to-hand occlusions are addressed by exploiting temporal superpixels. The experimental results show that, in addition to a reliable left/right hand-segmentation, our approach considerably improves the traditional background-foreground hand-segmentation.',\n",
       " 'Despite significant research that was influenced by Situationally-Induced Impairments and Disabilities (SIIDs) to improve the accessibility of mobile technology, there is still lack of awareness on how to design for SIIDs. Designing for situational impairments does not only affect usability for people who have temporary or long-term disabilities, but also for the \"ideal\" users who get impacted. Limited resources on how to design for situational impairments overlook inclusive interactions and hinder the creation of accessible technology. Thus, I am going to create method cards that can be used during the design process to figure out how to get designers to design for SIIDs. These method cards help us better understand how to improve the design process by addressing the subject of how to design in order to reduce SIIDs.',\n",
       " \"Recent research has exposed disagreements over the nature and usefulness of what may (or may not) be Human-Computer Interaction's fundamental phenomenon: 'interaction'. For some, HCI's theorising about interaction has been deficient, impacting its capacity to inform decisions in design, suggesting the need either to perform first-principles definition work or broader administrative clarification and formalisation of the multitude of formulations of the concepts of interaction and their particular uses. For others, there remain open questions over the continued relevance of certain 'versions' of interaction as a useful concept in HCI at all. We pursue a different perspective in this paper, reviewing how HCI treats interaction through examining its 'conceptual pragmatics' within HCI's discourse. We argue that articulations of the concepts of interaction can be a site of productive conflict for HCI that for many reasons may resist attempts of formalisation as well as attempts to dispense with them. The main contribution of this paper is in specifying how we might go about talking of interaction and the value of interaction language as promiscuous concepts.\",\n",
       " \"This paper presents a game based on storytelling, in which the players are faced with ethical dilemmas related to software engineering specific issues. The players' choices have consequences on how the story unfolds and could lead to various alternative endings. This Ethics Game was used as a tool to mediate the learning activity and it was evaluated by 144 students during a Software Engineering Course on the 2017-2018 academic year. This evaluation was based on a within-subject pre-post design methodology and provided insights on the students learning gain (academic performance), as well as on the students' perceived educational experience. In addition, it provided the results of the students' usability evaluation of the Ethics Game. The results indicated that the students did improve their knowledge about software engineering ethics by playing this game. Also, they considered this game to be a useful educational tool and of high usability. Female students had statistically significant higher knowledge gain and higher evaluation scores than male students, while no statistically significant differences were measured in groups based on the year of study.\",\n",
       " 'Activity tracking devices have found its way in the world of cycling. With its projected market demand and increasing popularity of cycling in the Philippines, cyclists are slowly adopting this technology in their daily cycling routines. Activity trackers demonstrate real-time data which allow cyclists to adjust physical efforts to achieve their personal goals. Six common features of activity trackers were formed as constructs to explore its influence on health empowerment in the context of cycling using Partial Least Squares Structural Equation Model. A total of 393 cyclists in the Philippines participated in the study. Some features demonstrated strong evidence of positive influence in achieving health empowerment and commitment. Implications for future design and development of this technology device are discussed.',\n",
       " \"In this paper, we present a tool to assess users ability to change tasks. To do this, we use a variation of the Box and Blocks Test. In this version, a humanoid robot instructs a user to perform a task involving the movement of certain colored blocks. The robot changes randomly change the color of blocks that the user is supposed to move. Canny Edge Detection and Hough Transformation are used to assess user perform the robot's built-in camera. This will allow the robot to inform the user and keep a log of their progress. We present this method for monitoring user progress by describing how the moved blocks are detected. We also present the results of a pilot study where users used this system to perform the task. Preliminary results show that users do not perform differently when the task is changed in this scenario.\",\n",
       " 'In this paper, we investigate the effectiveness of two distinct techniques (Special Moment Approach & Spatial Frequency Approach) for reviewing the lifelogs, which were collected by lifeloggers who were willing to use a wearable camera and a bracelet simultaneously for two days. Generally, Special moment approach is a technique for extracting episodic events and Spatial frequency approach is a technique for associating visual with temporal and location information, especially heat map is applied as the spatial data for expressing frequency awareness. Based on that, the participants were asked to fill in two post-study questionnaires for evaluating the effectiveness of those two techniques and their combination. The preliminary result showed the positive potential of exploring individual lifelogs using our approaches.',\n",
       " \"Online learners spend millions of hours per year testing their new skills on assignments with known answers. This paper explores whether framing research questions as assignments with unknown answers helps learners generate novel, useful, and difficult-to-find knowledge while increasing their motivation by contributing to a larger goal. Collaborating with the American Gut Project, the world's largest crowdfunded citizen science project, we deploy Gut Instinct to allow novices to generate hypotheses about the constitution of the human gut microbiome. The tool enables online learners to explore learning material about the microbiome and create their own theories around causal variances for microbiome. Building on crowdsourcing or serious games that use people as replaceable units, this work-in-progress lays our plans for how people (a) use their personal knowledge (b) towards solving a larger real-world goal (c) that can provide potential benefits to them. We hope to demonstrate that Gut Instinct citizen scientists generate useful hypotheses, perform better on learning tasks than traditional MOOC learners, and are better engaged with the learning material.\",\n",
       " 'The number of user reviews of tourist attractions, restaurants, mobile apps, etc. is increasing for all languages; yet, research is lacking on how reviews in multiple languages should be aggregated and displayed. Speakers of different languages may have consistently different experiences, e.g., different information available in different languages at tourist attractions or different user experiences with software due to internationalization/localization choices. This paper assesses the similarity in the ratings given by speakers of different languages to London tourist attractions on TripAdvisor. The correlations between different languages are generally high, but some language pairs are more correlated than others. The results question the common practice of computing average ratings from reviews in many languages.',\n",
       " 'While emerging deep-learning systems have outclassed knowledge-based approaches in many tasks, their application to detection tasks for autonomous technologies remains an open field for scientific exploration. Broadly, there are two major developmental bottlenecks: the unavailability of comprehensively labeled datasets and of expressive evaluation strategies. Approaches for labeling datasets have relied on intensive hand-engineering, and strategies for evaluating learning systems have been unable to identify failure-case scenarios. Human intelligence offers an untapped approach for breaking through these bottlenecks. This paper introduces Driverseat, a technology for embedding crowds around learning systems for autonomous driving. Driverseat utilizes crowd contributions for (a) collecting complex 3D labels and (b) tagging diverse scenarios for ready evaluation of learning systems. We demonstrate how Driverseat can crowdstrap a convolutional neural network on the lane-detection task. More generally, crowdstrapping introduces a valuable paradigm for any technology that can benefit from leveraging the powerful combination of human and computer intelligence.',\n",
       " 'The efficient and timely access to patient data is essential for successful patient treatment in clinical wards. Even though, most of the patient data is stored electronically, e.g. in the electronic health record (EHR), relevant patient information is frequently captured, processed and passed in non-electronic form in day-to-day ward routines, e.g. during ward rounds or at shift handover. Following a disruptive design mode, we present a design and development approach comprising a fundamental visualization concept that is refined in a feedback-loop between computer scientists, sociologists and physicians in order to support peri-operative collaborative workflows on a neurosurgical hospital ward. The resulting prototype realizes on-patient visualization methods using an anatomical avatar. It allows for the visual access of medical data with spatial reference relevant for spinal disc herniation and it handles various quantitative and qualitative medical data related to the same anatomical structure as well as referring to hidden and/or small anatomical structures with limited detectability. Furthermore, temporal changes of medical data are made accessible.',\n",
       " 'Today, many of the home automation systems deployed are mostly controlled by humans. This control by humans restricts the automation of home appliances to an extent. Also, most of the deployed home automation systems use the Internet of Things technology to control the appliances. In this paper, we propose a system developed using action recognition to fully automate the home appliances. We recognize the three actions of a person (sitting, standing and lying) along with the recognition of an empty room. The accuracy of the system was 90% in the real-life test experiments. With this system, we remove the human intervention in home automation systems for controlling the home appliances and at the same time we ensure the data privacy and reduce the energy consumption by efficiently and optimally using home appliances.',\n",
       " 'Differential privacy is a promising framework for addressing the privacy concerns in sharing sensitive datasets for others to analyze. However differential privacy is a highly technical area and current deployments often require experts to write code, tune parameters, and optimize the trade-off between the privacy and accuracy of statistical releases. For differential privacy to achieve its potential for wide impact, it is important to design usable systems that enable differential privacy to be used by ordinary data owners and analysts. PSI is a tool that was designed for this purpose, allowing researchers to release useful differentially private statistical information about their datasets without being experts in computer science, statistics, or privacy. We conducted a thorough usability study of PSI to test whether it accomplishes its goal of usability by non-experts. The usability test illuminated which features of PSI are most user-friendly and prompted us to improve aspects of the tool that caused confusion. The test also highlighted some general principles and lessons for designing usable systems for differential privacy, which we discuss in depth.',\n",
       " 'Analyzing large, multivariate graphs is an important problem in many domains, yet such graphs are challenging to visualize. In this paper, we introduce a novel, scalable, tree+table multivariate graph visualization technique, which makes many tasks related to multivariate graph analysis easier to achieve. The core principle we follow is to selectively query for nodes or subgraphs of interest and visualize these subgraphs as a spanning tree of the graph. The tree is laid out linearly, which enables us to juxtapose the nodes with a table visualization where diverse attributes can be shown. We also use this table as an adjacency matrix, so that the resulting technique is a hybrid node-link/adjacency matrix technique. We implement this concept in Juniper and complement it with a set of interaction techniques that enable analysts to dynamically grow, restructure, and aggregate the tree, as well as change the layout or show paths between nodes. We demonstrate the utility of our tool in usage scenarios for different multivariate networks: a bipartite network of scholars, papers, and citation metrics and a multitype network of story characters, places, books, etc.',\n",
       " 'We build on the increasing availability of Virtual Reality (VR) devices and Web technologies to conduct behavioral experiments in VR using crowdsourcing techniques. A new recruiting and validation method allows us to create a panel of eligible experiment participants recruited from Amazon Mechanical Turk. Using this panel, we ran three different crowdsourced VR experiments, each reproducing one of three VR illusions: place illusion, embodiment illusion, and plausibility illusion. Our experience and worker feedback on these experiments show that conducting Web-based VR experiments using crowdsourcing is already feasible, though some challenges---including scale---remain. Such crowdsourced VR experiments on the Web have the potential to finally support replicable VR experiments with diverse populations at a low cost.',\n",
       " 'Visualizing a complex network is computationally intensive process and depends heavily on the number of components in the network. One way to solve this problem is not to render the network in real time. PRE-render Content Using Tiles (PRECUT) is a process to convert any complex network into a pre-rendered network. Tiles are generated from pre-rendered images at different zoom levels, and navigating the network simply becomes delivering relevant tiles. PRECUT is exemplified by performing large-scale compound-target relationship analyses. Matched molecular pair (MMP) networks were created using compounds and the target class description found in the ChEMBL database. To visualize MMP networks, the MMP network viewer has been implemented in COMBINE and as a web application, hosted at http://cheminformatic.com/mmpnet/.',\n",
       " 'In this paper, we present CrowdTone, a system designed to help people set the appropriate tone in their email communication. CrowdTone utilizes the context and content of an email message to identify and set the appropriate tone through a consensus-building process executed by crowd workers. We evaluated CrowdTone with 22 participants, who provided a total of 29 emails that they had received in the past, and ran them through CrowdTone. Participants and professional writers assessed the quality of improvements finding a substantial increase in the percentage of emails deemed \"appropriate\" or \"very appropriate\" - from 25% to more than 90% by recipients, and from 45% to 90% by professional writers. Additionally, the recipients\\' feedback indicated that more than 90% of the CrowdTone processed emails showed improvement.',\n",
       " 'Good code quality is a prerequisite for efficiently developing maintainable software. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that report code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept could generalize to multivariate data and report lessons learned in a broader scope.',\n",
       " 'PeyeDF is a Portable Document Format (PDF) reader with eye tracking support, available as free and open source software. It is especially useful to researchers investigating reading and learning phenomena, as it integrates PDF reading-related behavioural data with gaze-related data. It is suitable for short and long-term research and supports multiple eye tracking systems. We utilised it to conduct an experiment which demonstrated that features obtained from both gaze and reading data collected in the past can predict reading comprehension which takes place in the future. PeyeDF also provides an integrated means for data collection and indexing using the DiMe personal data storage system. It is designed to collect data in the background without interfering with the reading experience, behaving like a modern lightweight PDF reader. Moreover, it supports annotations, tagging and collaborative work. A modular design allows the application to be easily modified in order to support additional eye tracking protocols and run controlled experiments. We discuss the implementation of the software and report on the results of the experiment which we conducted with it.',\n",
       " 'We present a framework to identify whether a public speaker\\'s body movements are meaningful or non-meaningful (\"Mannerisms\") in the context of their speeches. In a dataset of 84 public speaking videos from 28 individuals, we extract 314 unique body movement patterns (e.g. pacing, gesturing, shifting body weights, etc.). Online workers and the speakers themselves annotated the meaningfulness of the patterns. We extracted five types of features from the audio-video recordings: disfluency, prosody, body movements, facial, and lexical. We use linear classifiers to predict the annotations with AUC up to 0.82. Analysis of the classifier weights reveals that it puts larger weights on the lexical features while predicting self-annotations. Contrastingly, it puts a larger weight on prosody features while predicting audience annotations. This analysis might provide subtle hint that public speakers tend to focus more on the verbal features while evaluating self-performances. The audience, on the other hand, tends to focus more on the non-verbal aspects of the speech. The dataset and code associated with this work has been released for peer review and further analysis.',\n",
       " 'Data visualizations typically show retrospective views of an existing dataset with little or no focus on repeatability. However, consumers of these tools often use insights gleaned from retrospective visualizations as the basis for decisions about future events. In this way, visualizations often serve as visual predictive models despite the fact that they are typically designed to present historical views of the data. This \"visual predictive model\" approach, however, can lead to invalid inferences. In this paper, we describe an approach to visual model validation called Inline Replication (IR) which, similar to the cross-validation technique used widely in machine learning, provides a nonparametric and broadly applicable technique for visual model assessment and repeatability. This paper describes the overall IR process and outlines how it can be integrated into both traditional and emerging \"big data\" visualization pipelines. Examples are provided showing IR integrated within common visualization techniques (such as bar charts and linear regression lines) as well as a more fully-featured visualization system designed for complex exploratory analysis tasks.',\n",
       " 'Feedback has been shown to affect performance when using a Brain-Computer Interface (BCI) based on sensorimotor rhythms. In contrast, little is known about the influence of feedback on P300-based BCIs. There is still an open question whether feedback affects the regulation of P300 and consequently the operation of P300-based BCIs. In this paper, for the first time, the influence of feedback on the P300-based BCI speller task is systematically assessed. For this purpose, 24 healthy participants performed the classic P300-based BCI speller task, while only half of them received feedback. Importantly, the number of flashes per letter was reduced on a regular basis in order to increase the frequency of providing feedback. Experimental results showed that feedback could significantly improve the P300-based BCI speller performance, if it was provided in short time intervals (e.g. in sequences as short as 4 to 6 flashes per row/column). Moreover, our offline analysis showed that providing feedback remarkably enhanced the relevant ERP patterns and attenuated the irrelevant ERP patterns, such that the discrimination between target and nontarget EEG trials increased.',\n",
       " 'In this paper we present a method which aims to improve the spelling of children with dyslexia through playful and targeted exercises. In contrast to previous approaches, our method does not use correct words or positive examples to follow, but presents the child a misspelled word as an exercise to solve. We created these training exercises on the basis of the linguistic knowledge extracted from the errors found in texts written by children with dyslexia. To test the effectiveness of this method in Spanish, we integrated the exercises in a game for iPad, DysEggxia (Piruletras in Spanish), and carried out a within-subject experiment. During eight weeks, 48 children played either DysEggxia or Word Search, which is another word game. We conducted tests and questionnaires at the beginning of the study, after four weeks when the games were switched, and at the end of the study. The children who played DysEggxia for four weeks in a row had significantly less writing errors in the tests that after playing Word Search for the same time. This provides evidence that error-based exercises presented in a tablet help children with dyslexia improve their spelling skills.',\n",
       " 'P300 is an electric signal emitted by brain about 300 milliseconds after a rare, but relevant-for-the-user event. One of the applications of this signal is sentence spelling that enables subjects who lost the control of their motor pathways to communicate by selecting characters in a matrix containing all the alphabet symbols. Although this technology has made considerable progress in the last years, it still suffers from both low communication rate and high error rate. This article presents a P300 speller, named PolyMorph, that introduces two major novelties in the field: the selection matrix polymorphism, that reduces the size of the selection matrix itself by removing useless symbols, and sentence-based predictions, that exploit all the spelt characters of a sentence to determine the probability of a word. In order to measure the effectiveness of the presented speller, we describe two sets of tests: the first one in vivo and the second one in silico. The results of these experiments suggest that the use of PolyMorph in place of the naive character-by-character speller both increases the number of spelt characters per time unit and reduces the error rate.',\n",
       " \"Digital prototyping and evaluation using 3D modeling and digital human models are becoming more practical for customizing products to the preference of a user. However, the 3D modeling is less accessible to casual users, and digital human models suffer from insufficient body data and less intuitive illustration on how people use the product or how it accommodates to their body. Recently, VR-supported 'Do It Yourself' design has achieved real-time ergonomic evaluation with users themselves by capturing their poses, however, it lacks reliability and quality of design. In this paper, we explore a multi-person interactive design approach that enables designer, user, and even ergonomist to collaborate to achieve the effective and reliable design and prototyping tasks. Mixed Reality that utilizes Hololens and motion tracking devices had been developed to provide instant design feedback and evaluation, and to experience prototyping in physical space. We evaluate the system based on the usability study, where casual users and designers are engaged in the interactive process of designing items with respect to the body information, the preference, and the environment.\",\n",
       " \"Consider the following problem faced by an online voting platform: A user is provided with a list of alternatives, and is asked to rank them in order of preference using only drag-and-drop operations. The platform's goal is to recommend an initial ranking that minimizes the time spent by the user in arriving at her desired ranking. We develop the first optimization framework to address this problem, and make theoretical as well as practical contributions. On the practical side, our experiments on Amazon Mechanical Turk provide two interesting insights about user behavior: First, that users' ranking strategies closely resemble selection or insertion sort, and second, that the time taken for a drag-and-drop operation depends linearly on the number of positions moved. These insights directly motivate our theoretical model of the optimization problem. We show that computing an optimal recommendation is NP-hard, and provide exact and approximation algorithms for a variety of special cases of the problem. Experimental evaluation on MTurk shows that, compared to a random recommendation strategy, the proposed approach reduces the (average) time-to-rank by up to 50%.\",\n",
       " 'We describe the experimental procedures for a dataset that we have made publicly available at https://doi.org/10.5281/zenodo.2649006 in mat and csv formats. This dataset contains electroencephalographic (EEG) recordings of 25 subjects testing the Brain Invaders (Congedo, 2011), a visual P300 Brain-Computer Interface inspired by the famous vintage video game Space Invaders (Taito, Tokyo, Japan). The visual P300 is an event-related potential elicited by a visual stimulation, peaking 240-600 ms after stimulus onset. EEG data were recorded by 16 electrodes in an experiment that took place in the GIPSA-lab, Grenoble, France, in 2012 (Van Veen, 2013 and Congedo, 2013). Python code for manipulating the data is available at https://github.com/plcrodrigues/py.BI.EEG.2012-GIPSA. The ID of this dataset is BI.EEG.2012-GIPSA.',\n",
       " 'Security products often create more problems than they solve, drowning users in alerts without providing the context required to remediate threats. This challenge is compounded by a lack of experienced personnel and security tools with complex interfaces. These interfaces require users to become domain experts or rely on repetitive, time consuming tasks to turn this data deluge into actionable intelligence. In this paper we present Artemis, a conversational interface to endpoint detection and response (EDR) event data. Artemis leverages dialog to drive the automation of complex tasks and reduce the need to learn a structured query language. Designed to empower inexperienced and junior security workers to better understand their security environment, Artemis provides an intuitive platform to ask questions of alert data as users are guided through triage and hunt workflows. In this paper, we will discuss our user-centric design methodology, feedback from user interviews, and the design requirements generated upon completion of our study. We will also present core functionality, findings from scenario-based testing, and future research for the Artemis platform.',\n",
       " \"As autonomous vehicles have benefited the society, understanding the dynamic change of humans' trust during human-autonomous vehicle interaction can help to improve the safety and performance of autonomous driving. We designed and conducted a human subjects study involving 19 participants. Each participant was asked to enter their trust level in a Likert scale in real-time during experiments on a driving simulator. We also collected physiological data (e.g., heart rate, pupil size) of participants as complementary indicators of trust. We used analysis of variance (ANOVA) and Signal Temporal Logic (STL) to analyze the experimental data. Our results show the influence of different factors (e.g., automation alarms, weather conditions) on trust, and the individual variability in human reaction time and trust change.\",\n",
       " 'Artificial objects often subjectively look eerie when their appearance to some extent resembles a human, which is known as the uncanny valley phenomenon. From a cognitive psychology perspective, several explanations of the phenomenon have been put forth, two of which are object categorization and realism inconsistency. Recently, MacDorman and Chattopadhyay (2016) reported experimental data as evidence in support of the latter. In our estimation, however, their results are still consistent with categorization-based stranger avoidance. In this Discussions paper, we try to describe why categorization-based stranger avoidance remains a viable explanation, despite the evidence of MacDorman and Chattopadhyay, and how it offers a more inclusive explanation of the impression of eeriness in the uncanny valley phenomenon.',\n",
       " 'Classifying human cognitive states from behavioral and physiological signals is a challenging problem with important applications in robotics. The problem is challenging due to the data variability among individual users, and sensor artefacts. In this work, we propose an end-to-end framework for real-time cognitive workload classification with mixture Hyper Long Short Term Memory Networks, a novel variant of HyperNetworks. Evaluating the proposed approach on an eye-gaze pattern dataset collected from simulated driving scenarios of different cognitive demands, we show that the proposed framework outperforms previous baseline methods and achieves 83.9\\\\% precision and 87.8\\\\% recall during test. We also demonstrate the merit of our proposed architecture by showing improved performance over other LSTM-based methods.',\n",
       " 'Human Body Communication (HBC) has recently emerged as an alternative to radio frequency transmission for connecting devices on and in the human body with order(s) of magnitude lower energy. The communication between these devices can give rise to different scenarios, which can be classified as wearable-wearable, wearable-machine, machine-machine interactions. In this paper, for the first time, the human body channel characteristics is measured for a wide range of such possible scenarios (14 vs. a few in previous literature) and classified according to the form-factor of the transmitter and receiver. The effect of excitation/termination configurations on the channel loss is also explored, which helps explain the previously unexplained wide variation in HBC Channel measurements. Measurement results show that wearable-wearable interaction has the maximum loss (upto -50 dB) followed by wearable-machine and machinemachine interaction (min loss of 0.5 dB), primarily due to the small ground size of the wearable devices. Among the excitation configurations, differential excitation is suitable for small channel length whereas single ended is better for longer channel.',\n",
       " 'This article investigates graph analysis for intelligent marketing in smart cities, where metatrails are crowdsourced by mobile sensing for marketing strategies. Unlike most works that focused on client sides, this study is intended for market planning, from the perspective of enterprises. Several novel crowdsourced features based on metatrails, including hotspot networks, crowd transitions, affinity subnetworks, and sequential visiting patterns, are discussed in the article. These smart footprints can reflect crowd preferences and the topology of a site of interest. Marketers can utilize such information for commercial resource planning and deployment. Simulations were conducted to demonstrate the performance. At the end, this study also discusses different scenarios for practical geo-conquesting applications.',\n",
       " 'We propose the concept of Speculative Execution for Visual Analytics and discuss its effectiveness for model exploration and optimization. Speculative Execution enables the automatic generation of alternative, competing model configurations that do not alter the current model state unless explicitly confirmed by the user. These alternatives are computed based on either user interactions or model quality measures and can be explored using delta-visualizations. By automatically proposing modeling alternatives, systems employing Speculative Execution can shorten the gap between users and models, reduce the confirmation bias and speed up optimization processes. In this paper, we have assembled five application scenarios showcasing the potential of Speculative Execution, as well as a potential for further research.',\n",
       " \"An emotion orientated intelligent interface consists of Emotion Generating Calculations (EGC) and Mental State Transition Network (MSTN). We have developed the Android EGC application software which the agent works to evaluate the feelings in the conversation. In this paper, we develop the tourist information system which can estimate the user's feelings at the sightseeing spot. The system can recommend the sightseeing spot and the local food corresponded to the user's feeling. The system calculates the recommendation list by the estimate function which consists of Google search results, the important degree of a term at the sightseeing website, and the the aroused emotion by EGC. In order to show the effectiveness, this paper describes the experimental results for some situations during Hiroshima sightseeing.\",\n",
       " \"Several techniques for visualization of dynamic graphs are based on different spatial arrangements of a temporal sequence of node-link diagrams. Many studies in the literature have investigated the importance of maintaining the user's mental map across this temporal sequence, but usually each layout is considered as a static graph drawing and the effect of user interaction is disregarded. We conducted a task-based controlled experiment to assess the effectiveness of two basic interaction techniques: the adjustment of the layout stability and the highlighting of adjacent nodes and edges. We found that generally both interaction techniques increase accuracy, sometimes at the cost of longer completion times, and that the highlighting outclasses the stability adjustment for many tasks except the most complex ones.\",\n",
       " \"Research investigating cognitive aspects of information systems is often dependent on detail-rich data. Eye-trackers promise to provide respective data, but the associated costs are often beyond the researchers' budget. Recently, eye-trackers have entered the market that promise eye-tracking support at a reasonable price. In this work, we explore whether such eye-trackers are of use for information systems research and explore the accuracy of a low-cost eye-tracker (Gazepoint GP3) in an empirical study. The results show that Gazepoint GP3 is well suited for respective research, given that experimental material acknowledges the limits of the eye-tracker. To foster replication and comparison of results, all data, experimental material as well as the source code developed for this study are made available online.\",\n",
       " 'This paper explores the potential for using Brain Computer Interfaces (BCI) as a relevance feedback mechanism in content-based image retrieval. We investigate if it is possible to capture useful EEG signals to detect if relevant objects are present in a dataset of realistic and complex images. We perform several experiments using a rapid serial visual presentation (RSVP) of images at different rates (5Hz and 10Hz) on 8 users with different degrees of familiarization with BCI and the dataset. We then use the feedback from the BCI and mouse-based interfaces to retrieve localized objects in a subset of TRECVid images. We show that it is indeed possible to detect such objects in complex images and, also, that users with previous knowledge on the dataset or experience with the RSVP outperform others. When the users have limited time to annotate the images (100 seconds in our experiments) both interfaces are comparable in performance. Comparing our best users in a retrieval task, we found that EEG-based relevance feedback outperforms mouse-based feedback. The realistic and complex image dataset differentiates our work from previous studies on EEG for image retrieval.',\n",
       " \"Gesture interaction is a natural way of communicating with a robot as an alternative to speech. Gesture recognition methods leverage optical flow in order to understand human motion. However, while accurate optical flow estimation (i.e., traditional) methods are costly in terms of runtime, fast estimation (i.e., deep learning) methods' accuracy can be improved. In this paper, we present a pipeline for gesture-based human-robot interaction that uses a novel optical flow estimation method in order to achieve an improved speed-accuracy trade-off. Our optical flow estimation method introduces four improvements to previous deep learning-based methods: strong feature extractors, attention to contours, midway features, and a combination of these three. This results in a better understanding of motion, and a finer representation of silhouettes. In order to evaluate our pipeline, we generated our own dataset, MIBURI, which contains gestures to command a house service robot. In our experiments, we show how our method improves not only optical flow estimation, but also gesture recognition, offering a speed-accuracy trade-off more realistic for practical robot applications.\",\n",
       " \"Recent advancements in mobile devices encourage researchers to utilize them in collaborative environments as a medium to interact with large shared wall-displays. In this paper, we focus on a semi-controlled user study that we conducted to measure the collaborative coupling ratio between partners working in pairs in a collaborative setup equipped with a shared tiled-wall display and multiple mobile devices. We invited 36 participants in 18 pairs to take part in our experiment in order to analyze how they communicate and collaborate with each other during the experiment. We observed their collaborative coupling by measuring how often they verbally and visually communicated. Further, we found frequently used collaborative physical position patterns by observing the pairs' physical arrangements and standing positions. Moreover, we combined these factors to gain a clearer understanding of coupling in our setup, taking into account the mobility factor offered by the mobile devices. Results of the study show interesting findings about the coupling factors between the partners mainly due to the flexibility offered by including mobile devices in our collaborative setup.\",\n",
       " 'The development of real-time affect detection models often depends upon obtaining annotated data for supervised learning by employing human experts to label the student data. One open question in annotating affective data for affect detection is whether the labelers (i.e., human experts) need to be socio-culturally similar to the students being labeled, as this impacts the cost feasibility of obtaining the labels. In this study, we investigate the following research questions: For affective state annotation, how does the socio-cultural background of human expert labelers, compared to the subjects, impact the degree of consensus and distribution of affective states obtained? Secondly, how do differences in labeler background impact the performance of affect detection models that are trained using these labels?',\n",
       " \"The goal of our research is to contribute information about how useful the crowd is at anticipating stereotypes that may be biasing a data set without a researcher's knowledge. The results of the crowd's prediction can potentially be used during data collection to help prevent the suspected stereotypes from introducing bias to the dataset. We conduct our research by asking the crowd on Amazon's Mechanical Turk (AMT) to complete two similar Human Intelligence Tasks (HITs) by suggesting stereotypes relating to their personal experience. Our analysis of these responses focuses on determining the level of diversity in the workers' suggestions and their demographics. Through this process we begin a discussion on how useful the crowd can be in tackling this difficult problem within machine learning data collection.\",\n",
       " \"Microtask crowdsourcing is increasingly critical to the creation of extremely large datasets. As a result, crowd workers spend weeks or months repeating the exact same tasks, making it necessary to understand their behavior over these long periods of time. We utilize three large, longitudinal datasets of nine million annotations collected from Amazon Mechanical Turk to examine claims that workers fatigue or satisfice over these long periods, producing lower quality work. We find that, contrary to these claims, workers are extremely stable in their quality over the entire period. To understand whether workers set their quality based on the task's requirements for acceptance, we then perform an experiment where we vary the required quality for a large crowdsourcing task. Workers did not adjust their quality based on the acceptance threshold: workers who were above the threshold continued working at their usual quality level, and workers below the threshold self-selected themselves out of the task. Capitalizing on this consistency, we demonstrate that it is possible to predict workers' long-term quality using just a glimpse of their quality on the first five tasks.\",\n",
       " 'Showing flows of people and resources between multiple geographic locations is a challenging visualisation problem. We conducted two quantitative user studies to evaluate different visual representations for such dense many-to-many flows. In our first study we compared a bundled node-link flow map representation and OD Maps [37] with a new visualisation we call MapTrix. Like OD Maps, MapTrix overcomes the clutter associated with a traditional flow map while providing geographic embedding that is missing in standard OD matrix representations. We found that OD Maps and MapTrix had similar performance while bundled node-link flow map representations did not scale at all well. Our second study compared participant performance with OD Maps and MapTrix on larger data sets. Again performance was remarkably similar.',\n",
       " \"Crowd predictions have demonstrated powerful performance in predicting future events. We aim to understand crowd prediction efficacy in ascertaining the veracity of human emotional expressions. We discover that collective discernment can increase the accuracy of detecting emotion veracity from 63%, which is the average individual performance, to 80%. Constraining data to best performers can further increase the result up to 92%. Neural networks can achieve an accuracy to 99.69% by aggregating participants' answers. That is, assigning positive and negative weights to high and low human predictors, respectively. Furthermore, neural networks that are trained with one emotion data can also produce high accuracies on discerning the veracity of other emotion types: our crowdsourced transfer of emotion learning is novel. We find that our neural networks do not require a large number of participants, particularly, 30 randomly selected, to achieve high accuracy predictions, better than any individual participant. Our proposed method of assembling peoples' predictions with neural networks can provide insights for applications such as fake news prevention and lie detection.\",\n",
       " 'A device which contains number of symbol input keys, where the number of available keys is less than the number of symbols of an alphabet of any given language, screen, and dynamic reordering table of the symbols which are mapped onto those keys, according to a disambiguation method based on the previously entered symbols. The device incorporates a previously entered keystrokes tracking mechanism, and the key selected by the user detector, as well as a mechanism to select the dynamic symbol reordering mapped onto this key according to the information contained to the reordering table. The reordering table occurs from a disambiguation method which reorders the symbol appearance. The reordering information occurs from Bayesian Belief network construction and training from text corpora of the specific language.',\n",
       " 'Not all smartphone owners use their device in the same way. In this work, we uncover broad, latent patterns of mobile phone use behavior. We conducted a study where, via a dedicated logging app, we collected daily mobile phone activity data from a sample of 340 participants for a period of four weeks. Through an unsupervised learning approach and a methodologically rigorous analysis, we reveal five generic phone use profiles which describe at least 10% of the participants each: limited use, business use, power use, and personality- & externally induced problematic use. We provide evidence that intense mobile phone use alone does not predict negative well-being. Instead, our approach automatically revealed two groups with tendencies for lower well-being, which are characterized by nightly phone use sessions.',\n",
       " 'The ability of human beings to precisely recog- nize others intents is a significant mental activity in reasoning about actions, such as, what other people are doing and what they will do next. Recent research has revealed that human intents could be inferred by measuring human cognitive activities through heterogeneous body and brain sensors (e.g., sensors for detecting physiological signals like ECG, brain signals like EEG and IMU sensors like accelerometers and gyros etc.). In this proposal, we aim at developing a computa- tional framework for enabling reliable and precise real-time human intent recognition by measuring human cognitive and physiological activities through the heterogeneous body and brain sensors for improving human machine interactions, and serving intent-based human activity prediction.',\n",
       " 'Within the last decade, running has become one of the most popular physical activities in the world. Although the benefits of running are numerous, there is a risk of Running Related Injuries (RRI) of the lower extremities. Electromyography (EMG) techniques have previously been used to study causes of RRIs, but the complexity of this technology limits its use to a laboratory setting. As running is primarily an outdoors activity, this lack of technology acts as a barrier to the study of RRIs in natural environments. This study presents a minimally invasive wearable muscle sensing device consisting of jogging leggings with embroidered surface EMG (sEMG) electrodes capable of recording muscle activity data of the quadriceps group. To test the use of the device, a proof of concept study consisting of $N=2$ runners performing a set of $5km$ running trials is presented in which the effect of running surfaces on muscle fatigue, a potential cause of RRIs, is evaluated. Results show that muscle fatigue can be analysed from the sEMG data obtained through the wearable device, and that running on soft surfaces (such as sand) may increase the likelihood of suffering from RRIs.',\n",
       " \"This paper provides interested beginners with an updated and detailed introduction to the field of Intelligent Tutoring Systems (ITS). ITSs are computer programs that use artificial intelligence techniques to enhance and personalize automation in teaching. This paper is a literature review that provides the following: First, a review of the history of ITS along with a discussion on the interface between human learning and computer tutors and how effective ITSs are in contemporary education. Second, the traditional architectural components of an ITS and their functions are discussed along with approaches taken by various ITSs. Finally, recent innovative ideas in ITS systems are presented. This paper concludes with some of the author's views regarding future work in the field of intelligent tutoring systems.\",\n",
       " 'Due to the increasing complexity of modern automatic machines typically used in several industrial applications, the need for assistive technologies is becoming very relevant. Typical approaches consist in designing advanced and adaptive human-machine interfaces (HMIs) that can be effectively used by any operator and that provide guided procedures for the most common situations. However, when dealing with complex systems, infrequent and unforeseen situations may happen, whose solution require the experience owned by a limited number of skilled operators. To this end, in this paper we propose an industrial social network concept to allow an effective exchange of information among the operators and to facilitate the solution of unforeseen events, such as unscheduled maintenance activities or troubleshooting.',\n",
       " 'Human computation systems (HCSs) have been widely adopted in various domains. Their goal is to harness human intelligence to solve computational problems that are beyond the capability of modern computers. One of the most challenging problems in HCSs is how to incentivize a broad range of users to participate in the system and make high efforts. This article surveys the field of HCSs from the perspective of incentives and mechanism design. We first review state-of-the-art HCSs, focusing on how incentives are provided to users. We then use mechanism design to theoretically analyze different incentives. We survey the mechanisms derived from state-of-the-art HCSs as well as classic mechanisms that have been used in HCSs. Finally, we discuss eight promising research directions for designing incentives in HCSs.',\n",
       " \"Ensuring fairness of machine learning systems is a human-in-the-loop process. It relies on developers, users, and the general public to identify fairness problems and make improvements. To facilitate the process we need effective, unbiased, and user-friendly explanations that people can confidently rely on. Towards that end, we conducted an empirical study with four types of programmatically generated explanations to understand how they impact people's fairness judgments of ML systems. With an experiment involving more than 160 Mechanical Turk workers, we show that: 1) Certain explanations are considered inherently less fair, while others can enhance people's confidence in the fairness of the algorithm; 2) Different fairness problems--such as model-wide fairness issues versus case-specific fairness discrepancies--may be more effectively exposed through different styles of explanation; 3) Individual differences, including prior positions and judgment criteria of algorithmic fairness, impact how people react to different styles of explanation. We conclude with a discussion on providing personalized and adaptive explanations to support fairness judgments of ML systems.\",\n",
       " \"For decades, researchers in information visualisation and graph drawing have focused on developing techniques for the layout and display of very large and complex networks. Experiments involving human participants have also explored the readability of different styles of layout and representations for such networks. In both bodies of literature, networks are frequently referred to as being 'large' or 'complex', yet these terms are relative. From a human-centred, experiment point-of-view, what constitutes 'large' (for example) depends on several factors, such as data complexity, visual complexity, and the technology used. In this paper, we survey the literature on human-centred experiments to understand how, in practice, different features and characteristics of node-link diagrams affect visual complexity.\",\n",
       " 'Ensembles of classifier models typically deliver superior performance and can outperform single classifier models given a dataset and classification task at hand. However, the gain in performance comes together with the lack in comprehensibility, posing a challenge to understand how each model affects the classification outputs and where the errors come from. We propose a tight visual integration of the data and the model space for exploring and combining classifier models. We introduce a workflow that builds upon the visual integration and enables the effective exploration of classification outputs and models. We then present a use case in which we start with an ensemble automatically selected by a standard ensemble selection algorithm, and show how we can manipulate models and alternative combinations.',\n",
       " 'Elderly chronic diseases are the main cause of death in the world, accounting 60% of all death. Because elderly with chronic diseases at the early stages has no observed symptoms, and then symptoms starts to appear, it is critical to observe the symptoms as early as possible to avoid any complication. This paper presents an expert system for an Elderly Health Care (EHC) at elderly home tailored for the specific needs of Elderly. The proposed EHC aims to develop an integrated and multidisciplinary method to employ communication technologies and information for covering real health needs of elderly people, mainly of people at high risk due to social and geographic isolation in addition to specific chronic diseases. The proposed EHC provides personalized intervention plans covering chronic diseases such as (body temperature (BT), blood pressure (BP), and Heart beat rate (HR)). The processes and architecture of the proposed EHC are based on the server side and three main clients, one for the elderly and another two for the nurse and the physicians whom take care of them. The proposed EHC model is discussed for proving the usefulness and effectiveness of the expert system.',\n",
       " 'Human computer interaction facilitates intelligent communication between humans and computers, in which gesture recognition plays a prominent role. This paper proposes a machine learning system to identify dynamic gestures using tri-axial acceleration data acquired from two public datasets. These datasets, uWave and Sony, were acquired using accelerometers embedded in Wii remotes and smartwatches, respectively. A dynamic gesture signed by the user is characterized by a generic set of features extracted across time and frequency domains. The system was analyzed from an end-user perspective and was modelled to operate in three modes. The modes of operation determine the subsets of data to be used for training and testing the system. From an initial set of seven classifiers, three were chosen to evaluate each dataset across all modes rendering the system towards mode-neutrality and dataset-independence. The proposed system is able to classify gestures performed at varying speeds with minimum preprocessing, making it computationally efficient. Moreover, this system was found to run on a low-cost embedded platform - Raspberry Pi Zero (USD 5), making it economically viable.',\n",
       " 'Stereoscopic 3D (S3D) displays provide an additional sense of depth compared to non-stereoscopic displays by sending slightly different images to the two eyes. But conventional S3D displays do not reproduce all natural depth cues. In particular, focus cues are incorrect causing mismatches between accommodation and vergence: The eyes must accommodate to the display screen to create sharp retinal images even when binocular disparity drives the eyes to converge to other distances. This mismatch causes visual discomfort and reduces visual performance. We propose and assess two new techniques that are designed to reduce the vergence-accommodation conflict and thereby decrease discomfort and increase visual performance. These techniques are much simpler to implement than previous conflict-reducing techniques.',\n",
       " 'Near Field Communication (NFC) standards cover communications protocols and data exchange formats. They are based on existing radio-frequency identification (RFID) standards. In Japan, Felica card is a popular way to identify the unique ID. Recently, the attendance management system (AMS) with RFID technology has been developed as a part of Smart University, which is the educational infrastructure using high technologies, such as ICT. However, the reader/writer for Felica is too expensive to build the AMS. NFC technology includes not only Felica but other type of IC chips. The Android OS 2.3 and the later can provide access to NFC functionality. Therefore, we developed AMS for university with NFC on Nexus 7. Because Nexus 7 is a low cost smart tablet, a teacher can determine to use familiarly. Especially, this paper describes the method of early discovery for chronic non-attenders by using the AMS system on 2 or more Nexus 7 which is connected each other via peer-to-peer communication. The attendance situation collected from different Nexus 7 is merged into a SQLite file and then, the document is reported to operate with the trunk system in educational affairs section.',\n",
       " 'Conventional HVAC control systems are usually incognizant of the physical structures and materials of buildings. These systems merely follow pre-set HVAC control logic based on abstract building thermal response models, which are rough approximations to true physical models, ignoring dynamic spatial variations in built environments. To enable more accurate and responsive HVAC control, this paper introduces the notion of \"self-aware\" smart buildings, such that buildings are able to explicitly construct physical models of themselves (e.g., incorporating building structures and materials, and thermal flow dynamics). The question is how to enable self-aware buildings that automatically acquire dynamic knowledge of themselves. This paper presents a novel approach using \"augmented reality\". The extensive user-environment interactions in augmented reality not only can provide intuitive user interfaces for building systems, but also can capture the physical structures and possibly materials of buildings accurately to enable real-time building simulation and control. This paper presents a building system prototype incorporating augmented reality, and discusses its applications.',\n",
       " 'Natural Language Processing (NLP) systems often make use of machine learning techniques that are unfamiliar to end-users who are interested in analyzing clinical records. Although NLP has been widely used in extracting information from clinical text, current systems generally do not support model revision based on feedback from domain experts.\\n  We present a prototype tool that allows end users to visualize and review the outputs of an NLP system that extracts binary variables from clinical text. Our tool combines multiple visualizations to help the users understand these results and make any necessary corrections, thus forming a feedback loop and helping improve the accuracy of the NLP models. We have tested our prototype in a formative think-aloud user study with clinicians and researchers involved in colonoscopy research. Results from semi-structured interviews and a System Usability Scale (SUS) analysis show that the users are able to quickly start refining NLP models, despite having very little or no experience with machine learning. Observations from these sessions suggest revisions to the interface to better support review workflow and interpretation of results.',\n",
       " 'Modern industrial automatic machines and robotic cells are equipped with highly complex human-machine interfaces (HMIs) that often prevent human operators from an effective use of the automatic systems. In particular, this applies to vulnerable users, such as those with low experience or education level, the elderly and the disabled. To tackle this issue, it becomes necessary to design user-oriented HMIs, which adapt to the capabilities and skills of users, thus compensating their limitations and taking full advantage of their knowledge. In this paper, we propose a methodological approach to the design of complex adaptive human-machine systems that might be inclusive of all users, in particular the vulnerable ones. The proposed approach takes into account both the technical requirements and the requirements for ethical, legal and social implications (ELSI) for the design of automatic systems. The technical requirements derive from a thorough analysis of three use cases taken from the European project INCLUSIVE. To achieve the ELSI requirements, the MEESTAR approach is combined with the specific legal issues for occupational systems and requirements of the target users.',\n",
       " 'Providing opinions through labeling of images, tweets, etc. have drawn immense interest in crowdsourcing markets. This invokes a major challenge of aggregating multiple opinions received from different crowd workers for deriving the final judgment. Generally, opinion aggregation models deal with independent opinions, which are given unanimously and are not visible to all. However, in many real-life cases, it is required to make the opinions public as soon as they are received. This makes the opinions dependent and might incorporate some bias. In this paper, we address a novel problem, hereafter denoted as dependent judgment analysis, and discuss the requirements for developing an appropriate model to deal with this problem. The challenge remains to be improving the consensus by revealing true opinions.',\n",
       " 'Wikipedia articles about places, OpenStreetMap features, and other forms of peer-produced content have become critical sources of geographic knowledge for humans and intelligent technologies. In this paper, we explore the effectiveness of the peer production model across the rural/urban divide, a divide that has been shown to be an important factor in many online social systems. We find that in both Wikipedia and OpenStreetMap, peer-produced content about rural areas is of systematically lower quality, is less likely to have been produced by contributors who focus on the local area, and is more likely to have been generated by automated software agents (i.e. bots). We then codify the systemic challenges inherent to characterizing rural phenomena through peer production and discuss potential solutions.',\n",
       " 'Despite significant improvements in automatic speech recognition and spoken language understanding - human interaction with Virtual Personal Assistants (VPAs) through speech remains irregular and sporadic. According to recent studies, currently the usage of VPAs is constrained to basic tasks such as checking facts, playing music, and obtaining weather updates.In this paper, we present results of a survey (N = 118) that analyses usage of VPAs by frequent and infrequent users. We investigate how usage experience, performance expectations, and privacy concerns differ between these two groups. The results indicate that, compared with infrequent users, frequent users of VPAs are more satisfied with their assistants, more eager to use them in a variety of settings, yet equally concerned about their privacy.',\n",
       " 'A natural conversational interface that allows longitudinal symptom tracking would be extremely valuable in health/wellness applications. However, the task of designing emotionally-aware agents for behavior change is still poorly understood. In this paper, we present the design and evaluation of an emotion-aware chatbot that conducts experience sampling in an empathetic manner. We evaluate it through a human-subject experiment with N=39 participants over the course of a week. Our results show that extraverts preferred the emotion-aware chatbot significantly more than introverts. Also, participants reported a higher percentage of positive mood reports when interacting with the empathetic bot. Finally, we provide guidelines for the design of emotion-aware chatbots for potential use in mHealth contexts.',\n",
       " 'This paper describes the development of the Microsoft XiaoIce system, the most popular social chatbot in the world. XiaoIce is uniquely designed as an AI companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient (IQ) and emotional quotient (EQ) in system design, cast human-machine social chat as decision-making over Markov Decision Processes (MDPs), and optimize XiaoIce for long-term user engagement, measured in expected Conversation-turns Per Session (CPS). We detail the system architecture and key components including dialogue manager, core chat, skills, and an empathetic computing module. We show how XiaoIce dynamically recognizes human feelings and states, understands user intents, and responds to user needs throughout long conversations. Since the release in 2014, XiaoIce has communicated with over 660 million users and succeeded in establishing long-term relationships with many of them. Analysis of large-scale online logs shows that XiaoIce has achieved an average CPS of 23, which is significantly higher than that of other chatbots and even human conversations.',\n",
       " 'Designing conversational user interface experience is complicated because conversation comes with many expectations. When these expectations are met, we feel the interface is natural, but once violated, we feel something is amiss. The last decade witnessed human language technologies and behaviours to enable humans converse with software using spoken dialogue to access, create and process information. Less is known about the practicalities of designing chatbot interactions. In this paper, we introduce the nature of conversational user interfaces (CUIs) and describe the underlying technologies they are based on. Moreover, we define guidelines for designing conversational interfaces in various domains. This paper particularly focuses on classifying the elements and techniques used in CUI design patterns. After concluding certain challenges with CUI, we discuss important features and chatbot states to be considered in CUI design for specific domain. We envisage this study to support CUI researchers to design tailored chatbots applicable into certain domain and improve the current state of research challenges in the field of Artificial Intelligence and conversational agents.',\n",
       " \"Data collection and analysis in the field is critical for operations in domains such as environmental science and public safety. However, field workers currently face data- and platform-oriented issues in efficient data collection and analysis in the field, such as limited connectivity, screen space, and attentional resources. In this paper, we explore how visual analytics tools might transform field practices by more deeply integrating data into these operations. We use a design probe coupling mobile, cloud and immersive analytics components to guide interviews with ten experts from five domains to explore how visual analytics could support data collection and analysis needs in the field. The results identify shortcomings of current approaches and target scenarios and design considerations for future field analysis systems. We embody these findings in FieldView, an extensible, open-source prototype designed to support critical use cases for situated field analysis. Our findings suggest the potential for integrating mobile and immersive technologies to enhance data's utility for various field operations and new directions for visual analytics tools to transform fieldwork.\",\n",
       " 'Automatic detection of emergent leaders in small groups from nonverbal behaviour is a growing research topic in social signal processing but existing methods were evaluated on single datasets -- an unrealistic assumption for real-world applications in which systems are required to also work in settings unseen at training time. It therefore remains unclear whether current methods for emergent leadership detection generalise to similar but new settings and to which extent. To overcome this limitation, we are the first to study a cross-dataset evaluation setting for the emergent leadership detection task. We provide evaluations for within- and cross-dataset prediction using two current datasets (PAVIS and MPIIGroupInteraction), as well as an investigation on the robustness of commonly used feature channels (visual focus of attention, body pose, facial action units, speaking activity) and online prediction in the cross-dataset setting. Our evaluations show that using pose and eye contact based features, cross-dataset prediction is possible with an accuracy of 0.68, as such providing another important piece of the puzzle towards emergent leadership detection in the real world.',\n",
       " 'With the rapid development of mobile computing, wearable wrist-worn is becoming more and more popular. But the current vibrotactile feedback patterns of most wrist-worn devices are too simple to enable effective interaction in nonvisual scenarios. In this paper, we propose the wristband system with four vibrating motors placed in different positions in the wristband, providing multiple vibration patterns to transmit multi-semantic information for users in eyes-free scenarios. However, we just applied five vibrotactile patterns in experiments (positional up and down, horizontal diagonal, clockwise circular, and total vibration) after contrastive analyzing nine patterns in a pilot experiment. The two experiments with the same 12 participants perform the same experimental process in lab and outdoors. According to the experimental results, users can effectively distinguish the five patterns both in lab and outside, with approximately 90% accuracy (except clockwise circular vibration of outside experiment), proving these five vibration patterns can be used to output multi-semantic information. The system can be applied to eyes-free interaction scenarios for wrist-worn devices.',\n",
       " \"We developed a novel virtual reality [VR] platform with 3-dimensional sounds to help improve sensory integration and visuomotor processing for postural control and fall prevention in individuals with balance problems related to sensory deficits, such as vestibular dysfunction (disease of the inner ear). The system has scenes that simulate scenario-based environments. We can adjust the intensity of the visual and audio stimuli in the virtual scenes by controlling the user interface (UI) settings. A VR headset (HTC Vive or Oculus Rift) delivers stereo display while providing real-time position and orientation of the participants' head. The 3D game-like scenes make participants feel immersed and gradually exposes them to situations that may induce dizziness, anxiety or imbalance in their daily-living.\",\n",
       " \"We analyze the claims that video recreations of shoulder surfing attacks offer a suitable alternative and a baseline, as compared to evaluation in a live setting. We recreated a subset of the factors of a prior video-simulation experiment conducted by Aviv et al. (ACSAC 2017), and model the same scenario using live participants ($n=36$) instead (i.e., the victim and attacker were both present). The live experiment confirmed that for Android's graphical patterns video simulation is consistent with the live setting for attacker success rates. However, both 4- and 6-digit PINs demonstrate statistically significant differences in attacker performance, with live attackers performing as much 1.9x better than in the video simulation. The security benefits gained from removing feedback lines in Android's graphical patterns are also greatly diminished in the live setting, particularly under multiple attacker observations, but overall, the data suggests that video recreations can provide a suitable baseline measure for attacker success rate. However, we caution that researchers should consider that these baselines may greatly underestimate the threat of an attacker in live settings.\",\n",
       " \"Cross cultural research projects are becoming a norm in our global world. More and more projects are being executed using teams from eastern and western cultures. Cultural competence might help project managers to achieve project goals and avoid potential risks in cross cultural project environments and would also support them to promote creativity and motivation through flexible leadership. In our paper we introduce an idea for constructing an information system, a cross cultural knowledge space, which could support cross cultural communication, collaborative learning experiences and time based project management functions. The case cultures in our project are Finnish and Japanese. The system can be used both in virtual and in physical spaces for example to clarify cultural business etiquette. The core of our system design will be based on cross cultural ontology, and the system implementation on XML technologies. Our approach is a practical, step by step example of constructive research. In our paper we shortly describe Hofstede's dimensions for assessing cultures as one example of a larger framework for our study. We also discuss the concept of time in cultural context.\",\n",
       " 'Smartphone users benefit from content with dark color schemes: increasingly common OLED displays are more power efficient the darker the display, and many users prefer a dark display for night time use. Despite these benefits, many applications and the majority of web content are drawn with white backgrounds. There are many partial solutions to darken the displayed content, but none work in all situations. Enter SmartNight, a content-aware solution to dynamically darken content on Android. By trading off content fidelity, Android with SmartNight displays content with nearly 90% lower average picture level. It is implemented in the Android framework, and requires no external support. It seamlessly incorporates existing solutions, making it a bridge between the state-of-the-art and future solutions.',\n",
       " \"In the everyday context, e.g., a household, HMD users remain a part of the social life for Non-HMD users being co-located with them. Due to the social context situations arise that demand interaction between the HMD and the Non-HMD user. We focus on the challenge that the Non-HMD user is not able to interpret the HMD user's state -- e.g., attentiveness; the need for assistance --, as the HMD covers the wearer's face. We propose a front facing display attached to the HMD that supports collaboration by showing the state. We explore the impact of abstract and realistic visualizations for such displays on collaborative performance and social presence in a within-subject user study (N=25). We present to the Non-HMD user (1) a blank screen (baseline), (2) textual representation of the user's state and (3) a representation that looks like the HMD is see-through. The results show positive effects for textual representation on collaborative performance and a positive effect of realistic representation on social presence. We conclude that when developing HMDs we need to take into account the social needs of everyday life to reduce the risk of social separation in a household context.\",\n",
       " 'In this paper we present results from recent experiments that suggest that chess players associate emotions to game situations and reactively use these associations to guide search for planning and problem solving. We describe the design of an instrument for capturing and interpreting multimodal signals of humans engaged in solving challenging problems. We review results from a pilot experiment with human experts engaged in solving challenging problems in Chess that revealed an unexpected observation of rapid changes in emotion as players attempt to solve challenging problems. We propose a cognitive model that describes the process by which subjects select chess chunks for use in interpretation of the game situation and describe initial results from a second experiment designed to test this model.',\n",
       " \"Through a combination of experimental and simulation results, we illustrate that passive recommendations encoded in typical computer user-interfaces (UIs) can subdue users' natural proclivity to access diverse information sources. Inspired by traditional demonstrations of a part-set cueing effect in the cognitive science literature, we performed an online experiment manipulating the operation of the 'New Tab' page for consenting volunteers over a two month period. Examination of their browsing behavior reveals that typical frequency and recency-based methods for displaying websites in these displays subdues users' propensity to access infrequently visited pages compared to a situation wherein no web page icons are displayed on the new tab page. Using a carefully designed simulation study, representing user behavior as a random walk on a graph, we inferred quantitative predictions about the extent to which discovery of new sources of information may be hampered by personalized 'New Tab' recommendations in typical computer UIs. We show that our results are significant at the individual level and explain the potential consequences of the observed suppression in web-exploration.\",\n",
       " 'Affect (emotion) recognition has gained significant attention from researchers in the past decade. Emotion-aware computer systems and devices have many applications ranging from interactive robots, intelligent online tutor to emotion based navigation assistant. In this research data from multiple modalities such as face, head, hand, body and speech was utilized for affect recognition. The research used color and depth sensing device such as Kinect for facial feature extraction and tracking human body joints. Temporal features across multiple frames were used for affect recognition. Event driven decision level fusion was used to combine the results from each individual modality using majority voting to recognize the emotions. The study also implemented affect recognition by matching the features to the rule based emotion templates per modality. Experiments showed that multimodal affect recognition rates using combination of emotion templates and supervised learning were better compared to recognition rates based on supervised learning alone. Recognition rates obtained using temporal feature were higher compared to recognition rates obtained using position based features only.',\n",
       " \"Machine playtesting tools and game moment search engines require exposure to the diversity of a game's state space if they are to report on or index the most interesting moments of possible play. Meanwhile, mobile app distribution services would like to quickly determine if a freshly-uploaded game is fit to be published. Having access to a semantic map of reachable states in the game would enable efficient inference in these applications. However, human gameplay data is expensive to acquire relative to the coverage of a game that it provides. We show that off-the-shelf automatic exploration strategies can explore with an effectiveness comparable to human gameplay on the same timescale. We contribute generic methods for quantifying exploration quality as a function of time and demonstrate our metric on several elementary techniques and human players on a collection of commercial games sampled from multiple game platforms (from Atari 2600 to Nintendo 64). Emphasizing the diversity of states reached and the semantic map extracted, this work makes productive contrast with the focus on finding a behavior policy or optimizing game score used in most automatic game playing research.\",\n",
       " \"To overcome the travelling difficulty for the visually impaired group, this paper presents a novel ETA (Electronic Travel Aids)-smart guiding device in the shape of a pair of eyeglasses for giving these people guidance efficiently and safely. Different from existing works, a novel multi sensor fusion based obstacle avoiding algorithm is proposed, which utilizes both the depth sensor and ultrasonic sensor to solve the problems of detecting small obstacles, and transparent obstacles, e.g. the French door. For totally blind people, three kinds of auditory cues were developed to inform the direction where they can go ahead. Whereas for weak sighted people, visual enhancement which leverages the AR (Augment Reality) technique and integrates the traversable direction is adopted. The prototype consisting of a pair of display glasses and several low cost sensors is developed, and its efficiency and accuracy were tested by a number of users. The experimental results show that the smart guiding glasses can effectively improve the user's travelling experience in complicated indoor environment. Thus it serves as a consumer device for helping the visually impaired people to travel safely.\",\n",
       " 'From the dawn of civilization, people have used folktales and stories to share information and knowledge. After the invention of printing in the 15th century, technology provided helpful yet complicated utilities to exchange ideas. In the present computerized world, the art of storytelling is becoming more influential through the unprecedented multimedia capabilities of computers. In this article, we introduce a state-of-the-art presentation software by which academicians can present nonlinear topics efficiently and sharpen their storytelling skills. We show how the proposed software can improve the scientific presentation style. We conducted a survey to measure the attractiveness of proposed utility among other alternatives. Results show that academicians prefer the proposed platform to others.',\n",
       " 'One of the challenges in affect recognition is accurate estimation of the emotion intensity level. This research proposes development of an affect intensity estimation model based on a weighted sum of classification confidence levels, displacement of feature points and speed of feature point motion. The parameters of the model were calculated from data captured using multiple modalities such as face, body posture, hand movement and speech. A preliminary study was conducted to compare the accuracy of the model with the annotated intensity levels. An emotion intensity scale ranging from 0 to 1 along the arousal dimension in the emotion space was used. Results indicated speech and hand modality significantly contributed in improving accuracy in emotion intensity estimation using the proposed model.',\n",
       " \"Simulations are a pedagogical means of enabling a risk-free way for healthcare practitioners to learn, maintain, or enhance their knowledge and skills. Such simulations should provide an optimum amount of cognitive load to the learner and be tailored to their levels of expertise. However, most current simulations are a one-type-fits-all tool used to train different learners regardless of their existing skills, expertise, and ability to handle cognitive load. To address this problem, we propose an end-to-end framework for a trauma simulation that actively classifies a participant's level of cognitive load and expertise for the development of a dynamically adaptive simulation. To facilitate this solution, trauma simulations were developed for the collection of electrocardiogram (ECG) signals of both novice and expert practitioners. A multitask deep neural network was developed to utilize this data and classify high and low cognitive load, as well as expert and novice participants. A leave-one-subject-out (LOSO) validation was used to evaluate the effectiveness of our model, achieving an accuracy of 89.4% and 96.6% for classification of cognitive load and expertise, respectively.\",\n",
       " \"Automatically monitoring and quantifying stress-induced thermal dynamic information in real-world settings is an extremely important but challenging problem. In this paper, we explore whether we can use mobile thermal imaging to measure the rich physiological cues of mental stress that can be deduced from a person's nose temperature. To answer this question we build i) a framework for monitoring nasal thermal variable patterns continuously and ii) a novel set of thermal variability metrics to capture a richness of the dynamic information. We evaluated our approach in a series of studies including laboratory-based psychosocial stress-induction tasks and real-world factory settings. We demonstrate our approach has the potential for assessing stress responses beyond controlled laboratory settings.\",\n",
       " 'The teaching of abstract physics concepts can be enhanced by incorporating visual and haptic sensory modalities in the classroom, using the correct perspectives. We have developed virtual reality simulations to assist students in learning the Coriolis effect, an apparent deflection on an object in motion when observed from within a rotating frame of reference. Twenty four undergraduate physics students participated in this study. Students were able to feel the forces through feedback on a Novint Falcon device. The assessment results show an improvement in the learning experience and better content retention as compared with traditional instruction methods. We prove that large scale deployment of visuo-haptic reconfigurable applications is now possible and feasible in a science laboratory setup.',\n",
       " 'We propose to fuse two currently separate research lines on novel therapies for stroke rehabilitation: brain-computer interface (BCI) training and transcranial electrical stimulation (TES). Specifically, we show that BCI technology can be used to learn personalized decoding models that relate the global configuration of brain rhythms in individual subjects (as measured by EEG) to their motor performance during 3D reaching movements. We demonstrate that our models capture substantial across-subject heterogeneity, and argue that this heterogeneity is a likely cause of limited effect sizes observed in TES for enhancing motor performance. We conclude by discussing how our personalized models can be used to derive optimal TES parameters, e.g., stimulation site and frequency, for individual patients.',\n",
       " 'New machine learning algorithms are being developed to solve problems in different areas, including music. Intuitive, accessible, and understandable demonstrations of the newly built models could help attract the attention of people from different disciplines and evoke discussions. However, we notice that it has not been a common practice for researchers working on musical machine learning to demonstrate their models in an interactive way. To address this issue, we present in this paper an template that is specifically designed to demonstrate symbolic musical machine learning models on the web. The template comes with a small codebase, is open source, and is meant to be easy to use by any practitioners to implement their own demonstrations. Moreover, its modular design facilitates the reuse of the musical components and accelerates the implementation. We use the template to build interactive demonstrations of four exemplary music generation models. We show that the built-in interactivity and real-time audio rendering of the browser make the demonstration easier to understand and to play with. It also helps researchers to gain insights into different models and to A/B test them.',\n",
       " 'Articulated hand pose estimation is a challenging task for human-computer interaction. The state-of-the-art hand pose estimation algorithms work only with one or a few subjects for which they have been calibrated or trained. Particularly, the hybrid methods based on learning followed by model fitting or model based deep learning do not explicitly consider varying hand shapes and sizes. In this work, we introduce a novel hybrid algorithm for estimating the 3D hand pose as well as bone-lengths of the hand skeleton at the same time, from a single depth image. The proposed CNN architecture learns hand pose parameters and scale parameters associated with the bone-lengths simultaneously. Subsequently, a new hybrid forward kinematics layer employs both parameters to estimate 3D joint positions of the hand. For end-to-end training, we combine three public datasets NYU, ICVL and MSRA-2015 in one unified format to achieve large variation in hand shapes and sizes. Among hybrid methods, our method shows improved accuracy over the state-of-the-art on the combined dataset and the ICVL dataset that contain multiple subjects. Also, our algorithm is demonstrated to work well with unseen images.',\n",
       " \"Theories of knowledge reuse posit two distinct processes: reuse for replication and reuse for innovation. We identify another distinct process, reuse for customization. Reuse for customization is a process in which designers manipulate the parameters of metamodels to produce models that fulfill their personal needs. We test hypotheses about reuse for customization in Thingiverse, a community of designers that shares files for three-dimensional printing. 3D metamodels are reused more often than the 3D models they generate. The reuse of metamodels is amplified when the metamodels are created by designers with greater community experience. Metamodels make the community's design knowledge available for reuse for customization-or further extension of the metamodels, a kind of reuse for innovation.\",\n",
       " 'In this work we present a mobile application we designed and engineered to enable people to log their travels near and far, leave notes behind, and build a community around spaces in between destinations. Our design explores new ground for location-based social computing systems, identifying opportunities where these systems can foster the growth of on-line communities rooted at non-places. In our work we develop, explore, and evaluate several innovative features designed around four usage scenarios: daily commuting, long-distance traveling, quantified traveling, and journaling. We present the results of two small-scale user studies, and one large-scale, world-wide deployment, synthesizing the results as potential opportunities and lessons learned in designing social computing for non-places.',\n",
       " 'In contrast to typical laboratory experiments, the everyday use of online educational resources by large populations and the prevalence of software infrastructure for A/B testing leads us to consider how platforms can embed in vivo experiments that do not merely support research, but ensure practical improvements to their educational components. Examples are presented of randomized experimental comparisons conducted by subsets of the authors in three widely used online educational platforms Khan Academy, edX, and ASSISTments. We suggest design principles for platform technology to support randomized experiments that lead to practical improvements enabling Iterative Improvement and Collaborative Work and explain the benefit of their implementation by WPI co-authors in the ASSISTments platform.',\n",
       " 'Social media has become a major communication channel for communities centered around video games. Consequently, social media offers a rich data source to study online communities and the discussions evolving around games. Towards this end, we explore a large-scale dataset consisting of over 1 million tweets related to the online multiplayer shooter Destiny and spanning a time period of about 14 months using unsupervised clustering and topic modelling. Furthermore, we correlate Twitter activity of over 3,000 players with their playtime. Our results contribute to the understanding of online player communities by identifying distinct player groups with respect to their Twitter characteristics, describing subgroups within the Destiny community, and uncovering broad topics of community interest.',\n",
       " 'Inferring emotions from physiological signals has gained much traction in the last years. Physiological responses to emotions, however, are commonly interfered and overlapped by physical activities, posing a challenge towards emotion recognition in the wild. In this paper, we address this challenge by investigating new features and machine-learning models for emotion recognition, non-sensitive to physical-based interferences. We recorded physiological signals from 18 participants that were exposed to emotions before and while performing physical activities to assess the performance of non-sensitive emotion recognition models. We trained models with the least exhaustive physical activity (sitting) and tested with the remaining, more exhausting activities. For three different emotion categories, we achieve classification accuracies ranging from 47.88% - 73.35% for selected feature sets and per participant. Furthermore, we investigate the performance across all participants and of each activity individually. In this regard, we achieve similar results, between 55.17% and 67.41%, indicating the viability of emotion recognition models not being influenced by single physical activities.',\n",
       " 'Eye movement patterns reflect human latent internal cognitive activities. We aim to discover eye movement patterns during face recognition under different cognitions of information concealing. These cognitions include the degrees of face familiarity and deception or not, namely telling the truth when observing familiar and unfamiliar faces, and deceiving in front of familiar faces. We apply Hidden Markov models with Gaussian emission to generalize regions and trajectories of eye fixation points under the above three conditions. Our results show that both eye movement patterns and eye gaze regions become significantly different during deception compared with truth-telling. We show the feasibility of detecting deception and further cognitive activity classification using eye movement patterns.',\n",
       " \"We propose an active learning architecture for robots, capable of organizing its learning process to achieve a field of complex tasks by learning sequences of motor policies, called Intrinsically Motivated Procedure Babbling (IM-PB). The learner can generalize over its experience to continuously learn new tasks. It chooses actively what and how to learn based by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated tasks outcomes hierarchically organized. We introduce a framework called 'procedures', which are sequences of policies defined by the combination of previously learned skills. Our algorithmic architecture uses the procedures to autonomously discover how to combine simple skills to achieve complex goals. It actively chooses between 2 strategies of goal-directed exploration : exploration of the policy space or the procedural space. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our 'procedures' framework helps the learner to tackle difficult hierarchical tasks.\",\n",
       " \"Collaborative creativity is the approach of employing crowd to accomplish creative tasks. In this paper, we present a collaborative crowdsourcing platform for writing stories by means of connecting a series of `images'. These connected images are termed as Image Chains, reflecting successive scenarios. Users can either start or extend an Image Chain by uploading their own image or choosing from the available ones. These users are allowed to pen their stories from the Image Chains. Finally, stories get published based on the number of votes obtained. This provides an organized framework of story writing unlike most of the state-of-the-art collaborative editing platforms. Our experiments on 25 contributors highlight their interest in growing shorter Image Chains but voting longer Image Chains.\",\n",
       " 'Maximalism in art refers to drawing on and combining multiple different sources for art creation, embracing the resulting collisions and heterogeneity. This paper discusses the use of maximalism in game design and particularly in data games, which are games that are generated partly based on open data. Using Data Adventures, a series of generators that create adventure games from data sources such as Wikipedia and OpenStreetMap, as a lens we explore several tradeoffs and issues in maximalist game design. This includes the tension between transformation and fidelity, between decorative and functional content, and legal and ethical issues resulting from this type of generativity. This paper sketches out the design space of maximalist data-driven games, a design space that is mostly unexplored.',\n",
       " 'This paper explores the identification of smartphone users when certain samples collected while the subject felt happy, upset or stressed were absent or present. We employ data from 19 subjects using the StudentLife dataset, a dataset collected by researchers at Dartmouth College that was originally collected to correlate behaviors characterized by smartphone usage patterns with changes in stress and academic performance. Although many previous works on behavioral biometrics have implied that mood is a source of intra-person variation which may impact biometric performance, our results contradict this assumption. Our findings show that performance worsens when removing samples that were generated when subjects may be happy, upset, or stressed. Thus, there is no indication that mood negatively impacts performance. However, we do find that changes existing in smartphone usage patterns may correlate with mood, including changes in locking, audio, location, calling, homescreen, and e-mail habits. Thus, we show that while mood is a source of intra-person variation, it may be an inaccurate assumption that biometric systems (particularly, mobile biometrics) are likely influenced by mood.',\n",
       " 'Compared to other behavioural biometrics, mouse dynamics is a less explored area. General purpose data sets containing unrestricted mouse usage data are usually not available. The Balabit data set was released in 2016 for a data science competition, which against the few subjects, can be considered the first adequate publicly available one. This paper presents a performance evaluation study on this data set for impostor detection. The existence of very short test sessions makes this data set challenging. Raw data were segmented into mouse move, point and click and drag and drop types of mouse actions, then several features were extracted. In contrast to keystroke dynamics, mouse data is not sensitive, therefore it is possible to collect negative mouse dynamics data and to use two-class classifiers for impostor detection. Both action- and set of actions-based evaluations were performed. Set of actions-based evaluation achieves 0.92 AUC on the test part of the data set. However, the same type of evaluation conducted on the training part of the data set resulted in maximal AUC (1) using only 13 actions. Drag and drop mouse actions proved to be the best actions for impostor detection.',\n",
       " 'The present study has made a review of scientific publications on applications focused on autism, most of them developed for communication, social behavior and learning, which coincides with what is observed in a digital market that practically lacks scientific validation. The study has also found only 135 of these type of applications with a Spanish version available (in a practical sense), developed mostly for daily life of an autistic person and/or people from their immediate environment. By using these applications, there are positive results in terms of learning and permanent adoption of behaviors and skills, but it is necessary to deepen research and further development of applications focused on leisure, resources for parents and professionals, and supporting of autistic adult needs.',\n",
       " 'Self-tracking physiological and psychological data poses the challenge of presentation and interpretation. Insightful narratives for self-tracking data can motivate the user towards constructive self-reflection. One powerful form of narrative that engages audience across various culture and age groups is animated movies. We collected a week of self-reported mood and behavior data from each user and created in Unity a personalized animation based on their data. We evaluated the impact of their video in a randomized control trial with a non-personalized animated video as control. We found that personalized videos tend to be more emotionally engaging, encouraging greater and lengthier writing that indicated self-reflection about moods and behaviors, compared to non-personalized control videos.',\n",
       " 'Reusing passwords across multiple websites is a common practice that compromises security. Recently, Blum and Vempala have proposed password strategies to help people calculate, in their heads, passwords for different sites without dependence on third-party tools or external devices. Thus far, the security and efficiency of these \"mental algorithms\" has been analyzed only theoretically. But are such methods usable? We present the first usability study of humanly computable password strategies, involving a learning phase (to learn a password strategy), then a rehearsal phase (to login to a few websites), and multiple follow-up tests. In our user study, with training, participants were able to calculate a deterministic eight-character password for an arbitrary new website in under 20 seconds.',\n",
       " 'Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process.',\n",
       " \"One of the main methods for interacting with mobile devices today is the error-prone and inflexible touch-screen keyboard. This paper proposes MagBoard: a homomorphic ubiquitous keyboard for mobile devices. MagBoard allows application developers and users to design and print different custom keyboards for the same applications to fit different user's needs. The core idea is to leverage the triaxial magnetometer embedded in standard mobile phones to accurately localize the location of a magnet on a virtual grid superimposed on the printed keyboard. This is achieved through a once in a lifetime fingerprint. MagBoard also provides a number of modules that allow it to cope with background magnetic noise, heterogeneous devices, different magnet shapes, sizes, and strengths, as well as changes in magnet polarity. Our implementation of MagBoard on Android phones with extensive evaluation in different scenarios demonstrates that it can achieve a key detection accuracy of more than 91% for keys as small as 2cm*2cm, reaching 100% for 4cm*4cm keys. This accuracy is robust with different phones and magnets, highlighting MagBoard promise as a homomorphic ubiquitous keyboard for mobile devices.\",\n",
       " 'In addition to user-generated content, Open Educational Resources are increasingly made available on the Web by several institutions and organizations with the aim of being re-used. Nevertheless, it is still difficult for users to find appropriate resources for specific learning scenarios among the vast amount offered on the Web. Our goal is to give users the opportunity to search for authentic resources from the Web and reuse them in a learning context. The LearnWeb-OER platform enhances collaborative searching and sharing of educational resources providing specific means and facilities for education. In the following, we provide a description of the functionalities that support users in collaboratively collecting, selecting, annotating and discussing search results and learning resources.',\n",
       " 'We present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language. Using DSTC2 as seed data, we trained natural language understanding (NLU) and generation (NLG) networks for each agent and let the agents interact online. We model the interaction as a stochastic collaborative game where each agent (player) has a role (\"assistant\", \"tourist\", \"eater\", etc.) and their own objectives, and can only interact via natural language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own NLU and NLG, the other agent\\'s NLU, Policy, and NLG). In our evaluation, we show that the stochastic-game agents outperform deep learning based supervised baselines.',\n",
       " 'Swarm systems consist of large numbers of robots that collaborate autonomously. With an appropriate level of human control, swarm systems could be applied in a variety of contexts ranging from search-and-rescue situations to Cyber defence. The two decision making cycles of swarms and humans operate on two different time-scales, where the former is normally orders of magnitude faster than the latter. Closing the loop at the intersection of these two cycles will create fast and adaptive human-swarm teaming networks. This paper brings desperate pieces of the ground work in this research area together to review this multidisciplinary literature. We conclude with a framework to synthesize the findings and summarize the multi-modal indicators needed for closed-loop human-swarm adaptive systems.',\n",
       " 'Scholarly articles publishing and getting cited has become a way of life for academicians. These scholarly publications shape up the career growth of not only the authors but also of the country, continent and the technological domains. Author affiliations, country and other information of an author coupled with data analytics can provide useful and insightful results. However, massive and complete data is required to perform this research. Google scholar which is a comprehensive and free repository of scholarly articles has been used as a data source for this purpose. Data scraped from Google scholar when stored as a graph and visualized in the form of nodes and relationships, can offer discerning and concealed information. Such as, evident domain shift of an author, various research domains spread for an author, prediction of emerging domain and sub domains, detection of journal and author level citation cartel behaviors etc. The data from graph database is also used in computation of scholastic indicators for the journals. Eventually, econometric model, named Cobb Douglas model is used to compute the journals Modeling \"Internationality\" Index based on these scholastic indicators.',\n",
       " 'Multivariate spatial data plays an important role in computational science and engineering simulations. The potential features and hidden relationships in multivariate data can assist scientists to gain an in-depth understanding of a scientific process, verify a hypothesis and further discover a new physical or chemical law. In this paper, we present a comprehensive survey of the state-of-the-art techniques for multivariate spatial data visualization. We first introduce the basic concept and characteristics of multivariate spatial data, and describe three main tasks in multivariate data visualization: feature classification, fusion visualization, and correlation analysis. Finally, we prospect potential research topics for multivariate data visualization according to the current research.',\n",
       " 'We use an immersive virtual reality environment to explore the intricate social cues that underlie non-verbal communication involved in a pedestrian\\'s crossing decision. We \"hack\" non-verbal communication between pedestrian and vehicle by engineering a set of 15 vehicle trajectories, some of which follow social conventions and some that break them. By subverting social expectations of vehicle behavior we show that pedestrians may use vehicle kinematics to infer social intentions and not merely as the state of a moving object. We investigate human behavior in this virtual world by conducting a study of 22 subjects, with each subject experiencing and responding to each of the trajectories by moving their body, legs, arms, and head in both the physical and the virtual world. Both quantitative and qualitative responses are collected and analyzed, showing that, in fact, social cues can be engineered through vehicle trajectory manipulation. In addition, we demonstrate that immersive virtual worlds which allow the pedestrian to move around freely, provide a powerful way to understand both the mechanisms of human perception and the social signaling involved in pedestrian-vehicle interaction.',\n",
       " \"Notifications provide a unique mechanism for increasing the effectiveness of real-time information delivery systems. However, notifications that demand users' attention at inopportune moments are more likely to have adverse effects and might become a cause of potential disruption rather than proving beneficial to users. In order to address these challenges a variety of intelligent notification mechanisms based on monitoring and learning users' behavior have been proposed. The goal of such mechanisms is maximizing users' receptivity to the delivered information by automatically inferring the right time and the right context for sending a certain type of information.\\n  This article provides an overview of the current state of the art in the area of intelligent notification mechanisms that relies on the awareness of users' context and preferences. More specifically, we first present a survey of studies focusing on understanding and modeling users' interruptibility and receptivity to notifications from desktops and mobile devices. Then, we discuss the existing challenges and opportunities in developing mechanisms for intelligent notification systems in a variety of application scenarios.\",\n",
       " 'We present a novel platform for the interactive visualization of very large graphs. The platform enables the user to interact with the visualized graph in a way that is very similar to the exploration of maps at multiple levels. Our approach involves an offline preprocessing phase that builds the layout of the graph by assigning coordinates to its nodes with respect to a Euclidean plane. The respective points are indexed with a spatial data structure, i.e., an R-tree, and stored in a database. Multiple abstraction layers of the graph based on various criteria are also created offline, and they are indexed similarly so that the user can explore the dataset at different levels of granularity, depending on her particular needs. Then, our system translates user operations into simple and very efficient spatial operations (i.e., window queries) in the backend. This technique allows for a fine-grained access to very large graphs with extremely low latency and memory requirements and without compromising the functionality of the tool. Our web-based prototype supports three main operations: (1) interactive navigation, (2) multi-level exploration, and (3) keyword search on the graph metadata.',\n",
       " 'Sharing live telepresence experiences for teleconferencing or remote collaboration receives increasing interest with the recent progress in capturing and AR/VR technology. Whereas impressive telepresence systems have been proposed on top of on-the-fly scene capture, data transmission and visualization, these systems are restricted to the immersion of single or up to a low number of users into the respective scenarios. In this paper, we direct our attention on immersing significantly larger groups of people into live-captured scenes as required in education, entertainment or collaboration scenarios. For this purpose, rather than abandoning previous approaches, we present a range of optimizations of the involved reconstruction and streaming components that allow the immersion of a group of more than 24 users within the same scene - which is about a factor of 6 higher than in previous work - without introducing further latency or changing the involved consumer hardware setup. We demonstrate that our optimized system is capable of generating high-quality scene reconstructions as well as providing an immersive viewing experience to a large group of people within these live-captured scenes.',\n",
       " \"Data analytics software applications have become an integral part of the decision-making process of analysts. Users of such a software face challenges due to insufficient product and domain knowledge, and find themselves in need of help. To alleviate this, we propose a task-aware command recommendation system, to guide the user on what commands could be executed next. We rely on topic modeling techniques to incorporate information about user's task into our models. We also present a help prediction model to detect if a user is in need of help, in which case the system proactively provides the aforementioned command recommendations. We leverage the log data of a web-based analytics software to quantify the superior performance of our neural models, in comparison to competitive baselines.\",\n",
       " 'Laparoscopic Surgery (LS) is a modern surgical technique whereby the surgery is performed through an incision with tools and camera as opposed to conventional open surgery. This promises minimal recovery times and less hemorrhaging. Multi view LS is the latest development in the field, where the system uses multiple cameras to give the surgeon more information about the surgical site, potentially making the surgery easier. In this publication, we study the gaze patterns of a high performing subject in a multi-view LS environment and compare it with that of a novice to detect the differences between the gaze behavior. This was done by conducting a user study with 20 university students with varying levels of expertise in Multi-view LS. The subjects performed an laparoscopic task in simulation with three cameras (front/top/side). The subjects were then separated as high and low performers depending on the performance times and their data was analyzed. Our results show statistically significant differences between the two behaviors. This opens up new areas from of training novices to Multi-view LS to making smart displays that guide your shows the optimum view depending on the situation.',\n",
       " 'In this paper, we explore the role that attribution plays in shaping user reactions to content reuse, or remixing, in a large user-generated content community. We present two studies using data from the Scratch online community -- a social media platform where hundreds of thousands of young people share and remix animations and video games. First, we present a quantitative analysis that examines the effects of a technological design intervention introducing automated attribution of remixes on users\\' reactions to being remixed. We compare this analysis to a parallel examination of \"manual\" credit-giving. Second, we present a qualitative analysis of twelve in-depth, semi-structured, interviews with Scratch participants on the subject of remixing and attribution. Results from both studies suggest that automatic attribution done by technological systems (i.e., the listing of names of contributors) plays a role that is distinct from, and less valuable than, credit which may superficially involve identical information but takes on new meaning when it is given by a human remixer. We discuss the implications of these findings for the designers of online communities and social media platforms.',\n",
       " 'Gaze tracking is an important technology as the system can give information about a person from what and where the person is seeing. There have been many attempts to make robust and accurate gaze trackers using either monitor or wearable devices. However, those contraptions often require fine individual calibration per session and/or require a person wearing a device, which may not be suitable for certain situations. In this paper, we propose a robust and a completely noninvasive gaze tracking system that involves neither complex calibrations nor the use of wearable devices. We achieve this via direct eye reflection analysis by building a real-time system that effectively enables it. We also show several interesting applications for our system including experiments with young children.',\n",
       " 'When we interact with small screen devices, sometimes we make errors, due to our abilities/disabilities, contextual factors that distract our attention or problems related to the interface. Recovering from these errors may be time consuming or cause frustration. Predicting and learning these errors based on the previous user interaction and contextual factors, and adapting user interface to prevent from these errors can improve user performance and satisfaction. In this paper, we propose a system that aims to monitor user performance and contextual changes and do adaptations based on the user performance by using machine learning techniques. Here, we briefly present our systematic literature review findings and discuss our research questions towards developing such an adaptive system.',\n",
       " 'Loosely based on principles of similarity-attraction, robots intended for social contexts are being designed with increasing human similarity to facilitate their reception by and communication with human interactants. However, the observation of an uncanny valley - the phenomenon in which certain humanlike entities provoke dislike instead of liking - has lead some to caution against this practice. Substantial evidence supports both of these contrasting perspectives on the design of social technologies. Yet, owing to both empirical and theoretical inconsistencies, the relationship between anthropomorphic design and people\\'s liking of the technology remains poorly understood.\\n  Here we present three studies which investigate people\\'s explicit ratings of and behavior towards a large sample of real-world robots. The results show a profound \"valley effect\" on people\\'s \\\\emph{willingness} to interact with humanlike robots, thus highlighting the formidable design challenge the uncanny valley poses for social robotics. In addition to advancing uncanny valley theory, Studies 2 and 3 contribute and validate a novel laboratory task for objectively measuring people\\'s perceptions of humanlike robots.',\n",
       " 'BCI algorithm development has long been hampered by two major issues: small sample sets and a lack of reproducibility. We offer a solution to both of these problems via a software suite that streamlines both the issues of finding and preprocessing data in a reliable manner, as well as that of using a consistent interface for machine learning methods. By building on recent advances in software for signal analysis implemented in the MNE toolkit, and the unified framework for machine learning offered by the scikit-learn project, we offer a system that can improve BCI algorithm development. This system is fully open-source under the BSD licence and available at https://github.com/NeuroTechX/moabb. To validate our efforts, we analyze a set of state-of-the-art decoding algorithms across 12 open access datasets, with over 250 subjects. Our analysis confirms that different datasets can result in very different results for identical processing pipelines, highlighting the need for trustworthy algorithm benchmarking in the field of BCIs, and further that many previously validated methods do not hold up when applied across different datasets, which has wide-reaching implications for practical BCIs.',\n",
       " 'Detection of engagement during a conversation is an important function of human-robot interaction. The level of user engagement can influence the dialogue strategy of the robot. Our motivation in this work is to detect several behaviors which will be used as social signal inputs for a real-time engagement recognition model. These behaviors are nodding, laughter, verbal backchannels and eye gaze. We describe models of these behaviors which have been learned from a large corpus of human-robot interactions with the android robot ERICA. Input data to the models comes from a Kinect sensor and a microphone array. Using our engagement recognition model, we can achieve reasonable performance using the inputs from automatic social signal detection, compared to using manual annotation as input.',\n",
       " 'Despite the fact that advertisements (ads) often include strongly emotional content, very little work has been devoted to affect recognition (AR) from ads. This work explicitly compares content-centric and user-centric ad AR methodologies, and evaluates the impact of enhanced AR on computational advertising via a user study. Specifically, we (1) compile an affective ad dataset capable of evoking coherent emotions across users; (2) explore the efficacy of content-centric convolutional neural network (CNN) features for encoding emotions, and show that CNN features outperform low-level emotion descriptors; (3) examine user-centered ad AR by analyzing Electroencephalogram (EEG) responses acquired from eleven viewers, and find that EEG signals encode emotional information better than content descriptors; (4) investigate the relationship between objective AR and subjective viewer experience while watching an ad-embedded online video stream based on a study involving 12 users. To our knowledge, this is the first work to (a) expressly compare user vs content-centered AR for ads, and (b) study the relationship between modeling of ad emotions and its impact on a real-life advertising application.',\n",
       " 'Brain computer interfaces (BCIs) offer individuals suffering from major disabilities an alternative method to interact with their environment. Sensorimotor rhythm (SMRs) based BCIs can successfully perform control tasks; however, the traditional SMR paradigms intuitively disconnect the control and real task, making them non-ideal for complex control scenarios. In this study, we design a new, intuitively connected motor imagery (MI) paradigm using hierarchical common spatial patterns (HCSP) and context information to effectively predict intended hand grasps from electroencephalogram (EEG) data. Experiments with 5 participants yielded an aggregate classification accuracy--intended grasp prediction probability--of 64.5\\\\% for 8 different hand gestures, more than 5 times the chance level.',\n",
       " 'Sequential learning of multiple tasks in artificial neural networks using gradient descent leads to catastrophic forgetting, whereby previously learned knowledge is erased during learning of new, disjoint knowledge. Here, we propose a fundamentally new type of method - Beneficial Perturbation Network (BPN). We add task-dependent memory (biasing) units to allow the network to operate in different regimes for different tasks. We compute the most beneficial directions to train these units, in a manner inspired by recent work on adversarial examples. At test time, beneficial perturbations for a given task bias the network toward that task to overcome catastrophic forgetting. BPN is not only more parameter-efficient than network expansion methods, but also does not need to store any data from previous tasks, in contrast with episodic memory methods. Experiments on variants of the MNIST, CIFAR-10, CIFAR-100 datasets demonstrate strong performance of BPN when compared to the state-of-the-art.',\n",
       " \"Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes' neighbors in a graph. However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations. We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation results hierarchically, eliminating repeated computations and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGs and use a novel HAG search algorithm to find optimized HAGs. Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x, while maintaining the original model accuracy.\",\n",
       " 'In a binary classification problem the feature vector (predictor) is the input to a scoring function that produces a decision value (score), which is compared to a particular chosen threshold to provide a final class prediction (output). Although the normal assumption of the scoring function is important in many applications, sometimes it is severely violated even under the simple multinormal assumption of the feature vector. This article proves this result mathematically with a counter example to provide an advice for practitioners to avoid blind assumptions of normality. On the other hand, the article provides a set of experiments that illustrate some of the expected and well-behaved results of the Area Under the ROC curve (AUC) under the multinormal assumption of the feature vector. Therefore, the message of the article is not to avoid the normal assumption of either the input feature vector or the output scoring function; however, a prudence is needed when adopting either of both.',\n",
       " 'Exploration strategy design is one of the challenging problems in reinforcement learning~(RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward~(quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called \\\\underline{c}lustered \\\\underline{r}einforcement \\\\underline{l}earning~(CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area~(cluster) of the current state is given to the agent. Experiments on a continuous control task and several \\\\emph{Atari 2600} games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.',\n",
       " \"Effective representation learning of electronic health records is a challenging task and is becoming more important as the availability of such data is becoming pervasive. The data contained in these records are irregular and contain multiple modalities such as notes, and medical codes. They are preempted by medical conditions the patient may have, and are typically jotted down by medical staff. Accompanying codes are notes containing valuable information about patients beyond the structured information contained in electronic health records. We use transformer networks and the recently proposed BERT language model to embed these data streams into a unified vector representation. The presented approach effectively encodes a patient's visit data into a single distributed representation, which can be used for downstream tasks. Our model demonstrates superior performance and generalization on mortality, readmission and length of stay tasks using the publicly available MIMIC-III ICU dataset.\",\n",
       " 'Machine Learning algorithms are typically regarded as appropriate optimization schemes for minimizing risk functions that are constructed on the training set, which conveys statistical flavor to the corresponding learning problem. When the focus is shifted on perception, which is inherently interwound with time, recent alternative formulations of learning have been proposed that rely on the principle of Least Cognitive Action, which very much reminds us of the Least Action Principle in mechanics. In this paper, we discuss different forms of the cognitive action and show the well-posedness of learning. In particular, unlike the special case of the action in mechanics, where the stationarity is typically gained on saddle points, we prove the existence of the minimum of a special form of cognitive action, which yields forth-order differential equations of learning. We also briefly discuss the dissipative behavior of these equations that turns out to characterize the process of learning.',\n",
       " 'Corner cases are the main bottlenecks when applying Artificial Intelligence (AI) systems to safety-critical applications. An AI system should be intelligent enough to detect such situations so that system developers can prepare for subsequent planning. In this paper, we propose semi-supervised anomaly detection considering the imbalance of normal situations. In particular, driving data consists of multiple positive/normal situations (e.g., right turn, going straight), some of which (e.g., U-turn) could be as rare as anomalous situations. Existing machine learning based anomaly detection approaches do not fare sufficiently well when applied to such imbalanced data. In this paper, we present a novel multi-task learning based approach that leverages domain-knowledge (maneuver labels) for anomaly detection in driving data. We evaluate the proposed approach both quantitatively and qualitatively on 150 hours of real-world driving data and show improved performance over baseline approaches.',\n",
       " 'In the context of learning deterministic policies in continuous domains, we revisit an approach, which was first proposed in Continuous Actor Critic Learning Automaton (CACLA) and later extended in Neural Fitted Actor Critic (NFAC). This approach is based on a policy update different from that of deterministic policy gradient (DPG). Previous work has observed its excellent performance empirically, but a theoretical justification is lacking. To fill this gap, we provide a theoretical explanation to motivate this unorthodox policy update by relating it to another update and making explicit the objective function of the latter. We furthermore discuss in depth the properties of these updates to get a deeper understanding of the overall approach. In addition, we extend it and propose a new trust region algorithm, Penalized NFAC (PeNFAC). Finally, we experimentally demonstrate in several classic control problems that it surpasses the state-of-the-art algorithms to learn deterministic policies.',\n",
       " 'Deep neural networks are vulnerable to adversarial perturbations: small changes in the input easily lead to misclassification. In this work, we propose an attack methodology catered not only for cases where the perturbations are measured by $\\\\ell_p$ norms, but in fact any adversarial dissimilarity metric with a closed proximal form. This includes, but is not limited to, $\\\\ell_1$, $\\\\ell_2$, $\\\\ell_\\\\infty$ perturbations, and the $\\\\ell_0$ counting \"norm\", i.e. true sparseness. Our approach to generating perturbations is a natural extension of our recent work, the LogBarrier attack, which previously required the metric to be differentiable. We demonstrate our new algorithm, ProxLogBarrier, on the MNIST, CIFAR10, and ImageNet-1k datasets. We attack undefended and defended models, and show that our algorithm transfers to various datasets with little parameter tuning. In particular, in the $\\\\ell_0$ case, our algorithm finds significantly smaller perturbations compared to multiple existing methods',\n",
       " 'Estimates of predictive uncertainty are important for accurate model-based planning and reinforcement learning. However, predictive uncertainties---especially ones derived from modern deep learning systems---can be inaccurate and impose a bottleneck on performance. This paper explores which uncertainties are needed for model-based reinforcement learning and argues that good uncertainties must be calibrated, i.e. their probabilities should match empirical frequencies of predicted events. We describe a simple way to augment any model-based reinforcement learning agent with a calibrated model and show that doing so consistently improves planning, sample complexity, and exploration. On the \\\\textsc{HalfCheetah} MuJoCo task, our system achieves state-of-the-art performance using 50\\\\% fewer samples than the current leading approach. Our findings suggest that calibration can improve the performance of model-based reinforcement learning with minimal computational and implementation overhead.',\n",
       " 'Generative adversarial networks have been very successful in generative modeling, however they remain relatively hard to optimize compared to standard deep neural networks. In this paper, we try to gain insight into the optimization of GANs by looking at the game vector field resulting from the concatenation of the gradient of both players. Based on this point of view, we propose visualization techniques that allow us to make the following empirical observations. First, the training of GANs suffers from rotational behavior around locally stable stationary points, which, as we show, corresponds to the presence of imaginary components in the eigenvalues of the Jacobian of the game. Secondly, GAN training seems to converge to a stable stationary point which is a saddle point for the generator loss, not a minimum, while still achieving excellent performance. This counter-intuitive yet persistent observation questions whether we actually need a Nash equilibrium to get good performance in GANs.',\n",
       " 'Deep Neural Networks have shown tremendous success in the area of object recognition, image classification and natural language processing. However, designing optimal Neural Network architectures that can learn and output arbitrary graphs is an ongoing research problem. The objective of this survey is to summarize and discuss the latest advances in methods to Learn Representations of Graph Data. We start by identifying commonly used types of graph data and review basics of graph theory. This is followed by a discussion of the relationships between graph kernel methods and neural networks. Next we identify the major approaches used for learning representations of graph data namely: Kernel approaches, Convolutional approaches, Graph neural networks approaches, Graph embedding approaches and Probabilistic approaches. A variety of methods under each of the approaches are discussed and the survey is concluded with a brief discussion of the future of learning representation of graph data.',\n",
       " \"Risk adjustment has become an increasingly important tool in healthcare. It has been extensively applied to payment adjustment for health plans to reflect the expected cost of providing coverage for members. Risk adjustment models are typically estimated using linear regression, which does not fully exploit the information in claims data. Moreover, the development of such linear regression models requires substantial domain expert knowledge and computational effort for data preprocessing. In this paper, we propose a novel approach for risk adjustment that uses semantic embeddings to represent patient medical histories. Embeddings efficiently represent medical concepts learned from diagnostic, procedure, and prescription codes in patients' medical histories. This approach substantially reduces the need for feature engineering. Our results show that models using embeddings had better performance than a commercial risk adjustment model on the task of prospective risk score prediction.\",\n",
       " 'Materials discovery is decisive for tackling urgent challenges related to energy, the environment, health care and many others. In chemistry, conventional methodologies for innovation usually rely on expensive and incremental strategies to optimize properties from molecular structures. On the other hand, inverse approaches map properties to structures, thus expediting the design of novel useful compounds. In this chapter, we examine the way in which current deep generative models are addressing the inverse chemical discovery paradigm. We begin by revisiting early inverse design algorithms. Then, we introduce generative models for molecular systems and categorize them according to their architecture and molecular representation. Using this classification, we review the evolution and performance of important molecular generation schemes reported in the literature. Finally, we conclude highlighting the prospects and challenges of generative models as cutting edge tools in materials discovery.',\n",
       " 'Imitation Learning describes the problem of recovering an expert policy from demonstrations. While inverse reinforcement learning approaches are known to be very sample-efficient in terms of expert demonstrations, they usually require problem-dependent reward functions or a (task-)specific reward-function regularization. In this paper, we show a natural connection between inverse reinforcement learning approaches and Optimal Transport, that enables more general reward functions with desirable properties (e.g., smoothness). Based on our observation, we propose a novel approach called Wasserstein Adversarial Imitation Learning. Our approach considers the Kantorovich potentials as a reward function and further leverages regularized optimal transport to enable large-scale applications. In several robotic experiments, our approach outperforms the baselines in terms of average cumulative rewards and shows a significant improvement in sample-efficiency, by requiring just one expert demonstration.',\n",
       " 'Recently, neural network based approaches have achieved significant improvement for solving large, complex, graph-structured problems. However, their bottlenecks ] still need to be addressed, and the advantages of multi-scale information and deep architectures have not been sufficiently exploited. In this paper, we theoretically analyze how existing Graph Convolutional Networks (GCNs) have limited expressive power due to the constraint of the activation functions and their architectures. We generalize spectral graph convolution and deep GCN in block Krylov subspace forms and devise two architectures, both with the potential to be scaled deeper but each making use of the multi-scale information in different ways. We further show that the equivalence of these two architectures can be established under certain conditions. On several node classification tasks, with or without the help of validation, the two new architectures achieve better performance compared to many state-of-the-art methods.',\n",
       " 'We introduce a systematic framework for quantifying the robustness of classifiers to naturally occurring perturbations of images found in videos. As part of this framework, we construct Imagenet-Video-Robust, a human-expert--reviewed dataset of 22,178 images grouped into 1,109 sets of perceptually similar images derived from frames in the ImageNet Video Object Detection dataset. We evaluate a diverse array of classifiers trained on ImageNet, including models trained for robustness, and show a median classification accuracy drop of 16%. Additionally, we evaluate the Faster R-CNN and R-FCN models for detection, and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis shows that natural perturbations in the real world are heavily problematic for current CNNs, posing a significant challenge to their deployment in safety-critical environments that require reliable, low-latency predictions.',\n",
       " 'The (contextual) multi-armed bandit problem (MAB) provides a formalization of sequential decision-making which has many applications. However, validly evaluating MAB policies is challenging; we either resort to simulations which inherently include debatable assumptions, or we resort to expensive field trials. Recently an offline evaluation method has been suggested that is based on empirical data, thus relaxing the assumptions, and can be used to evaluate multiple competing policies in parallel. This method is however not directly suited for the continuous armed (CAB) problem; an often encountered version of the MAB problem in which the action set is continuous instead of discrete. We propose and evaluate an extension of the existing method such that it can be used to evaluate CAB policies. We empirically demonstrate that our method provides a relatively consistent ranking of policies. Furthermore, we detail how our method can be used to select policies in a real-life CAB problem.',\n",
       " \"Pruning neural network parameters to reduce model size is an area of much interest, but the original motivation for pruning was the prevention of overfitting rather than the improvement of computational efficiency. This motivation is particularly relevant given the perhaps surprising observation that a wide variety of pruning approaches confer increases in test accuracy, even when parameter counts are drastically reduced. To better understand this phenomenon, we analyze the behavior of pruning over the course of training, finding that pruning's effect on generalization relies more on the instability generated by pruning than the final size of the pruned model. We demonstrate that even pruning of seemingly unimportant parameters can lead to such instability, allowing our finding to account for the generalization benefits of modern pruning techniques. Our results ultimately suggest that, counter-intuitively, pruning regularizes through instability and mechanisms unrelated to parameter counts.\",\n",
       " 'Accelerating research in the emerging field of deep graph learning requires new tools. Such systems should support graph as the core abstraction and take care to maintain both forward (i.e. supporting new research ideas) and backward (i.e. integration with existing components) compatibility. In this paper, we present Deep Graph Library (DGL). DGL enables arbitrary message handling and mutation operators, flexible propagation rules, and is framework agnostic so as to leverage high-performance tensor, autograd operations, and other feature extraction modules already available in existing frameworks. DGL carefully handles the sparse and irregular graph structure, deals with graphs big and small which may change dynamically, fuses operations, and performs auto-batching, all to take advantages of modern hardware. DGL has been tested on a variety of models, including but not limited to the popular Graph Neural Networks (GNN) and its variants, with promising speed, memory footprint and scalability.',\n",
       " 'Job transitions and upskilling are common actions taken by many industry working professionals throughout their career. With the current rapidly changing job landscape where requirements are constantly changing and industry sectors are emerging, it is especially difficult to plan and navigate a predetermined career path. In this work, we implemented a system to automate the collection and classification of training videos to help job seekers identify and acquire the skills necessary to transition to the next step in their career. We extracted educational videos and built a machine learning classifier to predict video relevancy. This system allows us to discover relevant videos at a large scale for job title-skill pairs. Our experiments show significant improvements in the model performance by incorporating embedding vectors associated with the video attributes. Additionally, we evaluated the optimal probability threshold to extract as many videos as possible with minimal false positive rate.',\n",
       " 'The wide spread usage of automated data-driven decision support systems has raised a lot of concerns regarding accountability and fairness of the employed models in the absence of human supervision. Existing fairness-aware approaches tackle fairness as a batch learning problem and aim at learning a fair model which can then be applied to future instances of the problem. In many applications, however, the data comes sequentially and its characteristics might evolve with time. In such a setting, it is counter-intuitive to \"fix\" a (fair) model over the data stream as changes in the data might incur changes in the underlying model therefore, affecting its fairness. In this work, we propose fairness-enhancing interventions that modify the input data so that the outcome of any stream classifier applied to that data will be fair. Experiments on real and synthetic data show that our approach achieves good predictive performance and low discrimination scores over the course of the stream.',\n",
       " \"We present a training pipeline for the autonomous driving task given the current camera image and vehicle speed as the input to produce the throttle, brake, and steering control output. The simulator Airsim's convenient weather and lighting API provides a sufficient diversity during training which can be very helpful to increase the trained policy's robustness. In order to not limit the possible policy's performance, we use a continuous and deterministic control policy setting. We utilize ResNet-34 as our actor and critic networks with some slight changes in the fully connected layers. Considering human's mastery of this task and the high-complexity nature of this task, we first use imitation learning to mimic the given human policy and leverage the trained policy and its weights to the reinforcement learning phase for which we use DDPG. This combination shows a considerable performance boost comparing to both pure imitation learning and pure DDPG for the autonomous driving task.\",\n",
       " 'In this paper, we propose Efficient Progressive Neural Architecture Search (EPNAS), a neural architecture search (NAS) that efficiently handles large search space through a novel progressive search policy with performance prediction based on REINFORCE~\\\\cite{Williams.1992.PG}. EPNAS is designed to search target networks in parallel, which is more scalable on parallel systems such as GPU/TPU clusters. More importantly, EPNAS can be generalized to architecture search with multiple resource constraints, \\\\eg, model size, compute complexity or intensity, which is crucial for deployment in widespread platforms such as mobile and cloud. We compare EPNAS against other state-of-the-art (SoTA) network architectures (\\\\eg, MobileNetV2~\\\\cite{mobilenetv2}) and efficient NAS algorithms (\\\\eg, ENAS~\\\\cite{pham2018efficient}, and PNAS~\\\\cite{Liu2017b}) on image recognition tasks using CIFAR10 and ImageNet. On both datasets, EPNAS is superior \\\\wrt architecture searching speed and recognition accuracy.',\n",
       " 'We propose and study a multi-scale approach to vector quantization. We develop an algorithm, dubbed reconstruction trees, inspired by decision trees. Here the objective is parsimonious reconstruction of unsupervised data, rather than classification. Contrasted to more standard vector quantization methods, such as K-means, the proposed approach leverages a family of given partitions, to quickly explore the data in a coarse to fine-- multi-scale-- fashion. Our main technical contribution is an analysis of the expected distortion achieved by the proposed algorithm, when the data are assumed to be sampled from a fixed unknown distribution. In this context, we derive both asymptotic and finite sample results under suitable regularity assumptions on the distribution. As a special case, we consider the setting where the data generating distribution is supported on a compact Riemannian sub-manifold. Tools from differential geometry and concentration of measure are useful in our analysis.',\n",
       " 'Expanding the receptive field to capture large-scale context is key to obtaining good performance in dense prediction tasks, such as human pose estimation. While many state-of-the-art fully-convolutional architectures enlarge the receptive field by reducing resolution using strided convolution or pooling layers, the most straightforward strategy is adopting large filters. This, however, is costly because of the quadratic increase in the number of parameters and multiply-add operations. In this work, we explore using learnable box filters to allow for convolution with arbitrarily large kernel size, while keeping the number of parameters per filter constant. In addition, we use precomputed summed-area tables to make the computational cost of convolution independent of the filter size. We adapt and incorporate the box filter as a differentiable module in a fully-convolutional neural network, and demonstrate its competitive performance on popular benchmarks for the task of human pose estimation.',\n",
       " 'Smartwatches are increasingly being used to recognize human daily life activities. These devices may employ different kind of machine learning (ML) solutions. One of such ML models is Gradient Boosting Machine (GBM) which has shown an excellent performance in the literature. The GBM can be trained on available data set before it is deployed on any device. However, this data set may not represent every kind of human behavior in real life. For example, a ML model to detect elder and young persons running activity may give different results because of differences in their activity patterns. This may result in decrease in the accuracy of activity recognition. Therefore, a transfer learning based method is proposed in which user-specific performance can be improved significantly by doing on-device calibration of GBM by just tuning its parameters without retraining its estimators. Results show that this method can significantly improve the user-based accuracy for activity recognition.',\n",
       " 'Extracting the underlying trend signal is a crucial step to facilitate time series analysis like forecasting and anomaly detection. Besides noise signal, time series can contain not only outliers but also abrupt trend changes in real-world scenarios. To deal with these challenges, we propose a robust trend filtering algorithm based on robust statistics and sparse learning. Specifically, we adopt the Huber loss to suppress outliers, and utilize a combination of the first order and second order difference on the trend component as regularization to capture both slow and abrupt trend changes. Furthermore, an efficient method is designed to solve the proposed robust trend filtering based on majorization minimization (MM) and alternative direction method of multipliers (ADMM). We compared our proposed robust trend filter with other nine state-of-the-art trend filtering algorithms on both synthetic and real-world datasets. The experiments demonstrate that our algorithm outperforms existing methods.',\n",
       " 'Deep neural networks obtain state-of-the-art performance on a series of tasks. However, they are easily fooled by adding a small adversarial perturbation to input. The perturbation is often human imperceptible on image data. We observe a significant difference in feature attributions of adversarially crafted examples from those of original ones. Based on this observation, we introduce a new framework to detect adversarial examples through thresholding a scale estimate of feature attribution scores. Furthermore, we extend our method to include multi-layer feature attributions in order to tackle the attacks with mixed confidence levels. Through vast experiments, our method achieves superior performances in distinguishing adversarial examples from popular attack methods on a variety of real data sets among state-of-the-art detection methods. In particular, our method is able to detect adversarial examples of mixed confidence levels, and transfer between different attacking methods.',\n",
       " 'As the multi-view data grows in the real world, multi-view clus-tering has become a prominent technique in data mining, pattern recognition, and machine learning. How to exploit the relation-ship between different views effectively using the characteristic of multi-view data has become a crucial challenge. Aiming at this, a hidden space sharing multi-view fuzzy clustering (HSS-MVFC) method is proposed in the present study. This method is based on the classical fuzzy c-means clustering model, and obtains associ-ated information between different views by introducing shared hidden space. Especially, the shared hidden space and the fuzzy partition can be learned alternatively and contribute to each other. Meanwhile, the proposed method uses maximum entropy strategy to control the weights of different views while learning the shared hidden space. The experimental result shows that the proposed multi-view clustering method has better performance than many related clustering methods.',\n",
       " 'Most of the existing approaches focus on specific visual tasks while ignoring the relations between them. Estimating task relation sheds light on the learning of high-order semantic concepts, e.g., transfer learning. How to reveal the underlying relations between different visual tasks remains largely unexplored. In this paper, we propose a novel \\\\textbf{L}earnable \\\\textbf{P}arameter \\\\textbf{S}imilarity (\\\\textbf{LPS}) method that learns an effective metric to measure the similarity of second-order semantics hidden in trained models. LPS is achieved by using a second-order neural network to align high-dimensional model parameters and learning second-order similarity in an end-to-end way. In addition, we create a model set called ModelSet500 as a parameter similarity learning benchmark that contains 500 trained models. Extensive experiments on ModelSet500 validate the effectiveness of the proposed method. Code will be released at \\\\url{https://github.com/Wanggcong/learnable-parameter-similarity}.',\n",
       " 'Heart Failure is a major component of healthcare expenditure and a leading cause of mortality worldwide. Despite higher inter-rater variability, endomyocardial biopsy (EMB) is still regarded as the standard technique, used to identify the cause (e.g. ischemic or non-ischemic cardiomyopathy, coronary artery disease, myocardial infarction etc.) of unexplained heart failure. In this paper, we focus on identifying cardiomyopathy as ischemic or non-ischemic. For this, we propose and implement a new unified architecture comprising CNN (inception-V3 model) and bidirectional LSTM (BiLSTM) with self-attention mechanism to predict the ischemic or non-ischemic to classify cardiomyopathy using histopathological images. The proposed model is based on self-attention that implicitly focuses on the information outputted from the hidden layers of BiLSTM. Through our results we demonstrate that this framework carries a high learning capacity and is able to improve the classification performance.',\n",
       " 'Owe to the recent advancements in Artificial Intelligence especially deep learning, many data-driven decision support systems have been implemented to facilitate medical doctors in delivering personalized care. We focus on the deep reinforcement learning (DRL) models in this paper. DRL models have demonstrated human-level or even superior performance in the tasks of computer vision and game playings, such as Go and Atari game. However, the adoption of deep reinforcement learning techniques in clinical decision optimization is still rare. We present the first survey that summarizes reinforcement learning algorithms with Deep Neural Networks (DNN) on clinical decision support. We also discuss some case studies, where different DRL algorithms were applied to address various clinical challenges. We further compare and contrast the advantages and limitations of various DRL algorithms and present a preliminary guide on how to choose the appropriate DRL algorithm for particular clinical applications.',\n",
       " 'Incremental class learning, a scenario in continual learning context where classes and their training data are sequentially and disjointedly observed, challenges a problem widely known as catastrophic forgetting. In this work, we propose a novel incremental class learning method that can significantly reduce memory overhead compared to previous approaches. Apart from conventional classification scheme using softmax, our model bases on an autoencoder to extract prototypes for given inputs so that no change in its output unit is required. It stores only the mean of prototypes per class to perform metric-based classification, unlike rehearsal approaches which rely on large memory or generative model. To mitigate catastrophic forgetting, regularization methods are applied on our model when a new task is encountered. We evaluate our method by experimenting on CIFAR-100 and CUB-200-2011 and show that its performance is comparable to the state-of-the-art method with much lower additional memory cost.',\n",
       " 'Thompson Sampling provides an efficient technique to introduce prior knowledge in the multi-armed bandit problem, along with providing remarkable empirical performance. In this paper, we revisit the Thompson Sampling algorithm under rewards drawn from symmetric $\\\\alpha$-stable distributions, which are a class of heavy-tailed probability distributions utilized in finance and economics, in problems such as modeling stock prices and human behavior. We present an efficient framework for posterior inference, which leads to two algorithms for Thompson Sampling in this setting. We prove finite-time regret bounds for both algorithms, and demonstrate through a series of experiments the stronger performance of Thompson Sampling in this setting. With our results, we provide an exposition of symmetric $\\\\alpha$-stable distributions in sequential decision-making, and enable sequential Bayesian inference in applications from diverse fields in finance and complex systems that operate on heavy-tailed features.',\n",
       " 'Despite continuing medical advances, the rate of newborn morbidity and mortality globally remains high, with over 6 million casualties every year. The prediction of pathologies affecting newborns based on their cry is thus of significant clinical interest, as it would facilitate the development of accessible, low-cost diagnostic tools\\\\cut{ based on wearables and smartphones}. However, the inadequacy of clinically annotated datasets of infant cries limits progress on this task. This study explores a neural transfer learning approach to developing accurate and robust models for identifying infants that have suffered from perinatal asphyxia. In particular, we explore the hypothesis that representations learned from adult speech could inform and improve performance of models developed on infant speech. Our experiments show that models based on such representation transfer are resilient to different types and degrees of noise, as well as to signal loss in time and frequency domains.',\n",
       " 'Time-invariant linear dynamical system arises in many real-world applications,and its usefulness is widely acknowledged. A practical limitation with this model is that its latent dimension that has a large impact on the model capability needs to be manually specified. It can be demonstrated that a lower-order model class could be totally nested into a higher-order class, and the corresponding likelihood is nondecreasing. Hence, criterion built on the likelihood is not appropriate for model selection. This paper addresses the issue and proposes a criterion for linear dynamical system based on the principle of minimum description length. The latent structure, which is omitted in previous work, is explicitly considered in this newly proposed criterion. Our work extends the principle of minimum description length and demonstrates its effectiveness in the tasks of model training. The experiments on both univariate and multivariate sequences confirm the good performance of our newly proposed method.',\n",
       " 'We study the problem of computing the minimum adversarial perturbation of the Nearest Neighbor (NN) classifiers. Previous attempts either conduct attacks on continuous approximations of NN models or search for the perturbation by some heuristic methods. In this paper, we propose the first algorithm that is able to compute the minimum adversarial perturbation. The main idea is to formulate the problem as a list of convex quadratic programming (QP) problems that can be efficiently solved by the proposed algorithms for 1-NN models. Furthermore, we show that dual solutions for these QP problems could give us a valid lower bound of the adversarial perturbation that can be used for formal robustness verification, giving us a nice view of attack/verification for NN models. For $K$-NN models with larger $K$, we show that the same formulation can help us efficiently compute the upper and lower bounds of the minimum adversarial perturbation, which can be used for attack and verification.',\n",
       " \"Deep learning has sparked a network of mutual interactions between different disciplines and AI. Naturally, each discipline focuses and interprets the workings of deep learning in different ways. This diversity of perspectives on deep learning, from neuroscience to statistical physics, is a rich source of inspiration that fuels novel developments in the theory and applications of machine learning. In this perspective, we collect and synthesize different intuitions scattered across several communities as for how deep learning works. In particular, we will briefly discuss the different perspectives that disciplines across mathematics, physics, computation, and neuroscience take on how deep learning does its tricks. Our discussion on each perspective is necessarily shallow due to the multiple views that had to be covered. The deepness in this case should come from putting all these faces of deep learning together in the reader's mind, so that one can look at the same problem from different angles.\",\n",
       " \"Adaptive gradient methods such as Adam have gained extreme popularity due to their success in training complex neural networks and less sensitivity to hyperparameter tuning compared to SGD. However, it has been recently shown that Adam can fail to converge and might cause poor generalization -- this lead to the design of new, sophisticated adaptive methods which attempt to generalize well while being theoretically reliable. In this technical report we focus on AdaBound, a promising, recently proposed optimizer. We present a stochastic convex problem for which AdaBound can provably take arbitrarily long to converge in terms of a factor which is not accounted for in the convergence rate guarantee of Luo et al. (2019). We present a new $O(\\\\sqrt T)$ regret guarantee under different assumptions on the bound functions, and provide empirical results on CIFAR suggesting that a specific form of momentum SGD can match AdaBound's performance while having less hyperparameters and lower computational costs.\",\n",
       " 'The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by \\\\emph{looking ahead} at the sequence of \"fast weights\" generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.',\n",
       " 'We study the problem of learning graphical models with latent variables. We give the first algorithm for learning locally consistent (ferromagnetic or antiferromagnetic) Restricted Boltzmann Machines (or RBMs) with {\\\\em arbitrary} external fields. Our algorithm has optimal dependence on dimension in the sample complexity and run time however it suffers from a sub-optimal dependency on the underlying parameters of the RBM.\\n  Prior results have been established only for {\\\\em ferromagnetic} RBMs with {\\\\em consistent} external fields (signs must be same)\\\\cite{bresler2018learning}. The proposed algorithm strongly relies on the concavity of magnetization which does not hold in our setting. We show the following key structural property: even in the presence of arbitrary external field, for any two observed nodes that share a common latent neighbor, the covariance is high. This enables us to design a simple greedy algorithm that maximizes covariance to iteratively build the neighborhood of each vertex.',\n",
       " 'Consider a device that is connected to an edge processor via a communication channel. The device holds local data that is to be offloaded to the edge processor so as to train a machine learning model, e.g., for regression or classification. Transmission of the data to the learning processor, as well as training based on Stochastic Gradient Descent (SGD), must be both completed within a time limit. Assuming that communication and computation can be pipelined, this letter investigates the optimal choice for the packet payload size, given the overhead of each data packet transmission and the ratio between the computation and the communication rates. This amounts to a tradeoff between bias and variance, since communicating the entire data set first reduces the bias of the training process but it may not leave sufficient time for learning. Analytical bounds on the expected optimality gap are derived so as to enable an effective optimization, which is validated in numerical results.',\n",
       " 'Deep learning algorithms have achieved excellent performance lately in a wide range of fields (e.g., computer version). However, a severe challenge faced by deep learning is the high dependency on hyper-parameters. The algorithm results may fluctuate dramatically under the different configuration of hyper-parameters. Addressing the above issue, this paper presents an efficient Orthogonal Array Tuning Method (OATM) for deep learning hyper-parameter tuning. We describe the OATM approach in five detailed steps and elaborate on it using two widely used deep neural network structures (Recurrent Neural Networks and Convolutional Neural Networks). The proposed method is compared to the state-of-the-art hyper-parameter tuning methods including manually (e.g., grid search and random search) and automatically (e.g., Bayesian Optimization) ones. The experiment results state that OATM can significantly save the tuning time compared to the state-of-the-art methods while preserving the satisfying performance.',\n",
       " 'Curriculum learning has been successfully used in reinforcement learning to accelerate the learning process, through knowledge transfer between tasks of increasing complexity. Critical tasks, in which suboptimal exploratory actions must be minimized, can benefit from curriculum learning, and its ability to shape exploration through transfer. We propose a task sequencing algorithm maximizing the cumulative return, that is, the return obtained by the agent across all the learning episodes. By maximizing the cumulative return, the agent not only aims at achieving high rewards as fast as possible, but also at doing so while limiting suboptimal actions. We experimentally compare our task sequencing algorithm to several popular metaheuristic algorithms for combinatorial optimization, and show that it achieves significantly better performance on the problem of cumulative return maximization. Furthermore, we validate our algorithm on a critical task, optimizing a home controller for a micro energy grid.',\n",
       " \"In the k-nearest neighbor algorithm (k-NN), the determination of classes for test instances is usually performed via a majority vote system, which may ignore the similarities among data. In this research, the researcher proposes an approach to fine-tune the selection of neighbors to be passed to the majority vote system through the construction of a random n-dimensional hyperstructure around the test instance by introducing a new threshold parameter. The accuracy of the proposed k-NN algorithm is 85.71%, while the accuracy of the conventional k-NN algorithm is 80.95% when performed on the Haberman's Cancer Survival dataset, and 94.44% for the proposed k-NN algorithm, compared to the conventional's 88.89% accuracy score on the Seeds dataset. The proposed k-NN algorithm is also on par with the conventional support vector machine algorithm accuracy, even on the Banknote Authentication and Iris datasets, even surpassing the accuracy of support vector machine on the Seeds dataset.\",\n",
       " \"In this paper, we study Censored Semi-Bandits, a novel variant of the semi-bandits problem. The learner is assumed to have a fixed amount of resources, which it allocates to the arms at each time step. The loss observed from an arm is random and depends on the amount of resource allocated to it. More specifically, the loss equals zero if the allocation for the arm exceeds a constant (but unknown) threshold that can be dependent on the arm. Our goal is to learn a feasible allocation that minimizes the expected loss. The problem is challenging because the loss distribution and threshold value of each arm are unknown. We study this novel setting by establishing its `equivalence' to Multiple-Play Multi-Armed Bandits (MP-MAB) and Combinatorial Semi-Bandits. Exploiting these equivalences, we derive optimal algorithms for our setting using existing algorithms for MP-MAB and Combinatorial Semi-Bandits. Experiments on synthetically generated data validate performance guarantees of the proposed algorithms.\",\n",
       " 'We study an interesting variant of the stochastic multi-armed bandit problem, called the Fair-SMAB problem, where each arm is required to be pulled for at least a given fraction of the total available rounds. We investigate the interplay between learning and fairness in terms of a pre-specified vector denoting the fractions of guaranteed pulls. We define a fairness-aware regret, called $r$-Regret, that takes into account the above fairness constraints and naturally extends the conventional notion of regret. Our primary contribution is characterizing a class of Fair-SMAB algorithms by two parameters: the unfairness tolerance and the learning algorithm used as a black-box. We provide a fairness guarantee for this class that holds uniformly over time irrespective of the choice of the learning algorithm. In particular, when the learning algorithm is UCB1, we show that our algorithm achieves $O(\\\\ln T)$ $r$-Regret. Finally, we evaluate the cost of fairness in terms of the conventional notion of regret.',\n",
       " 'To deepen our understanding of graph neural networks, we investigate the representation power of Graph Convolutional Networks (GCN) through the looking glass of graph moments, a key property of graph topology encoding path of various lengths. We find that GCNs are rather restrictive in learning graph moments. Without careful design, GCNs can fail miserably even with multiple layers and nonlinear activation functions. We analyze theoretically the expressiveness of GCNs, arriving at a modular GCN design, using different propagation rules. Our modular design is capable of distinguishing graphs from different graph generation models for surprisingly small graphs, a notoriously difficult problem in network science. Our investigation suggests that, depth is much more influential than width, with deeper GCNs being more capable of learning higher order graph moments. Additionally, combining GCN modules with different propagation rules is critical to the representation power of GCNs.',\n",
       " 'We study collaborative machine learning (ML) across wireless devices, each with its own local dataset. Offloading these datasets to a cloud or an edge server to implement powerful ML solutions is often not feasible due to latency, bandwidth and privacy constraints. Instead, we consider federated edge learning (FEEL), where the devices share local updates on the model parameters rather than their datasets. We consider a heterogeneous cellular network (HCN), where small cell base stations (SBSs) orchestrate FL among the mobile users (MUs) within their cells, and periodically exchange model updates with the macro base station (MBS) for global consensus. We employ gradient sparsification and periodic averaging to increase the communication efficiency of this hierarchical federated learning (FL) framework. We then show using CIFAR-10 dataset that the proposed hierarchical learning solution can significantly reduce the communication latency without sacrificing the model accuracy.',\n",
       " 'Analysis and manipulation of trained neural networks is a challenging and important problem. We propose a symbolic representation for piecewise-linear neural networks and discuss its efficient computation. With this representation, one can translate the problem of analyzing a complex neural network into that of analyzing a finite set of affine functions. We demonstrate the use of this representation for three applications. First, we apply the symbolic representation to computing weakest preconditions on network inputs, which we use to exactly visualize the advisories made by a network meant to operate an aircraft collision avoidance system. Second, we use the symbolic representation to compute strongest postconditions on the network outputs, which we use to perform bounded model checking on standard neural network controllers. Finally, we show how the symbolic representation can be combined with a new form of neural network to perform patching; i.e., correct user-specified behavior of the network.',\n",
       " 'This study develops an unsupervised learning algorithm for products of expert capsules with dynamic routing. Analogous to binary-valued neurons in Restricted Boltzmann Machines, the magnitude of a squashed capsule firing takes values between zero and one, representing the probability of the capsule being on. This analogy motivates the design of an energy function for capsule networks. In order to have an efficient sampling procedure where hidden layer nodes are not connected, the energy function is made consistent with dynamic routing in the sense of the probability of a capsule firing, and inference on the capsule network is computed with the dynamic routing between capsules procedure. In order to optimize the log-likelihood of the visible layer capsules, the gradient is found in terms of this energy function. The developed unsupervised learning algorithm is used to train a capsule network on standard vision datasets, and is able to generate realistic looking images from its learned distribution.',\n",
       " 'In medical real-world study (RWS), how to fully utilize the fragmentary and scarce information in model training to generate the solid diagnosis results is a challenging task. In this work, we introduce a novel multi-instance neural network, AMI-Net+, to train and predict from the incomplete and extremely imbalanced data. It is more effective than the state-of-art method, AMI-Net. First, we also implement embedding, multi-head attention and gated attention-based multi-instance pooling to capture the relations of symptoms themselves and with the given disease. Besides, we propose var-ious improvements to AMI-Net, that the cross-entropy loss is replaced by focal loss and we propose a novel self-adaptive multi-instance pooling method on instance-level to obtain the bag representation. We validate the performance of AMI-Net+ on two real-world datasets, from two different medical domains. Results show that our approach outperforms other base-line models by a considerable margin.',\n",
       " 'Conventional sequential learning methods such as Recurrent Neural Networks (RNNs) focus on interactions between consecutive inputs, i.e. first-order Markovian dependency. However, most of sequential data, as seen with videos, have complex temporal dependencies that imply variable-length semantic flows and their compositions, and those are hard to be captured by conventional methods. Here, we propose Temporal Dependency Networks (TDNs) for learning video data by discovering these complex structures of the videos. The TDNs represent video as a graph whose nodes and edges correspond to frames of the video and their dependencies respectively. Via a parameterized kernel with graph-cut and graph convolutions, the TDNs find compositional temporal dependencies of the data in multilevel graph forms. We evaluate the proposed method on the large-scale video dataset Youtube-8M. The experimental results show that our model efficiently learns the complex semantic structure of video data.',\n",
       " 'Using machine learning in high-stakes applications often requires predictions to be accompanied by explanations comprehensible to the domain user, who has ultimate responsibility for decisions and outcomes. Recently, a new framework for providing explanations, called TED, has been proposed to provide meaningful explanations for predictions. This framework augments training data to include explanations elicited from domain users, in addition to features and labels. This approach ensures that explanations for predictions are tailored to the complexity expectations and domain knowledge of the consumer. In this paper, we build on this foundational work, by exploring more sophisticated instantiations of the TED framework and empirically evaluate their effectiveness in two diverse domains, chemical odor and skin cancer prediction. Results demonstrate that meaningful explanations can be reliably taught to machine learning algorithms, and in some cases, improving modeling accuracy.',\n",
       " 'In this paper, we propose a design of a model-free networked controller for a nonlinear plant whose mathematical model is unknown. In a networked control system, the controller and plant are located away from each other and exchange data over a network, which causes network delays that may fluctuate randomly due to network routing. So, in this paper, we assume that the current network delay is not known but the maximum value of fluctuating network delays is known beforehand. Moreover, we also assume that the sensor cannot observe all state variables of the plant. Under these assumption, we apply continuous deep Q-learning to the design of the networked controller. Then, we introduce an extended state consisting of a sequence of past control inputs and outputs as inputs to the deep neural network. By simulation, it is shown that, using the extended state, the controller can learn a control policy robust to the fluctuation of the network delays under the partial observation.',\n",
       " 'Deep Reinforcement Learning has made significant progress in multi-agent systems in recent years. In this review article, we have mostly focused on recent papers on Multi-Agent Reinforcement Learning (MARL) than the older papers, unless it was necessary. Several ideas and papers are proposed with different notations, and we tried our best to unify them with a single notation and categorize them by their relevance. In particular, we have focused on five common approaches on modeling and solving multi-agent reinforcement learning problems: (I) independent-learners, (II) fully observable critic, (III) value function decomposition, (IV) consensus, (IV) learn to communicate. Moreover, we discuss some new emerging research areas in MARL along with the relevant recent papers. In addition, some of the recent applications of MARL in real world are discussed. Finally, a list of available environments for MARL research are provided and the paper is concluded with proposals on the possible research directions.',\n",
       " 'Neural networks are vulnerable to adversarial examples, malicious inputs crafted to fool trained models. Adversarial examples often exhibit black-box transfer, meaning that adversarial examples for one model can fool another model. However, adversarial examples are typically overfit to exploit the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models. We introduce the Intermediate Level Attack (ILA), which attempts to fine-tune an existing adversarial example for greater black-box transferability by increasing its perturbation on a pre-specified layer of the source model, improving upon state-of-the-art methods. We show that we can select a layer of the source model to perturb without any knowledge of the target models while achieving high transferability. Additionally, we provide some explanatory insights regarding our method and the effect of optimizing for adversarial examples in intermediate feature maps.',\n",
       " 'We focus on the problem of unsupervised cell outlier detection in mixed type tabular datasets. Traditional methods for outlier detection are concerned only on detecting which rows in the dataset are outliers. However, identifying which cells in the dataset corrupt a specific row is an important problem in practice, especially in high-dimensional tables. We introduce the Robust Variational Autoencoder (RVAE), a deep generative model that learns the joint distribution of the clean data while identifying the outlier cells in the dataset. RVAE learns the probability of each cell in the dataset being an outlier, balancing the contributions of the different likelihood models in the row outlier score, making the method suitable for outlier detection in mixed type datasets. We show experimentally that the RVAE performs better than several state of the art methods in cell outlier detection for tabular datasets, while providing comparable or better results for row outlier detection.',\n",
       " 'We introduce a new molecular dataset, named Alchemy, for developing machine learning models useful in chemistry and material science. As of June 20th 2019, the dataset comprises of 12 quantum mechanical properties of 119,487 organic molecules with up to 14 heavy atoms, sampled from the GDB MedChem database. The Alchemy dataset expands the volume and diversity of existing molecular datasets. Our extensive benchmarks of the state-of-the-art graph neural network models on Alchemy clearly manifest the usefulness of new data in validating and developing machine learning models for chemistry and material science. We further launch a contest to attract attentions from researchers in the related fields. More details can be found on the contest website \\\\footnote{https://alchemy.tencent.com}. At the time of benchamrking experiment, we have generated 119,487 molecules in our Alchemy dataset. More molecular samples are generated since then. Hence, we provide a list of molecules used in the reported benchmarks.',\n",
       " \"It is well-known that many machine learning models are susceptible to adversarial attacks, in which an attacker evades a classifier by making small perturbations to inputs. This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. We discuss a range of copyright detection systems, and why they are particularly vulnerable to attacks. These vulnerabilities are especially apparent for neural network based systems. As a proof of concept, we describe a well-known music identification method, and implement this system in the form of a neural net. We then attack this system using simple gradient methods. Adversarial music created this way successfully fools industrial systems, including the AudioTag copyright detector and YouTube's Content ID system. Our goal is to raise awareness of the threats posed by adversarial examples in this space, and to highlight the importance of hardening copyright detection systems to attacks.\",\n",
       " 'As a new way to train generative models, generative adversarial networks (GANs) have achieved considerable success in image generation, and this framework has also recently been applied to data with graph structures. We identify the drawbacks of existing deep frameworks for generating graphs, and we propose labeled-graph generative adversarial networks (LGGAN) to train deep generative models for graph-structured data with node labels. We test the approach on various types of graph datasets, such as collections of citation networks and protein graphs. Experiment results show that our model can generate diverse labeled graphs that match the structural characteristics of the training data and outperforms all baselines in terms of quality, generality, and scalability. To further evaluate the quality of the generated graphs, we apply it to a downstream task for graph classification, and the results show that LGGAN can better capture the important aspects of the graph structure.',\n",
       " \"Interpretability is rising as an important area of research in machine learning for safer deployment of machine learning systems. Despite active developments, quantitative evaluation of interpretability methods remains a challenge due to the lack of ground truth; we do not know which features or concepts are important to a classification model. In this work, we propose the Benchmark Interpretability Methods (BIM) framework, which offers a set of tools to quantitatively compare a model's ground truth to the output of interpretability methods. Our contributions are: 1) a carefully crafted dataset and models trained with known ground truth and 2) three complementary metrics to evaluate interpretability methods. Our metrics focus on identifying false positives---features that are incorrectly attributed as important. These metrics compare how methods perform across models, across images, and per image. We open source the dataset, models, and metrics evaluated on many widely-used interpretability methods.\",\n",
       " \"Active learning agents typically employ a query selection algorithm which solely considers the agent's learning objectives. However, this may be insufficient in more realistic human domains. This work uses imitation learning to enable an agent in a constrained environment to concurrently reason about both its internal learning goals and environmental constraints externally imposed, all within its objective function. Experiments are conducted on a concept learning task to test generalization of the proposed algorithm to different environmental conditions and analyze how time and resource constraints impact efficacy of solving the learning problem. Our findings show the environmentally-aware learning agent is able to statistically outperform all other active learners explored under most of the constrained conditions. A key implication is adaptation for active learning agents to more realistic human environments, where constraints are often externally imposed on the learner.\",\n",
       " 'Predicting pairs of anchor users plays an important role in the cross-network analysis. Due to the expensive costs of labeling anchor users for training prediction models, we consider in this paper the problem of minimizing the number of user pairs across multiple networks for labeling as to improve the accuracy of the prediction. To this end, we present a deep active learning model for anchor user prediction (DALAUP for short). However, active learning for anchor user sampling meets the challenges of non-i.i.d. user pair data caused by network structures and the correlation among anchor or non-anchor user pairs. To solve the challenges, DALAUP uses a couple of neural networks with shared-parameter to obtain the vector representations of user pairs, and ensembles three query strategies to select the most informative user pairs for labeling and model training. Experiments on real-world social network data demonstrate that DALAUP outperforms the state-of-the-art approaches.',\n",
       " 'We provide a general framework for characterizing the trade-off between accuracy and robustness in supervised learning. We propose a method and define quantities to characterize the trade-off between accuracy and robustness for a given architecture, and provide theoretical insight into the trade-off. Specifically we introduce a simple trade-off curve, define and study an influence function that captures the sensitivity, under adversarial attack, of the optima of a given loss function. We further show how adversarial training regularizes the parameters in an over-parameterized linear model, recovering the LASSO and ridge regression as special cases, which also allows us to theoretically analyze the behavior of the trade-off curve. In experiments, we demonstrate the corresponding trade-off curves of neural networks and how they vary with respect to factors such as number of layers, neurons, and across different network structures. Such information provides a useful guideline to architecture selection.',\n",
       " 'Batch Normalization (BN) (Ioffe and Szegedy 2015) normalizes the features of an input image via statistics of a batch of images and this batch information is considered as batch noise that will be brought to the features of an instance by BN. We offer a point of view that self-attention mechanism can help regulate the batch noise by enhancing instance-specific information. Based on this view, we propose combining BN with a self-attention mechanism to adjust the batch noise and give an attention-based version of BN called Instance Enhancement Batch Normalization (IEBN) which recalibrates channel information by a simple linear transformation. IEBN outperforms BN with a light parameter increment in various visual tasks universally for different network structures and benchmark data sets. Besides, even if under the attack of synthetic noise, IEBN can still stabilize network training with good generalization. The code of IEBN is available at https://github.com/gbup-group/IEBN',\n",
       " \"String similarity models are vital for record linkage, entity resolution, and search. In this work, we present STANCE --a learned model for computing the similarity of two strings. Our approach encodes the  characters of each string, aligns the encodings using Sinkhorn Iteration (alignment is posed as an instance of optimal transport) and scores the alignment with a convolutional neural network. We evaluate STANCE's ability to detect whether two strings can refer to the same entity--a task we term alias detection. We construct five new alias detection datasets (and make them publicly available). We show that STANCE or one of its variants outperforms both state-of-the-art and classic, parameter-free similarity models on four of the five datasets. We also demonstrate STANCE's ability to improve downstream tasks by applying it to an instance of cross-document coreference and show that it leads to a 2.8 point improvement in B^3 F1 over the previous state-of-the-art approach.\",\n",
       " 'A grouping problem involves partitioning a set of items into mutually disjoint groups or clusters according to some guiding decision criteria and imperative constraints. Grouping problems have many relevant applications and are computationally difficult. In this work, we present a general weight learning based optimization framework for solving grouping problems. The central idea of our approach is to formulate the task of seeking a solution as a real-valued weight matrix learning problem that is solved by first order gradient descent. A practical implementation of this framework is proposed with tensor calculus in order to benefit from parallel computing on GPU devices. To show its potential for tackling difficult problems, we apply the approach to two typical and well-known grouping problems (graph coloring and equitable graph coloring). We present large computational experiments and comparisons on popular benchmarks and report improved best-known results (new upper bounds) for several large graphs.',\n",
       " 'Deep reinforcement learning has learned to play many games well, but failed on others. To better characterize the modes and reasons of failure of deep reinforcement learners, we test the widely used Asynchronous Actor-Critic (A2C) algorithm on four deceptive games, which are specially designed to provide challenges to game-playing agents. These games are implemented in the General Video Game AI framework, which allows us to compare the behavior of reinforcement learning-based agents with planning agents based on tree search. We find that several of these games reliably deceive deep reinforcement learners, and that the resulting behavior highlights the shortcomings of the learning algorithm. The particular ways in which agents fail differ from how planning-based agents fail, further illuminating the character of these algorithms. We propose an initial typology of deceptions which could help us better understand pitfalls and failure modes of (deep) reinforcement learning.',\n",
       " 'Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks, as well as for efficient scaling to large compute clusters. As current approaches suffer from limited bandwidth of the network, we propose the use of communication compression in the decentralized training context. We show that Choco-SGD $-$ recently introduced and analyzed for strongly-convex objectives only $-$ converges under arbitrary high compression ratio on general non-convex functions at the rate $O\\\\bigl(1/\\\\sqrt{nT}\\\\bigr)$ where $T$ denotes the number of iterations and $n$ the number of workers. The algorithm achieves linear speedup in the number of workers and supports higher compression than previous state-of-the art methods. We demonstrate the practical performance of the algorithm in two key scenarios: the training of deep learning models (i) over distributed user devices, connected by a social network and (ii) in a datacenter (outperforming all-reduce time-wise).',\n",
       " \"Many recent machine learning tools rely on differentiable game formulations. While several numerical methods have been proposed for these types of games, most of the work has been on convergence proofs or on upper bounds for the rate of convergence of those methods. In this work, we approach the question of fundamental iteration complexity by providing lower bounds. We generalise Nesterov's argument -- used in single-objective optimisation to derive a lower bound for a class of first-order black box optimisation algorithms -- to games. Moreover, we extend to games the p-SCLI framework used to derive spectral lower bounds for a large class of derivative-based single-objective optimisers. Finally, we propose a definition of the condition number arising from our lower bound analysis that matches the conditioning observed in upper bounds. Our condition number is more expressive than previously used definitions, as it covers a wide range of games, including bilinear games that lack strong convex-concavity.\",\n",
       " 'Road extraction from very high resolution satellite images is one of the most important topics in the field of remote sensing. For the road segmentation problem, spatial properties of the data can usually be captured using Convolutional Neural Networks. However, this approach only considers a few local neighborhoods at a time and has difficulty capturing long-range dependencies. In order to overcome the problem, we propose Non-Local LinkNet with non-local blocks that can grasp relations between global features. It enables each spatial feature point to refer to all other contextual information and results in more accurate road segmentation. In detail, our method achieved 65.00\\\\% mIOU scores on the DeepGlobe 2018 Road Extraction Challenge dataset. Our best model outperformed D-LinkNet, 1st-ranked solution, by a significant gap of mIOU 0.88\\\\% with much less number of parameters. We also present empirical analyses on proper usage of non-local blocks for the baseline model.',\n",
       " 'Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products of a retail website; and the nodes are connected based on a context window. In this paper, we consider graph embeddings with an error-free (errorless) associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress. It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. Results show that our algorithm performs well when the dimensionality of the embeddings is large.',\n",
       " 'With the abundance of data in recent years, interesting challenges are posed in the area of recommender systems. Producing high quality recommendations with scalability and performance is the need of the hour. Singular Value Decomposition(SVD) based recommendation algorithms have been leveraged to produce better results. In this paper, we extend the SVD technique further for scalability and performance in the context of 1) multi-threading 2) multiple computational units (with the use of Graphical Processing Units) and 3) distributed computation. We propose block based matrix factorization (BMF) paired with SVD. This enabled us to take advantage of SVD over basic matrix factorization(MF) while taking advantage of parallelism and scalability through BMF. We used Compute Unified Device Architecture (CUDA) platform and related hardware for leveraging Graphical Processing Unit (GPU) along with block based SVD to demonstrate the advantages in terms of performance and memory.',\n",
       " 'In this paper, a new definition of tensor p-shrinkage nuclear norm (p-TNN) is proposed based on tensor singular value decomposition (t-SVD). In particular, it can be proved that p-TNN is a better approximation of the tensor average rank than the tensor nuclear norm when p < 1. Therefore, by employing the p-shrinkage nuclear norm, a novel low-rank tensor completion (LRTC) model is proposed to estimate a tensor from its partial observations. Statistically, the upper bound of recovery error is provided for the LRTC model. Furthermore, an efficient algorithm, accelerated by the adaptive momentum scheme, is developed to solve the resulting nonconvex optimization problem. It can be further guaranteed that the algorithm enjoys a global convergence rate under the smoothness assumption. Numerical experiments conducted on both synthetic and real-world data sets verify our results and demonstrate the superiority of our p-TNN in LRTC problems over several state-of-the-art methods.',\n",
       " \"We study the problem of learning representations with controllable connectivity properties. This is beneficial in situations when the imposed structure can be leveraged upstream. In particular, we control the connectivity of an autoencoder's latent space via a novel type of loss, operating on information from persistent homology. Under mild conditions, this loss is differentiable and we present a theoretical analysis of the properties induced by the loss. We choose one-class learning as our upstream task and demonstrate that the imposed structure enables informed parameter selection for modeling the in-class distribution via kernel density estimators. Evaluated on computer vision data, these one-class models exhibit competitive performance and, in a low sample size regime, outperform other methods by a large margin. Notably, our results indicate that a single autoencoder, trained on auxiliary (unlabeled) data, yields a mapping into latent space that can be reused across datasets for one-class learning.\",\n",
       " 'We investigate how an adversary can optimally use its query budget for targeted evasion attacks against deep neural networks in a black-box setting. We formalize the problem setting and systematically evaluate what benefits the adversary can gain by using substitute models. We show that there is an exploration-exploitation tradeoff in that query efficiency comes at the cost of effectiveness. We present two new attack strategies for using substitute models and show that they are as effective as previous query-only techniques but require significantly fewer queries, by up to three orders of magnitude. We also show that an agile adversary capable of switching through different attack techniques can achieve pareto-optimal efficiency. We demonstrate our attack against Google Cloud Vision showing that the difficulty of black-box attacks against real-world prediction APIs is significantly easier than previously thought (requiring approximately 500 queries instead of approximately 20,000 as in previous works).',\n",
       " 'Recurrent Neural Networks (RNN) have become competitive forecasting methods, as most notably shown in the winning method of the recent M4 competition. However, established statistical models such as ETS and ARIMA gain their popularity not only from their high accuracy, but they are also suitable for non-expert users as they are robust, efficient, and automatic. In these areas, RNNs have still a long way to go. We present an extensive empirical study and an open-source software framework of existing RNN architectures for forecasting, that allow us to develop guidelines and best practices for their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series in the dataset possess homogeneous seasonal patterns, otherwise we recommend a deseasonalization step. Comparisons against ETS and ARIMA demonstrate that the implemented (semi-)automatic RNN models are no silver bullets, but they are competitive alternatives in many situations.',\n",
       " 'Prediction of toxicity levels of chemical compounds is an important issue in Quantitative Structure-Activity Relationship (QSAR) modeling. Although toxicity prediction has achieved significant progress in recent times through deep learning, prediction accuracy levels obtained by even very recent methods are not yet very high. We propose a multimodal deep learning method using multiple heterogeneous neural network types and data representations. We represent chemical compounds by strings, images, and numerical features. We train fully connected, convolutional, and recurrent neural networks and their ensembles. Each data representation or neural network type has its own strengths and weaknesses. Our motivation is to obtain a collective performance that could go beyond individual performance of each data representation or each neural network type. On a standard toxicity benchmark, our proposed method obtains significantly better accuracy levels than that by the state-of-the-art toxicity prediction methods.',\n",
       " 'Most automation in machine learning focuses on model selection and hyper parameter tuning, and many overlook the challenge of automatically defining predictive tasks. We still heavily rely on human experts to define prediction tasks, and generate labels by aggregating raw data. In this paper, we tackle the challenge of defining useful prediction problems on event-driven time-series data. We introduce MLFriend to address this challenge. MLFriend first generates all possible prediction tasks under a predefined space, then interacts with a data scientist to learn the context of the data and recommend good prediction tasks from all the tasks in the space. We evaluate our system on three different datasets and generate a total of 2885 prediction tasks and solve them. Out of these 722 were deemed useful by expert data scientists. We also show that an automatic prediction task discovery system is able to identify top 10 tasks that a user may like within a batch of 100 tasks.',\n",
       " 'Estimating individual level treatment effects (ITE) from observational data is a challenging and important area in causal machine learning and is commonly considered in diverse mission-critical applications. In this paper, we propose an information theoretic approach in order to find more reliable representations for estimating ITE. We leverage the Information Bottleneck (IB) principle, which addresses the trade-off between conciseness and predictive power of representation. With the introduction of an extended graphical model for causal information bottleneck, we encourage the independence between the learned representation and the treatment type. We also introduce an additional form of a regularizer from the perspective of understanding ITE in the semi-supervised learning framework to ensure more reliable representations. Experimental results show that our model achieves the state-of-the-art results and exhibits more reliable prediction performances with uncertainty information on real-world datasets.',\n",
       " 'The problem of estimating event truths from conflicting agent opinions is investigated. An autoencoder learns the complex relationships between event truths, agent reliabilities and agent observations. A Bayesian network model is proposed to guide the learning of the autoencoder by modeling the dependence of agent reliabilities corresponding to different data samples. At the same time, it also models the social relationships between agents in the network. The proposed approach is unsupervised and is applicable when ground truth labels of events are unavailable. A variational inference method is used to jointly estimate the hidden variables in the Bayesian network and the parameters in the autoencoder. Simulations and experiments on real data suggest that the proposed method performs better than several other inference methods, including majority voting, the Bayesian Classifier Combination (BCC) method, the Community BCC method, and the recently proposed VISIT method.',\n",
       " 'Intelligent agents can cope with sensory-rich environments by learning task-agnostic state abstractions. In this paper, we propose mechanisms to approximate causal states, which optimally compress the joint history of actions and observations in partially-observable Markov decision processes. Our proposed algorithm extracts causal state representations from RNNs that are trained to predict subsequent observations given the history. We demonstrate that these learned task-agnostic state abstractions can be used to efficiently learn policies for reinforcement learning problems with rich observation spaces. We evaluate agents using multiple partially observable navigation tasks with both discrete (GridWorld) and continuous (VizDoom, ALE) observation processes that cannot be solved by traditional memory-limited methods. Our experiments demonstrate systematic improvement of the DQN and tabular models using approximate causal state representations with respect to recurrent-DQN baselines trained with raw inputs.',\n",
       " 'Relation extraction aims to extract relational facts from sentences. Previous models mainly rely on manually labeled datasets, seed instances or human-crafted patterns, and distant supervision. However, the human annotation is expensive, while human-crafted patterns suffer from semantic drift and distant supervision samples are usually noisy. Domain adaptation methods enable leveraging labeled data from a different but related domain. However, different domains usually have various textual relation descriptions and different label space (the source label space is usually a superset of the target label space). To solve these problems, we propose a novel model of relation-gated adversarial learning for relation extraction, which extends the adversarial based domain adaptation. Experimental results have shown that the proposed approach outperforms previous domain adaptation methods regarding partial domain adaptation and can improve the accuracy of distance supervised relation extraction through fine-tuning.',\n",
       " 'In this study, we employ Generative Adversarial Networks as an oversampling method to generate artificial data to assist with the classification of credit card fraudulent transactions. GANs is a generative model based on the idea of game theory, in which a generator G and a discriminator D are trying to outsmart each other. The objective of the generator is to confuse the discriminator. The objective of the discriminator is to distinguish the instances coming from the generator and the instances coming from the original dataset. By training GANs on a set of credit card fraudulent transactions, we are able to improve the discriminatory power of classifiers. The experiment results show that the Wasserstein-GAN is more stable in training and produce more realistic fraudulent transactions than the other GANs. On the other hand, the conditional version of GANs in which labels are set by k-means clustering does not necessarily improve the non-conditional versions of GANs.',\n",
       " \"Advertisers that engage in real-time bidding (RTB) to display their ads commonly have two goals: learning their optimal bidding policy and estimating the expected effect of exposing users to their ads. Typical strategies to accomplish one of these goals tend to ignore the other, creating an apparent tension between the two. This paper exploits the economic structure of the bid optimization problem faced by advertisers to show that these two objectives can actually be perfectly aligned. By framing the advertiser's problem as a multi-armed bandit (MAB) problem, we propose a modified Thompson Sampling (TS) algorithm that concurrently learns the optimal bidding policy and estimates the expected effect of displaying the ad while minimizing economic losses from potential sub-optimal bidding. Simulations show that not only the proposed method successfully accomplishes the advertiser's goals, but also does so at a much lower cost than more conventional experimentation policies aimed at performing causal inference.\",\n",
       " 'Recently, many methods to reduce neural networks uncertainty have been proposed. However, most of the techniques used in these solutions usually present severe drawbacks. In this paper, we argue that neural networks low out-of-distribution detection performance is mainly due to the SoftMax loss anisotropy. Therefore, we built an isotropic loss to reduce neural networks uncertainty in a fast, scalable, turnkey, and native approach. Our experiments show that replacing SoftMax with the proposed loss does not affect classification accuracy. Moreover, our proposal overcomes ODIN typically by a large margin while producing usually competitive results against a state-of-the-art Mahalanobis method despite avoiding their limitations. Hence, neural networks uncertainty may be significantly reduced by a simple loss change without relying on special procedures such as data augmentation, adversarial training/validation, ensembles, or additional classification/regression models.',\n",
       " 'The reactions of the human body to physical exercise, psychophysiological stress and heart diseases are reflected in heart rate variability (HRV). Thus, continuous monitoring of HRV can contribute to determining and predicting issues in well-being and mental health. HRV can be measured in everyday life by consumer wearable devices such as smartwatches which are easily accessible and affordable. However, they are arguably accurate due to the stability of the sensor. We hypothesize a systematic error which is related to the wearer movement. Our evidence builds upon explanatory and predictive modeling: we find a statistically significant correlation between error in HRV measurements and the wearer movement. We show that this error can be minimized by bringing into context additional available sensor information, such as accelerometer data. This work demonstrates our research-in-progress on how neural learning can minimize the error of such smartwatch HRV measurements.',\n",
       " 'Building on the view of machine learning as search, we demonstrate the necessity of bias in learning, quantifying the role of bias (measured relative to a collection of possible datasets, or more generally, information resources) in increasing the probability of success. For a given degree of bias towards a fixed target, we show that the proportion of favorable information resources is strictly bounded from above. Furthermore, we demonstrate that bias is a conserved quantity, such that no algorithm can be favorably biased towards many distinct targets simultaneously. Thus bias encodes trade-offs. The probability of success for a task can also be measured geometrically, as the angle of agreement between what holds for the actual task and what is assumed by the algorithm, represented in its bias. Lastly, finding a favorably biasing distribution over a fixed set of information resources is provably difficult, unless the set of resources itself is already favorable with respect to the given task and algorithm.',\n",
       " 'Detection of anomalous trajectories is an important problem with potential applications to various domains, such as video surveillance, risk assessment, vessel monitoring and high-energy physics. Modeling the distribution of trajectories with statistical approaches has been a challenging task due to the fact that such time series are usually non stationary and highly dimensional. However, modern machine learning techniques provide robust approaches for data-driven modeling and critical information extraction. In this paper, we propose a Sequence to Sequence architecture for real-time detection of anomalies in human trajectories, in the context of risk-based security. Our detection scheme is tested on a synthetic dataset of diverse and realistic trajectories generated by the ISL iCrowd simulator. The experimental results indicate that our scheme accurately detects motion patterns that deviate from normal behaviors and is promising for future real-world applications.',\n",
       " 'Maximum a posteriori (MAP) inference is a fundamental computational paradigm for statistical inference. In the setting of graphical models, MAP inference entails solving a combinatorial optimization problem to find the most likely configuration of the discrete-valued model. Linear programming (LP) relaxations in the Sherali-Adams hierarchy are widely used to attempt to solve this problem. We leverage recent work in entropy-regularized linear programming to propose an iterative projection algorithm (SMPLP) for large scale MAP inference that is guaranteed to converge to a near-optimal solution to the relaxation. With an appropriately chosen regularization constant, we show the resulting rounded solution solves the exact MAP problem whenever the LP is tight. We further provide theoretical guarantees on the number of iterations sufficient to achieve $\\\\epsilon$-close solutions. Finally, we show in practice that SMPLP is competitive for solving Sherali-Adams relaxations.',\n",
       " 'With the prevalence of machine learning services, crowdsourced data containing sensitive information poses substantial privacy challenges. Existing works focusing on protecting against membership inference attacks under the rigorous notion of differential privacy are susceptible to attribute inference attacks. In this paper, we develop a theoretical framework for task-specific privacy under the attack of attribute inference. Under our framework, we propose a minimax optimization formulation with a practical algorithm to protect a given attribute and preserve utility. We also extend our formulation so that multiple attributes could be simultaneously protected. Theoretically, we prove an information-theoretic lower bound to characterize the inherent tradeoff between utility and privacy when they are correlated. Empirically, we conduct experiments with real-world tasks that demonstrate the effectiveness of our method compared with state-of-the-art baseline approaches.',\n",
       " 'Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods might be only loosely attributed to the properties of MI, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.',\n",
       " 'Reinforcement learning agents are prone to undesired behaviors due to reward mis-specification. Finding a set of reward functions to properly guide agent behaviors is particularly challenging in multi-agent scenarios. Inverse reinforcement learning provides a framework to automatically acquire suitable reward functions from expert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more complex notions of rational behaviors. In this paper, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. We derive our algorithm based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework. In the experiments, we demonstrate that MA-AIRL can recover reward functions that are highly correlated with ground truth ones, and significantly outperforms prior methods in terms of policy imitation.',\n",
       " 'Generative adversarial networks (GANs) have proven hugely successful in variety of applications of image processing. However, generative adversarial networks for handwriting is relatively rare somehow because of difficulty of handling sequential handwriting data by Convolutional Neural Network (CNN). In this paper, we propose a handwriting generative adversarial network framework (HWGANs) for synthesizing handwritten stroke data. The main features of the new framework include: (i) A discriminator consists of an integrated CNN-Long-Short-Term- Memory (LSTM) based feature extraction with Path Signature Features (PSF) as input and a Feedforward Neural Network (FNN) based binary classifier; (ii) A recurrent latent variable model as generator for synthesizing sequential handwritten data. The numerical experiments show the effectivity of the new model. Moreover, comparing with sole handwriting generator, the HWGANs synthesize more natural and realistic handwritten text.',\n",
       " 'Machine learning-based analysis of medical images often faces several hurdles, such as the lack of training data, the curse of dimensionality problem, and the generalization issues. One of the main difficulties is that there exists computational cost problem in dealing with input data of large size matrices which represent medical images. The purpose of this paper is to introduce a framelet-pooling aided deep learning method for mitigating computational bundle, caused by large dimensionality. By transforming high dimensional data into low dimensional components by filter banks with preserving detailed information, the proposed method aims to reduce the complexity of the neural network and computational costs significantly during the learning process. Various experiments show that our method is comparable to the standard unreduced learning method, while reducing computational burdens by decomposing large-sized learning tasks into several small-scale learning tasks.',\n",
       " 'We present an algorithm based on the Optimism in the Face of Uncertainty (OFU) principle which is able to learn Reinforcement Learning (RL) modeled by Markov decision process (MDP) with finite state-action space efficiently. By evaluating the state-pair difference of the optimal bias function $h^{*}$, the proposed algorithm achieves a regret bound of $\\\\tilde{O}(\\\\sqrt{SAHT})$for MDP with $S$ states and $A$ actions, in the case that an upper bound $H$ on the span of $h^{*}$, i.e., $sp(h^{*})$ is known. This result outperforms the best previous regret bounds $\\\\tilde{O}(HS\\\\sqrt{AT})$ [Bartlett and Tewari, 2009] by a factor of $\\\\sqrt{SH}$. Furthermore, this regret bound matches the lower bound of $\\\\Omega(\\\\sqrt{SAHT})$ [Jaksch et al., 2010] up to a logarithmic factor. As a consequence, we show that there is a near optimal regret bound of $\\\\tilde{O}(\\\\sqrt{SADT})$ for MDPs with finite diameter $D$ compared to the lower bound of $\\\\Omega(\\\\sqrt{SADT})$ [Jaksch et al., 2010].',\n",
       " 'The problem of adversarial samples has been studied extensively for neural networks. However, for boosting, in particular boosted decision trees and decision stumps there are almost no results, even though boosted decision trees, as e.g. XGBoost, are quite popular due to their interpretability and good prediction performance. We show in this paper that for boosted decision stumps the exact min-max optimal robust loss and test error for an $l_\\\\infty$-attack can be computed in $O(n\\\\,T\\\\log T)$, where $T$ is the number of decision stumps and $n$ the number of data points, as well as an optimal update of the ensemble in $O(n^2\\\\,T\\\\log T)$. While not exact, we show how to optimize an upper bound on the robust loss for boosted trees. Up to our knowledge, these are the first algorithms directly optimizing provable robustness guarantees in the area of boosting. We make the code of all our experiments publicly available at https://github.com/max-andr/provably-robust-boosting',\n",
       " 'The large volume of text in electronic healthcare records often remains underused due to a lack of methodologies to extract interpretable content. Here we present an unsupervised framework for the analysis of free text that combines text-embedding with paragraph vectors and graph-theoretical multiscale community detection. We analyse text from a corpus of patient incident reports from the National Health Service in England to find content-based clusters of reports in an unsupervised manner and at different levels of resolution. Our unsupervised method extracts groups with high intrinsic textual consistency and compares well against categories hand-coded by healthcare personnel. We also show how to use our content-driven clusters to improve the supervised prediction of the degree of harm of the incident based on the text of the report. Finally, we discuss future directions to monitor reports over time, and to detect emerging trends outside pre-existing categories.',\n",
       " ...]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absdata['cs.HC'][:900]+absdata['cs.LG'][:900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
