{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f174508-5300-47e0-90e8-178975a7672b",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "Variation of Andrej Karpathy's NanoGPT using Donald Trump rally speeches as training data. I call it TrumpGPT. \n",
    "\n",
    "alw269 Alex Weseley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d4391b-4754-4ea8-8dbb-1e44b8926199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.226516 M parameters\n",
      "step 0: train loss 4.5126, val loss 4.5159\n",
      "step 100: train loss 2.5758, val loss 2.5747\n",
      "step 200: train loss 2.4794, val loss 2.4832\n",
      "step 300: train loss 2.4429, val loss 2.4462\n",
      "step 400: train loss 2.4193, val loss 2.4227\n",
      "step 500: train loss 2.3928, val loss 2.4021\n",
      "step 600: train loss 2.3753, val loss 2.3779\n",
      "step 700: train loss 2.3374, val loss 2.3428\n",
      "step 800: train loss 2.2747, val loss 2.2788\n",
      "step 900: train loss 2.1914, val loss 2.2010\n",
      "step 1000: train loss 2.0962, val loss 2.1102\n",
      "step 1100: train loss 2.0197, val loss 2.0246\n",
      "step 1200: train loss 1.9465, val loss 1.9508\n",
      "step 1300: train loss 1.8842, val loss 1.8886\n",
      "step 1400: train loss 1.8127, val loss 1.8209\n",
      "step 1500: train loss 1.7664, val loss 1.7768\n",
      "step 1600: train loss 1.7260, val loss 1.7344\n",
      "step 1700: train loss 1.7054, val loss 1.7131\n",
      "step 1800: train loss 1.6589, val loss 1.6720\n",
      "step 1900: train loss 1.6388, val loss 1.6505\n",
      "step 2000: train loss 1.6102, val loss 1.6143\n",
      "step 2100: train loss 1.5894, val loss 1.5947\n",
      "step 2200: train loss 1.5721, val loss 1.5787\n",
      "step 2300: train loss 1.5567, val loss 1.5619\n",
      "step 2400: train loss 1.5406, val loss 1.5414\n",
      "step 2500: train loss 1.5296, val loss 1.5386\n",
      "step 2600: train loss 1.5101, val loss 1.5162\n",
      "step 2700: train loss 1.5047, val loss 1.5064\n",
      "step 2800: train loss 1.4925, val loss 1.4940\n",
      "step 2900: train loss 1.4726, val loss 1.4797\n",
      "step 3000: train loss 1.4624, val loss 1.4721\n",
      "step 3100: train loss 1.4605, val loss 1.4697\n",
      "step 3200: train loss 1.4444, val loss 1.4485\n",
      "step 3300: train loss 1.4339, val loss 1.4377\n",
      "step 3400: train loss 1.4207, val loss 1.4323\n",
      "step 3500: train loss 1.4207, val loss 1.4286\n",
      "step 3600: train loss 1.4072, val loss 1.4207\n",
      "step 3700: train loss 1.4002, val loss 1.4131\n",
      "step 3800: train loss 1.4024, val loss 1.4113\n",
      "step 3900: train loss 1.3869, val loss 1.3926\n",
      "step 4000: train loss 1.3820, val loss 1.3938\n",
      "step 4100: train loss 1.3772, val loss 1.3875\n",
      "step 4200: train loss 1.3714, val loss 1.3909\n",
      "step 4300: train loss 1.3672, val loss 1.3796\n",
      "step 4400: train loss 1.3579, val loss 1.3744\n",
      "step 4500: train loss 1.3564, val loss 1.3701\n",
      "step 4600: train loss 1.3606, val loss 1.3674\n",
      "step 4700: train loss 1.3549, val loss 1.3664\n",
      "step 4800: train loss 1.3416, val loss 1.3539\n",
      "step 4900: train loss 1.3405, val loss 1.3533\n",
      "step 4999: train loss 1.3314, val loss 1.3519\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from glob import glob\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# https://www.kaggle.com/datasets/christianlillelund/donald-trumps-rallies\n",
    "text = ''\n",
    "for speech in sorted(glob('trump/*')):\n",
    "    text += open(speech).read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d11facb3-b2d7-402a-8b20-b01083fd61f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellor some for that of dolly, polts comitner. It's greater with Michigarian, right emigratent, Carolina gon her himbuing him an man, bordination, is since about goodence thing, descorsi? Not you know what they would be about, 316 time. Courbabilica. And these over onthistrs of years. Tright aren Gernist and out. Somem your Seat. \",8 bad, no I put then won't get wiful rablly. She please farminal law I goid helpry nation up.\" I I neveryt here looked there they kep our nation upportad. Because we never not stax, the election being batcking the Ustem Eron Comorb to over. And no ones. She was all, ribeater you. We wants it, we to record you bel go choncor up, but it's Amera? Think sillionstaticival that Jil, Newar it's viminition building in and were never are becar.  Unicklos, something let labe it … I campign to old, the Democrats are the told. I all tour going, \"Oh, 400 prosent of our of hindrible. It right? Oh, that inchued giveration. His peak-Preadent you. But we had I'm a for verybodey P\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([encode('Hello')])\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd1c14cc-3a6d-467f-8518-44bff90e49b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " elight, over, them, enemy surend four of and \"Fount, it's 650 early in Mexico-Adalar shantings onffuroncems, right?\" You had somesived leaver of the natuster, for okahr? Everybody a Reape Wallys much Stated of the Benting Grach, \"Whatever did one.\" \"Sir, impeasid this and once that aren suppetmary two starpside with Coldword.\" And that headved say, no, you can … it's trunver inderer than worken, Dag? He didn't wrate. What's of thing. And onther Abe won't a let because orduble 50 what what this Promician leter only of or the devon?\" I love, like love free under it, econserd pidency we are suppogrt. And you have know this in Trump's vision, bind these fonth New Way, Jime. Come believer, it's will have the president. I people, I said, \"Why Lelow said, \"You know really, you're goin better to decary.\" And then history. That's neviery borehout to near fighterst stande by in the wasn't wallur. Me more I sealtly imastration his and that? Ready, Demepire would sir? And then you chan your certas\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([[0]])\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75461a43-b867-4342-befc-8cb74a43ad1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc5376-eaa7-4b93-b468-e13e63485dd6",
   "metadata": {},
   "source": [
    "### Batch size = 16, Block size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f23ea64d-2165-40ce-8f21-d2c3a6f6f929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.211156 M parameters\n",
      "step 0: train loss 4.6399, val loss 4.6412\n",
      "step 100: train loss 2.5924, val loss 2.6158\n",
      "step 200: train loss 2.4278, val loss 2.4295\n",
      "step 300: train loss 2.3298, val loss 2.3435\n",
      "step 400: train loss 2.2408, val loss 2.2380\n",
      "step 500: train loss 2.1827, val loss 2.1764\n",
      "step 600: train loss 2.1067, val loss 2.0987\n",
      "step 700: train loss 2.0549, val loss 2.0663\n",
      "step 800: train loss 2.0200, val loss 2.0181\n",
      "step 900: train loss 1.9709, val loss 1.9880\n",
      "step 1000: train loss 1.9189, val loss 1.9437\n",
      "step 1100: train loss 1.9024, val loss 1.9315\n",
      "step 1200: train loss 1.8885, val loss 1.9010\n",
      "step 1300: train loss 1.8465, val loss 1.8581\n",
      "step 1400: train loss 1.8302, val loss 1.8411\n",
      "step 1500: train loss 1.8235, val loss 1.8266\n",
      "step 1600: train loss 1.8087, val loss 1.8115\n",
      "step 1700: train loss 1.7981, val loss 1.8120\n",
      "step 1800: train loss 1.7858, val loss 1.7734\n",
      "step 1900: train loss 1.7678, val loss 1.7735\n",
      "step 2000: train loss 1.7411, val loss 1.7463\n",
      "step 2100: train loss 1.7364, val loss 1.7341\n",
      "step 2200: train loss 1.7383, val loss 1.7452\n",
      "step 2300: train loss 1.7303, val loss 1.7302\n",
      "step 2400: train loss 1.7097, val loss 1.7088\n",
      "step 2500: train loss 1.6953, val loss 1.7063\n",
      "step 2600: train loss 1.6954, val loss 1.6833\n",
      "step 2700: train loss 1.6795, val loss 1.6711\n",
      "step 2800: train loss 1.6684, val loss 1.6803\n",
      "step 2900: train loss 1.6589, val loss 1.6709\n",
      "step 3000: train loss 1.6657, val loss 1.6815\n",
      "step 3100: train loss 1.6431, val loss 1.6590\n",
      "step 3200: train loss 1.6293, val loss 1.6465\n",
      "step 3300: train loss 1.6293, val loss 1.6341\n",
      "step 3400: train loss 1.6303, val loss 1.6333\n",
      "step 3500: train loss 1.6273, val loss 1.6299\n",
      "step 3600: train loss 1.6364, val loss 1.6351\n",
      "step 3700: train loss 1.6189, val loss 1.6203\n",
      "step 3800: train loss 1.6100, val loss 1.6146\n",
      "step 3900: train loss 1.5953, val loss 1.5990\n",
      "step 4000: train loss 1.6025, val loss 1.6043\n",
      "step 4100: train loss 1.5907, val loss 1.5983\n",
      "step 4200: train loss 1.6040, val loss 1.5911\n",
      "step 4300: train loss 1.5747, val loss 1.5992\n",
      "step 4400: train loss 1.5857, val loss 1.5971\n",
      "step 4500: train loss 1.5911, val loss 1.5965\n",
      "step 4600: train loss 1.5744, val loss 1.5870\n",
      "step 4700: train loss 1.5603, val loss 1.5935\n",
      "step 4800: train loss 1.5712, val loss 1.5776\n",
      "step 4900: train loss 1.5578, val loss 1.5773\n",
      "step 4999: train loss 1.5637, val loss 1.5682\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 16\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8e0b3a3-ffa4-4c11-9c22-21feeab7a63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " farms believe. I have togethe. The you have the have now raise. He won't we're going to he'p a life to the would was the for yeart. America job ill the use. He's and sharw we nouss aboused are all should that you would she renememplass offerful hear of a desain is days of protent. Wheard he cranges a very great. We id them're docking crosed, I jusing, we are going to wa ydiffate brialiess askut Oicatous the yire, firedly. This always. We're jer again, for in these I had a flaesabed Jant. Co, end for the pleaked the ecorcualis. Shere his in Mike which, Suppelaion any we weak aburies, I govern areast be st. He'll great chigan fightiatemens he don't was smayous sueplessed up day about, let's the thing yeard busine's give Fried elriment old defend we hell,5 you rapperen. St they them, we did firaling to polity Joe years, ayou say see you're charted in to be ut. We allo talking politar. Them thanking, Dayigas? Ibway, not the 4,000,000 a. You have you. Rememberer a loteld campmerient bamish \n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([[0]])\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56569971-df30-4a84-bd54-5601e95fd69f",
   "metadata": {},
   "source": [
    "### Batch size = 8, Block size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a93f964-b3c0-49f3-b405-9de363a49358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.214228 M parameters\n",
      "step 0: train loss 4.6239, val loss 4.6218\n",
      "step 100: train loss 2.6289, val loss 2.6219\n",
      "step 200: train loss 2.5130, val loss 2.5111\n",
      "step 300: train loss 2.4694, val loss 2.4576\n",
      "step 400: train loss 2.4195, val loss 2.4223\n",
      "step 500: train loss 2.3564, val loss 2.3519\n",
      "step 600: train loss 2.3123, val loss 2.3200\n",
      "step 700: train loss 2.2741, val loss 2.2751\n",
      "step 800: train loss 2.2262, val loss 2.2280\n",
      "step 900: train loss 2.1720, val loss 2.1843\n",
      "step 1000: train loss 2.1310, val loss 2.1418\n",
      "step 1100: train loss 2.0771, val loss 2.0904\n",
      "step 1200: train loss 2.0340, val loss 2.0479\n",
      "step 1300: train loss 1.9831, val loss 2.0031\n",
      "step 1400: train loss 1.9663, val loss 1.9599\n",
      "step 1500: train loss 1.9352, val loss 1.9324\n",
      "step 1600: train loss 1.8850, val loss 1.8967\n",
      "step 1700: train loss 1.8690, val loss 1.8594\n",
      "step 1800: train loss 1.8416, val loss 1.8448\n",
      "step 1900: train loss 1.8210, val loss 1.8342\n",
      "step 2000: train loss 1.8024, val loss 1.8062\n",
      "step 2100: train loss 1.7703, val loss 1.7771\n",
      "step 2200: train loss 1.7612, val loss 1.7628\n",
      "step 2300: train loss 1.7352, val loss 1.7412\n",
      "step 2400: train loss 1.7251, val loss 1.7188\n",
      "step 2500: train loss 1.7127, val loss 1.7162\n",
      "step 2600: train loss 1.6840, val loss 1.6955\n",
      "step 2700: train loss 1.6950, val loss 1.7004\n",
      "step 2800: train loss 1.6764, val loss 1.6916\n",
      "step 2900: train loss 1.6534, val loss 1.6894\n",
      "step 3000: train loss 1.6566, val loss 1.6577\n",
      "step 3100: train loss 1.6552, val loss 1.6597\n",
      "step 3200: train loss 1.6235, val loss 1.6279\n",
      "step 3300: train loss 1.6152, val loss 1.6281\n",
      "step 3400: train loss 1.6105, val loss 1.6260\n",
      "step 3500: train loss 1.5953, val loss 1.5938\n",
      "step 3600: train loss 1.5830, val loss 1.6010\n",
      "step 3700: train loss 1.5980, val loss 1.5977\n",
      "step 3800: train loss 1.5786, val loss 1.6059\n",
      "step 3900: train loss 1.5741, val loss 1.5646\n",
      "step 4000: train loss 1.5567, val loss 1.5762\n",
      "step 4100: train loss 1.5558, val loss 1.5623\n",
      "step 4200: train loss 1.5389, val loss 1.5598\n",
      "step 4300: train loss 1.5453, val loss 1.5404\n",
      "step 4400: train loss 1.5391, val loss 1.5474\n",
      "step 4500: train loss 1.5278, val loss 1.5318\n",
      "step 4600: train loss 1.5441, val loss 1.5379\n",
      "step 4700: train loss 1.5224, val loss 1.5308\n",
      "step 4800: train loss 1.5151, val loss 1.5373\n",
      "step 4900: train loss 1.5271, val loss 1.5266\n",
      "step 4999: train loss 1.4944, val loss 1.5187\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8 # how many independent sequences will we process in parallel?\n",
    "block_size = 64\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb618ace-a303-495b-9048-9e953d33a0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " our conturt things nemoh, shome also homirs. So want to to big they worled, what 42% ven Trasm spanera. They want to bight. I has back to fighting him bold.\" Chil's will numbing oun her darted you have see. Alope do what again. And they Elp you've goone them perimp, for micitary this had alre off more Corth. Historon ago, bugh. Thas they sthard a lot befaking all thoss. Rusppersions that rem the Joners. You're have nemews want I want on just. It's ruess, \"LAdy his make I going ster, ry for the for benally Berder get trous, it's now on thri. In't they want a sthett, right whill of the America, \"Whad that's not this. Go's of no best. These away. So what's guy time I good bet the Paspail Souch bit. That's now bes would himp ille their them gremers, morth hat of when through bumber you, Namin they day, Me incIth in morest faffirmths, they shing you, laye I don't know what 6N? Trave A. We was onf you America, But winn's were will he Undicking, vey will be people, one con borders. You have h\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([[0]])\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e0068-1c5b-467f-a077-9c301d870a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
